{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5aYFAEuYutm"
      },
      "source": [
        "## Text generation using RNN - Character Level\n",
        "\n",
        "to generate text using RNN , we need to convert raw text to a supervised learning problem format .\n",
        "take , for exemple the following corpus :\n",
        "\"Her brother shook his head incredulously\"\n",
        "First we need to divide the data into tabular format containing input(X) and output(y) sequences . in case of a character level model , the X and y will look like this :     \n",
        "X\n",
        "\n",
        "(1) : her b , (2): er br , (3): r bro , (4):  brot\n",
        "\n",
        "Y\n",
        "\n",
        "(1) : r     , (2): o     , (3) : t    , (4):  e\n",
        "\n",
        "--> Note that in the above problem , the sequence length of X is five characters and that of y is one character , this many to one architecture . we can , however change the number of input characters to any number of characters depending on the type of problem .\n",
        "\n",
        "--> a model is trained on such data . To generate text , we simply give the model any five characters using which it predicts the next character .\n",
        "\n",
        "then it appends the predicted character to the input sequence ( on the extreme right of the sequence ) and discards the first character on (the extreme left of the sequence ) . then ut predicts again the new sequence and the cycle continues until a fix number of iterations . an exemple is shown below :      \n",
        "\n",
        "X\n",
        "\n",
        "(1) : incre ,\n",
        "\n",
        "(2): ncre<predicted character 1> ,\n",
        "\n",
        "\n",
        "(3): cre<predicted character 1><predicted character 2> ,\n",
        "\n",
        "(4):  re<predicted character 1><predicted character 2><predicted character 3>,\n",
        "\n",
        "Y\n",
        "\n",
        "(1) : <predicted character 1>,     \n",
        "\n",
        "(2): <predicted character 2>,     \n",
        "\n",
        "(3) : <predicted character 3>,    \n",
        "\n",
        "(4):  <predicted character 4>,\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ilox5GCZByQ"
      },
      "source": [
        "# Notebook overiew\n",
        "1 preprocess data\n",
        "\n",
        "2 LSTM model\n",
        "\n",
        "3 Generate code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deN8XMGBYstH"
      },
      "outputs": [],
      "source": [
        "# import Libraries\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Activation\n",
        "from keras.layers import LSTM , GRU , Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqAQxkDEJPva",
        "outputId": "8bc0652c-3fe4-4c3c-e471-bfa488a78e3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# access to Google drive in colab to get the dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06NdOgRL9rdz"
      },
      "source": [
        "# 1.Preprocess data\n",
        "\n",
        "We are going to build a C code generator by training an RNN on huge corpus of C code (the linux kernel code) . You can downoload the C code used as source text from the following link : [https://github.com/torvalds/linux/tree/master](https://) .\n",
        "\n",
        "\n",
        "We have already downloaded the entire kernel folder and stored in a local directory .\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uQU1ygW_wrY"
      },
      "source": [
        "# Load C code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWxbGl20_vPB",
        "outputId": "fd6704d1-ae16-4e6b-a854-b006d38307f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['async.c', '.gitignore', 'acct.c', 'audit.c', 'Kconfig.kexec', 'Kconfig.freezer', 'Kconfig.hz', 'Makefile', 'Kconfig.locks', 'audit_tree.c', 'audit.h', 'Kconfig.preempt', 'audit_fsnotify.c', 'kexec.c', 'freezer.c', 'fork.c', 'cred.c', 'delayacct.c', 'kallsyms_selftest.c', 'kallsyms_internal.h', 'kexec_elf.c', 'auditfilter.c', 'kexec_internal.h', 'kallsyms_selftest.h', 'iomem.c', 'kexec_file.c', 'extable.c', 'auditsc.c', 'dma.c', 'exit.h', 'fail_function.c', 'kcmp.c', 'crash_reserve.c', 'kexec_core.c', 'gen_kheaders.sh', 'bounds.c', 'exit.c', 'elfcorehdr.c', 'kheaders.c', 'cfi.c', 'groups.c', 'crash_core.c', 'cpu.c', 'hung_task.c', 'irq_work.c', 'jump_label.c', 'kcov.c', 'configs.c', 'backtracetest.c', 'kallsyms.c', 'audit_watch.c', 'exec_domain.c', 'compat.c', 'cpu_pm.c', 'capability.c', 'context_tracking.c', 'scs.c', 'sysctl-test.c', 'signal.c', 'seccomp.c', 'rseq.c', 'nsproxy.c', 'stop_machine.c', 'sysctl.c', 'latencytop.c', 'module_signature.c', 'panic.c', 'regset.c', 'stacktrace.c', 'stackleak.c', 'smpboot.h', 'tsacct.c', 'resource.c', 'ksysfs.c', 'softirq.c', 'padata.c', 'static_call_inline.c', 'static_call.c', 'notifier.c', 'taskstats.c', 'sys.c', 'params.c', 'kthread.c', 'pid_sysctl.h', 'scftorture.c', 'ptrace.c', 'tracepoint.c', 'reboot.c', 'ucount.c', 'pid.c', 'smpboot.c', 'resource_kunit.c', 'relay.c', 'range.c', 'smp.c', 'ksyms_common.c', 'pid_namespace.c', 'profile.c', 'torture.c', 'kprobes.c', 'task_work.c', 'sys_ni.c', 'utsname_sysctl.c', 'user.c', 'watchdog_perf.c', 'usermode_driver.c', 'uid16.c', 'user-return-notifier.c', 'watchdog_buddy.c', 'vhost_task.c', 'vmcore_info.c', 'workqueue_internal.h', 'umh.c', 'workqueue.c', 'uid16.h', 'utsname.c', 'user_namespace.c', 'watchdog.c', 'up.c', 'watch_queue.c', 'dma', 'debug', 'cgroup', 'bpf', 'configs', 'irq', 'entry', 'gcov', 'events', 'futex', 'printk', 'sched', 'rcu', 'livepatch', 'locking', 'trace', 'module', 'power', 'kcsan', 'time']\n"
          ]
        }
      ],
      "source": [
        "# set path where C files reside\n",
        "\n",
        "path=r'/content/drive/MyDrive/Colab Notebooks/RNNs/kernel'\n",
        "\n",
        "os.chdir(path)\n",
        "\n",
        "files_names = os.listdir(path)\n",
        "\n",
        "print(files_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct4IZg5oABUY",
        "outputId": "d3129f46-22fd-4c7a-81b1-ccb5e962214e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['async.c', 'acct.c', 'audit.c', 'audit_tree.c', 'audit_fsnotify.c', 'kexec.c', 'freezer.c', 'fork.c', 'cred.c', 'delayacct.c', 'kallsyms_selftest.c', 'kexec_elf.c', 'auditfilter.c', 'iomem.c', 'kexec_file.c', 'extable.c', 'auditsc.c', 'dma.c', 'fail_function.c', 'kcmp.c', 'crash_reserve.c', 'kexec_core.c', 'bounds.c', 'exit.c', 'elfcorehdr.c', 'kheaders.c', 'cfi.c', 'groups.c', 'crash_core.c', 'cpu.c', 'hung_task.c', 'irq_work.c', 'jump_label.c', 'kcov.c', 'configs.c', 'backtracetest.c', 'kallsyms.c', 'audit_watch.c', 'exec_domain.c', 'compat.c', 'cpu_pm.c', 'capability.c', 'context_tracking.c', 'scs.c', 'sysctl-test.c', 'signal.c', 'seccomp.c', 'rseq.c', 'nsproxy.c', 'stop_machine.c', 'sysctl.c', 'latencytop.c', 'module_signature.c', 'panic.c', 'regset.c', 'stacktrace.c', 'stackleak.c', 'tsacct.c', 'resource.c', 'ksysfs.c', 'softirq.c', 'padata.c', 'static_call_inline.c', 'static_call.c', 'notifier.c', 'taskstats.c', 'sys.c', 'params.c', 'kthread.c', 'scftorture.c', 'ptrace.c', 'tracepoint.c', 'reboot.c', 'ucount.c', 'pid.c', 'smpboot.c', 'resource_kunit.c', 'relay.c', 'range.c', 'smp.c', 'ksyms_common.c', 'pid_namespace.c', 'profile.c', 'torture.c', 'kprobes.c', 'task_work.c', 'sys_ni.c', 'utsname_sysctl.c', 'user.c', 'watchdog_perf.c', 'usermode_driver.c', 'uid16.c', 'user-return-notifier.c', 'watchdog_buddy.c', 'vhost_task.c', 'vmcore_info.c', 'umh.c', 'workqueue.c', 'utsname.c', 'user_namespace.c', 'watchdog.c', 'up.c', 'watch_queue.c']\n"
          ]
        }
      ],
      "source": [
        "# use regex to filter .c files\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "c_names = r\"\\.c$\"\n",
        "\n",
        "c_files = list()\n",
        "\n",
        "for file in files_names :\n",
        "    if re.search(c_names , file):  # Use re.search instead of re.match\n",
        "        c_files.append(file)\n",
        "\n",
        "print(c_files)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEylYOrVAgRJ"
      },
      "outputs": [],
      "source": [
        "# load all c code in a list\n",
        "\n",
        "full_code = list()\n",
        "\n",
        "for file in c_files :\n",
        "    code = open(file , \"r\" , encoding=\"utf-8\")\n",
        "    full_code.append(code.read())\n",
        "    code.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ik54NWJAy9p",
        "outputId": "0bfe4c60-b860-49e8-da33-562d264e3d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "// SPDX-License-Identifier: GPL-2.0-only\n",
            "/*\n",
            " * crash.c - kernel crash support code.\n",
            " * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>\n",
            " */\n",
            "\n",
            "#include <linux/buildid.h>\n",
            "#include <linux/init.h>\n",
            "#include <linux/utsname.h>\n",
            "#include <linux/vmalloc.h>\n",
            "#include <linux/sizes.h>\n",
            "#include <linux/kexec.h>\n",
            "#include <linux/memory.h>\n",
            "#include <linux/cpuhotplug.h>\n",
            "#include <linux/memblock.h>\n",
            "#include <linux/kmemleak.h>\n",
            "\n",
            "#include <asm/page.h>\n",
            "#include <asm/sections.h>\n",
            "\n",
            "#include <crypto/sha1.h>\n",
            "\n",
            "#include \"kallsyms_internal.h\"\n",
            "#include \"kexec_internal.h\"\n",
            "\n",
            "/* Location of the reserved area for the crash kernel */\n",
            "struct resource crashk_res = {\n",
            "\t.name  = \"Crash kernel\",\n",
            "\t.start = 0,\n",
            "\t.end   = 0,\n",
            "\t.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,\n",
            "\t.desc  = IORES_DESC_CRASH_KERNEL\n",
            "};\n",
            "struct resource crashk_low_res = {\n",
            "\t.name  = \"Crash kernel\",\n",
            "\t.start = 0,\n",
            "\t.end   = 0,\n",
            "\t.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,\n",
            "\t.desc  = IORES_DESC_CRASH_KERNEL\n",
            "};\n",
            "\n",
            "/*\n",
            " * parsing the \"crashkernel\" commandline\n",
            " *\n",
            " * this code is intended to be called from architecture specific code\n",
            " */\n",
            "\n",
            "\n",
            "/*\n",
            " * This function parses command lines in the format\n",
            " *\n",
            " *   crashkernel=ramsize-range:size[,...][@offset]\n",
            " *\n",
            " * The function returns 0 on success and -EINVAL on failure.\n",
            " */\n",
            "static int __init parse_crashkernel_mem(char *cmdline,\n",
            "\t\t\t\t\tunsigned long long system_ram,\n",
            "\t\t\t\t\tunsigned long long *crash_size,\n",
            "\t\t\t\t\tunsigned long long *crash_base)\n",
            "{\n",
            "\tchar *cur = cmdline, *tmp;\n",
            "\tunsigned long long total_mem = system_ram;\n",
            "\n",
            "\t/*\n",
            "\t * Firmware sometimes reserves some memory regions for its own use,\n",
            "\t * so the system memory size is less than the actual physical memory\n",
            "\t * size. Work around this by rounding up the total size to 128M,\n",
            "\t * which is enough for most test cases.\n",
            "\t */\n",
            "\ttotal_mem = roundup(total_mem, SZ_128M);\n",
            "\n",
            "\t/* for each entry of the comma-separated list */\n",
            "\tdo {\n",
            "\t\tunsigned long long start, end = ULLONG_MAX, size;\n",
            "\n",
            "\t\t/* get the start of the range */\n",
            "\t\tstart = memparse(cur, &tmp);\n",
            "\t\tif (cur == tmp) {\n",
            "\t\t\tpr_warn(\"crashkernel: Memory value expected\\n\");\n",
            "\t\t\treturn -EINVAL;\n",
            "\t\t}\n",
            "\t\tcur = tmp;\n",
            "\t\tif (*cur != '-') {\n",
            "\t\t\tpr_warn(\"crashkernel: '-' expected\\n\");\n",
            "\t\t\treturn -EINVAL;\n",
            "\t\t}\n",
            "\t\tcur++;\n",
            "\n",
            "\t\t/* if no ':' is here, than we read the end */\n",
            "\t\tif (*cur != ':') {\n",
            "\t\t\tend = memparse(cur, &tmp);\n",
            "\t\t\tif (cur == tmp) {\n",
            "\t\t\t\tpr_warn(\"crashkernel: Memory value expected\\n\");\n",
            "\t\t\t\treturn -EINVAL;\n",
            "\t\t\t}\n",
            "\t\t\tcur = tmp;\n",
            "\t\t\tif (end <= start) {\n",
            "\t\t\t\tpr_warn(\"crashkernel: end <= start\\n\");\n",
            "\t\t\t\treturn -EINVAL;\n",
            "\t\t\t}\n",
            "\t\t}\n",
            "\n",
            "\t\tif (*cur != ':') {\n",
            "\t\t\tpr_warn(\"crashkernel: ':' expected\\n\");\n",
            "\t\t\treturn -EINVAL;\n",
            "\t\t}\n",
            "\t\tcur++;\n",
            "\n",
            "\t\tsize = memparse(cur, &tmp);\n",
            "\t\tif (cur == tmp) {\n",
            "\t\t\tpr_warn(\"crashkernel: Memory value expected\\n\");\n",
            "\t\t\treturn -EINVAL;\n",
            "\t\t}\n",
            "\t\tcur = tmp;\n",
            "\t\tif (size >= total_mem) {\n",
            "\t\t\tpr_warn(\"crashkernel: invalid size\\n\");\n",
            "\t\t\treturn -EINVAL;\n",
            "\t\t}\n",
            "\n",
            "\t\t/* match ? */\n",
            "\t\tif (total_mem >= start && total_mem < end) {\n",
            "\t\t\t*crash_size = size;\n",
            "\t\t\tbreak;\n",
            "\t\t}\n",
            "\t} while (*cur++ == ',');\n",
            "\n",
            "\tif (*crash_size > 0) {\n",
            "\t\twhile (*cur && *cur != ' ' && *cur != '@')\n",
            "\t\t\tcur++;\n",
            "\t\tif (*cur == '@') {\n",
            "\t\t\tcur++;\n",
            "\t\t\t*crash_base = memparse(cur, &tmp);\n",
            "\t\t\tif (cur == tmp) {\n",
            "\t\t\t\tpr_warn(\"crahskernel: Memory value expected after '@'\\n\");\n",
            "\t\t\t\treturn -EINVAL;\n",
            "\t\t\t}\n",
            "\t\t}\n",
            "\t} else\n",
            "\t\tpr_info(\"crashkernel size resulted in zero bytes\\n\");\n",
            "\n",
            "\treturn 0;\n",
            "}\n",
            "\n",
            "/*\n",
            " * That function parses \"simple\" (old) crashkernel command lines like\n",
            " *\n",
            " *\tcrashkernel=size[@offset]\n",
            " *\n",
            " * It returns 0 on success and -EINVAL on failure.\n",
            " */\n",
            "static int __init parse_crashkernel_simple(char *cmdline,\n",
            "\t\t\t\t\t   unsigned long long *crash_size,\n",
            "\t\t\t\t\t   unsigned long long *crash_base)\n",
            "{\n",
            "\tchar *cur = cmdline;\n",
            "\n",
            "\t*crash_size = memparse(cmdline, &cur);\n",
            "\tif (cmdline == cur) {\n",
            "\t\tpr_warn(\"crashkernel: memory value expected\\n\");\n",
            "\t\treturn -EINVAL;\n",
            "\t}\n",
            "\n",
            "\tif (*cur == '@')\n",
            "\t\t*crash_base = memparse(cur+1, &cur);\n",
            "\telse if (*cur != ' ' && *cur != '\\0') {\n",
            "\t\tpr_warn(\"crashkernel: unrecognized char: %c\\n\", *cur);\n",
            "\t\treturn -EINVAL;\n",
            "\t}\n",
            "\n",
            "\treturn 0;\n",
            "}\n",
            "\n",
            "#define SUFFIX_HIGH 0\n",
            "#define SUFFIX_LOW  1\n",
            "#define SUFFIX_NULL 2\n",
            "static __initdata char *suffix_tbl[] = {\n",
            "\t[SUFFIX_HIGH] = \",high\",\n",
            "\t[SUFFIX_LOW]  = \",low\",\n",
            "\t[SUFFIX_NULL] = NULL,\n",
            "};\n",
            "\n",
            "/*\n",
            " * That function parses \"suffix\"  crashkernel command lines like\n",
            " *\n",
            " *\tcrashkernel=size,[high|low]\n",
            " *\n",
            " * It returns 0 on success and -EINVAL on failure.\n",
            " */\n",
            "static int __init parse_crashkernel_suffix(char *cmdline,\n",
            "\t\t\t\t\t   unsigned long long *crash_size,\n",
            "\t\t\t\t\t   const char *suffix)\n",
            "{\n",
            "\tchar *cur = cmdline;\n",
            "\n",
            "\t*crash_size = memparse(cmdline, &cur);\n",
            "\tif (cmdline == cur) {\n",
            "\t\tpr_warn(\"crashkernel: memory value expected\\n\");\n",
            "\t\treturn -EINVAL;\n",
            "\t}\n",
            "\n",
            "\t/* check with suffix */\n",
            "\tif (strncmp(cur, suffix, strlen(suffix))) {\n",
            "\t\tpr_warn(\"crashkernel: unrecognized char: %c\\n\", *cur);\n",
            "\t\treturn -EINVAL;\n",
            "\t}\n",
            "\tcur += strlen(suffix);\n",
            "\tif (*cur != ' ' && *cur != '\\0') {\n",
            "\t\tpr_warn(\"crashkernel: unrecognized char: %c\\n\", *cur);\n",
            "\t\treturn -EINVAL;\n",
            "\t}\n",
            "\n",
            "\treturn 0;\n",
            "}\n",
            "\n",
            "static __init char *get_last_crashkernel(char *cmdline,\n",
            "\t\t\t     const char *name,\n",
            "\t\t\t     const char *suffix)\n",
            "{\n",
            "\tchar *p = cmdline, *ck_cmdline = NULL;\n",
            "\n",
            "\t/* find crashkernel and use the last one if there are more */\n",
            "\tp = strstr(p, name);\n",
            "\twhile (p) {\n",
            "\t\tchar *end_p = strchr(p, ' ');\n",
            "\t\tchar *q;\n",
            "\n",
            "\t\tif (!end_p)\n",
            "\t\t\tend_p = p + strlen(p);\n",
            "\n",
            "\t\tif (!suffix) {\n",
            "\t\t\tint i;\n",
            "\n",
            "\t\t\t/* skip the one with any known suffix */\n",
            "\t\t\tfor (i = 0; suffix_tbl[i]; i++) {\n",
            "\t\t\t\tq = end_p - strlen(suffix_tbl[i]);\n",
            "\t\t\t\tif (!strncmp(q, suffix_tbl[i],\n",
            "\t\t\t\t\t     strlen(suffix_tbl[i])))\n",
            "\t\t\t\t\tgoto next;\n",
            "\t\t\t}\n",
            "\t\t\tck_cmdline = p;\n",
            "\t\t} else {\n",
            "\t\t\tq = end_p - strlen(suffix);\n",
            "\t\t\tif (!strncmp(q, suffix, strlen(suffix)))\n",
            "\t\t\t\tck_cmdline = p;\n",
            "\t\t}\n",
            "next:\n",
            "\t\tp = strstr(p+1, name);\n",
            "\t}\n",
            "\n",
            "\treturn ck_cmdline;\n",
            "}\n",
            "\n",
            "static int __init __parse_crashkernel(char *cmdline,\n",
            "\t\t\t     unsigned long long system_ram,\n",
            "\t\t\t     unsigned long long *crash_size,\n",
            "\t\t\t     unsigned long long *crash_base,\n",
            "\t\t\t     const char *suffix)\n",
            "{\n",
            "\tchar *first_colon, *first_space;\n",
            "\tchar *ck_cmdline;\n",
            "\tchar *name = \"crashkernel=\";\n",
            "\n",
            "\tBUG_ON(!crash_size || !crash_base);\n",
            "\t*crash_size = 0;\n",
            "\t*crash_base = 0;\n",
            "\n",
            "\tck_cmdline = get_last_crashkernel(cmdline, name, suffix);\n",
            "\tif (!ck_cmdline)\n",
            "\t\treturn -ENOENT;\n",
            "\n",
            "\tck_cmdline += strlen(name);\n",
            "\n",
            "\tif (suffix)\n",
            "\t\treturn parse_crashkernel_suffix(ck_cmdline, crash_size,\n",
            "\t\t\t\tsuffix);\n",
            "\t/*\n",
            "\t * if the commandline contains a ':', then that's the extended\n",
            "\t * syntax -- if not, it must be the classic syntax\n",
            "\t */\n",
            "\tfirst_colon = strchr(ck_cmdline, ':');\n",
            "\tfirst_space = strchr(ck_cmdline, ' ');\n",
            "\tif (first_colon && (!first_space || first_colon < first_space))\n",
            "\t\treturn parse_crashkernel_mem(ck_cmdline, system_ram,\n",
            "\t\t\t\tcrash_size, crash_base);\n",
            "\n",
            "\treturn parse_crashkernel_simple(ck_cmdline, crash_size, crash_base);\n",
            "}\n",
            "\n",
            "/*\n",
            " * That function is the entry point for command line parsing and should be\n",
            " * called from the arch-specific code.\n",
            " *\n",
            " * If crashkernel=,high|low is supported on architecture, non-NULL values\n",
            " * should be passed to parameters 'low_size' and 'high'.\n",
            " */\n",
            "int __init parse_crashkernel(char *cmdline,\n",
            "\t\t\t     unsigned long long system_ram,\n",
            "\t\t\t     unsigned long long *crash_size,\n",
            "\t\t\t     unsigned long long *crash_base,\n",
            "\t\t\t     unsigned long long *low_size,\n",
            "\t\t\t     bool *high)\n",
            "{\n",
            "\tint ret;\n",
            "\n",
            "\t/* crashkernel=X[@offset] */\n",
            "\tret = __parse_crashkernel(cmdline, system_ram, crash_size,\n",
            "\t\t\t\tcrash_base, NULL);\n",
            "#ifdef CONFIG_ARCH_HAS_GENERIC_CRASHKERNEL_RESERVATION\n",
            "\t/*\n",
            "\t * If non-NULL 'high' passed in and no normal crashkernel\n",
            "\t * setting detected, try parsing crashkernel=,high|low.\n",
            "\t */\n",
            "\tif (high && ret == -ENOENT) {\n",
            "\t\tret = __parse_crashkernel(cmdline, 0, crash_size,\n",
            "\t\t\t\tcrash_base, suffix_tbl[SUFFIX_HIGH]);\n",
            "\t\tif (ret || !*crash_size)\n",
            "\t\t\treturn -EINVAL;\n",
            "\n",
            "\t\t/*\n",
            "\t\t * crashkernel=Y,low can be specified or not, but invalid value\n",
            "\t\t * is not allowed.\n",
            "\t\t */\n",
            "\t\tret = __parse_crashkernel(cmdline, 0, low_size,\n",
            "\t\t\t\tcrash_base, suffix_tbl[SUFFIX_LOW]);\n",
            "\t\tif (ret == -ENOENT) {\n",
            "\t\t\t*low_size = DEFAULT_CRASH_KERNEL_LOW_SIZE;\n",
            "\t\t\tret = 0;\n",
            "\t\t} else if (ret) {\n",
            "\t\t\treturn ret;\n",
            "\t\t}\n",
            "\n",
            "\t\t*high = true;\n",
            "\t}\n",
            "#endif\n",
            "\tif (!*crash_size)\n",
            "\t\tret = -EINVAL;\n",
            "\n",
            "\tif (*crash_size >= system_ram)\n",
            "\t\tret = -EINVAL;\n",
            "\n",
            "\treturn ret;\n",
            "}\n",
            "\n",
            "/*\n",
            " * Add a dummy early_param handler to mark crashkernel= as a known command line\n",
            " * parameter and suppress incorrect warnings in init/main.c.\n",
            " */\n",
            "static int __init parse_crashkernel_dummy(char *arg)\n",
            "{\n",
            "\treturn 0;\n",
            "}\n",
            "early_param(\"crashkernel\", parse_crashkernel_dummy);\n",
            "\n",
            "#ifdef CONFIG_ARCH_HAS_GENERIC_CRASHKERNEL_RESERVATION\n",
            "static int __init reserve_crashkernel_low(unsigned long long low_size)\n",
            "{\n",
            "#ifdef CONFIG_64BIT\n",
            "\tunsigned long long low_base;\n",
            "\n",
            "\tlow_base = memblock_phys_alloc_range(low_size, CRASH_ALIGN, 0, CRASH_ADDR_LOW_MAX);\n",
            "\tif (!low_base) {\n",
            "\t\tpr_err(\"cannot allocate crashkernel low memory (size:0x%llx).\\n\", low_size);\n",
            "\t\treturn -ENOMEM;\n",
            "\t}\n",
            "\n",
            "\tpr_info(\"crashkernel low memory reserved: 0x%08llx - 0x%08llx (%lld MB)\\n\",\n",
            "\t\tlow_base, low_base + low_size, low_size >> 20);\n",
            "\n",
            "\tcrashk_low_res.start = low_base;\n",
            "\tcrashk_low_res.end   = low_base + low_size - 1;\n",
            "#ifdef HAVE_ARCH_ADD_CRASH_RES_TO_IOMEM_EARLY\n",
            "\tinsert_resource(&iomem_resource, &crashk_low_res);\n",
            "#endif\n",
            "#endif\n",
            "\treturn 0;\n",
            "}\n",
            "\n",
            "void __init reserve_crashkernel_generic(char *cmdline,\n",
            "\t\t\t     unsigned long long crash_size,\n",
            "\t\t\t     unsigned long long crash_base,\n",
            "\t\t\t     unsigned long long crash_low_size,\n",
            "\t\t\t     bool high)\n",
            "{\n",
            "\tunsigned long long search_end = CRASH_ADDR_LOW_MAX, search_base = 0;\n",
            "\tbool fixed_base = false;\n",
            "\n",
            "\t/* User specifies base address explicitly. */\n",
            "\tif (crash_base) {\n",
            "\t\tfixed_base = true;\n",
            "\t\tsearch_base = crash_base;\n",
            "\t\tsearch_end = crash_base + crash_size;\n",
            "\t} else if (high) {\n",
            "\t\tsearch_base = CRASH_ADDR_LOW_MAX;\n",
            "\t\tsearch_end = CRASH_ADDR_HIGH_MAX;\n",
            "\t}\n",
            "\n",
            "retry:\n",
            "\tcrash_base = memblock_phys_alloc_range(crash_size, CRASH_ALIGN,\n",
            "\t\t\t\t\t       search_base, search_end);\n",
            "\tif (!crash_base) {\n",
            "\t\t/*\n",
            "\t\t * For crashkernel=size[KMG]@offset[KMG], print out failure\n",
            "\t\t * message if can't reserve the specified region.\n",
            "\t\t */\n",
            "\t\tif (fixed_base) {\n",
            "\t\t\tpr_warn(\"crashkernel reservation failed - memory is in use.\\n\");\n",
            "\t\t\treturn;\n",
            "\t\t}\n",
            "\n",
            "\t\t/*\n",
            "\t\t * For crashkernel=size[KMG], if the first attempt was for\n",
            "\t\t * low memory, fall back to high memory, the minimum required\n",
            "\t\t * low memory will be reserved later.\n",
            "\t\t */\n",
            "\t\tif (!high && search_end == CRASH_ADDR_LOW_MAX) {\n",
            "\t\t\tsearch_end = CRASH_ADDR_HIGH_MAX;\n",
            "\t\t\tsearch_base = CRASH_ADDR_LOW_MAX;\n",
            "\t\t\tcrash_low_size = DEFAULT_CRASH_KERNEL_LOW_SIZE;\n",
            "\t\t\tgoto retry;\n",
            "\t\t}\n",
            "\n",
            "\t\t/*\n",
            "\t\t * For crashkernel=size[KMG],high, if the first attempt was\n",
            "\t\t * for high memory, fall back to low memory.\n",
            "\t\t */\n",
            "\t\tif (high && search_end == CRASH_ADDR_HIGH_MAX) {\n",
            "\t\t\tsearch_end = CRASH_ADDR_LOW_MAX;\n",
            "\t\t\tsearch_base = 0;\n",
            "\t\t\tif (search_end != CRASH_ADDR_HIGH_MAX)\n",
            "\t\t\t\tgoto retry;\n",
            "\t\t}\n",
            "\t\tpr_warn(\"cannot allocate crashkernel (size:0x%llx)\\n\",\n",
            "\t\t\tcrash_size);\n",
            "\t\treturn;\n",
            "\t}\n",
            "\n",
            "\tif ((crash_base >= CRASH_ADDR_LOW_MAX) &&\n",
            "\t     crash_low_size && reserve_crashkernel_low(crash_low_size)) {\n",
            "\t\tmemblock_phys_free(crash_base, crash_size);\n",
            "\t\treturn;\n",
            "\t}\n",
            "\n",
            "\tpr_info(\"crashkernel reserved: 0x%016llx - 0x%016llx (%lld MB)\\n\",\n",
            "\t\tcrash_base, crash_base + crash_size, crash_size >> 20);\n",
            "\n",
            "\t/*\n",
            "\t * The crashkernel memory will be removed from the kernel linear\n",
            "\t * map. Inform kmemleak so that it won't try to access it.\n",
            "\t */\n",
            "\tkmemleak_ignore_phys(crash_base);\n",
            "\tif (crashk_low_res.end)\n",
            "\t\tkmemleak_ignore_phys(crashk_low_res.start);\n",
            "\n",
            "\tcrashk_res.start = crash_base;\n",
            "\tcrashk_res.end = crash_base + crash_size - 1;\n",
            "#ifdef HAVE_ARCH_ADD_CRASH_RES_TO_IOMEM_EARLY\n",
            "\tinsert_resource(&iomem_resource, &crashk_res);\n",
            "#endif\n",
            "}\n",
            "\n",
            "#ifndef HAVE_ARCH_ADD_CRASH_RES_TO_IOMEM_EARLY\n",
            "static __init int insert_crashkernel_resources(void)\n",
            "{\n",
            "\tif (crashk_res.start < crashk_res.end)\n",
            "\t\tinsert_resource(&iomem_resource, &crashk_res);\n",
            "\n",
            "\tif (crashk_low_res.start < crashk_low_res.end)\n",
            "\t\tinsert_resource(&iomem_resource, &crashk_low_res);\n",
            "\n",
            "\treturn 0;\n",
            "}\n",
            "early_initcall(insert_crashkernel_resources);\n",
            "#endif\n",
            "#endif\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at how a typical c code looks like\n",
        "\n",
        "print(full_code[20])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXj7yPOLB8_Y"
      },
      "source": [
        "## if you have set of characters or set of words is high then use word embedding if  less then go to one-hot encodding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POCW2XbQBZH3",
        "outputId": "03a123e3-2b87-4923-dfde-414c0b8cbf78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of characters in entire code 2230677\n"
          ]
        }
      ],
      "source": [
        "# merge different c codes into one big c code\n",
        "\n",
        "text = \"\\n\".join(full_code)\n",
        "\n",
        "print(f\"Total number of characters in entire code {len(text)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "hrp4gYn7CQhW",
        "outputId": "58a90854-1a28-4b14-fcf4-c3c1a23ed602"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * async.c: Asynchronous function calls for boot performance\\n *\\n * (C) Copyright 2009 Intel Corporation\\n * Author: Arjan van de Ven <arjan@linux.intel.com>\\n */\\n\\n\\n/*\\n\\nGoals and Theory of Operation\\n\\nThe primary goal of this feature is to reduce the kernel boot time,\\nby doing various independent hardware delays and discovery operations\\ndecoupled and not strictly serialized.\\n\\nMore specifically, the asynchronous function call concept allows\\ncertain operations (primarily during system boot) to happen\\nasynchronously, out of order, while these operations still\\nhave their externally visible parts happen sequentially and in-order.\\n(not unlike how out-of-order CPUs retire their instructions in order)\\n\\nKey to the asynchronous function call implementation is the concept of\\na \"sequence cookie\" (which, although it has an abstracted type, can be\\nthought of as a monotonically incrementing number).\\n\\nThe async core will assign each scheduled event such a sequence cookie and\\npass this to the called functions.\\n\\nThe asynchronously called function should before doing a globally visible\\noperation, such as registering device numbers, call the\\nasync_synchronize_cookie() function and pass in its own cookie. The\\nasync_synchronize_cookie() function will make sure that all asynchronous\\noperations that were scheduled prior to the operation corresponding with the\\ncookie have completed.\\n\\nSubsystem/driver initialization code that scheduled asynchronous probe\\nfunctions, but which shares global resources with other drivers/subsystems\\nthat do not use the asynchronous call feature, need to do a full\\nsynchronization with the async_synchronize_full() function, before returning\\nfrom their init function. This is to maintain strict ordering between the\\nasynchronous and synchronous parts of the kernel.\\n\\n*/\\n\\n#include <linux/async.h>\\n#include <linux/atomic.h>\\n#include <linux/export.h>\\n#include <linux/ktime.h>\\n#include <linux/pid.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/wait.h>\\n#include <linux/workqueue.h>\\n\\n#include \"workqueue_internal.h\"\\n\\nstatic async_cookie_t next_cookie = 1;\\n\\n#define MAX_WORK\\t\\t32768\\n#define ASYNC_COOKIE_MAX\\tULLONG_MAX\\t/* infinity cookie */\\n\\nstatic LIST_HEAD(async_global_pending);\\t/* pending from all registered doms */\\nstatic ASYNC_DOMAIN(async_dfl_domain);\\nstatic DEFINE_SPINLOCK(async_lock);\\nstatic struct workqueue_struct *async_wq;\\n\\nstruct async_entry {\\n\\tstruct list_head\\tdomain_list;\\n\\tstruct list_head\\tglobal_list;\\n\\tstruct work_struct\\twork;\\n\\tasync_cookie_t\\t\\tcookie;\\n\\tasync_func_t\\t\\tfunc;\\n\\tvoid\\t\\t\\t*data;\\n\\tstruct async_domain\\t*domain;\\n};\\n\\nstatic DECLARE_WAIT_QUEUE_HEAD(async_done);\\n\\nstatic atomic_t entry_count;\\n\\nstatic long long microseconds_since(ktime_t start)\\n{\\n\\tktime_t now = ktime_get();\\n\\treturn ktime_to_ns(ktime_sub(now, start)) >> 10;\\n}\\n\\nstatic async_cookie_t lowest_in_progress(struct async_domain *domain)\\n{\\n\\tstruct async_entry *first = NULL;\\n\\tasync_cookie_t ret = ASYNC_COOKIE_MAX;\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&async_lock, flags);\\n\\n\\tif (domain) {\\n\\t\\tif (!list_empty(&domain->pending))\\n\\t\\t\\tfirst = list_first_entry(&domain->pending,\\n\\t\\t\\t\\t\\tstruct async_entry, domain_list);\\n\\t} else {\\n\\t\\tif (!list_empty(&async_global_pending))\\n\\t\\t\\tfirst = list_first_entry(&async_global_pending,\\n\\t\\t\\t\\t\\tstruct async_entry, global_list);\\n\\t}\\n\\n\\tif (first)\\n\\t\\tret = first->cookie;\\n\\n\\tspin_unlock_irqrestore(&async_lock, flags);\\n\\treturn ret;\\n}\\n\\n/*\\n * pick the first pending entry and run it\\n */\\nstatic void async_run_entry_fn(struct work_struct *work)\\n{\\n\\tstruct async_entry *entry =\\n\\t\\tcontainer_of(work, struct async_entry, work);\\n\\tunsigned long flags;\\n\\tktime_t calltime;\\n\\n\\t/* 1) run (and print duration) */\\n\\tpr_debug(\"calling  %lli_%pS @ %i\\\\n\", (long long)entry->cookie,\\n\\t\\t entry->func, task_pid_nr(current));\\n\\tcalltime = ktime_get();\\n\\n\\tentry->func(entry->data, entry->cookie);\\n\\n\\tpr_debug(\"initcall %lli_%pS returned after %lld usecs\\\\n\",\\n\\t\\t (long long)entry->cookie, entry->func,\\n\\t\\t microseconds_since(calltime));\\n\\n\\t/* 2) remove self from the pending queues */\\n\\tspin_lock_irqsave(&async_lock, flags);\\n\\tlist_del_init(&entry->domain_list);\\n\\tlist_del_init(&entry->global_list);\\n\\n\\t/* 3) free the entry */\\n\\tkfree(entry);\\n\\tatomic_dec(&entry_count);\\n\\n\\tspin_unlock_irqrestore(&async_lock, flags);\\n\\n\\t/* 4) wake up any waiters */\\n\\twake_up(&async_done);\\n}\\n\\nstatic async_cookie_t __async_schedule_node_domain(async_func_t func,\\n\\t\\t\\t\\t\\t\\t   void *data, int node,\\n\\t\\t\\t\\t\\t\\t   struct async_domain *domain,\\n\\t\\t\\t\\t\\t\\t   struct async_entry *entry)\\n{\\n\\tasync_cookie_t newcookie;\\n\\tunsigned long flags;\\n\\n\\tINIT_LIST_HEAD(&entry->domain_list);\\n\\tINIT_LIST_HEAD(&entry->global_list);\\n\\tINIT_WORK(&entry->work, async_run_entry_fn);\\n\\tentry->func = func;\\n\\tentry->data = data;\\n\\tentry->domain = domain;\\n\\n\\tspin_lock_irqsave(&async_lock, flags);\\n\\n\\t/* allocate cookie and queue */\\n\\tnewcookie = entry->cookie = next_cookie++;\\n\\n\\tlist_add_tail(&entry->domain_list, &domain->pending);\\n\\tif (domain->registered)\\n\\t\\tlist_add_tail(&entry->global_list, &async_global_pending);\\n\\n\\tatomic_inc(&entry_count);\\n\\tspin_unlock_irqrestore(&async_lock, flags);\\n\\n\\t/* schedule for execution */\\n\\tqueue_work_node(node, async_wq, &entry->work);\\n\\n\\treturn newcookie;\\n}\\n\\n/**\\n * async_schedule_node_domain - NUMA specific version of async_schedule_domain\\n * @func: function to execute asynchronously\\n * @data: data pointer to pass to the function\\n * @node: NUMA node that we want to schedule this on or close to\\n * @domain: the domain\\n *\\n * Returns an async_cookie_t that may be used for checkpointing later.\\n * @domain may be used in the async_synchronize_*_domain() functions to\\n * wait within a certain synchronization domain rather than globally.\\n *\\n * Note: This function may be called from atomic or non-atomic contexts.\\n *\\n * The node requested will be honored on a best effort basis. If the node\\n * has no CPUs associated with it then the work is distributed among all\\n * available CPUs.\\n */\\nasync_cookie_t async_schedule_node_domain(async_func_t func, void *data,\\n\\t\\t\\t\\t\\t  int node, struct async_domain *domain)\\n{\\n\\tstruct async_entry *entry;\\n\\tunsigned long flags;\\n\\tasync_cookie_t newcookie;\\n\\n\\t/* allow irq-off callers */\\n\\tentry = kzalloc(sizeof(struct async_entry), GFP_ATOMIC);\\n\\n\\t/*\\n\\t * If we\\'re out of memory or if there\\'s too much work\\n\\t * pending already, we execute synchronously.\\n\\t */\\n\\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {\\n\\t\\tkfree(entry);\\n\\t\\tspin_lock_irqsave(&async_lock, flags);\\n\\t\\tnewcookie = next_cookie++;\\n\\t\\tspin_unlock_irqrestore(&async_lock, flags);\\n\\n\\t\\t/* low on memory.. run synchronously */\\n\\t\\tfunc(data, newcookie);\\n\\t\\treturn newcookie;\\n\\t}\\n\\n\\treturn __async_schedule_node_domain(func, data, node, domain, entry);\\n}\\nEXPORT_SYMBOL_GPL(async_schedule_node_domain);\\n\\n/**\\n * async_schedule_node - NUMA specific version of async_schedule\\n * @func: function to execute asynchronously\\n * @data: data pointer to pass to the function\\n * @node: NUMA node that we want to schedule this on or close to\\n *\\n * Returns an async_cookie_t that may be used for checkpointing later.\\n * Note: This function may be called from atomic or non-atomic contexts.\\n *\\n * The node requested will be honored on a best effort basis. If the node\\n * has no CPUs associated with it then the work is distributed among all\\n * available CPUs.\\n */\\nasync_cookie_t async_schedule_node(async_func_t func, void *data, int node)\\n{\\n\\treturn async_schedule_node_domain(func, data, node, &async_dfl_domain);\\n}\\nEXPORT_SYMBOL_GPL(async_schedule_node);\\n\\n/**\\n * async_schedule_dev_nocall - A simplified variant of async_schedule_dev()\\n * @func: function to execute asynchronously\\n * @dev: device argument to be passed to function\\n *\\n * @dev is used as both the argument for the function and to provide NUMA\\n * context for where to run the function.\\n *\\n * If the asynchronous execution of @func is scheduled successfully, return\\n * true. Otherwise, do nothing and return false, unlike async_schedule_dev()\\n * that will run the function synchronously then.\\n */\\nbool async_schedule_dev_nocall(async_func_t func, struct device *dev)\\n{\\n\\tstruct async_entry *entry;\\n\\n\\tentry = kzalloc(sizeof(struct async_entry), GFP_KERNEL);\\n\\n\\t/* Give up if there is no memory or too much work. */\\n\\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {\\n\\t\\tkfree(entry);\\n\\t\\treturn false;\\n\\t}\\n\\n\\t__async_schedule_node_domain(func, dev, dev_to_node(dev),\\n\\t\\t\\t\\t     &async_dfl_domain, entry);\\n\\treturn true;\\n}\\n\\n/**\\n * async_synchronize_full - synchronize all asynchronous function calls\\n *\\n * This function waits until all asynchronous function calls have been done.\\n */\\nvoid async_synchronize_full(void)\\n{\\n\\tasync_synchronize_full_domain(NULL);\\n}\\nEXPORT_SYMBOL_GPL(async_synchronize_full);\\n\\n/**\\n * async_synchronize_full_domain - synchronize all asynchronous function within a certain domain\\n * @domain: the domain to synchronize\\n *\\n * This function waits until all asynchronous function calls for the\\n * synchronization domain specified by @domain have been done.\\n */\\nvoid async_synchronize_full_domain(struct async_domain *domain)\\n{\\n\\tasync_synchronize_cookie_domain(ASYNC_COOKIE_MAX, domain);\\n}\\nEXPORT_SYMBOL_GPL(async_synchronize_full_domain);\\n\\n/**\\n * async_synchronize_cookie_domain - synchronize asynchronous function calls within a certain domain with cookie checkpointing\\n * @cookie: async_cookie_t to use as checkpoint\\n * @domain: the domain to synchronize (%NULL for all registered domains)\\n *\\n * This function waits until all asynchronous function calls for the\\n * synchronization domain specified by @domain submitted prior to @cookie\\n * have been done.\\n */\\nvoid async_synchronize_cookie_domain(async_cookie_t cookie, struct async_domain *domain)\\n{\\n\\tktime_t starttime;\\n\\n\\tpr_debug(\"async_waiting @ %i\\\\n\", task_pid_nr(current));\\n\\tstarttime = ktime_get();\\n\\n\\twait_event(async_done, lowest_in_progress(domain) >= cookie);\\n\\n\\tpr_debug(\"async_continuing @ %i after %lli usec\\\\n\", task_pid_nr(current),\\n\\t\\t microseconds_since(starttime));\\n}\\nEXPORT_SYMBOL_GPL(async_synchronize_cookie_domain);\\n\\n/**\\n * async_synchronize_cookie - synchronize asynchronous function calls with cookie checkpointing\\n * @cookie: async_cookie_t to use as checkpoint\\n *\\n * This function waits until all asynchronous function calls prior to @cookie\\n * have been done.\\n */\\nvoid async_synchronize_cookie(async_cookie_t cookie)\\n{\\n\\tasync_synchronize_cookie_domain(cookie, &async_dfl_domain);\\n}\\nEXPORT_SYMBOL_GPL(async_synchronize_cookie);\\n\\n/**\\n * current_is_async - is %current an async worker task?\\n *\\n * Returns %true if %current is an async worker task.\\n */\\nbool current_is_async(void)\\n{\\n\\tstruct worker *worker = current_wq_worker();\\n\\n\\treturn worker && worker->current_func == async_run_entry_fn;\\n}\\nEXPORT_SYMBOL_GPL(current_is_async);\\n\\nvoid __init async_init(void)\\n{\\n\\t/*\\n\\t * Async can schedule a number of interdependent work items. However,\\n\\t * unbound workqueues can handle only upto min_active interdependent\\n\\t * work items. The default min_active of 8 isn\\'t sufficient for async\\n\\t * and can lead to stalls. Let\\'s use a dedicated workqueue with raised\\n\\t * min_active.\\n\\t */\\n\\tasync_wq = alloc_workqueue(\"async\", WQ_UNBOUND, 0);\\n\\tBUG_ON(!async_wq);\\n\\tworkqueue_set_min_active(async_wq, WQ_DFL_ACTIVE);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n *  linux/kernel/acct.c\\n *\\n *  BSD Process Accounting for Linux\\n *\\n *  Author: Marco van Wieringen <mvw@planets.elm.net>\\n *\\n *  Some code based on ideas and code from:\\n *  Thomas K. Dyas <tdyas@eden.rutgers.edu>\\n *\\n *  This file implements BSD-style process accounting. Whenever any\\n *  process exits, an accounting record of type \"struct acct\" is\\n *  written to the file specified with the acct() system call. It is\\n *  up to user-level programs to do useful things with the accounting\\n *  log. The kernel just provides the raw accounting information.\\n *\\n * (C) Copyright 1995 - 1997 Marco van Wieringen - ELM Consultancy B.V.\\n *\\n *  Plugged two leaks. 1) It didn\\'t return acct_file into the free_filps if\\n *  the file happened to be read-only. 2) If the accounting was suspended\\n *  due to the lack of space it happily allowed to reopen it and completely\\n *  lost the old acct_file. 3/10/98, Al Viro.\\n *\\n *  Now we silently close acct_file on attempt to reopen. Cleaned sys_acct().\\n *  XTerms and EMACS are manifestations of pure evil. 21/10/98, AV.\\n *\\n *  Fixed a nasty interaction with sys_umount(). If the accounting\\n *  was suspeneded we failed to stop it on umount(). Messy.\\n *  Another one: remount to readonly didn\\'t stop accounting.\\n *\\tQuestion: what should we do if we have CAP_SYS_ADMIN but not\\n *  CAP_SYS_PACCT? Current code does the following: umount returns -EBUSY\\n *  unless we are messing with the root. In that case we are getting a\\n *  real mess with do_remount_sb(). 9/11/98, AV.\\n *\\n *  Fixed a bunch of races (and pair of leaks). Probably not the best way,\\n *  but this one obviously doesn\\'t introduce deadlocks. Later. BTW, found\\n *  one race (and leak) in BSD implementation.\\n *  OK, that\\'s better. ANOTHER race and leak in BSD variant. There always\\n *  is one more bug... 10/11/98, AV.\\n *\\n *\\tOh, fsck... Oopsable SMP race in do_process_acct() - we must hold\\n * ->mmap_lock to walk the vma list of current->mm. Nasty, since it leaks\\n * a struct file opened for write. Fixed. 2/6/2000, AV.\\n */\\n\\n#include <linux/mm.h>\\n#include <linux/slab.h>\\n#include <linux/acct.h>\\n#include <linux/capability.h>\\n#include <linux/file.h>\\n#include <linux/tty.h>\\n#include <linux/security.h>\\n#include <linux/vfs.h>\\n#include <linux/jiffies.h>\\n#include <linux/times.h>\\n#include <linux/syscalls.h>\\n#include <linux/mount.h>\\n#include <linux/uaccess.h>\\n#include <linux/sched/cputime.h>\\n\\n#include <asm/div64.h>\\n#include <linux/pid_namespace.h>\\n#include <linux/fs_pin.h>\\n\\n/*\\n * These constants control the amount of freespace that suspend and\\n * resume the process accounting system, and the time delay between\\n * each check.\\n * Turned into sysctl-controllable parameters. AV, 12/11/98\\n */\\n\\nstatic int acct_parm[3] = {4, 2, 30};\\n#define RESUME\\t\\t(acct_parm[0])\\t/* >foo% free space - resume */\\n#define SUSPEND\\t\\t(acct_parm[1])\\t/* <foo% free space - suspend */\\n#define ACCT_TIMEOUT\\t(acct_parm[2])\\t/* foo second timeout between checks */\\n\\n#ifdef CONFIG_SYSCTL\\nstatic struct ctl_table kern_acct_table[] = {\\n\\t{\\n\\t\\t.procname       = \"acct\",\\n\\t\\t.data           = &acct_parm,\\n\\t\\t.maxlen         = 3*sizeof(int),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = proc_dointvec,\\n\\t},\\n};\\n\\nstatic __init int kernel_acct_sysctls_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", kern_acct_table);\\n\\treturn 0;\\n}\\nlate_initcall(kernel_acct_sysctls_init);\\n#endif /* CONFIG_SYSCTL */\\n\\n/*\\n * External references and all of the globals.\\n */\\n\\nstruct bsd_acct_struct {\\n\\tstruct fs_pin\\t\\tpin;\\n\\tatomic_long_t\\t\\tcount;\\n\\tstruct rcu_head\\t\\trcu;\\n\\tstruct mutex\\t\\tlock;\\n\\tint\\t\\t\\tactive;\\n\\tunsigned long\\t\\tneedcheck;\\n\\tstruct file\\t\\t*file;\\n\\tstruct pid_namespace\\t*ns;\\n\\tstruct work_struct\\twork;\\n\\tstruct completion\\tdone;\\n};\\n\\nstatic void do_acct_process(struct bsd_acct_struct *acct);\\n\\n/*\\n * Check the amount of free space and suspend/resume accordingly.\\n */\\nstatic int check_free_space(struct bsd_acct_struct *acct)\\n{\\n\\tstruct kstatfs sbuf;\\n\\n\\tif (time_is_after_jiffies(acct->needcheck))\\n\\t\\tgoto out;\\n\\n\\t/* May block */\\n\\tif (vfs_statfs(&acct->file->f_path, &sbuf))\\n\\t\\tgoto out;\\n\\n\\tif (acct->active) {\\n\\t\\tu64 suspend = sbuf.f_blocks * SUSPEND;\\n\\t\\tdo_div(suspend, 100);\\n\\t\\tif (sbuf.f_bavail <= suspend) {\\n\\t\\t\\tacct->active = 0;\\n\\t\\t\\tpr_info(\"Process accounting paused\\\\n\");\\n\\t\\t}\\n\\t} else {\\n\\t\\tu64 resume = sbuf.f_blocks * RESUME;\\n\\t\\tdo_div(resume, 100);\\n\\t\\tif (sbuf.f_bavail >= resume) {\\n\\t\\t\\tacct->active = 1;\\n\\t\\t\\tpr_info(\"Process accounting resumed\\\\n\");\\n\\t\\t}\\n\\t}\\n\\n\\tacct->needcheck = jiffies + ACCT_TIMEOUT*HZ;\\nout:\\n\\treturn acct->active;\\n}\\n\\nstatic void acct_put(struct bsd_acct_struct *p)\\n{\\n\\tif (atomic_long_dec_and_test(&p->count))\\n\\t\\tkfree_rcu(p, rcu);\\n}\\n\\nstatic inline struct bsd_acct_struct *to_acct(struct fs_pin *p)\\n{\\n\\treturn p ? container_of(p, struct bsd_acct_struct, pin) : NULL;\\n}\\n\\nstatic struct bsd_acct_struct *acct_get(struct pid_namespace *ns)\\n{\\n\\tstruct bsd_acct_struct *res;\\nagain:\\n\\tsmp_rmb();\\n\\trcu_read_lock();\\n\\tres = to_acct(READ_ONCE(ns->bacct));\\n\\tif (!res) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn NULL;\\n\\t}\\n\\tif (!atomic_long_inc_not_zero(&res->count)) {\\n\\t\\trcu_read_unlock();\\n\\t\\tcpu_relax();\\n\\t\\tgoto again;\\n\\t}\\n\\trcu_read_unlock();\\n\\tmutex_lock(&res->lock);\\n\\tif (res != to_acct(READ_ONCE(ns->bacct))) {\\n\\t\\tmutex_unlock(&res->lock);\\n\\t\\tacct_put(res);\\n\\t\\tgoto again;\\n\\t}\\n\\treturn res;\\n}\\n\\nstatic void acct_pin_kill(struct fs_pin *pin)\\n{\\n\\tstruct bsd_acct_struct *acct = to_acct(pin);\\n\\tmutex_lock(&acct->lock);\\n\\tdo_acct_process(acct);\\n\\tschedule_work(&acct->work);\\n\\twait_for_completion(&acct->done);\\n\\tcmpxchg(&acct->ns->bacct, pin, NULL);\\n\\tmutex_unlock(&acct->lock);\\n\\tpin_remove(pin);\\n\\tacct_put(acct);\\n}\\n\\nstatic void close_work(struct work_struct *work)\\n{\\n\\tstruct bsd_acct_struct *acct = container_of(work, struct bsd_acct_struct, work);\\n\\tstruct file *file = acct->file;\\n\\tif (file->f_op->flush)\\n\\t\\tfile->f_op->flush(file, NULL);\\n\\t__fput_sync(file);\\n\\tcomplete(&acct->done);\\n}\\n\\nstatic int acct_on(struct filename *pathname)\\n{\\n\\tstruct file *file;\\n\\tstruct vfsmount *mnt, *internal;\\n\\tstruct pid_namespace *ns = task_active_pid_ns(current);\\n\\tstruct bsd_acct_struct *acct;\\n\\tstruct fs_pin *old;\\n\\tint err;\\n\\n\\tacct = kzalloc(sizeof(struct bsd_acct_struct), GFP_KERNEL);\\n\\tif (!acct)\\n\\t\\treturn -ENOMEM;\\n\\n\\t/* Difference from BSD - they don\\'t do O_APPEND */\\n\\tfile = file_open_name(pathname, O_WRONLY|O_APPEND|O_LARGEFILE, 0);\\n\\tif (IS_ERR(file)) {\\n\\t\\tkfree(acct);\\n\\t\\treturn PTR_ERR(file);\\n\\t}\\n\\n\\tif (!S_ISREG(file_inode(file)->i_mode)) {\\n\\t\\tkfree(acct);\\n\\t\\tfilp_close(file, NULL);\\n\\t\\treturn -EACCES;\\n\\t}\\n\\n\\tif (!(file->f_mode & FMODE_CAN_WRITE)) {\\n\\t\\tkfree(acct);\\n\\t\\tfilp_close(file, NULL);\\n\\t\\treturn -EIO;\\n\\t}\\n\\tinternal = mnt_clone_internal(&file->f_path);\\n\\tif (IS_ERR(internal)) {\\n\\t\\tkfree(acct);\\n\\t\\tfilp_close(file, NULL);\\n\\t\\treturn PTR_ERR(internal);\\n\\t}\\n\\terr = mnt_get_write_access(internal);\\n\\tif (err) {\\n\\t\\tmntput(internal);\\n\\t\\tkfree(acct);\\n\\t\\tfilp_close(file, NULL);\\n\\t\\treturn err;\\n\\t}\\n\\tmnt = file->f_path.mnt;\\n\\tfile->f_path.mnt = internal;\\n\\n\\tatomic_long_set(&acct->count, 1);\\n\\tinit_fs_pin(&acct->pin, acct_pin_kill);\\n\\tacct->file = file;\\n\\tacct->needcheck = jiffies;\\n\\tacct->ns = ns;\\n\\tmutex_init(&acct->lock);\\n\\tINIT_WORK(&acct->work, close_work);\\n\\tinit_completion(&acct->done);\\n\\tmutex_lock_nested(&acct->lock, 1);\\t/* nobody has seen it yet */\\n\\tpin_insert(&acct->pin, mnt);\\n\\n\\trcu_read_lock();\\n\\told = xchg(&ns->bacct, &acct->pin);\\n\\tmutex_unlock(&acct->lock);\\n\\tpin_kill(old);\\n\\tmnt_put_write_access(mnt);\\n\\tmntput(mnt);\\n\\treturn 0;\\n}\\n\\nstatic DEFINE_MUTEX(acct_on_mutex);\\n\\n/**\\n * sys_acct - enable/disable process accounting\\n * @name: file name for accounting records or NULL to shutdown accounting\\n *\\n * sys_acct() is the only system call needed to implement process\\n * accounting. It takes the name of the file where accounting records\\n * should be written. If the filename is NULL, accounting will be\\n * shutdown.\\n *\\n * Returns: 0 for success or negative errno values for failure.\\n */\\nSYSCALL_DEFINE1(acct, const char __user *, name)\\n{\\n\\tint error = 0;\\n\\n\\tif (!capable(CAP_SYS_PACCT))\\n\\t\\treturn -EPERM;\\n\\n\\tif (name) {\\n\\t\\tstruct filename *tmp = getname(name);\\n\\n\\t\\tif (IS_ERR(tmp))\\n\\t\\t\\treturn PTR_ERR(tmp);\\n\\t\\tmutex_lock(&acct_on_mutex);\\n\\t\\terror = acct_on(tmp);\\n\\t\\tmutex_unlock(&acct_on_mutex);\\n\\t\\tputname(tmp);\\n\\t} else {\\n\\t\\trcu_read_lock();\\n\\t\\tpin_kill(task_active_pid_ns(current)->bacct);\\n\\t}\\n\\n\\treturn error;\\n}\\n\\nvoid acct_exit_ns(struct pid_namespace *ns)\\n{\\n\\trcu_read_lock();\\n\\tpin_kill(ns->bacct);\\n}\\n\\n/*\\n *  encode an u64 into a comp_t\\n *\\n *  This routine has been adopted from the encode_comp_t() function in\\n *  the kern_acct.c file of the FreeBSD operating system. The encoding\\n *  is a 13-bit fraction with a 3-bit (base 8) exponent.\\n */\\n\\n#define\\tMANTSIZE\\t13\\t\\t\\t/* 13 bit mantissa. */\\n#define\\tEXPSIZE\\t\\t3\\t\\t\\t/* Base 8 (3 bit) exponent. */\\n#define\\tMAXFRACT\\t((1 << MANTSIZE) - 1)\\t/* Maximum fractional value. */\\n\\nstatic comp_t encode_comp_t(u64 value)\\n{\\n\\tint exp, rnd;\\n\\n\\texp = rnd = 0;\\n\\twhile (value > MAXFRACT) {\\n\\t\\trnd = value & (1 << (EXPSIZE - 1));\\t/* Round up? */\\n\\t\\tvalue >>= EXPSIZE;\\t/* Base 8 exponent == 3 bit shift. */\\n\\t\\texp++;\\n\\t}\\n\\n\\t/*\\n\\t * If we need to round up, do it (and handle overflow correctly).\\n\\t */\\n\\tif (rnd && (++value > MAXFRACT)) {\\n\\t\\tvalue >>= EXPSIZE;\\n\\t\\texp++;\\n\\t}\\n\\n\\tif (exp > (((comp_t) ~0U) >> MANTSIZE))\\n\\t\\treturn (comp_t) ~0U;\\n\\t/*\\n\\t * Clean it up and polish it off.\\n\\t */\\n\\texp <<= MANTSIZE;\\t\\t/* Shift the exponent into place */\\n\\texp += value;\\t\\t\\t/* and add on the mantissa. */\\n\\treturn exp;\\n}\\n\\n#if ACCT_VERSION == 1 || ACCT_VERSION == 2\\n/*\\n * encode an u64 into a comp2_t (24 bits)\\n *\\n * Format: 5 bit base 2 exponent, 20 bits mantissa.\\n * The leading bit of the mantissa is not stored, but implied for\\n * non-zero exponents.\\n * Largest encodable value is 50 bits.\\n */\\n\\n#define MANTSIZE2       20                      /* 20 bit mantissa. */\\n#define EXPSIZE2        5                       /* 5 bit base 2 exponent. */\\n#define MAXFRACT2       ((1ul << MANTSIZE2) - 1) /* Maximum fractional value. */\\n#define MAXEXP2         ((1 << EXPSIZE2) - 1)    /* Maximum exponent. */\\n\\nstatic comp2_t encode_comp2_t(u64 value)\\n{\\n\\tint exp, rnd;\\n\\n\\texp = (value > (MAXFRACT2>>1));\\n\\trnd = 0;\\n\\twhile (value > MAXFRACT2) {\\n\\t\\trnd = value & 1;\\n\\t\\tvalue >>= 1;\\n\\t\\texp++;\\n\\t}\\n\\n\\t/*\\n\\t * If we need to round up, do it (and handle overflow correctly).\\n\\t */\\n\\tif (rnd && (++value > MAXFRACT2)) {\\n\\t\\tvalue >>= 1;\\n\\t\\texp++;\\n\\t}\\n\\n\\tif (exp > MAXEXP2) {\\n\\t\\t/* Overflow. Return largest representable number instead. */\\n\\t\\treturn (1ul << (MANTSIZE2+EXPSIZE2-1)) - 1;\\n\\t} else {\\n\\t\\treturn (value & (MAXFRACT2>>1)) | (exp << (MANTSIZE2-1));\\n\\t}\\n}\\n#elif ACCT_VERSION == 3\\n/*\\n * encode an u64 into a 32 bit IEEE float\\n */\\nstatic u32 encode_float(u64 value)\\n{\\n\\tunsigned exp = 190;\\n\\tunsigned u;\\n\\n\\tif (value == 0)\\n\\t\\treturn 0;\\n\\twhile ((s64)value > 0) {\\n\\t\\tvalue <<= 1;\\n\\t\\texp--;\\n\\t}\\n\\tu = (u32)(value >> 40) & 0x7fffffu;\\n\\treturn u | (exp << 23);\\n}\\n#endif\\n\\n/*\\n *  Write an accounting entry for an exiting process\\n *\\n *  The acct_process() call is the workhorse of the process\\n *  accounting system. The struct acct is built here and then written\\n *  into the accounting file. This function should only be called from\\n *  do_exit() or when switching to a different output file.\\n */\\n\\nstatic void fill_ac(acct_t *ac)\\n{\\n\\tstruct pacct_struct *pacct = &current->signal->pacct;\\n\\tu64 elapsed, run_time;\\n\\ttime64_t btime;\\n\\tstruct tty_struct *tty;\\n\\n\\t/*\\n\\t * Fill the accounting struct with the needed info as recorded\\n\\t * by the different kernel functions.\\n\\t */\\n\\tmemset(ac, 0, sizeof(acct_t));\\n\\n\\tac->ac_version = ACCT_VERSION | ACCT_BYTEORDER;\\n\\tstrscpy(ac->ac_comm, current->comm, sizeof(ac->ac_comm));\\n\\n\\t/* calculate run_time in nsec*/\\n\\trun_time = ktime_get_ns();\\n\\trun_time -= current->group_leader->start_time;\\n\\t/* convert nsec -> AHZ */\\n\\telapsed = nsec_to_AHZ(run_time);\\n#if ACCT_VERSION == 3\\n\\tac->ac_etime = encode_float(elapsed);\\n#else\\n\\tac->ac_etime = encode_comp_t(elapsed < (unsigned long) -1l ?\\n\\t\\t\\t\\t(unsigned long) elapsed : (unsigned long) -1l);\\n#endif\\n#if ACCT_VERSION == 1 || ACCT_VERSION == 2\\n\\t{\\n\\t\\t/* new enlarged etime field */\\n\\t\\tcomp2_t etime = encode_comp2_t(elapsed);\\n\\n\\t\\tac->ac_etime_hi = etime >> 16;\\n\\t\\tac->ac_etime_lo = (u16) etime;\\n\\t}\\n#endif\\n\\tdo_div(elapsed, AHZ);\\n\\tbtime = ktime_get_real_seconds() - elapsed;\\n\\tac->ac_btime = clamp_t(time64_t, btime, 0, U32_MAX);\\n#if ACCT_VERSION == 2\\n\\tac->ac_ahz = AHZ;\\n#endif\\n\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\ttty = current->signal->tty;\\t/* Safe as we hold the siglock */\\n\\tac->ac_tty = tty ? old_encode_dev(tty_devnum(tty)) : 0;\\n\\tac->ac_utime = encode_comp_t(nsec_to_AHZ(pacct->ac_utime));\\n\\tac->ac_stime = encode_comp_t(nsec_to_AHZ(pacct->ac_stime));\\n\\tac->ac_flag = pacct->ac_flag;\\n\\tac->ac_mem = encode_comp_t(pacct->ac_mem);\\n\\tac->ac_minflt = encode_comp_t(pacct->ac_minflt);\\n\\tac->ac_majflt = encode_comp_t(pacct->ac_majflt);\\n\\tac->ac_exitcode = pacct->ac_exitcode;\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n}\\n/*\\n *  do_acct_process does all actual work. Caller holds the reference to file.\\n */\\nstatic void do_acct_process(struct bsd_acct_struct *acct)\\n{\\n\\tacct_t ac;\\n\\tunsigned long flim;\\n\\tconst struct cred *orig_cred;\\n\\tstruct file *file = acct->file;\\n\\n\\t/*\\n\\t * Accounting records are not subject to resource limits.\\n\\t */\\n\\tflim = rlimit(RLIMIT_FSIZE);\\n\\tcurrent->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;\\n\\t/* Perform file operations on behalf of whoever enabled accounting */\\n\\torig_cred = override_creds(file->f_cred);\\n\\n\\t/*\\n\\t * First check to see if there is enough free_space to continue\\n\\t * the process accounting system.\\n\\t */\\n\\tif (!check_free_space(acct))\\n\\t\\tgoto out;\\n\\n\\tfill_ac(&ac);\\n\\t/* we really need to bite the bullet and change layout */\\n\\tac.ac_uid = from_kuid_munged(file->f_cred->user_ns, orig_cred->uid);\\n\\tac.ac_gid = from_kgid_munged(file->f_cred->user_ns, orig_cred->gid);\\n#if ACCT_VERSION == 1 || ACCT_VERSION == 2\\n\\t/* backward-compatible 16 bit fields */\\n\\tac.ac_uid16 = ac.ac_uid;\\n\\tac.ac_gid16 = ac.ac_gid;\\n#elif ACCT_VERSION == 3\\n\\t{\\n\\t\\tstruct pid_namespace *ns = acct->ns;\\n\\n\\t\\tac.ac_pid = task_tgid_nr_ns(current, ns);\\n\\t\\trcu_read_lock();\\n\\t\\tac.ac_ppid = task_tgid_nr_ns(rcu_dereference(current->real_parent),\\n\\t\\t\\t\\t\\t     ns);\\n\\t\\trcu_read_unlock();\\n\\t}\\n#endif\\n\\t/*\\n\\t * Get freeze protection. If the fs is frozen, just skip the write\\n\\t * as we could deadlock the system otherwise.\\n\\t */\\n\\tif (file_start_write_trylock(file)) {\\n\\t\\t/* it\\'s been opened O_APPEND, so position is irrelevant */\\n\\t\\tloff_t pos = 0;\\n\\t\\t__kernel_write(file, &ac, sizeof(acct_t), &pos);\\n\\t\\tfile_end_write(file);\\n\\t}\\nout:\\n\\tcurrent->signal->rlim[RLIMIT_FSIZE].rlim_cur = flim;\\n\\trevert_creds(orig_cred);\\n}\\n\\n/**\\n * acct_collect - collect accounting information into pacct_struct\\n * @exitcode: task exit code\\n * @group_dead: not 0, if this thread is the last one in the process.\\n */\\nvoid acct_collect(long exitcode, int group_dead)\\n{\\n\\tstruct pacct_struct *pacct = &current->signal->pacct;\\n\\tu64 utime, stime;\\n\\tunsigned long vsize = 0;\\n\\n\\tif (group_dead && current->mm) {\\n\\t\\tstruct mm_struct *mm = current->mm;\\n\\t\\tVMA_ITERATOR(vmi, mm, 0);\\n\\t\\tstruct vm_area_struct *vma;\\n\\n\\t\\tmmap_read_lock(mm);\\n\\t\\tfor_each_vma(vmi, vma)\\n\\t\\t\\tvsize += vma->vm_end - vma->vm_start;\\n\\t\\tmmap_read_unlock(mm);\\n\\t}\\n\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\tif (group_dead)\\n\\t\\tpacct->ac_mem = vsize / 1024;\\n\\tif (thread_group_leader(current)) {\\n\\t\\tpacct->ac_exitcode = exitcode;\\n\\t\\tif (current->flags & PF_FORKNOEXEC)\\n\\t\\t\\tpacct->ac_flag |= AFORK;\\n\\t}\\n\\tif (current->flags & PF_SUPERPRIV)\\n\\t\\tpacct->ac_flag |= ASU;\\n\\tif (current->flags & PF_DUMPCORE)\\n\\t\\tpacct->ac_flag |= ACORE;\\n\\tif (current->flags & PF_SIGNALED)\\n\\t\\tpacct->ac_flag |= AXSIG;\\n\\n\\ttask_cputime(current, &utime, &stime);\\n\\tpacct->ac_utime += utime;\\n\\tpacct->ac_stime += stime;\\n\\tpacct->ac_minflt += current->min_flt;\\n\\tpacct->ac_majflt += current->maj_flt;\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n}\\n\\nstatic void slow_acct_process(struct pid_namespace *ns)\\n{\\n\\tfor ( ; ns; ns = ns->parent) {\\n\\t\\tstruct bsd_acct_struct *acct = acct_get(ns);\\n\\t\\tif (acct) {\\n\\t\\t\\tdo_acct_process(acct);\\n\\t\\t\\tmutex_unlock(&acct->lock);\\n\\t\\t\\tacct_put(acct);\\n\\t\\t}\\n\\t}\\n}\\n\\n/**\\n * acct_process - handles process accounting for an exiting task\\n */\\nvoid acct_process(void)\\n{\\n\\tstruct pid_namespace *ns;\\n\\n\\t/*\\n\\t * This loop is safe lockless, since current is still\\n\\t * alive and holds its namespace, which in turn holds\\n\\t * its parent.\\n\\t */\\n\\tfor (ns = task_active_pid_ns(current); ns != NULL; ns = ns->parent) {\\n\\t\\tif (ns->bacct)\\n\\t\\t\\tbreak;\\n\\t}\\n\\tif (unlikely(ns))\\n\\t\\tslow_acct_process(ns);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/* audit.c -- Auditing support\\n * Gateway between the kernel (e.g., selinux) and the user-space audit daemon.\\n * System-call specific features have moved to auditsc.c\\n *\\n * Copyright 2003-2007 Red Hat Inc., Durham, North Carolina.\\n * All Rights Reserved.\\n *\\n * Written by Rickard E. (Rik) Faith <faith@redhat.com>\\n *\\n * Goals: 1) Integrate fully with Security Modules.\\n *\\t  2) Minimal run-time overhead:\\n *\\t     a) Minimal when syscall auditing is disabled (audit_enable=0).\\n *\\t     b) Small when syscall auditing is enabled and no audit record\\n *\\t\\tis generated (defer as much work as possible to record\\n *\\t\\tgeneration time):\\n *\\t\\ti) context is allocated,\\n *\\t\\tii) names from getname are stored without a copy, and\\n *\\t\\tiii) inode information stored from path_lookup.\\n *\\t  3) Ability to disable syscall auditing at boot time (audit=0).\\n *\\t  4) Usable by other parts of the kernel (if audit_log* is called,\\n *\\t     then a syscall record will be generated automatically for the\\n *\\t     current syscall).\\n *\\t  5) Netlink interface to user-space.\\n *\\t  6) Support low-overhead kernel-based filtering to minimize the\\n *\\t     information that must be passed to user-space.\\n *\\n * Audit userspace, documentation, tests, and bug/issue trackers:\\n * \\thttps://github.com/linux-audit\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/file.h>\\n#include <linux/init.h>\\n#include <linux/types.h>\\n#include <linux/atomic.h>\\n#include <linux/mm.h>\\n#include <linux/export.h>\\n#include <linux/slab.h>\\n#include <linux/err.h>\\n#include <linux/kthread.h>\\n#include <linux/kernel.h>\\n#include <linux/syscalls.h>\\n#include <linux/spinlock.h>\\n#include <linux/rcupdate.h>\\n#include <linux/mutex.h>\\n#include <linux/gfp.h>\\n#include <linux/pid.h>\\n\\n#include <linux/audit.h>\\n\\n#include <net/sock.h>\\n#include <net/netlink.h>\\n#include <linux/skbuff.h>\\n#include <linux/security.h>\\n#include <linux/freezer.h>\\n#include <linux/pid_namespace.h>\\n#include <net/netns/generic.h>\\n\\n#include \"audit.h\"\\n\\n/* No auditing will take place until audit_initialized == AUDIT_INITIALIZED.\\n * (Initialization happens after skb_init is called.) */\\n#define AUDIT_DISABLED\\t\\t-1\\n#define AUDIT_UNINITIALIZED\\t0\\n#define AUDIT_INITIALIZED\\t1\\nstatic int\\taudit_initialized = AUDIT_UNINITIALIZED;\\n\\nu32\\t\\taudit_enabled = AUDIT_OFF;\\nbool\\t\\taudit_ever_enabled = !!AUDIT_OFF;\\n\\nEXPORT_SYMBOL_GPL(audit_enabled);\\n\\n/* Default state when kernel boots without any parameters. */\\nstatic u32\\taudit_default = AUDIT_OFF;\\n\\n/* If auditing cannot proceed, audit_failure selects what happens. */\\nstatic u32\\taudit_failure = AUDIT_FAIL_PRINTK;\\n\\n/* private audit network namespace index */\\nstatic unsigned int audit_net_id;\\n\\n/**\\n * struct audit_net - audit private network namespace data\\n * @sk: communication socket\\n */\\nstruct audit_net {\\n\\tstruct sock *sk;\\n};\\n\\n/**\\n * struct auditd_connection - kernel/auditd connection state\\n * @pid: auditd PID\\n * @portid: netlink portid\\n * @net: the associated network namespace\\n * @rcu: RCU head\\n *\\n * Description:\\n * This struct is RCU protected; you must either hold the RCU lock for reading\\n * or the associated spinlock for writing.\\n */\\nstruct auditd_connection {\\n\\tstruct pid *pid;\\n\\tu32 portid;\\n\\tstruct net *net;\\n\\tstruct rcu_head rcu;\\n};\\nstatic struct auditd_connection __rcu *auditd_conn;\\nstatic DEFINE_SPINLOCK(auditd_conn_lock);\\n\\n/* If audit_rate_limit is non-zero, limit the rate of sending audit records\\n * to that number per second.  This prevents DoS attacks, but results in\\n * audit records being dropped. */\\nstatic u32\\taudit_rate_limit;\\n\\n/* Number of outstanding audit_buffers allowed.\\n * When set to zero, this means unlimited. */\\nstatic u32\\taudit_backlog_limit = 64;\\n#define AUDIT_BACKLOG_WAIT_TIME (60 * HZ)\\nstatic u32\\taudit_backlog_wait_time = AUDIT_BACKLOG_WAIT_TIME;\\n\\n/* The identity of the user shutting down the audit system. */\\nstatic kuid_t\\t\\taudit_sig_uid = INVALID_UID;\\nstatic pid_t\\t\\taudit_sig_pid = -1;\\nstatic struct lsm_prop\\taudit_sig_lsm;\\n\\n/* Records can be lost in several ways:\\n   0) [suppressed in audit_alloc]\\n   1) out of memory in audit_log_start [kmalloc of struct audit_buffer]\\n   2) out of memory in audit_log_move [alloc_skb]\\n   3) suppressed due to audit_rate_limit\\n   4) suppressed due to audit_backlog_limit\\n*/\\nstatic atomic_t\\taudit_lost = ATOMIC_INIT(0);\\n\\n/* Monotonically increasing sum of time the kernel has spent\\n * waiting while the backlog limit is exceeded.\\n */\\nstatic atomic_t audit_backlog_wait_time_actual = ATOMIC_INIT(0);\\n\\n/* Hash for inode-based rules */\\nstruct list_head audit_inode_hash[AUDIT_INODE_BUCKETS];\\n\\nstatic struct kmem_cache *audit_buffer_cache;\\n\\n/* queue msgs to send via kauditd_task */\\nstatic struct sk_buff_head audit_queue;\\n/* queue msgs due to temporary unicast send problems */\\nstatic struct sk_buff_head audit_retry_queue;\\n/* queue msgs waiting for new auditd connection */\\nstatic struct sk_buff_head audit_hold_queue;\\n\\n/* queue servicing thread */\\nstatic struct task_struct *kauditd_task;\\nstatic DECLARE_WAIT_QUEUE_HEAD(kauditd_wait);\\n\\n/* waitqueue for callers who are blocked on the audit backlog */\\nstatic DECLARE_WAIT_QUEUE_HEAD(audit_backlog_wait);\\n\\nstatic struct audit_features af = {.vers = AUDIT_FEATURE_VERSION,\\n\\t\\t\\t\\t   .mask = -1,\\n\\t\\t\\t\\t   .features = 0,\\n\\t\\t\\t\\t   .lock = 0,};\\n\\nstatic char *audit_feature_names[2] = {\\n\\t\"only_unset_loginuid\",\\n\\t\"loginuid_immutable\",\\n};\\n\\n/**\\n * struct audit_ctl_mutex - serialize requests from userspace\\n * @lock: the mutex used for locking\\n * @owner: the task which owns the lock\\n *\\n * Description:\\n * This is the lock struct used to ensure we only process userspace requests\\n * in an orderly fashion.  We can\\'t simply use a mutex/lock here because we\\n * need to track lock ownership so we don\\'t end up blocking the lock owner in\\n * audit_log_start() or similar.\\n */\\nstatic struct audit_ctl_mutex {\\n\\tstruct mutex lock;\\n\\tvoid *owner;\\n} audit_cmd_mutex;\\n\\n/* AUDIT_BUFSIZ is the size of the temporary buffer used for formatting\\n * audit records.  Since printk uses a 1024 byte buffer, this buffer\\n * should be at least that large. */\\n#define AUDIT_BUFSIZ 1024\\n\\n/* The audit_buffer is used when formatting an audit record.  The caller\\n * locks briefly to get the record off the freelist or to allocate the\\n * buffer, and locks briefly to send the buffer to the netlink layer or\\n * to place it on a transmit queue.  Multiple audit_buffers can be in\\n * use simultaneously. */\\nstruct audit_buffer {\\n\\tstruct sk_buff       *skb;\\t/* formatted skb ready to send */\\n\\tstruct audit_context *ctx;\\t/* NULL or associated context */\\n\\tgfp_t\\t\\t     gfp_mask;\\n};\\n\\nstruct audit_reply {\\n\\t__u32 portid;\\n\\tstruct net *net;\\n\\tstruct sk_buff *skb;\\n};\\n\\n/**\\n * auditd_test_task - Check to see if a given task is an audit daemon\\n * @task: the task to check\\n *\\n * Description:\\n * Return 1 if the task is a registered audit daemon, 0 otherwise.\\n */\\nint auditd_test_task(struct task_struct *task)\\n{\\n\\tint rc;\\n\\tstruct auditd_connection *ac;\\n\\n\\trcu_read_lock();\\n\\tac = rcu_dereference(auditd_conn);\\n\\trc = (ac && ac->pid == task_tgid(task) ? 1 : 0);\\n\\trcu_read_unlock();\\n\\n\\treturn rc;\\n}\\n\\n/**\\n * audit_ctl_lock - Take the audit control lock\\n */\\nvoid audit_ctl_lock(void)\\n{\\n\\tmutex_lock(&audit_cmd_mutex.lock);\\n\\taudit_cmd_mutex.owner = current;\\n}\\n\\n/**\\n * audit_ctl_unlock - Drop the audit control lock\\n */\\nvoid audit_ctl_unlock(void)\\n{\\n\\taudit_cmd_mutex.owner = NULL;\\n\\tmutex_unlock(&audit_cmd_mutex.lock);\\n}\\n\\n/**\\n * audit_ctl_owner_current - Test to see if the current task owns the lock\\n *\\n * Description:\\n * Return true if the current task owns the audit control lock, false if it\\n * doesn\\'t own the lock.\\n */\\nstatic bool audit_ctl_owner_current(void)\\n{\\n\\treturn (current == audit_cmd_mutex.owner);\\n}\\n\\n/**\\n * auditd_pid_vnr - Return the auditd PID relative to the namespace\\n *\\n * Description:\\n * Returns the PID in relation to the namespace, 0 on failure.\\n */\\nstatic pid_t auditd_pid_vnr(void)\\n{\\n\\tpid_t pid;\\n\\tconst struct auditd_connection *ac;\\n\\n\\trcu_read_lock();\\n\\tac = rcu_dereference(auditd_conn);\\n\\tif (!ac || !ac->pid)\\n\\t\\tpid = 0;\\n\\telse\\n\\t\\tpid = pid_vnr(ac->pid);\\n\\trcu_read_unlock();\\n\\n\\treturn pid;\\n}\\n\\n/**\\n * audit_get_sk - Return the audit socket for the given network namespace\\n * @net: the destination network namespace\\n *\\n * Description:\\n * Returns the sock pointer if valid, NULL otherwise.  The caller must ensure\\n * that a reference is held for the network namespace while the sock is in use.\\n */\\nstatic struct sock *audit_get_sk(const struct net *net)\\n{\\n\\tstruct audit_net *aunet;\\n\\n\\tif (!net)\\n\\t\\treturn NULL;\\n\\n\\taunet = net_generic(net, audit_net_id);\\n\\treturn aunet->sk;\\n}\\n\\nvoid audit_panic(const char *message)\\n{\\n\\tswitch (audit_failure) {\\n\\tcase AUDIT_FAIL_SILENT:\\n\\t\\tbreak;\\n\\tcase AUDIT_FAIL_PRINTK:\\n\\t\\tif (printk_ratelimit())\\n\\t\\t\\tpr_err(\"%s\\\\n\", message);\\n\\t\\tbreak;\\n\\tcase AUDIT_FAIL_PANIC:\\n\\t\\tpanic(\"audit: %s\\\\n\", message);\\n\\t\\tbreak;\\n\\t}\\n}\\n\\nstatic inline int audit_rate_check(void)\\n{\\n\\tstatic unsigned long\\tlast_check = 0;\\n\\tstatic int\\t\\tmessages   = 0;\\n\\tstatic DEFINE_SPINLOCK(lock);\\n\\tunsigned long\\t\\tflags;\\n\\tunsigned long\\t\\tnow;\\n\\tint\\t\\t\\tretval\\t   = 0;\\n\\n\\tif (!audit_rate_limit)\\n\\t\\treturn 1;\\n\\n\\tspin_lock_irqsave(&lock, flags);\\n\\tif (++messages < audit_rate_limit) {\\n\\t\\tretval = 1;\\n\\t} else {\\n\\t\\tnow = jiffies;\\n\\t\\tif (time_after(now, last_check + HZ)) {\\n\\t\\t\\tlast_check = now;\\n\\t\\t\\tmessages   = 0;\\n\\t\\t\\tretval     = 1;\\n\\t\\t}\\n\\t}\\n\\tspin_unlock_irqrestore(&lock, flags);\\n\\n\\treturn retval;\\n}\\n\\n/**\\n * audit_log_lost - conditionally log lost audit message event\\n * @message: the message stating reason for lost audit message\\n *\\n * Emit at least 1 message per second, even if audit_rate_check is\\n * throttling.\\n * Always increment the lost messages counter.\\n*/\\nvoid audit_log_lost(const char *message)\\n{\\n\\tstatic unsigned long\\tlast_msg = 0;\\n\\tstatic DEFINE_SPINLOCK(lock);\\n\\tunsigned long\\t\\tflags;\\n\\tunsigned long\\t\\tnow;\\n\\tint\\t\\t\\tprint;\\n\\n\\tatomic_inc(&audit_lost);\\n\\n\\tprint = (audit_failure == AUDIT_FAIL_PANIC || !audit_rate_limit);\\n\\n\\tif (!print) {\\n\\t\\tspin_lock_irqsave(&lock, flags);\\n\\t\\tnow = jiffies;\\n\\t\\tif (time_after(now, last_msg + HZ)) {\\n\\t\\t\\tprint = 1;\\n\\t\\t\\tlast_msg = now;\\n\\t\\t}\\n\\t\\tspin_unlock_irqrestore(&lock, flags);\\n\\t}\\n\\n\\tif (print) {\\n\\t\\tif (printk_ratelimit())\\n\\t\\t\\tpr_warn(\"audit_lost=%u audit_rate_limit=%u audit_backlog_limit=%u\\\\n\",\\n\\t\\t\\t\\tatomic_read(&audit_lost),\\n\\t\\t\\t\\taudit_rate_limit,\\n\\t\\t\\t\\taudit_backlog_limit);\\n\\t\\taudit_panic(message);\\n\\t}\\n}\\n\\nstatic int audit_log_config_change(char *function_name, u32 new, u32 old,\\n\\t\\t\\t\\t   int allow_changes)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tint rc = 0;\\n\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_CONFIG_CHANGE);\\n\\tif (unlikely(!ab))\\n\\t\\treturn rc;\\n\\taudit_log_format(ab, \"op=set %s=%u old=%u \", function_name, new, old);\\n\\taudit_log_session_info(ab);\\n\\trc = audit_log_task_context(ab);\\n\\tif (rc)\\n\\t\\tallow_changes = 0; /* Something weird, deny request */\\n\\taudit_log_format(ab, \" res=%d\", allow_changes);\\n\\taudit_log_end(ab);\\n\\treturn rc;\\n}\\n\\nstatic int audit_do_config_change(char *function_name, u32 *to_change, u32 new)\\n{\\n\\tint allow_changes, rc = 0;\\n\\tu32 old = *to_change;\\n\\n\\t/* check if we are locked */\\n\\tif (audit_enabled == AUDIT_LOCKED)\\n\\t\\tallow_changes = 0;\\n\\telse\\n\\t\\tallow_changes = 1;\\n\\n\\tif (audit_enabled != AUDIT_OFF) {\\n\\t\\trc = audit_log_config_change(function_name, new, old, allow_changes);\\n\\t\\tif (rc)\\n\\t\\t\\tallow_changes = 0;\\n\\t}\\n\\n\\t/* If we are allowed, make the change */\\n\\tif (allow_changes == 1)\\n\\t\\t*to_change = new;\\n\\t/* Not allowed, update reason */\\n\\telse if (rc == 0)\\n\\t\\trc = -EPERM;\\n\\treturn rc;\\n}\\n\\nstatic int audit_set_rate_limit(u32 limit)\\n{\\n\\treturn audit_do_config_change(\"audit_rate_limit\", &audit_rate_limit, limit);\\n}\\n\\nstatic int audit_set_backlog_limit(u32 limit)\\n{\\n\\treturn audit_do_config_change(\"audit_backlog_limit\", &audit_backlog_limit, limit);\\n}\\n\\nstatic int audit_set_backlog_wait_time(u32 timeout)\\n{\\n\\treturn audit_do_config_change(\"audit_backlog_wait_time\",\\n\\t\\t\\t\\t      &audit_backlog_wait_time, timeout);\\n}\\n\\nstatic int audit_set_enabled(u32 state)\\n{\\n\\tint rc;\\n\\tif (state > AUDIT_LOCKED)\\n\\t\\treturn -EINVAL;\\n\\n\\trc =  audit_do_config_change(\"audit_enabled\", &audit_enabled, state);\\n\\tif (!rc)\\n\\t\\taudit_ever_enabled |= !!state;\\n\\n\\treturn rc;\\n}\\n\\nstatic int audit_set_failure(u32 state)\\n{\\n\\tif (state != AUDIT_FAIL_SILENT\\n\\t    && state != AUDIT_FAIL_PRINTK\\n\\t    && state != AUDIT_FAIL_PANIC)\\n\\t\\treturn -EINVAL;\\n\\n\\treturn audit_do_config_change(\"audit_failure\", &audit_failure, state);\\n}\\n\\n/**\\n * auditd_conn_free - RCU helper to release an auditd connection struct\\n * @rcu: RCU head\\n *\\n * Description:\\n * Drop any references inside the auditd connection tracking struct and free\\n * the memory.\\n */\\nstatic void auditd_conn_free(struct rcu_head *rcu)\\n{\\n\\tstruct auditd_connection *ac;\\n\\n\\tac = container_of(rcu, struct auditd_connection, rcu);\\n\\tput_pid(ac->pid);\\n\\tput_net(ac->net);\\n\\tkfree(ac);\\n}\\n\\n/**\\n * auditd_set - Set/Reset the auditd connection state\\n * @pid: auditd PID\\n * @portid: auditd netlink portid\\n * @net: auditd network namespace pointer\\n * @skb: the netlink command from the audit daemon\\n * @ack: netlink ack flag, cleared if ack\\'d here\\n *\\n * Description:\\n * This function will obtain and drop network namespace references as\\n * necessary.  Returns zero on success, negative values on failure.\\n */\\nstatic int auditd_set(struct pid *pid, u32 portid, struct net *net,\\n\\t\\t      struct sk_buff *skb, bool *ack)\\n{\\n\\tunsigned long flags;\\n\\tstruct auditd_connection *ac_old, *ac_new;\\n\\tstruct nlmsghdr *nlh;\\n\\n\\tif (!pid || !net)\\n\\t\\treturn -EINVAL;\\n\\n\\tac_new = kzalloc(sizeof(*ac_new), GFP_KERNEL);\\n\\tif (!ac_new)\\n\\t\\treturn -ENOMEM;\\n\\tac_new->pid = get_pid(pid);\\n\\tac_new->portid = portid;\\n\\tac_new->net = get_net(net);\\n\\n\\t/* send the ack now to avoid a race with the queue backlog */\\n\\tif (*ack) {\\n\\t\\tnlh = nlmsg_hdr(skb);\\n\\t\\tnetlink_ack(skb, nlh, 0, NULL);\\n\\t\\t*ack = false;\\n\\t}\\n\\n\\tspin_lock_irqsave(&auditd_conn_lock, flags);\\n\\tac_old = rcu_dereference_protected(auditd_conn,\\n\\t\\t\\t\\t\\t   lockdep_is_held(&auditd_conn_lock));\\n\\trcu_assign_pointer(auditd_conn, ac_new);\\n\\tspin_unlock_irqrestore(&auditd_conn_lock, flags);\\n\\n\\tif (ac_old)\\n\\t\\tcall_rcu(&ac_old->rcu, auditd_conn_free);\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * kauditd_printk_skb - Print the audit record to the ring buffer\\n * @skb: audit record\\n *\\n * Whatever the reason, this packet may not make it to the auditd connection\\n * so write it via printk so the information isn\\'t completely lost.\\n */\\nstatic void kauditd_printk_skb(struct sk_buff *skb)\\n{\\n\\tstruct nlmsghdr *nlh = nlmsg_hdr(skb);\\n\\tchar *data = nlmsg_data(nlh);\\n\\n\\tif (nlh->nlmsg_type != AUDIT_EOE && printk_ratelimit())\\n\\t\\tpr_notice(\"type=%d %s\\\\n\", nlh->nlmsg_type, data);\\n}\\n\\n/**\\n * kauditd_rehold_skb - Handle a audit record send failure in the hold queue\\n * @skb: audit record\\n * @error: error code (unused)\\n *\\n * Description:\\n * This should only be used by the kauditd_thread when it fails to flush the\\n * hold queue.\\n */\\nstatic void kauditd_rehold_skb(struct sk_buff *skb, __always_unused int error)\\n{\\n\\t/* put the record back in the queue */\\n\\tskb_queue_tail(&audit_hold_queue, skb);\\n}\\n\\n/**\\n * kauditd_hold_skb - Queue an audit record, waiting for auditd\\n * @skb: audit record\\n * @error: error code\\n *\\n * Description:\\n * Queue the audit record, waiting for an instance of auditd.  When this\\n * function is called we haven\\'t given up yet on sending the record, but things\\n * are not looking good.  The first thing we want to do is try to write the\\n * record via printk and then see if we want to try and hold on to the record\\n * and queue it, if we have room.  If we want to hold on to the record, but we\\n * don\\'t have room, record a record lost message.\\n */\\nstatic void kauditd_hold_skb(struct sk_buff *skb, int error)\\n{\\n\\t/* at this point it is uncertain if we will ever send this to auditd so\\n\\t * try to send the message via printk before we go any further */\\n\\tkauditd_printk_skb(skb);\\n\\n\\t/* can we just silently drop the message? */\\n\\tif (!audit_default)\\n\\t\\tgoto drop;\\n\\n\\t/* the hold queue is only for when the daemon goes away completely,\\n\\t * not -EAGAIN failures; if we are in a -EAGAIN state requeue the\\n\\t * record on the retry queue unless it\\'s full, in which case drop it\\n\\t */\\n\\tif (error == -EAGAIN) {\\n\\t\\tif (!audit_backlog_limit ||\\n\\t\\t    skb_queue_len(&audit_retry_queue) < audit_backlog_limit) {\\n\\t\\t\\tskb_queue_tail(&audit_retry_queue, skb);\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\taudit_log_lost(\"kauditd retry queue overflow\");\\n\\t\\tgoto drop;\\n\\t}\\n\\n\\t/* if we have room in the hold queue, queue the message */\\n\\tif (!audit_backlog_limit ||\\n\\t    skb_queue_len(&audit_hold_queue) < audit_backlog_limit) {\\n\\t\\tskb_queue_tail(&audit_hold_queue, skb);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* we have no other options - drop the message */\\n\\taudit_log_lost(\"kauditd hold queue overflow\");\\ndrop:\\n\\tkfree_skb(skb);\\n}\\n\\n/**\\n * kauditd_retry_skb - Queue an audit record, attempt to send again to auditd\\n * @skb: audit record\\n * @error: error code (unused)\\n *\\n * Description:\\n * Not as serious as kauditd_hold_skb() as we still have a connected auditd,\\n * but for some reason we are having problems sending it audit records so\\n * queue the given record and attempt to resend.\\n */\\nstatic void kauditd_retry_skb(struct sk_buff *skb, __always_unused int error)\\n{\\n\\tif (!audit_backlog_limit ||\\n\\t    skb_queue_len(&audit_retry_queue) < audit_backlog_limit) {\\n\\t\\tskb_queue_tail(&audit_retry_queue, skb);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* we have to drop the record, send it via printk as a last effort */\\n\\tkauditd_printk_skb(skb);\\n\\taudit_log_lost(\"kauditd retry queue overflow\");\\n\\tkfree_skb(skb);\\n}\\n\\n/**\\n * auditd_reset - Disconnect the auditd connection\\n * @ac: auditd connection state\\n *\\n * Description:\\n * Break the auditd/kauditd connection and move all the queued records into the\\n * hold queue in case auditd reconnects.  It is important to note that the @ac\\n * pointer should never be dereferenced inside this function as it may be NULL\\n * or invalid, you can only compare the memory address!  If @ac is NULL then\\n * the connection will always be reset.\\n */\\nstatic void auditd_reset(const struct auditd_connection *ac)\\n{\\n\\tunsigned long flags;\\n\\tstruct sk_buff *skb;\\n\\tstruct auditd_connection *ac_old;\\n\\n\\t/* if it isn\\'t already broken, break the connection */\\n\\tspin_lock_irqsave(&auditd_conn_lock, flags);\\n\\tac_old = rcu_dereference_protected(auditd_conn,\\n\\t\\t\\t\\t\\t   lockdep_is_held(&auditd_conn_lock));\\n\\tif (ac && ac != ac_old) {\\n\\t\\t/* someone already registered a new auditd connection */\\n\\t\\tspin_unlock_irqrestore(&auditd_conn_lock, flags);\\n\\t\\treturn;\\n\\t}\\n\\trcu_assign_pointer(auditd_conn, NULL);\\n\\tspin_unlock_irqrestore(&auditd_conn_lock, flags);\\n\\n\\tif (ac_old)\\n\\t\\tcall_rcu(&ac_old->rcu, auditd_conn_free);\\n\\n\\t/* flush the retry queue to the hold queue, but don\\'t touch the main\\n\\t * queue since we need to process that normally for multicast */\\n\\twhile ((skb = skb_dequeue(&audit_retry_queue)))\\n\\t\\tkauditd_hold_skb(skb, -ECONNREFUSED);\\n}\\n\\n/**\\n * auditd_send_unicast_skb - Send a record via unicast to auditd\\n * @skb: audit record\\n *\\n * Description:\\n * Send a skb to the audit daemon, returns positive/zero values on success and\\n * negative values on failure; in all cases the skb will be consumed by this\\n * function.  If the send results in -ECONNREFUSED the connection with auditd\\n * will be reset.  This function may sleep so callers should not hold any locks\\n * where this would cause a problem.\\n */\\nstatic int auditd_send_unicast_skb(struct sk_buff *skb)\\n{\\n\\tint rc;\\n\\tu32 portid;\\n\\tstruct net *net;\\n\\tstruct sock *sk;\\n\\tstruct auditd_connection *ac;\\n\\n\\t/* NOTE: we can\\'t call netlink_unicast while in the RCU section so\\n\\t *       take a reference to the network namespace and grab local\\n\\t *       copies of the namespace, the sock, and the portid; the\\n\\t *       namespace and sock aren\\'t going to go away while we hold a\\n\\t *       reference and if the portid does become invalid after the RCU\\n\\t *       section netlink_unicast() should safely return an error */\\n\\n\\trcu_read_lock();\\n\\tac = rcu_dereference(auditd_conn);\\n\\tif (!ac) {\\n\\t\\trcu_read_unlock();\\n\\t\\tkfree_skb(skb);\\n\\t\\trc = -ECONNREFUSED;\\n\\t\\tgoto err;\\n\\t}\\n\\tnet = get_net(ac->net);\\n\\tsk = audit_get_sk(net);\\n\\tportid = ac->portid;\\n\\trcu_read_unlock();\\n\\n\\trc = netlink_unicast(sk, skb, portid, 0);\\n\\tput_net(net);\\n\\tif (rc < 0)\\n\\t\\tgoto err;\\n\\n\\treturn rc;\\n\\nerr:\\n\\tif (ac && rc == -ECONNREFUSED)\\n\\t\\tauditd_reset(ac);\\n\\treturn rc;\\n}\\n\\n/**\\n * kauditd_send_queue - Helper for kauditd_thread to flush skb queues\\n * @sk: the sending sock\\n * @portid: the netlink destination\\n * @queue: the skb queue to process\\n * @retry_limit: limit on number of netlink unicast failures\\n * @skb_hook: per-skb hook for additional processing\\n * @err_hook: hook called if the skb fails the netlink unicast send\\n *\\n * Description:\\n * Run through the given queue and attempt to send the audit records to auditd,\\n * returns zero on success, negative values on failure.  It is up to the caller\\n * to ensure that the @sk is valid for the duration of this function.\\n *\\n */\\nstatic int kauditd_send_queue(struct sock *sk, u32 portid,\\n\\t\\t\\t      struct sk_buff_head *queue,\\n\\t\\t\\t      unsigned int retry_limit,\\n\\t\\t\\t      void (*skb_hook)(struct sk_buff *skb),\\n\\t\\t\\t      void (*err_hook)(struct sk_buff *skb, int error))\\n{\\n\\tint rc = 0;\\n\\tstruct sk_buff *skb = NULL;\\n\\tstruct sk_buff *skb_tail;\\n\\tunsigned int failed = 0;\\n\\n\\t/* NOTE: kauditd_thread takes care of all our locking, we just use\\n\\t *       the netlink info passed to us (e.g. sk and portid) */\\n\\n\\tskb_tail = skb_peek_tail(queue);\\n\\twhile ((skb != skb_tail) && (skb = skb_dequeue(queue))) {\\n\\t\\t/* call the skb_hook for each skb we touch */\\n\\t\\tif (skb_hook)\\n\\t\\t\\t(*skb_hook)(skb);\\n\\n\\t\\t/* can we send to anyone via unicast? */\\n\\t\\tif (!sk) {\\n\\t\\t\\tif (err_hook)\\n\\t\\t\\t\\t(*err_hook)(skb, -ECONNREFUSED);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\nretry:\\n\\t\\t/* grab an extra skb reference in case of error */\\n\\t\\tskb_get(skb);\\n\\t\\trc = netlink_unicast(sk, skb, portid, 0);\\n\\t\\tif (rc < 0) {\\n\\t\\t\\t/* send failed - try a few times unless fatal error */\\n\\t\\t\\tif (++failed >= retry_limit ||\\n\\t\\t\\t    rc == -ECONNREFUSED || rc == -EPERM) {\\n\\t\\t\\t\\tsk = NULL;\\n\\t\\t\\t\\tif (err_hook)\\n\\t\\t\\t\\t\\t(*err_hook)(skb, rc);\\n\\t\\t\\t\\tif (rc == -EAGAIN)\\n\\t\\t\\t\\t\\trc = 0;\\n\\t\\t\\t\\t/* continue to drain the queue */\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t} else\\n\\t\\t\\t\\tgoto retry;\\n\\t\\t} else {\\n\\t\\t\\t/* skb sent - drop the extra reference and continue */\\n\\t\\t\\tconsume_skb(skb);\\n\\t\\t\\tfailed = 0;\\n\\t\\t}\\n\\t}\\n\\n\\treturn (rc >= 0 ? 0 : rc);\\n}\\n\\n/*\\n * kauditd_send_multicast_skb - Send a record to any multicast listeners\\n * @skb: audit record\\n *\\n * Description:\\n * Write a multicast message to anyone listening in the initial network\\n * namespace.  This function doesn\\'t consume an skb as might be expected since\\n * it has to copy it anyways.\\n */\\nstatic void kauditd_send_multicast_skb(struct sk_buff *skb)\\n{\\n\\tstruct sk_buff *copy;\\n\\tstruct sock *sock = audit_get_sk(&init_net);\\n\\tstruct nlmsghdr *nlh;\\n\\n\\t/* NOTE: we are not taking an additional reference for init_net since\\n\\t *       we don\\'t have to worry about it going away */\\n\\n\\tif (!netlink_has_listeners(sock, AUDIT_NLGRP_READLOG))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * The seemingly wasteful skb_copy() rather than bumping the refcount\\n\\t * using skb_get() is necessary because non-standard mods are made to\\n\\t * the skb by the original kaudit unicast socket send routine.  The\\n\\t * existing auditd daemon assumes this breakage.  Fixing this would\\n\\t * require co-ordinating a change in the established protocol between\\n\\t * the kaudit kernel subsystem and the auditd userspace code.  There is\\n\\t * no reason for new multicast clients to continue with this\\n\\t * non-compliance.\\n\\t */\\n\\tcopy = skb_copy(skb, GFP_KERNEL);\\n\\tif (!copy)\\n\\t\\treturn;\\n\\tnlh = nlmsg_hdr(copy);\\n\\tnlh->nlmsg_len = skb->len;\\n\\n\\tnlmsg_multicast(sock, copy, 0, AUDIT_NLGRP_READLOG, GFP_KERNEL);\\n}\\n\\n/**\\n * kauditd_thread - Worker thread to send audit records to userspace\\n * @dummy: unused\\n */\\nstatic int kauditd_thread(void *dummy)\\n{\\n\\tint rc;\\n\\tu32 portid = 0;\\n\\tstruct net *net = NULL;\\n\\tstruct sock *sk = NULL;\\n\\tstruct auditd_connection *ac;\\n\\n#define UNICAST_RETRIES 5\\n\\n\\tset_freezable();\\n\\twhile (!kthread_should_stop()) {\\n\\t\\t/* NOTE: see the lock comments in auditd_send_unicast_skb() */\\n\\t\\trcu_read_lock();\\n\\t\\tac = rcu_dereference(auditd_conn);\\n\\t\\tif (!ac) {\\n\\t\\t\\trcu_read_unlock();\\n\\t\\t\\tgoto main_queue;\\n\\t\\t}\\n\\t\\tnet = get_net(ac->net);\\n\\t\\tsk = audit_get_sk(net);\\n\\t\\tportid = ac->portid;\\n\\t\\trcu_read_unlock();\\n\\n\\t\\t/* attempt to flush the hold queue */\\n\\t\\trc = kauditd_send_queue(sk, portid,\\n\\t\\t\\t\\t\\t&audit_hold_queue, UNICAST_RETRIES,\\n\\t\\t\\t\\t\\tNULL, kauditd_rehold_skb);\\n\\t\\tif (rc < 0) {\\n\\t\\t\\tsk = NULL;\\n\\t\\t\\tauditd_reset(ac);\\n\\t\\t\\tgoto main_queue;\\n\\t\\t}\\n\\n\\t\\t/* attempt to flush the retry queue */\\n\\t\\trc = kauditd_send_queue(sk, portid,\\n\\t\\t\\t\\t\\t&audit_retry_queue, UNICAST_RETRIES,\\n\\t\\t\\t\\t\\tNULL, kauditd_hold_skb);\\n\\t\\tif (rc < 0) {\\n\\t\\t\\tsk = NULL;\\n\\t\\t\\tauditd_reset(ac);\\n\\t\\t\\tgoto main_queue;\\n\\t\\t}\\n\\nmain_queue:\\n\\t\\t/* process the main queue - do the multicast send and attempt\\n\\t\\t * unicast, dump failed record sends to the retry queue; if\\n\\t\\t * sk == NULL due to previous failures we will just do the\\n\\t\\t * multicast send and move the record to the hold queue */\\n\\t\\trc = kauditd_send_queue(sk, portid, &audit_queue, 1,\\n\\t\\t\\t\\t\\tkauditd_send_multicast_skb,\\n\\t\\t\\t\\t\\t(sk ?\\n\\t\\t\\t\\t\\t kauditd_retry_skb : kauditd_hold_skb));\\n\\t\\tif (ac && rc < 0)\\n\\t\\t\\tauditd_reset(ac);\\n\\t\\tsk = NULL;\\n\\n\\t\\t/* drop our netns reference, no auditd sends past this line */\\n\\t\\tif (net) {\\n\\t\\t\\tput_net(net);\\n\\t\\t\\tnet = NULL;\\n\\t\\t}\\n\\n\\t\\t/* we have processed all the queues so wake everyone */\\n\\t\\twake_up(&audit_backlog_wait);\\n\\n\\t\\t/* NOTE: we want to wake up if there is anything on the queue,\\n\\t\\t *       regardless of if an auditd is connected, as we need to\\n\\t\\t *       do the multicast send and rotate records from the\\n\\t\\t *       main queue to the retry/hold queues */\\n\\t\\twait_event_freezable(kauditd_wait,\\n\\t\\t\\t\\t     (skb_queue_len(&audit_queue) ? 1 : 0));\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nint audit_send_list_thread(void *_dest)\\n{\\n\\tstruct audit_netlink_list *dest = _dest;\\n\\tstruct sk_buff *skb;\\n\\tstruct sock *sk = audit_get_sk(dest->net);\\n\\n\\t/* wait for parent to finish and send an ACK */\\n\\taudit_ctl_lock();\\n\\taudit_ctl_unlock();\\n\\n\\twhile ((skb = __skb_dequeue(&dest->q)) != NULL)\\n\\t\\tnetlink_unicast(sk, skb, dest->portid, 0);\\n\\n\\tput_net(dest->net);\\n\\tkfree(dest);\\n\\n\\treturn 0;\\n}\\n\\nstruct sk_buff *audit_make_reply(int seq, int type, int done,\\n\\t\\t\\t\\t int multi, const void *payload, int size)\\n{\\n\\tstruct sk_buff\\t*skb;\\n\\tstruct nlmsghdr\\t*nlh;\\n\\tvoid\\t\\t*data;\\n\\tint\\t\\tflags = multi ? NLM_F_MULTI : 0;\\n\\tint\\t\\tt     = done  ? NLMSG_DONE  : type;\\n\\n\\tskb = nlmsg_new(size, GFP_KERNEL);\\n\\tif (!skb)\\n\\t\\treturn NULL;\\n\\n\\tnlh\\t= nlmsg_put(skb, 0, seq, t, size, flags);\\n\\tif (!nlh)\\n\\t\\tgoto out_kfree_skb;\\n\\tdata = nlmsg_data(nlh);\\n\\tmemcpy(data, payload, size);\\n\\treturn skb;\\n\\nout_kfree_skb:\\n\\tkfree_skb(skb);\\n\\treturn NULL;\\n}\\n\\nstatic void audit_free_reply(struct audit_reply *reply)\\n{\\n\\tif (!reply)\\n\\t\\treturn;\\n\\n\\tkfree_skb(reply->skb);\\n\\tif (reply->net)\\n\\t\\tput_net(reply->net);\\n\\tkfree(reply);\\n}\\n\\nstatic int audit_send_reply_thread(void *arg)\\n{\\n\\tstruct audit_reply *reply = (struct audit_reply *)arg;\\n\\n\\taudit_ctl_lock();\\n\\taudit_ctl_unlock();\\n\\n\\t/* Ignore failure. It\\'ll only happen if the sender goes away,\\n\\t   because our timeout is set to infinite. */\\n\\tnetlink_unicast(audit_get_sk(reply->net), reply->skb, reply->portid, 0);\\n\\treply->skb = NULL;\\n\\taudit_free_reply(reply);\\n\\treturn 0;\\n}\\n\\n/**\\n * audit_send_reply - send an audit reply message via netlink\\n * @request_skb: skb of request we are replying to (used to target the reply)\\n * @seq: sequence number\\n * @type: audit message type\\n * @done: done (last) flag\\n * @multi: multi-part message flag\\n * @payload: payload data\\n * @size: payload size\\n *\\n * Allocates a skb, builds the netlink message, and sends it to the port id.\\n */\\nstatic void audit_send_reply(struct sk_buff *request_skb, int seq, int type, int done,\\n\\t\\t\\t     int multi, const void *payload, int size)\\n{\\n\\tstruct task_struct *tsk;\\n\\tstruct audit_reply *reply;\\n\\n\\treply = kzalloc(sizeof(*reply), GFP_KERNEL);\\n\\tif (!reply)\\n\\t\\treturn;\\n\\n\\treply->skb = audit_make_reply(seq, type, done, multi, payload, size);\\n\\tif (!reply->skb)\\n\\t\\tgoto err;\\n\\treply->net = get_net(sock_net(NETLINK_CB(request_skb).sk));\\n\\treply->portid = NETLINK_CB(request_skb).portid;\\n\\n\\ttsk = kthread_run(audit_send_reply_thread, reply, \"audit_send_reply\");\\n\\tif (IS_ERR(tsk))\\n\\t\\tgoto err;\\n\\n\\treturn;\\n\\nerr:\\n\\taudit_free_reply(reply);\\n}\\n\\n/*\\n * Check for appropriate CAP_AUDIT_ capabilities on incoming audit\\n * control messages.\\n */\\nstatic int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\\n{\\n\\tint err = 0;\\n\\n\\t/* Only support initial user namespace for now. */\\n\\t/*\\n\\t * We return ECONNREFUSED because it tricks userspace into thinking\\n\\t * that audit was not configured into the kernel.  Lots of users\\n\\t * configure their PAM stack (because that\\'s what the distro does)\\n\\t * to reject login if unable to send messages to audit.  If we return\\n\\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\\n\\t * configured in and will let login proceed.  If we return EPERM\\n\\t * userspace will reject all logins.  This should be removed when we\\n\\t * support non init namespaces!!\\n\\t */\\n\\tif (current_user_ns() != &init_user_ns)\\n\\t\\treturn -ECONNREFUSED;\\n\\n\\tswitch (msg_type) {\\n\\tcase AUDIT_LIST:\\n\\tcase AUDIT_ADD:\\n\\tcase AUDIT_DEL:\\n\\t\\treturn -EOPNOTSUPP;\\n\\tcase AUDIT_GET:\\n\\tcase AUDIT_SET:\\n\\tcase AUDIT_GET_FEATURE:\\n\\tcase AUDIT_SET_FEATURE:\\n\\tcase AUDIT_LIST_RULES:\\n\\tcase AUDIT_ADD_RULE:\\n\\tcase AUDIT_DEL_RULE:\\n\\tcase AUDIT_SIGNAL_INFO:\\n\\tcase AUDIT_TTY_GET:\\n\\tcase AUDIT_TTY_SET:\\n\\tcase AUDIT_TRIM:\\n\\tcase AUDIT_MAKE_EQUIV:\\n\\t\\t/* Only support auditd and auditctl in initial pid namespace\\n\\t\\t * for now. */\\n\\t\\tif (task_active_pid_ns(current) != &init_pid_ns)\\n\\t\\t\\treturn -EPERM;\\n\\n\\t\\tif (!netlink_capable(skb, CAP_AUDIT_CONTROL))\\n\\t\\t\\terr = -EPERM;\\n\\t\\tbreak;\\n\\tcase AUDIT_USER:\\n\\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\\n\\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\\n\\t\\tif (!netlink_capable(skb, CAP_AUDIT_WRITE))\\n\\t\\t\\terr = -EPERM;\\n\\t\\tbreak;\\n\\tdefault:  /* bad msg */\\n\\t\\terr = -EINVAL;\\n\\t}\\n\\n\\treturn err;\\n}\\n\\nstatic void audit_log_common_recv_msg(struct audit_context *context,\\n\\t\\t\\t\\t\\tstruct audit_buffer **ab, u16 msg_type)\\n{\\n\\tuid_t uid = from_kuid(&init_user_ns, current_uid());\\n\\tpid_t pid = task_tgid_nr(current);\\n\\n\\tif (!audit_enabled && msg_type != AUDIT_USER_AVC) {\\n\\t\\t*ab = NULL;\\n\\t\\treturn;\\n\\t}\\n\\n\\t*ab = audit_log_start(context, GFP_KERNEL, msg_type);\\n\\tif (unlikely(!*ab))\\n\\t\\treturn;\\n\\taudit_log_format(*ab, \"pid=%d uid=%u \", pid, uid);\\n\\taudit_log_session_info(*ab);\\n\\taudit_log_task_context(*ab);\\n}\\n\\nstatic inline void audit_log_user_recv_msg(struct audit_buffer **ab,\\n\\t\\t\\t\\t\\t   u16 msg_type)\\n{\\n\\taudit_log_common_recv_msg(NULL, ab, msg_type);\\n}\\n\\nstatic int is_audit_feature_set(int i)\\n{\\n\\treturn af.features & AUDIT_FEATURE_TO_MASK(i);\\n}\\n\\n\\nstatic int audit_get_feature(struct sk_buff *skb)\\n{\\n\\tu32 seq;\\n\\n\\tseq = nlmsg_hdr(skb)->nlmsg_seq;\\n\\n\\taudit_send_reply(skb, seq, AUDIT_GET_FEATURE, 0, 0, &af, sizeof(af));\\n\\n\\treturn 0;\\n}\\n\\nstatic void audit_log_feature_change(int which, u32 old_feature, u32 new_feature,\\n\\t\\t\\t\\t     u32 old_lock, u32 new_lock, int res)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (audit_enabled == AUDIT_OFF)\\n\\t\\treturn;\\n\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_FEATURE_CHANGE);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\taudit_log_task_info(ab);\\n\\taudit_log_format(ab, \" feature=%s old=%u new=%u old_lock=%u new_lock=%u res=%d\",\\n\\t\\t\\t audit_feature_names[which], !!old_feature, !!new_feature,\\n\\t\\t\\t !!old_lock, !!new_lock, res);\\n\\taudit_log_end(ab);\\n}\\n\\nstatic int audit_set_feature(struct audit_features *uaf)\\n{\\n\\tint i;\\n\\n\\tBUILD_BUG_ON(AUDIT_LAST_FEATURE + 1 > ARRAY_SIZE(audit_feature_names));\\n\\n\\t/* if there is ever a version 2 we should handle that here */\\n\\n\\tfor (i = 0; i <= AUDIT_LAST_FEATURE; i++) {\\n\\t\\tu32 feature = AUDIT_FEATURE_TO_MASK(i);\\n\\t\\tu32 old_feature, new_feature, old_lock, new_lock;\\n\\n\\t\\t/* if we are not changing this feature, move along */\\n\\t\\tif (!(feature & uaf->mask))\\n\\t\\t\\tcontinue;\\n\\n\\t\\told_feature = af.features & feature;\\n\\t\\tnew_feature = uaf->features & feature;\\n\\t\\tnew_lock = (uaf->lock | af.lock) & feature;\\n\\t\\told_lock = af.lock & feature;\\n\\n\\t\\t/* are we changing a locked feature? */\\n\\t\\tif (old_lock && (new_feature != old_feature)) {\\n\\t\\t\\taudit_log_feature_change(i, old_feature, new_feature,\\n\\t\\t\\t\\t\\t\\t old_lock, new_lock, 0);\\n\\t\\t\\treturn -EPERM;\\n\\t\\t}\\n\\t}\\n\\t/* nothing invalid, do the changes */\\n\\tfor (i = 0; i <= AUDIT_LAST_FEATURE; i++) {\\n\\t\\tu32 feature = AUDIT_FEATURE_TO_MASK(i);\\n\\t\\tu32 old_feature, new_feature, old_lock, new_lock;\\n\\n\\t\\t/* if we are not changing this feature, move along */\\n\\t\\tif (!(feature & uaf->mask))\\n\\t\\t\\tcontinue;\\n\\n\\t\\told_feature = af.features & feature;\\n\\t\\tnew_feature = uaf->features & feature;\\n\\t\\told_lock = af.lock & feature;\\n\\t\\tnew_lock = (uaf->lock | af.lock) & feature;\\n\\n\\t\\tif (new_feature != old_feature)\\n\\t\\t\\taudit_log_feature_change(i, old_feature, new_feature,\\n\\t\\t\\t\\t\\t\\t old_lock, new_lock, 1);\\n\\n\\t\\tif (new_feature)\\n\\t\\t\\taf.features |= feature;\\n\\t\\telse\\n\\t\\t\\taf.features &= ~feature;\\n\\t\\taf.lock |= new_lock;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic int audit_replace(struct pid *pid)\\n{\\n\\tpid_t pvnr;\\n\\tstruct sk_buff *skb;\\n\\n\\tpvnr = pid_vnr(pid);\\n\\tskb = audit_make_reply(0, AUDIT_REPLACE, 0, 0, &pvnr, sizeof(pvnr));\\n\\tif (!skb)\\n\\t\\treturn -ENOMEM;\\n\\treturn auditd_send_unicast_skb(skb);\\n}\\n\\nstatic int audit_receive_msg(struct sk_buff *skb, struct nlmsghdr *nlh,\\n\\t\\t\\t     bool *ack)\\n{\\n\\tu32\\t\\t\\tseq;\\n\\tvoid\\t\\t\\t*data;\\n\\tint\\t\\t\\tdata_len;\\n\\tint\\t\\t\\terr;\\n\\tstruct audit_buffer\\t*ab;\\n\\tu16\\t\\t\\tmsg_type = nlh->nlmsg_type;\\n\\tstruct audit_sig_info   *sig_data;\\n\\tchar\\t\\t\\t*ctx = NULL;\\n\\tu32\\t\\t\\tlen;\\n\\n\\terr = audit_netlink_ok(skb, msg_type);\\n\\tif (err)\\n\\t\\treturn err;\\n\\n\\tseq  = nlh->nlmsg_seq;\\n\\tdata = nlmsg_data(nlh);\\n\\tdata_len = nlmsg_len(nlh);\\n\\n\\tswitch (msg_type) {\\n\\tcase AUDIT_GET: {\\n\\t\\tstruct audit_status\\ts;\\n\\t\\tmemset(&s, 0, sizeof(s));\\n\\t\\ts.enabled\\t\\t   = audit_enabled;\\n\\t\\ts.failure\\t\\t   = audit_failure;\\n\\t\\t/* NOTE: use pid_vnr() so the PID is relative to the current\\n\\t\\t *       namespace */\\n\\t\\ts.pid\\t\\t\\t   = auditd_pid_vnr();\\n\\t\\ts.rate_limit\\t\\t   = audit_rate_limit;\\n\\t\\ts.backlog_limit\\t\\t   = audit_backlog_limit;\\n\\t\\ts.lost\\t\\t\\t   = atomic_read(&audit_lost);\\n\\t\\ts.backlog\\t\\t   = skb_queue_len(&audit_queue);\\n\\t\\ts.feature_bitmap\\t   = AUDIT_FEATURE_BITMAP_ALL;\\n\\t\\ts.backlog_wait_time\\t   = audit_backlog_wait_time;\\n\\t\\ts.backlog_wait_time_actual = atomic_read(&audit_backlog_wait_time_actual);\\n\\t\\taudit_send_reply(skb, seq, AUDIT_GET, 0, 0, &s, sizeof(s));\\n\\t\\tbreak;\\n\\t}\\n\\tcase AUDIT_SET: {\\n\\t\\tstruct audit_status\\ts;\\n\\t\\tmemset(&s, 0, sizeof(s));\\n\\t\\t/* guard against past and future API changes */\\n\\t\\tmemcpy(&s, data, min_t(size_t, sizeof(s), data_len));\\n\\t\\tif (s.mask & AUDIT_STATUS_ENABLED) {\\n\\t\\t\\terr = audit_set_enabled(s.enabled);\\n\\t\\t\\tif (err < 0)\\n\\t\\t\\t\\treturn err;\\n\\t\\t}\\n\\t\\tif (s.mask & AUDIT_STATUS_FAILURE) {\\n\\t\\t\\terr = audit_set_failure(s.failure);\\n\\t\\t\\tif (err < 0)\\n\\t\\t\\t\\treturn err;\\n\\t\\t}\\n\\t\\tif (s.mask & AUDIT_STATUS_PID) {\\n\\t\\t\\t/* NOTE: we are using the vnr PID functions below\\n\\t\\t\\t *       because the s.pid value is relative to the\\n\\t\\t\\t *       namespace of the caller; at present this\\n\\t\\t\\t *       doesn\\'t matter much since you can really only\\n\\t\\t\\t *       run auditd from the initial pid namespace, but\\n\\t\\t\\t *       something to keep in mind if this changes */\\n\\t\\t\\tpid_t new_pid = s.pid;\\n\\t\\t\\tpid_t auditd_pid;\\n\\t\\t\\tstruct pid *req_pid = task_tgid(current);\\n\\n\\t\\t\\t/* Sanity check - PID values must match. Setting\\n\\t\\t\\t * pid to 0 is how auditd ends auditing. */\\n\\t\\t\\tif (new_pid && (new_pid != pid_vnr(req_pid)))\\n\\t\\t\\t\\treturn -EINVAL;\\n\\n\\t\\t\\t/* test the auditd connection */\\n\\t\\t\\taudit_replace(req_pid);\\n\\n\\t\\t\\tauditd_pid = auditd_pid_vnr();\\n\\t\\t\\tif (auditd_pid) {\\n\\t\\t\\t\\t/* replacing a healthy auditd is not allowed */\\n\\t\\t\\t\\tif (new_pid) {\\n\\t\\t\\t\\t\\taudit_log_config_change(\"audit_pid\",\\n\\t\\t\\t\\t\\t\\t\\tnew_pid, auditd_pid, 0);\\n\\t\\t\\t\\t\\treturn -EEXIST;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t/* only current auditd can unregister itself */\\n\\t\\t\\t\\tif (pid_vnr(req_pid) != auditd_pid) {\\n\\t\\t\\t\\t\\taudit_log_config_change(\"audit_pid\",\\n\\t\\t\\t\\t\\t\\t\\tnew_pid, auditd_pid, 0);\\n\\t\\t\\t\\t\\treturn -EACCES;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\n\\t\\t\\tif (new_pid) {\\n\\t\\t\\t\\t/* register a new auditd connection */\\n\\t\\t\\t\\terr = auditd_set(req_pid,\\n\\t\\t\\t\\t\\t\\t NETLINK_CB(skb).portid,\\n\\t\\t\\t\\t\\t\\t sock_net(NETLINK_CB(skb).sk),\\n\\t\\t\\t\\t\\t\\t skb, ack);\\n\\t\\t\\t\\tif (audit_enabled != AUDIT_OFF)\\n\\t\\t\\t\\t\\taudit_log_config_change(\"audit_pid\",\\n\\t\\t\\t\\t\\t\\t\\t\\tnew_pid,\\n\\t\\t\\t\\t\\t\\t\\t\\tauditd_pid,\\n\\t\\t\\t\\t\\t\\t\\t\\terr ? 0 : 1);\\n\\t\\t\\t\\tif (err)\\n\\t\\t\\t\\t\\treturn err;\\n\\n\\t\\t\\t\\t/* try to process any backlog */\\n\\t\\t\\t\\twake_up_interruptible(&kauditd_wait);\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tif (audit_enabled != AUDIT_OFF)\\n\\t\\t\\t\\t\\taudit_log_config_change(\"audit_pid\",\\n\\t\\t\\t\\t\\t\\t\\t\\tnew_pid,\\n\\t\\t\\t\\t\\t\\t\\t\\tauditd_pid, 1);\\n\\n\\t\\t\\t\\t/* unregister the auditd connection */\\n\\t\\t\\t\\tauditd_reset(NULL);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif (s.mask & AUDIT_STATUS_RATE_LIMIT) {\\n\\t\\t\\terr = audit_set_rate_limit(s.rate_limit);\\n\\t\\t\\tif (err < 0)\\n\\t\\t\\t\\treturn err;\\n\\t\\t}\\n\\t\\tif (s.mask & AUDIT_STATUS_BACKLOG_LIMIT) {\\n\\t\\t\\terr = audit_set_backlog_limit(s.backlog_limit);\\n\\t\\t\\tif (err < 0)\\n\\t\\t\\t\\treturn err;\\n\\t\\t}\\n\\t\\tif (s.mask & AUDIT_STATUS_BACKLOG_WAIT_TIME) {\\n\\t\\t\\tif (sizeof(s) > (size_t)nlh->nlmsg_len)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tif (s.backlog_wait_time > 10*AUDIT_BACKLOG_WAIT_TIME)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\terr = audit_set_backlog_wait_time(s.backlog_wait_time);\\n\\t\\t\\tif (err < 0)\\n\\t\\t\\t\\treturn err;\\n\\t\\t}\\n\\t\\tif (s.mask == AUDIT_STATUS_LOST) {\\n\\t\\t\\tu32 lost = atomic_xchg(&audit_lost, 0);\\n\\n\\t\\t\\taudit_log_config_change(\"lost\", 0, lost, 1);\\n\\t\\t\\treturn lost;\\n\\t\\t}\\n\\t\\tif (s.mask == AUDIT_STATUS_BACKLOG_WAIT_TIME_ACTUAL) {\\n\\t\\t\\tu32 actual = atomic_xchg(&audit_backlog_wait_time_actual, 0);\\n\\n\\t\\t\\taudit_log_config_change(\"backlog_wait_time_actual\", 0, actual, 1);\\n\\t\\t\\treturn actual;\\n\\t\\t}\\n\\t\\tbreak;\\n\\t}\\n\\tcase AUDIT_GET_FEATURE:\\n\\t\\terr = audit_get_feature(skb);\\n\\t\\tif (err)\\n\\t\\t\\treturn err;\\n\\t\\tbreak;\\n\\tcase AUDIT_SET_FEATURE:\\n\\t\\tif (data_len < sizeof(struct audit_features))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terr = audit_set_feature(data);\\n\\t\\tif (err)\\n\\t\\t\\treturn err;\\n\\t\\tbreak;\\n\\tcase AUDIT_USER:\\n\\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\\n\\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\\n\\t\\tif (!audit_enabled && msg_type != AUDIT_USER_AVC)\\n\\t\\t\\treturn 0;\\n\\t\\t/* exit early if there isn\\'t at least one character to print */\\n\\t\\tif (data_len < 2)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\terr = audit_filter(msg_type, AUDIT_FILTER_USER);\\n\\t\\tif (err == 1) { /* match or error */\\n\\t\\t\\tchar *str = data;\\n\\n\\t\\t\\terr = 0;\\n\\t\\t\\tif (msg_type == AUDIT_USER_TTY) {\\n\\t\\t\\t\\terr = tty_audit_push();\\n\\t\\t\\t\\tif (err)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\taudit_log_user_recv_msg(&ab, msg_type);\\n\\t\\t\\tif (msg_type != AUDIT_USER_TTY) {\\n\\t\\t\\t\\t/* ensure NULL termination */\\n\\t\\t\\t\\tstr[data_len - 1] = \\'\\\\0\\';\\n\\t\\t\\t\\taudit_log_format(ab, \" msg=\\'%.*s\\'\",\\n\\t\\t\\t\\t\\t\\t AUDIT_MESSAGE_TEXT_MAX,\\n\\t\\t\\t\\t\\t\\t str);\\n\\t\\t\\t} else {\\n\\t\\t\\t\\taudit_log_format(ab, \" data=\");\\n\\t\\t\\t\\tif (str[data_len - 1] == \\'\\\\0\\')\\n\\t\\t\\t\\t\\tdata_len--;\\n\\t\\t\\t\\taudit_log_n_untrustedstring(ab, str, data_len);\\n\\t\\t\\t}\\n\\t\\t\\taudit_log_end(ab);\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase AUDIT_ADD_RULE:\\n\\tcase AUDIT_DEL_RULE:\\n\\t\\tif (data_len < sizeof(struct audit_rule_data))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (audit_enabled == AUDIT_LOCKED) {\\n\\t\\t\\taudit_log_common_recv_msg(audit_context(), &ab,\\n\\t\\t\\t\\t\\t\\t  AUDIT_CONFIG_CHANGE);\\n\\t\\t\\taudit_log_format(ab, \" op=%s audit_enabled=%d res=0\",\\n\\t\\t\\t\\t\\t msg_type == AUDIT_ADD_RULE ?\\n\\t\\t\\t\\t\\t\\t\"add_rule\" : \"remove_rule\",\\n\\t\\t\\t\\t\\t audit_enabled);\\n\\t\\t\\taudit_log_end(ab);\\n\\t\\t\\treturn -EPERM;\\n\\t\\t}\\n\\t\\terr = audit_rule_change(msg_type, seq, data, data_len);\\n\\t\\tbreak;\\n\\tcase AUDIT_LIST_RULES:\\n\\t\\terr = audit_list_rules_send(skb, seq);\\n\\t\\tbreak;\\n\\tcase AUDIT_TRIM:\\n\\t\\taudit_trim_trees();\\n\\t\\taudit_log_common_recv_msg(audit_context(), &ab,\\n\\t\\t\\t\\t\\t  AUDIT_CONFIG_CHANGE);\\n\\t\\taudit_log_format(ab, \" op=trim res=1\");\\n\\t\\taudit_log_end(ab);\\n\\t\\tbreak;\\n\\tcase AUDIT_MAKE_EQUIV: {\\n\\t\\tvoid *bufp = data;\\n\\t\\tu32 sizes[2];\\n\\t\\tsize_t msglen = data_len;\\n\\t\\tchar *old, *new;\\n\\n\\t\\terr = -EINVAL;\\n\\t\\tif (msglen < 2 * sizeof(u32))\\n\\t\\t\\tbreak;\\n\\t\\tmemcpy(sizes, bufp, 2 * sizeof(u32));\\n\\t\\tbufp += 2 * sizeof(u32);\\n\\t\\tmsglen -= 2 * sizeof(u32);\\n\\t\\told = audit_unpack_string(&bufp, &msglen, sizes[0]);\\n\\t\\tif (IS_ERR(old)) {\\n\\t\\t\\terr = PTR_ERR(old);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tnew = audit_unpack_string(&bufp, &msglen, sizes[1]);\\n\\t\\tif (IS_ERR(new)) {\\n\\t\\t\\terr = PTR_ERR(new);\\n\\t\\t\\tkfree(old);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\t/* OK, here comes... */\\n\\t\\terr = audit_tag_tree(old, new);\\n\\n\\t\\taudit_log_common_recv_msg(audit_context(), &ab,\\n\\t\\t\\t\\t\\t  AUDIT_CONFIG_CHANGE);\\n\\t\\taudit_log_format(ab, \" op=make_equiv old=\");\\n\\t\\taudit_log_untrustedstring(ab, old);\\n\\t\\taudit_log_format(ab, \" new=\");\\n\\t\\taudit_log_untrustedstring(ab, new);\\n\\t\\taudit_log_format(ab, \" res=%d\", !err);\\n\\t\\taudit_log_end(ab);\\n\\t\\tkfree(old);\\n\\t\\tkfree(new);\\n\\t\\tbreak;\\n\\t}\\n\\tcase AUDIT_SIGNAL_INFO:\\n\\t\\tlen = 0;\\n\\t\\tif (lsmprop_is_set(&audit_sig_lsm)) {\\n\\t\\t\\terr = security_lsmprop_to_secctx(&audit_sig_lsm, &ctx,\\n\\t\\t\\t\\t\\t\\t\\t &len);\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\treturn err;\\n\\t\\t}\\n\\t\\tsig_data = kmalloc(struct_size(sig_data, ctx, len), GFP_KERNEL);\\n\\t\\tif (!sig_data) {\\n\\t\\t\\tif (lsmprop_is_set(&audit_sig_lsm))\\n\\t\\t\\t\\tsecurity_release_secctx(ctx, len);\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\t}\\n\\t\\tsig_data->uid = from_kuid(&init_user_ns, audit_sig_uid);\\n\\t\\tsig_data->pid = audit_sig_pid;\\n\\t\\tif (lsmprop_is_set(&audit_sig_lsm)) {\\n\\t\\t\\tmemcpy(sig_data->ctx, ctx, len);\\n\\t\\t\\tsecurity_release_secctx(ctx, len);\\n\\t\\t}\\n\\t\\taudit_send_reply(skb, seq, AUDIT_SIGNAL_INFO, 0, 0,\\n\\t\\t\\t\\t sig_data, struct_size(sig_data, ctx, len));\\n\\t\\tkfree(sig_data);\\n\\t\\tbreak;\\n\\tcase AUDIT_TTY_GET: {\\n\\t\\tstruct audit_tty_status s;\\n\\t\\tunsigned int t;\\n\\n\\t\\tt = READ_ONCE(current->signal->audit_tty);\\n\\t\\ts.enabled = t & AUDIT_TTY_ENABLE;\\n\\t\\ts.log_passwd = !!(t & AUDIT_TTY_LOG_PASSWD);\\n\\n\\t\\taudit_send_reply(skb, seq, AUDIT_TTY_GET, 0, 0, &s, sizeof(s));\\n\\t\\tbreak;\\n\\t}\\n\\tcase AUDIT_TTY_SET: {\\n\\t\\tstruct audit_tty_status s, old;\\n\\t\\tstruct audit_buffer\\t*ab;\\n\\t\\tunsigned int t;\\n\\n\\t\\tmemset(&s, 0, sizeof(s));\\n\\t\\t/* guard against past and future API changes */\\n\\t\\tmemcpy(&s, data, min_t(size_t, sizeof(s), data_len));\\n\\t\\t/* check if new data is valid */\\n\\t\\tif ((s.enabled != 0 && s.enabled != 1) ||\\n\\t\\t    (s.log_passwd != 0 && s.log_passwd != 1))\\n\\t\\t\\terr = -EINVAL;\\n\\n\\t\\tif (err)\\n\\t\\t\\tt = READ_ONCE(current->signal->audit_tty);\\n\\t\\telse {\\n\\t\\t\\tt = s.enabled | (-s.log_passwd & AUDIT_TTY_LOG_PASSWD);\\n\\t\\t\\tt = xchg(&current->signal->audit_tty, t);\\n\\t\\t}\\n\\t\\told.enabled = t & AUDIT_TTY_ENABLE;\\n\\t\\told.log_passwd = !!(t & AUDIT_TTY_LOG_PASSWD);\\n\\n\\t\\taudit_log_common_recv_msg(audit_context(), &ab,\\n\\t\\t\\t\\t\\t  AUDIT_CONFIG_CHANGE);\\n\\t\\taudit_log_format(ab, \" op=tty_set old-enabled=%d new-enabled=%d\"\\n\\t\\t\\t\\t \" old-log_passwd=%d new-log_passwd=%d res=%d\",\\n\\t\\t\\t\\t old.enabled, s.enabled, old.log_passwd,\\n\\t\\t\\t\\t s.log_passwd, !err);\\n\\t\\taudit_log_end(ab);\\n\\t\\tbreak;\\n\\t}\\n\\tdefault:\\n\\t\\terr = -EINVAL;\\n\\t\\tbreak;\\n\\t}\\n\\n\\treturn err < 0 ? err : 0;\\n}\\n\\n/**\\n * audit_receive - receive messages from a netlink control socket\\n * @skb: the message buffer\\n *\\n * Parse the provided skb and deal with any messages that may be present,\\n * malformed skbs are discarded.\\n */\\nstatic void audit_receive(struct sk_buff *skb)\\n{\\n\\tstruct nlmsghdr *nlh;\\n\\tbool ack;\\n\\t/*\\n\\t * len MUST be signed for nlmsg_next to be able to dec it below 0\\n\\t * if the nlmsg_len was not aligned\\n\\t */\\n\\tint len;\\n\\tint err;\\n\\n\\tnlh = nlmsg_hdr(skb);\\n\\tlen = skb->len;\\n\\n\\taudit_ctl_lock();\\n\\twhile (nlmsg_ok(nlh, len)) {\\n\\t\\tack = nlh->nlmsg_flags & NLM_F_ACK;\\n\\t\\terr = audit_receive_msg(skb, nlh, &ack);\\n\\n\\t\\t/* send an ack if the user asked for one and audit_receive_msg\\n\\t\\t * didn\\'t already do it, or if there was an error. */\\n\\t\\tif (ack || err)\\n\\t\\t\\tnetlink_ack(skb, nlh, err, NULL);\\n\\n\\t\\tnlh = nlmsg_next(nlh, &len);\\n\\t}\\n\\taudit_ctl_unlock();\\n\\n\\t/* can\\'t block with the ctrl lock, so penalize the sender now */\\n\\tif (audit_backlog_limit &&\\n\\t    (skb_queue_len(&audit_queue) > audit_backlog_limit)) {\\n\\t\\tDECLARE_WAITQUEUE(wait, current);\\n\\n\\t\\t/* wake kauditd to try and flush the queue */\\n\\t\\twake_up_interruptible(&kauditd_wait);\\n\\n\\t\\tadd_wait_queue_exclusive(&audit_backlog_wait, &wait);\\n\\t\\tset_current_state(TASK_UNINTERRUPTIBLE);\\n\\t\\tschedule_timeout(audit_backlog_wait_time);\\n\\t\\tremove_wait_queue(&audit_backlog_wait, &wait);\\n\\t}\\n}\\n\\n/* Log information about who is connecting to the audit multicast socket */\\nstatic void audit_log_multicast(int group, const char *op, int err)\\n{\\n\\tconst struct cred *cred;\\n\\tstruct tty_struct *tty;\\n\\tchar comm[sizeof(current->comm)];\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_EVENT_LISTENER);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tcred = current_cred();\\n\\ttty = audit_get_tty();\\n\\taudit_log_format(ab, \"pid=%u uid=%u auid=%u tty=%s ses=%u\",\\n\\t\\t\\t task_tgid_nr(current),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->uid),\\n\\t\\t\\t from_kuid(&init_user_ns, audit_get_loginuid(current)),\\n\\t\\t\\t tty ? tty_name(tty) : \"(none)\",\\n\\t\\t\\t audit_get_sessionid(current));\\n\\taudit_put_tty(tty);\\n\\taudit_log_task_context(ab); /* subj= */\\n\\taudit_log_format(ab, \" comm=\");\\n\\taudit_log_untrustedstring(ab, get_task_comm(comm, current));\\n\\taudit_log_d_path_exe(ab, current->mm); /* exe= */\\n\\taudit_log_format(ab, \" nl-mcgrp=%d op=%s res=%d\", group, op, !err);\\n\\taudit_log_end(ab);\\n}\\n\\n/* Run custom bind function on netlink socket group connect or bind requests. */\\nstatic int audit_multicast_bind(struct net *net, int group)\\n{\\n\\tint err = 0;\\n\\n\\tif (!capable(CAP_AUDIT_READ))\\n\\t\\terr = -EPERM;\\n\\taudit_log_multicast(group, \"connect\", err);\\n\\treturn err;\\n}\\n\\nstatic void audit_multicast_unbind(struct net *net, int group)\\n{\\n\\taudit_log_multicast(group, \"disconnect\", 0);\\n}\\n\\nstatic int __net_init audit_net_init(struct net *net)\\n{\\n\\tstruct netlink_kernel_cfg cfg = {\\n\\t\\t.input\\t= audit_receive,\\n\\t\\t.bind\\t= audit_multicast_bind,\\n\\t\\t.unbind\\t= audit_multicast_unbind,\\n\\t\\t.flags\\t= NL_CFG_F_NONROOT_RECV,\\n\\t\\t.groups\\t= AUDIT_NLGRP_MAX,\\n\\t};\\n\\n\\tstruct audit_net *aunet = net_generic(net, audit_net_id);\\n\\n\\taunet->sk = netlink_kernel_create(net, NETLINK_AUDIT, &cfg);\\n\\tif (aunet->sk == NULL) {\\n\\t\\taudit_panic(\"cannot initialize netlink socket in namespace\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\t/* limit the timeout in case auditd is blocked/stopped */\\n\\taunet->sk->sk_sndtimeo = HZ / 10;\\n\\n\\treturn 0;\\n}\\n\\nstatic void __net_exit audit_net_exit(struct net *net)\\n{\\n\\tstruct audit_net *aunet = net_generic(net, audit_net_id);\\n\\n\\t/* NOTE: you would think that we would want to check the auditd\\n\\t * connection and potentially reset it here if it lives in this\\n\\t * namespace, but since the auditd connection tracking struct holds a\\n\\t * reference to this namespace (see auditd_set()) we are only ever\\n\\t * going to get here after that connection has been released */\\n\\n\\tnetlink_kernel_release(aunet->sk);\\n}\\n\\nstatic struct pernet_operations audit_net_ops __net_initdata = {\\n\\t.init = audit_net_init,\\n\\t.exit = audit_net_exit,\\n\\t.id = &audit_net_id,\\n\\t.size = sizeof(struct audit_net),\\n};\\n\\n/* Initialize audit support at boot time. */\\nstatic int __init audit_init(void)\\n{\\n\\tint i;\\n\\n\\tif (audit_initialized == AUDIT_DISABLED)\\n\\t\\treturn 0;\\n\\n\\taudit_buffer_cache = KMEM_CACHE(audit_buffer, SLAB_PANIC);\\n\\n\\tskb_queue_head_init(&audit_queue);\\n\\tskb_queue_head_init(&audit_retry_queue);\\n\\tskb_queue_head_init(&audit_hold_queue);\\n\\n\\tfor (i = 0; i < AUDIT_INODE_BUCKETS; i++)\\n\\t\\tINIT_LIST_HEAD(&audit_inode_hash[i]);\\n\\n\\tmutex_init(&audit_cmd_mutex.lock);\\n\\taudit_cmd_mutex.owner = NULL;\\n\\n\\tpr_info(\"initializing netlink subsys (%s)\\\\n\",\\n\\t\\tstr_enabled_disabled(audit_default));\\n\\tregister_pernet_subsys(&audit_net_ops);\\n\\n\\taudit_initialized = AUDIT_INITIALIZED;\\n\\n\\tkauditd_task = kthread_run(kauditd_thread, NULL, \"kauditd\");\\n\\tif (IS_ERR(kauditd_task)) {\\n\\t\\tint err = PTR_ERR(kauditd_task);\\n\\t\\tpanic(\"audit: failed to start the kauditd thread (%d)\\\\n\", err);\\n\\t}\\n\\n\\taudit_log(NULL, GFP_KERNEL, AUDIT_KERNEL,\\n\\t\\t\"state=initialized audit_enabled=%u res=1\",\\n\\t\\t audit_enabled);\\n\\n\\treturn 0;\\n}\\npostcore_initcall(audit_init);\\n\\n/*\\n * Process kernel command-line parameter at boot time.\\n * audit={0|off} or audit={1|on}.\\n */\\nstatic int __init audit_enable(char *str)\\n{\\n\\tif (!strcasecmp(str, \"off\") || !strcmp(str, \"0\"))\\n\\t\\taudit_default = AUDIT_OFF;\\n\\telse if (!strcasecmp(str, \"on\") || !strcmp(str, \"1\"))\\n\\t\\taudit_default = AUDIT_ON;\\n\\telse {\\n\\t\\tpr_err(\"audit: invalid \\'audit\\' parameter value (%s)\\\\n\", str);\\n\\t\\taudit_default = AUDIT_ON;\\n\\t}\\n\\n\\tif (audit_default == AUDIT_OFF)\\n\\t\\taudit_initialized = AUDIT_DISABLED;\\n\\tif (audit_set_enabled(audit_default))\\n\\t\\tpr_err(\"audit: error setting audit state (%d)\\\\n\",\\n\\t\\t       audit_default);\\n\\n\\tpr_info(\"%s\\\\n\", audit_default ?\\n\\t\\t\"enabled (after initialization)\" : \"disabled (until reboot)\");\\n\\n\\treturn 1;\\n}\\n__setup(\"audit=\", audit_enable);\\n\\n/* Process kernel command-line parameter at boot time.\\n * audit_backlog_limit=<n> */\\nstatic int __init audit_backlog_limit_set(char *str)\\n{\\n\\tu32 audit_backlog_limit_arg;\\n\\n\\tpr_info(\"audit_backlog_limit: \");\\n\\tif (kstrtouint(str, 0, &audit_backlog_limit_arg)) {\\n\\t\\tpr_cont(\"using default of %u, unable to parse %s\\\\n\",\\n\\t\\t\\taudit_backlog_limit, str);\\n\\t\\treturn 1;\\n\\t}\\n\\n\\taudit_backlog_limit = audit_backlog_limit_arg;\\n\\tpr_cont(\"%d\\\\n\", audit_backlog_limit);\\n\\n\\treturn 1;\\n}\\n__setup(\"audit_backlog_limit=\", audit_backlog_limit_set);\\n\\nstatic void audit_buffer_free(struct audit_buffer *ab)\\n{\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tkfree_skb(ab->skb);\\n\\tkmem_cache_free(audit_buffer_cache, ab);\\n}\\n\\nstatic struct audit_buffer *audit_buffer_alloc(struct audit_context *ctx,\\n\\t\\t\\t\\t\\t       gfp_t gfp_mask, int type)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tab = kmem_cache_alloc(audit_buffer_cache, gfp_mask);\\n\\tif (!ab)\\n\\t\\treturn NULL;\\n\\n\\tab->skb = nlmsg_new(AUDIT_BUFSIZ, gfp_mask);\\n\\tif (!ab->skb)\\n\\t\\tgoto err;\\n\\tif (!nlmsg_put(ab->skb, 0, 0, type, 0, 0))\\n\\t\\tgoto err;\\n\\n\\tab->ctx = ctx;\\n\\tab->gfp_mask = gfp_mask;\\n\\n\\treturn ab;\\n\\nerr:\\n\\taudit_buffer_free(ab);\\n\\treturn NULL;\\n}\\n\\n/**\\n * audit_serial - compute a serial number for the audit record\\n *\\n * Compute a serial number for the audit record.  Audit records are\\n * written to user-space as soon as they are generated, so a complete\\n * audit record may be written in several pieces.  The timestamp of the\\n * record and this serial number are used by the user-space tools to\\n * determine which pieces belong to the same audit record.  The\\n * (timestamp,serial) tuple is unique for each syscall and is live from\\n * syscall entry to syscall exit.\\n *\\n * NOTE: Another possibility is to store the formatted records off the\\n * audit context (for those records that have a context), and emit them\\n * all at syscall exit.  However, this could delay the reporting of\\n * significant errors until syscall exit (or never, if the system\\n * halts).\\n */\\nunsigned int audit_serial(void)\\n{\\n\\tstatic atomic_t serial = ATOMIC_INIT(0);\\n\\n\\treturn atomic_inc_return(&serial);\\n}\\n\\nstatic inline void audit_get_stamp(struct audit_context *ctx,\\n\\t\\t\\t\\t   struct timespec64 *t, unsigned int *serial)\\n{\\n\\tif (!ctx || !auditsc_get_stamp(ctx, t, serial)) {\\n\\t\\tktime_get_coarse_real_ts64(t);\\n\\t\\t*serial = audit_serial();\\n\\t}\\n}\\n\\n/**\\n * audit_log_start - obtain an audit buffer\\n * @ctx: audit_context (may be NULL)\\n * @gfp_mask: type of allocation\\n * @type: audit message type\\n *\\n * Returns audit_buffer pointer on success or NULL on error.\\n *\\n * Obtain an audit buffer.  This routine does locking to obtain the\\n * audit buffer, but then no locking is required for calls to\\n * audit_log_*format.  If the task (ctx) is a task that is currently in a\\n * syscall, then the syscall is marked as auditable and an audit record\\n * will be written at syscall exit.  If there is no associated task, then\\n * task context (ctx) should be NULL.\\n */\\nstruct audit_buffer *audit_log_start(struct audit_context *ctx, gfp_t gfp_mask,\\n\\t\\t\\t\\t     int type)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tstruct timespec64 t;\\n\\tunsigned int serial;\\n\\n\\tif (audit_initialized != AUDIT_INITIALIZED)\\n\\t\\treturn NULL;\\n\\n\\tif (unlikely(!audit_filter(type, AUDIT_FILTER_EXCLUDE)))\\n\\t\\treturn NULL;\\n\\n\\t/* NOTE: don\\'t ever fail/sleep on these two conditions:\\n\\t * 1. auditd generated record - since we need auditd to drain the\\n\\t *    queue; also, when we are checking for auditd, compare PIDs using\\n\\t *    task_tgid_vnr() since auditd_pid is set in audit_receive_msg()\\n\\t *    using a PID anchored in the caller\\'s namespace\\n\\t * 2. generator holding the audit_cmd_mutex - we don\\'t want to block\\n\\t *    while holding the mutex, although we do penalize the sender\\n\\t *    later in audit_receive() when it is safe to block\\n\\t */\\n\\tif (!(auditd_test_task(current) || audit_ctl_owner_current())) {\\n\\t\\tlong stime = audit_backlog_wait_time;\\n\\n\\t\\twhile (audit_backlog_limit &&\\n\\t\\t       (skb_queue_len(&audit_queue) > audit_backlog_limit)) {\\n\\t\\t\\t/* wake kauditd to try and flush the queue */\\n\\t\\t\\twake_up_interruptible(&kauditd_wait);\\n\\n\\t\\t\\t/* sleep if we are allowed and we haven\\'t exhausted our\\n\\t\\t\\t * backlog wait limit */\\n\\t\\t\\tif (gfpflags_allow_blocking(gfp_mask) && (stime > 0)) {\\n\\t\\t\\t\\tlong rtime = stime;\\n\\n\\t\\t\\t\\tDECLARE_WAITQUEUE(wait, current);\\n\\n\\t\\t\\t\\tadd_wait_queue_exclusive(&audit_backlog_wait,\\n\\t\\t\\t\\t\\t\\t\\t &wait);\\n\\t\\t\\t\\tset_current_state(TASK_UNINTERRUPTIBLE);\\n\\t\\t\\t\\tstime = schedule_timeout(rtime);\\n\\t\\t\\t\\tatomic_add(rtime - stime, &audit_backlog_wait_time_actual);\\n\\t\\t\\t\\tremove_wait_queue(&audit_backlog_wait, &wait);\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tif (audit_rate_check() && printk_ratelimit())\\n\\t\\t\\t\\t\\tpr_warn(\"audit_backlog=%d > audit_backlog_limit=%d\\\\n\",\\n\\t\\t\\t\\t\\t\\tskb_queue_len(&audit_queue),\\n\\t\\t\\t\\t\\t\\taudit_backlog_limit);\\n\\t\\t\\t\\taudit_log_lost(\"backlog limit exceeded\");\\n\\t\\t\\t\\treturn NULL;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\tab = audit_buffer_alloc(ctx, gfp_mask, type);\\n\\tif (!ab) {\\n\\t\\taudit_log_lost(\"out of memory in audit_log_start\");\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\taudit_get_stamp(ab->ctx, &t, &serial);\\n\\t/* cancel dummy context to enable supporting records */\\n\\tif (ctx)\\n\\t\\tctx->dummy = 0;\\n\\taudit_log_format(ab, \"audit(%llu.%03lu:%u): \",\\n\\t\\t\\t (unsigned long long)t.tv_sec, t.tv_nsec/1000000, serial);\\n\\n\\treturn ab;\\n}\\n\\n/**\\n * audit_expand - expand skb in the audit buffer\\n * @ab: audit_buffer\\n * @extra: space to add at tail of the skb\\n *\\n * Returns 0 (no space) on failed expansion, or available space if\\n * successful.\\n */\\nstatic inline int audit_expand(struct audit_buffer *ab, int extra)\\n{\\n\\tstruct sk_buff *skb = ab->skb;\\n\\tint oldtail = skb_tailroom(skb);\\n\\tint ret = pskb_expand_head(skb, 0, extra, ab->gfp_mask);\\n\\tint newtail = skb_tailroom(skb);\\n\\n\\tif (ret < 0) {\\n\\t\\taudit_log_lost(\"out of memory in audit_expand\");\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tskb->truesize += newtail - oldtail;\\n\\treturn newtail;\\n}\\n\\n/*\\n * Format an audit message into the audit buffer.  If there isn\\'t enough\\n * room in the audit buffer, more room will be allocated and vsnprint\\n * will be called a second time.  Currently, we assume that a printk\\n * can\\'t format message larger than 1024 bytes, so we don\\'t either.\\n */\\nstatic void audit_log_vformat(struct audit_buffer *ab, const char *fmt,\\n\\t\\t\\t      va_list args)\\n{\\n\\tint len, avail;\\n\\tstruct sk_buff *skb;\\n\\tva_list args2;\\n\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tBUG_ON(!ab->skb);\\n\\tskb = ab->skb;\\n\\tavail = skb_tailroom(skb);\\n\\tif (avail == 0) {\\n\\t\\tavail = audit_expand(ab, AUDIT_BUFSIZ);\\n\\t\\tif (!avail)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\tva_copy(args2, args);\\n\\tlen = vsnprintf(skb_tail_pointer(skb), avail, fmt, args);\\n\\tif (len >= avail) {\\n\\t\\t/* The printk buffer is 1024 bytes long, so if we get\\n\\t\\t * here and AUDIT_BUFSIZ is at least 1024, then we can\\n\\t\\t * log everything that printk could have logged. */\\n\\t\\tavail = audit_expand(ab,\\n\\t\\t\\tmax_t(unsigned, AUDIT_BUFSIZ, 1+len-avail));\\n\\t\\tif (!avail)\\n\\t\\t\\tgoto out_va_end;\\n\\t\\tlen = vsnprintf(skb_tail_pointer(skb), avail, fmt, args2);\\n\\t}\\n\\tif (len > 0)\\n\\t\\tskb_put(skb, len);\\nout_va_end:\\n\\tva_end(args2);\\nout:\\n\\treturn;\\n}\\n\\n/**\\n * audit_log_format - format a message into the audit buffer.\\n * @ab: audit_buffer\\n * @fmt: format string\\n * @...: optional parameters matching @fmt string\\n *\\n * All the work is done in audit_log_vformat.\\n */\\nvoid audit_log_format(struct audit_buffer *ab, const char *fmt, ...)\\n{\\n\\tva_list args;\\n\\n\\tif (!ab)\\n\\t\\treturn;\\n\\tva_start(args, fmt);\\n\\taudit_log_vformat(ab, fmt, args);\\n\\tva_end(args);\\n}\\n\\n/**\\n * audit_log_n_hex - convert a buffer to hex and append it to the audit skb\\n * @ab: the audit_buffer\\n * @buf: buffer to convert to hex\\n * @len: length of @buf to be converted\\n *\\n * No return value; failure to expand is silently ignored.\\n *\\n * This function will take the passed buf and convert it into a string of\\n * ascii hex digits. The new string is placed onto the skb.\\n */\\nvoid audit_log_n_hex(struct audit_buffer *ab, const unsigned char *buf,\\n\\t\\tsize_t len)\\n{\\n\\tint i, avail, new_len;\\n\\tunsigned char *ptr;\\n\\tstruct sk_buff *skb;\\n\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tBUG_ON(!ab->skb);\\n\\tskb = ab->skb;\\n\\tavail = skb_tailroom(skb);\\n\\tnew_len = len<<1;\\n\\tif (new_len >= avail) {\\n\\t\\t/* Round the buffer request up to the next multiple */\\n\\t\\tnew_len = AUDIT_BUFSIZ*(((new_len-avail)/AUDIT_BUFSIZ) + 1);\\n\\t\\tavail = audit_expand(ab, new_len);\\n\\t\\tif (!avail)\\n\\t\\t\\treturn;\\n\\t}\\n\\n\\tptr = skb_tail_pointer(skb);\\n\\tfor (i = 0; i < len; i++)\\n\\t\\tptr = hex_byte_pack_upper(ptr, buf[i]);\\n\\t*ptr = 0;\\n\\tskb_put(skb, len << 1); /* new string is twice the old string */\\n}\\n\\n/*\\n * Format a string of no more than slen characters into the audit buffer,\\n * enclosed in quote marks.\\n */\\nvoid audit_log_n_string(struct audit_buffer *ab, const char *string,\\n\\t\\t\\tsize_t slen)\\n{\\n\\tint avail, new_len;\\n\\tunsigned char *ptr;\\n\\tstruct sk_buff *skb;\\n\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tBUG_ON(!ab->skb);\\n\\tskb = ab->skb;\\n\\tavail = skb_tailroom(skb);\\n\\tnew_len = slen + 3;\\t/* enclosing quotes + null terminator */\\n\\tif (new_len > avail) {\\n\\t\\tavail = audit_expand(ab, new_len);\\n\\t\\tif (!avail)\\n\\t\\t\\treturn;\\n\\t}\\n\\tptr = skb_tail_pointer(skb);\\n\\t*ptr++ = \\'\"\\';\\n\\tmemcpy(ptr, string, slen);\\n\\tptr += slen;\\n\\t*ptr++ = \\'\"\\';\\n\\t*ptr = 0;\\n\\tskb_put(skb, slen + 2);\\t/* don\\'t include null terminator */\\n}\\n\\n/**\\n * audit_string_contains_control - does a string need to be logged in hex\\n * @string: string to be checked\\n * @len: max length of the string to check\\n */\\nbool audit_string_contains_control(const char *string, size_t len)\\n{\\n\\tconst unsigned char *p;\\n\\tfor (p = string; p < (const unsigned char *)string + len; p++) {\\n\\t\\tif (*p == \\'\"\\' || *p < 0x21 || *p > 0x7e)\\n\\t\\t\\treturn true;\\n\\t}\\n\\treturn false;\\n}\\n\\n/**\\n * audit_log_n_untrustedstring - log a string that may contain random characters\\n * @ab: audit_buffer\\n * @string: string to be logged\\n * @len: length of string (not including trailing null)\\n *\\n * This code will escape a string that is passed to it if the string\\n * contains a control character, unprintable character, double quote mark,\\n * or a space. Unescaped strings will start and end with a double quote mark.\\n * Strings that are escaped are printed in hex (2 digits per char).\\n *\\n * The caller specifies the number of characters in the string to log, which may\\n * or may not be the entire string.\\n */\\nvoid audit_log_n_untrustedstring(struct audit_buffer *ab, const char *string,\\n\\t\\t\\t\\t size_t len)\\n{\\n\\tif (audit_string_contains_control(string, len))\\n\\t\\taudit_log_n_hex(ab, string, len);\\n\\telse\\n\\t\\taudit_log_n_string(ab, string, len);\\n}\\n\\n/**\\n * audit_log_untrustedstring - log a string that may contain random characters\\n * @ab: audit_buffer\\n * @string: string to be logged\\n *\\n * Same as audit_log_n_untrustedstring(), except that strlen is used to\\n * determine string length.\\n */\\nvoid audit_log_untrustedstring(struct audit_buffer *ab, const char *string)\\n{\\n\\taudit_log_n_untrustedstring(ab, string, strlen(string));\\n}\\n\\n/* This is a helper-function to print the escaped d_path */\\nvoid audit_log_d_path(struct audit_buffer *ab, const char *prefix,\\n\\t\\t      const struct path *path)\\n{\\n\\tchar *p, *pathname;\\n\\n\\tif (prefix)\\n\\t\\taudit_log_format(ab, \"%s\", prefix);\\n\\n\\t/* We will allow 11 spaces for \\' (deleted)\\' to be appended */\\n\\tpathname = kmalloc(PATH_MAX+11, ab->gfp_mask);\\n\\tif (!pathname) {\\n\\t\\taudit_log_format(ab, \"\\\\\"<no_memory>\\\\\"\");\\n\\t\\treturn;\\n\\t}\\n\\tp = d_path(path, pathname, PATH_MAX+11);\\n\\tif (IS_ERR(p)) { /* Should never happen since we send PATH_MAX */\\n\\t\\t/* FIXME: can we save some information here? */\\n\\t\\taudit_log_format(ab, \"\\\\\"<too_long>\\\\\"\");\\n\\t} else\\n\\t\\taudit_log_untrustedstring(ab, p);\\n\\tkfree(pathname);\\n}\\n\\nvoid audit_log_session_info(struct audit_buffer *ab)\\n{\\n\\tunsigned int sessionid = audit_get_sessionid(current);\\n\\tuid_t auid = from_kuid(&init_user_ns, audit_get_loginuid(current));\\n\\n\\taudit_log_format(ab, \"auid=%u ses=%u\", auid, sessionid);\\n}\\n\\nvoid audit_log_key(struct audit_buffer *ab, char *key)\\n{\\n\\taudit_log_format(ab, \" key=\");\\n\\tif (key)\\n\\t\\taudit_log_untrustedstring(ab, key);\\n\\telse\\n\\t\\taudit_log_format(ab, \"(null)\");\\n}\\n\\nint audit_log_task_context(struct audit_buffer *ab)\\n{\\n\\tstruct lsm_prop prop;\\n\\tchar *ctx = NULL;\\n\\tunsigned len;\\n\\tint error;\\n\\n\\tsecurity_current_getlsmprop_subj(&prop);\\n\\tif (!lsmprop_is_set(&prop))\\n\\t\\treturn 0;\\n\\n\\terror = security_lsmprop_to_secctx(&prop, &ctx, &len);\\n\\tif (error) {\\n\\t\\tif (error != -EINVAL)\\n\\t\\t\\tgoto error_path;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\taudit_log_format(ab, \" subj=%s\", ctx);\\n\\tsecurity_release_secctx(ctx, len);\\n\\treturn 0;\\n\\nerror_path:\\n\\taudit_panic(\"error in audit_log_task_context\");\\n\\treturn error;\\n}\\nEXPORT_SYMBOL(audit_log_task_context);\\n\\nvoid audit_log_d_path_exe(struct audit_buffer *ab,\\n\\t\\t\\t  struct mm_struct *mm)\\n{\\n\\tstruct file *exe_file;\\n\\n\\tif (!mm)\\n\\t\\tgoto out_null;\\n\\n\\texe_file = get_mm_exe_file(mm);\\n\\tif (!exe_file)\\n\\t\\tgoto out_null;\\n\\n\\taudit_log_d_path(ab, \" exe=\", &exe_file->f_path);\\n\\tfput(exe_file);\\n\\treturn;\\nout_null:\\n\\taudit_log_format(ab, \" exe=(null)\");\\n}\\n\\nstruct tty_struct *audit_get_tty(void)\\n{\\n\\tstruct tty_struct *tty = NULL;\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&current->sighand->siglock, flags);\\n\\tif (current->signal)\\n\\t\\ttty = tty_kref_get(current->signal->tty);\\n\\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\\n\\treturn tty;\\n}\\n\\nvoid audit_put_tty(struct tty_struct *tty)\\n{\\n\\ttty_kref_put(tty);\\n}\\n\\nvoid audit_log_task_info(struct audit_buffer *ab)\\n{\\n\\tconst struct cred *cred;\\n\\tchar comm[sizeof(current->comm)];\\n\\tstruct tty_struct *tty;\\n\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tcred = current_cred();\\n\\ttty = audit_get_tty();\\n\\taudit_log_format(ab,\\n\\t\\t\\t \" ppid=%d pid=%d auid=%u uid=%u gid=%u\"\\n\\t\\t\\t \" euid=%u suid=%u fsuid=%u\"\\n\\t\\t\\t \" egid=%u sgid=%u fsgid=%u tty=%s ses=%u\",\\n\\t\\t\\t task_ppid_nr(current),\\n\\t\\t\\t task_tgid_nr(current),\\n\\t\\t\\t from_kuid(&init_user_ns, audit_get_loginuid(current)),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->uid),\\n\\t\\t\\t from_kgid(&init_user_ns, cred->gid),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->euid),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->suid),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->fsuid),\\n\\t\\t\\t from_kgid(&init_user_ns, cred->egid),\\n\\t\\t\\t from_kgid(&init_user_ns, cred->sgid),\\n\\t\\t\\t from_kgid(&init_user_ns, cred->fsgid),\\n\\t\\t\\t tty ? tty_name(tty) : \"(none)\",\\n\\t\\t\\t audit_get_sessionid(current));\\n\\taudit_put_tty(tty);\\n\\taudit_log_format(ab, \" comm=\");\\n\\taudit_log_untrustedstring(ab, get_task_comm(comm, current));\\n\\taudit_log_d_path_exe(ab, current->mm);\\n\\taudit_log_task_context(ab);\\n}\\nEXPORT_SYMBOL(audit_log_task_info);\\n\\n/**\\n * audit_log_path_denied - report a path restriction denial\\n * @type: audit message type (AUDIT_ANOM_LINK, AUDIT_ANOM_CREAT, etc)\\n * @operation: specific operation name\\n */\\nvoid audit_log_path_denied(int type, const char *operation)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled || audit_dummy_context())\\n\\t\\treturn;\\n\\n\\t/* Generate log with subject, operation, outcome. */\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL, type);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\taudit_log_format(ab, \"op=%s\", operation);\\n\\taudit_log_task_info(ab);\\n\\taudit_log_format(ab, \" res=0\");\\n\\taudit_log_end(ab);\\n}\\n\\n/* global counter which is incremented every time something logs in */\\nstatic atomic_t session_id = ATOMIC_INIT(0);\\n\\nstatic int audit_set_loginuid_perm(kuid_t loginuid)\\n{\\n\\t/* if we are unset, we don\\'t need privs */\\n\\tif (!audit_loginuid_set(current))\\n\\t\\treturn 0;\\n\\t/* if AUDIT_FEATURE_LOGINUID_IMMUTABLE means never ever allow a change*/\\n\\tif (is_audit_feature_set(AUDIT_FEATURE_LOGINUID_IMMUTABLE))\\n\\t\\treturn -EPERM;\\n\\t/* it is set, you need permission */\\n\\tif (!capable(CAP_AUDIT_CONTROL))\\n\\t\\treturn -EPERM;\\n\\t/* reject if this is not an unset and we don\\'t allow that */\\n\\tif (is_audit_feature_set(AUDIT_FEATURE_ONLY_UNSET_LOGINUID)\\n\\t\\t\\t\\t && uid_valid(loginuid))\\n\\t\\treturn -EPERM;\\n\\treturn 0;\\n}\\n\\nstatic void audit_log_set_loginuid(kuid_t koldloginuid, kuid_t kloginuid,\\n\\t\\t\\t\\t   unsigned int oldsessionid,\\n\\t\\t\\t\\t   unsigned int sessionid, int rc)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tuid_t uid, oldloginuid, loginuid;\\n\\tstruct tty_struct *tty;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_LOGIN);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tuid = from_kuid(&init_user_ns, task_uid(current));\\n\\toldloginuid = from_kuid(&init_user_ns, koldloginuid);\\n\\tloginuid = from_kuid(&init_user_ns, kloginuid);\\n\\ttty = audit_get_tty();\\n\\n\\taudit_log_format(ab, \"pid=%d uid=%u\", task_tgid_nr(current), uid);\\n\\taudit_log_task_context(ab);\\n\\taudit_log_format(ab, \" old-auid=%u auid=%u tty=%s old-ses=%u ses=%u res=%d\",\\n\\t\\t\\t oldloginuid, loginuid, tty ? tty_name(tty) : \"(none)\",\\n\\t\\t\\t oldsessionid, sessionid, !rc);\\n\\taudit_put_tty(tty);\\n\\taudit_log_end(ab);\\n}\\n\\n/**\\n * audit_set_loginuid - set current task\\'s loginuid\\n * @loginuid: loginuid value\\n *\\n * Returns 0.\\n *\\n * Called (set) from fs/proc/base.c::proc_loginuid_write().\\n */\\nint audit_set_loginuid(kuid_t loginuid)\\n{\\n\\tunsigned int oldsessionid, sessionid = AUDIT_SID_UNSET;\\n\\tkuid_t oldloginuid;\\n\\tint rc;\\n\\n\\toldloginuid = audit_get_loginuid(current);\\n\\toldsessionid = audit_get_sessionid(current);\\n\\n\\trc = audit_set_loginuid_perm(loginuid);\\n\\tif (rc)\\n\\t\\tgoto out;\\n\\n\\t/* are we setting or clearing? */\\n\\tif (uid_valid(loginuid)) {\\n\\t\\tsessionid = (unsigned int)atomic_inc_return(&session_id);\\n\\t\\tif (unlikely(sessionid == AUDIT_SID_UNSET))\\n\\t\\t\\tsessionid = (unsigned int)atomic_inc_return(&session_id);\\n\\t}\\n\\n\\tcurrent->sessionid = sessionid;\\n\\tcurrent->loginuid = loginuid;\\nout:\\n\\taudit_log_set_loginuid(oldloginuid, loginuid, oldsessionid, sessionid, rc);\\n\\treturn rc;\\n}\\n\\n/**\\n * audit_signal_info - record signal info for shutting down audit subsystem\\n * @sig: signal value\\n * @t: task being signaled\\n *\\n * If the audit subsystem is being terminated, record the task (pid)\\n * and uid that is doing that.\\n */\\nint audit_signal_info(int sig, struct task_struct *t)\\n{\\n\\tkuid_t uid = current_uid(), auid;\\n\\n\\tif (auditd_test_task(t) &&\\n\\t    (sig == SIGTERM || sig == SIGHUP ||\\n\\t     sig == SIGUSR1 || sig == SIGUSR2)) {\\n\\t\\taudit_sig_pid = task_tgid_nr(current);\\n\\t\\tauid = audit_get_loginuid(current);\\n\\t\\tif (uid_valid(auid))\\n\\t\\t\\taudit_sig_uid = auid;\\n\\t\\telse\\n\\t\\t\\taudit_sig_uid = uid;\\n\\t\\tsecurity_current_getlsmprop_subj(&audit_sig_lsm);\\n\\t}\\n\\n\\treturn audit_signal_info_syscall(t);\\n}\\n\\n/**\\n * audit_log_end - end one audit record\\n * @ab: the audit_buffer\\n *\\n * We can not do a netlink send inside an irq context because it blocks (last\\n * arg, flags, is not set to MSG_DONTWAIT), so the audit buffer is placed on a\\n * queue and a kthread is scheduled to remove them from the queue outside the\\n * irq context.  May be called in any context.\\n */\\nvoid audit_log_end(struct audit_buffer *ab)\\n{\\n\\tstruct sk_buff *skb;\\n\\tstruct nlmsghdr *nlh;\\n\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tif (audit_rate_check()) {\\n\\t\\tskb = ab->skb;\\n\\t\\tab->skb = NULL;\\n\\n\\t\\t/* setup the netlink header, see the comments in\\n\\t\\t * kauditd_send_multicast_skb() for length quirks */\\n\\t\\tnlh = nlmsg_hdr(skb);\\n\\t\\tnlh->nlmsg_len = skb->len - NLMSG_HDRLEN;\\n\\n\\t\\t/* queue the netlink packet and poke the kauditd thread */\\n\\t\\tskb_queue_tail(&audit_queue, skb);\\n\\t\\twake_up_interruptible(&kauditd_wait);\\n\\t} else\\n\\t\\taudit_log_lost(\"rate limit exceeded\");\\n\\n\\taudit_buffer_free(ab);\\n}\\n\\n/**\\n * audit_log - Log an audit record\\n * @ctx: audit context\\n * @gfp_mask: type of allocation\\n * @type: audit message type\\n * @fmt: format string to use\\n * @...: variable parameters matching the format string\\n *\\n * This is a convenience function that calls audit_log_start,\\n * audit_log_vformat, and audit_log_end.  It may be called\\n * in any context.\\n */\\nvoid audit_log(struct audit_context *ctx, gfp_t gfp_mask, int type,\\n\\t       const char *fmt, ...)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tva_list args;\\n\\n\\tab = audit_log_start(ctx, gfp_mask, type);\\n\\tif (ab) {\\n\\t\\tva_start(args, fmt);\\n\\t\\taudit_log_vformat(ab, fmt, args);\\n\\t\\tva_end(args);\\n\\t\\taudit_log_end(ab);\\n\\t}\\n}\\n\\nEXPORT_SYMBOL(audit_log_start);\\nEXPORT_SYMBOL(audit_log_end);\\nEXPORT_SYMBOL(audit_log_format);\\nEXPORT_SYMBOL(audit_log);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#include \"audit.h\"\\n#include <linux/fsnotify_backend.h>\\n#include <linux/namei.h>\\n#include <linux/mount.h>\\n#include <linux/kthread.h>\\n#include <linux/refcount.h>\\n#include <linux/slab.h>\\n\\nstruct audit_tree;\\nstruct audit_chunk;\\n\\nstruct audit_tree {\\n\\trefcount_t count;\\n\\tint goner;\\n\\tstruct audit_chunk *root;\\n\\tstruct list_head chunks;\\n\\tstruct list_head rules;\\n\\tstruct list_head list;\\n\\tstruct list_head same_root;\\n\\tstruct rcu_head head;\\n\\tchar pathname[];\\n};\\n\\nstruct audit_chunk {\\n\\tstruct list_head hash;\\n\\tunsigned long key;\\n\\tstruct fsnotify_mark *mark;\\n\\tstruct list_head trees;\\t\\t/* with root here */\\n\\tint count;\\n\\tatomic_long_t refs;\\n\\tstruct rcu_head head;\\n\\tstruct audit_node {\\n\\t\\tstruct list_head list;\\n\\t\\tstruct audit_tree *owner;\\n\\t\\tunsigned index;\\t\\t/* index; upper bit indicates \\'will prune\\' */\\n\\t} owners[] __counted_by(count);\\n};\\n\\nstruct audit_tree_mark {\\n\\tstruct fsnotify_mark mark;\\n\\tstruct audit_chunk *chunk;\\n};\\n\\nstatic LIST_HEAD(tree_list);\\nstatic LIST_HEAD(prune_list);\\nstatic struct task_struct *prune_thread;\\n\\n/*\\n * One struct chunk is attached to each inode of interest through\\n * audit_tree_mark (fsnotify mark). We replace struct chunk on tagging /\\n * untagging, the mark is stable as long as there is chunk attached. The\\n * association between mark and chunk is protected by hash_lock and\\n * audit_tree_group->mark_mutex. Thus as long as we hold\\n * audit_tree_group->mark_mutex and check that the mark is alive by\\n * FSNOTIFY_MARK_FLAG_ATTACHED flag check, we are sure the mark points to\\n * the current chunk.\\n *\\n * Rules have pointer to struct audit_tree.\\n * Rules have struct list_head rlist forming a list of rules over\\n * the same tree.\\n * References to struct chunk are collected at audit_inode{,_child}()\\n * time and used in AUDIT_TREE rule matching.\\n * These references are dropped at the same time we are calling\\n * audit_free_names(), etc.\\n *\\n * Cyclic lists galore:\\n * tree.chunks anchors chunk.owners[].list\\t\\t\\thash_lock\\n * tree.rules anchors rule.rlist\\t\\t\\t\\taudit_filter_mutex\\n * chunk.trees anchors tree.same_root\\t\\t\\t\\thash_lock\\n * chunk.hash is a hash with middle bits of watch.inode as\\n * a hash function.\\t\\t\\t\\t\\t\\tRCU, hash_lock\\n *\\n * tree is refcounted; one reference for \"some rules on rules_list refer to\\n * it\", one for each chunk with pointer to it.\\n *\\n * chunk is refcounted by embedded .refs. Mark associated with the chunk holds\\n * one chunk reference. This reference is dropped either when a mark is going\\n * to be freed (corresponding inode goes away) or when chunk attached to the\\n * mark gets replaced. This reference must be dropped using\\n * audit_mark_put_chunk() to make sure the reference is dropped only after RCU\\n * grace period as it protects RCU readers of the hash table.\\n *\\n * node.index allows to get from node.list to containing chunk.\\n * MSB of that sucker is stolen to mark taggings that we might have to\\n * revert - several operations have very unpleasant cleanup logics and\\n * that makes a difference.  Some.\\n */\\n\\nstatic struct fsnotify_group *audit_tree_group __ro_after_init;\\nstatic struct kmem_cache *audit_tree_mark_cachep __ro_after_init;\\n\\nstatic struct audit_tree *alloc_tree(const char *s)\\n{\\n\\tstruct audit_tree *tree;\\n\\n\\ttree = kmalloc(struct_size(tree, pathname, strlen(s) + 1), GFP_KERNEL);\\n\\tif (tree) {\\n\\t\\trefcount_set(&tree->count, 1);\\n\\t\\ttree->goner = 0;\\n\\t\\tINIT_LIST_HEAD(&tree->chunks);\\n\\t\\tINIT_LIST_HEAD(&tree->rules);\\n\\t\\tINIT_LIST_HEAD(&tree->list);\\n\\t\\tINIT_LIST_HEAD(&tree->same_root);\\n\\t\\ttree->root = NULL;\\n\\t\\tstrcpy(tree->pathname, s);\\n\\t}\\n\\treturn tree;\\n}\\n\\nstatic inline void get_tree(struct audit_tree *tree)\\n{\\n\\trefcount_inc(&tree->count);\\n}\\n\\nstatic inline void put_tree(struct audit_tree *tree)\\n{\\n\\tif (refcount_dec_and_test(&tree->count))\\n\\t\\tkfree_rcu(tree, head);\\n}\\n\\n/* to avoid bringing the entire thing in audit.h */\\nconst char *audit_tree_path(struct audit_tree *tree)\\n{\\n\\treturn tree->pathname;\\n}\\n\\nstatic void free_chunk(struct audit_chunk *chunk)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < chunk->count; i++) {\\n\\t\\tif (chunk->owners[i].owner)\\n\\t\\t\\tput_tree(chunk->owners[i].owner);\\n\\t}\\n\\tkfree(chunk);\\n}\\n\\nvoid audit_put_chunk(struct audit_chunk *chunk)\\n{\\n\\tif (atomic_long_dec_and_test(&chunk->refs))\\n\\t\\tfree_chunk(chunk);\\n}\\n\\nstatic void __put_chunk(struct rcu_head *rcu)\\n{\\n\\tstruct audit_chunk *chunk = container_of(rcu, struct audit_chunk, head);\\n\\taudit_put_chunk(chunk);\\n}\\n\\n/*\\n * Drop reference to the chunk that was held by the mark. This is the reference\\n * that gets dropped after we\\'ve removed the chunk from the hash table and we\\n * use it to make sure chunk cannot be freed before RCU grace period expires.\\n */\\nstatic void audit_mark_put_chunk(struct audit_chunk *chunk)\\n{\\n\\tcall_rcu(&chunk->head, __put_chunk);\\n}\\n\\nstatic inline struct audit_tree_mark *audit_mark(struct fsnotify_mark *mark)\\n{\\n\\treturn container_of(mark, struct audit_tree_mark, mark);\\n}\\n\\nstatic struct audit_chunk *mark_chunk(struct fsnotify_mark *mark)\\n{\\n\\treturn audit_mark(mark)->chunk;\\n}\\n\\nstatic void audit_tree_destroy_watch(struct fsnotify_mark *mark)\\n{\\n\\tkmem_cache_free(audit_tree_mark_cachep, audit_mark(mark));\\n}\\n\\nstatic struct fsnotify_mark *alloc_mark(void)\\n{\\n\\tstruct audit_tree_mark *amark;\\n\\n\\tamark = kmem_cache_zalloc(audit_tree_mark_cachep, GFP_KERNEL);\\n\\tif (!amark)\\n\\t\\treturn NULL;\\n\\tfsnotify_init_mark(&amark->mark, audit_tree_group);\\n\\tamark->mark.mask = FS_IN_IGNORED;\\n\\treturn &amark->mark;\\n}\\n\\nstatic struct audit_chunk *alloc_chunk(int count)\\n{\\n\\tstruct audit_chunk *chunk;\\n\\tint i;\\n\\n\\tchunk = kzalloc(struct_size(chunk, owners, count), GFP_KERNEL);\\n\\tif (!chunk)\\n\\t\\treturn NULL;\\n\\n\\tINIT_LIST_HEAD(&chunk->hash);\\n\\tINIT_LIST_HEAD(&chunk->trees);\\n\\tchunk->count = count;\\n\\tatomic_long_set(&chunk->refs, 1);\\n\\tfor (i = 0; i < count; i++) {\\n\\t\\tINIT_LIST_HEAD(&chunk->owners[i].list);\\n\\t\\tchunk->owners[i].index = i;\\n\\t}\\n\\treturn chunk;\\n}\\n\\nenum {HASH_SIZE = 128};\\nstatic struct list_head chunk_hash_heads[HASH_SIZE];\\nstatic __cacheline_aligned_in_smp DEFINE_SPINLOCK(hash_lock);\\n\\n/* Function to return search key in our hash from inode. */\\nstatic unsigned long inode_to_key(const struct inode *inode)\\n{\\n\\t/* Use address pointed to by connector->obj as the key */\\n\\treturn (unsigned long)&inode->i_fsnotify_marks;\\n}\\n\\nstatic inline struct list_head *chunk_hash(unsigned long key)\\n{\\n\\tunsigned long n = key / L1_CACHE_BYTES;\\n\\treturn chunk_hash_heads + n % HASH_SIZE;\\n}\\n\\n/* hash_lock & mark->group->mark_mutex is held by caller */\\nstatic void insert_hash(struct audit_chunk *chunk)\\n{\\n\\tstruct list_head *list;\\n\\n\\t/*\\n\\t * Make sure chunk is fully initialized before making it visible in the\\n\\t * hash. Pairs with a data dependency barrier in READ_ONCE() in\\n\\t * audit_tree_lookup().\\n\\t */\\n\\tsmp_wmb();\\n\\tWARN_ON_ONCE(!chunk->key);\\n\\tlist = chunk_hash(chunk->key);\\n\\tlist_add_rcu(&chunk->hash, list);\\n}\\n\\n/* called under rcu_read_lock */\\nstruct audit_chunk *audit_tree_lookup(const struct inode *inode)\\n{\\n\\tunsigned long key = inode_to_key(inode);\\n\\tstruct list_head *list = chunk_hash(key);\\n\\tstruct audit_chunk *p;\\n\\n\\tlist_for_each_entry_rcu(p, list, hash) {\\n\\t\\t/*\\n\\t\\t * We use a data dependency barrier in READ_ONCE() to make sure\\n\\t\\t * the chunk we see is fully initialized.\\n\\t\\t */\\n\\t\\tif (READ_ONCE(p->key) == key) {\\n\\t\\t\\tatomic_long_inc(&p->refs);\\n\\t\\t\\treturn p;\\n\\t\\t}\\n\\t}\\n\\treturn NULL;\\n}\\n\\nbool audit_tree_match(struct audit_chunk *chunk, struct audit_tree *tree)\\n{\\n\\tint n;\\n\\tfor (n = 0; n < chunk->count; n++)\\n\\t\\tif (chunk->owners[n].owner == tree)\\n\\t\\t\\treturn true;\\n\\treturn false;\\n}\\n\\n/* tagging and untagging inodes with trees */\\n\\nstatic struct audit_chunk *find_chunk(struct audit_node *p)\\n{\\n\\tint index = p->index & ~(1U<<31);\\n\\tp -= index;\\n\\treturn container_of(p, struct audit_chunk, owners[0]);\\n}\\n\\nstatic void replace_mark_chunk(struct fsnotify_mark *mark,\\n\\t\\t\\t       struct audit_chunk *chunk)\\n{\\n\\tstruct audit_chunk *old;\\n\\n\\tassert_spin_locked(&hash_lock);\\n\\told = mark_chunk(mark);\\n\\taudit_mark(mark)->chunk = chunk;\\n\\tif (chunk)\\n\\t\\tchunk->mark = mark;\\n\\tif (old)\\n\\t\\told->mark = NULL;\\n}\\n\\nstatic void replace_chunk(struct audit_chunk *new, struct audit_chunk *old)\\n{\\n\\tstruct audit_tree *owner;\\n\\tint i, j;\\n\\n\\tnew->key = old->key;\\n\\tlist_splice_init(&old->trees, &new->trees);\\n\\tlist_for_each_entry(owner, &new->trees, same_root)\\n\\t\\towner->root = new;\\n\\tfor (i = j = 0; j < old->count; i++, j++) {\\n\\t\\tif (!old->owners[j].owner) {\\n\\t\\t\\ti--;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\towner = old->owners[j].owner;\\n\\t\\tnew->owners[i].owner = owner;\\n\\t\\tnew->owners[i].index = old->owners[j].index - j + i;\\n\\t\\tif (!owner) /* result of earlier fallback */\\n\\t\\t\\tcontinue;\\n\\t\\tget_tree(owner);\\n\\t\\tlist_replace_init(&old->owners[j].list, &new->owners[i].list);\\n\\t}\\n\\treplace_mark_chunk(old->mark, new);\\n\\t/*\\n\\t * Make sure chunk is fully initialized before making it visible in the\\n\\t * hash. Pairs with a data dependency barrier in READ_ONCE() in\\n\\t * audit_tree_lookup().\\n\\t */\\n\\tsmp_wmb();\\n\\tlist_replace_rcu(&old->hash, &new->hash);\\n}\\n\\nstatic void remove_chunk_node(struct audit_chunk *chunk, struct audit_node *p)\\n{\\n\\tstruct audit_tree *owner = p->owner;\\n\\n\\tif (owner->root == chunk) {\\n\\t\\tlist_del_init(&owner->same_root);\\n\\t\\towner->root = NULL;\\n\\t}\\n\\tlist_del_init(&p->list);\\n\\tp->owner = NULL;\\n\\tput_tree(owner);\\n}\\n\\nstatic int chunk_count_trees(struct audit_chunk *chunk)\\n{\\n\\tint i;\\n\\tint ret = 0;\\n\\n\\tfor (i = 0; i < chunk->count; i++)\\n\\t\\tif (chunk->owners[i].owner)\\n\\t\\t\\tret++;\\n\\treturn ret;\\n}\\n\\nstatic void untag_chunk(struct audit_chunk *chunk, struct fsnotify_mark *mark)\\n{\\n\\tstruct audit_chunk *new;\\n\\tint size;\\n\\n\\tfsnotify_group_lock(audit_tree_group);\\n\\t/*\\n\\t * mark_mutex stabilizes chunk attached to the mark so we can check\\n\\t * whether it didn\\'t change while we\\'ve dropped hash_lock.\\n\\t */\\n\\tif (!(mark->flags & FSNOTIFY_MARK_FLAG_ATTACHED) ||\\n\\t    mark_chunk(mark) != chunk)\\n\\t\\tgoto out_mutex;\\n\\n\\tsize = chunk_count_trees(chunk);\\n\\tif (!size) {\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tlist_del_init(&chunk->trees);\\n\\t\\tlist_del_rcu(&chunk->hash);\\n\\t\\treplace_mark_chunk(mark, NULL);\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tfsnotify_detach_mark(mark);\\n\\t\\tfsnotify_group_unlock(audit_tree_group);\\n\\t\\taudit_mark_put_chunk(chunk);\\n\\t\\tfsnotify_free_mark(mark);\\n\\t\\treturn;\\n\\t}\\n\\n\\tnew = alloc_chunk(size);\\n\\tif (!new)\\n\\t\\tgoto out_mutex;\\n\\n\\tspin_lock(&hash_lock);\\n\\t/*\\n\\t * This has to go last when updating chunk as once replace_chunk() is\\n\\t * called, new RCU readers can see the new chunk.\\n\\t */\\n\\treplace_chunk(new, chunk);\\n\\tspin_unlock(&hash_lock);\\n\\tfsnotify_group_unlock(audit_tree_group);\\n\\taudit_mark_put_chunk(chunk);\\n\\treturn;\\n\\nout_mutex:\\n\\tfsnotify_group_unlock(audit_tree_group);\\n}\\n\\n/* Call with group->mark_mutex held, releases it */\\nstatic int create_chunk(struct inode *inode, struct audit_tree *tree)\\n{\\n\\tstruct fsnotify_mark *mark;\\n\\tstruct audit_chunk *chunk = alloc_chunk(1);\\n\\n\\tif (!chunk) {\\n\\t\\tfsnotify_group_unlock(audit_tree_group);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tmark = alloc_mark();\\n\\tif (!mark) {\\n\\t\\tfsnotify_group_unlock(audit_tree_group);\\n\\t\\tkfree(chunk);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tif (fsnotify_add_inode_mark_locked(mark, inode, 0)) {\\n\\t\\tfsnotify_group_unlock(audit_tree_group);\\n\\t\\tfsnotify_put_mark(mark);\\n\\t\\tkfree(chunk);\\n\\t\\treturn -ENOSPC;\\n\\t}\\n\\n\\tspin_lock(&hash_lock);\\n\\tif (tree->goner) {\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tfsnotify_detach_mark(mark);\\n\\t\\tfsnotify_group_unlock(audit_tree_group);\\n\\t\\tfsnotify_free_mark(mark);\\n\\t\\tfsnotify_put_mark(mark);\\n\\t\\tkfree(chunk);\\n\\t\\treturn 0;\\n\\t}\\n\\treplace_mark_chunk(mark, chunk);\\n\\tchunk->owners[0].index = (1U << 31);\\n\\tchunk->owners[0].owner = tree;\\n\\tget_tree(tree);\\n\\tlist_add(&chunk->owners[0].list, &tree->chunks);\\n\\tif (!tree->root) {\\n\\t\\ttree->root = chunk;\\n\\t\\tlist_add(&tree->same_root, &chunk->trees);\\n\\t}\\n\\tchunk->key = inode_to_key(inode);\\n\\t/*\\n\\t * Inserting into the hash table has to go last as once we do that RCU\\n\\t * readers can see the chunk.\\n\\t */\\n\\tinsert_hash(chunk);\\n\\tspin_unlock(&hash_lock);\\n\\tfsnotify_group_unlock(audit_tree_group);\\n\\t/*\\n\\t * Drop our initial reference. When mark we point to is getting freed,\\n\\t * we get notification through ->freeing_mark callback and cleanup\\n\\t * chunk pointing to this mark.\\n\\t */\\n\\tfsnotify_put_mark(mark);\\n\\treturn 0;\\n}\\n\\n/* the first tagged inode becomes root of tree */\\nstatic int tag_chunk(struct inode *inode, struct audit_tree *tree)\\n{\\n\\tstruct fsnotify_mark *mark;\\n\\tstruct audit_chunk *chunk, *old;\\n\\tstruct audit_node *p;\\n\\tint n;\\n\\n\\tfsnotify_group_lock(audit_tree_group);\\n\\tmark = fsnotify_find_inode_mark(inode, audit_tree_group);\\n\\tif (!mark)\\n\\t\\treturn create_chunk(inode, tree);\\n\\n\\t/*\\n\\t * Found mark is guaranteed to be attached and mark_mutex protects mark\\n\\t * from getting detached and thus it makes sure there is chunk attached\\n\\t * to the mark.\\n\\t */\\n\\t/* are we already there? */\\n\\tspin_lock(&hash_lock);\\n\\told = mark_chunk(mark);\\n\\tfor (n = 0; n < old->count; n++) {\\n\\t\\tif (old->owners[n].owner == tree) {\\n\\t\\t\\tspin_unlock(&hash_lock);\\n\\t\\t\\tfsnotify_group_unlock(audit_tree_group);\\n\\t\\t\\tfsnotify_put_mark(mark);\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\t}\\n\\tspin_unlock(&hash_lock);\\n\\n\\tchunk = alloc_chunk(old->count + 1);\\n\\tif (!chunk) {\\n\\t\\tfsnotify_group_unlock(audit_tree_group);\\n\\t\\tfsnotify_put_mark(mark);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tspin_lock(&hash_lock);\\n\\tif (tree->goner) {\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tfsnotify_group_unlock(audit_tree_group);\\n\\t\\tfsnotify_put_mark(mark);\\n\\t\\tkfree(chunk);\\n\\t\\treturn 0;\\n\\t}\\n\\tp = &chunk->owners[chunk->count - 1];\\n\\tp->index = (chunk->count - 1) | (1U<<31);\\n\\tp->owner = tree;\\n\\tget_tree(tree);\\n\\tlist_add(&p->list, &tree->chunks);\\n\\tif (!tree->root) {\\n\\t\\ttree->root = chunk;\\n\\t\\tlist_add(&tree->same_root, &chunk->trees);\\n\\t}\\n\\t/*\\n\\t * This has to go last when updating chunk as once replace_chunk() is\\n\\t * called, new RCU readers can see the new chunk.\\n\\t */\\n\\treplace_chunk(chunk, old);\\n\\tspin_unlock(&hash_lock);\\n\\tfsnotify_group_unlock(audit_tree_group);\\n\\tfsnotify_put_mark(mark); /* pair to fsnotify_find_mark */\\n\\taudit_mark_put_chunk(old);\\n\\n\\treturn 0;\\n}\\n\\nstatic void audit_tree_log_remove_rule(struct audit_context *context,\\n\\t\\t\\t\\t       struct audit_krule *rule)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_CONFIG_CHANGE);\\n\\tif (unlikely(!ab))\\n\\t\\treturn;\\n\\taudit_log_format(ab, \"op=remove_rule dir=\");\\n\\taudit_log_untrustedstring(ab, rule->tree->pathname);\\n\\taudit_log_key(ab, rule->filterkey);\\n\\taudit_log_format(ab, \" list=%d res=1\", rule->listnr);\\n\\taudit_log_end(ab);\\n}\\n\\nstatic void kill_rules(struct audit_context *context, struct audit_tree *tree)\\n{\\n\\tstruct audit_krule *rule, *next;\\n\\tstruct audit_entry *entry;\\n\\n\\tlist_for_each_entry_safe(rule, next, &tree->rules, rlist) {\\n\\t\\tentry = container_of(rule, struct audit_entry, rule);\\n\\n\\t\\tlist_del_init(&rule->rlist);\\n\\t\\tif (rule->tree) {\\n\\t\\t\\t/* not a half-baked one */\\n\\t\\t\\taudit_tree_log_remove_rule(context, rule);\\n\\t\\t\\tif (entry->rule.exe)\\n\\t\\t\\t\\taudit_remove_mark(entry->rule.exe);\\n\\t\\t\\trule->tree = NULL;\\n\\t\\t\\tlist_del_rcu(&entry->list);\\n\\t\\t\\tlist_del(&entry->rule.list);\\n\\t\\t\\tcall_rcu(&entry->rcu, audit_free_rule_rcu);\\n\\t\\t}\\n\\t}\\n}\\n\\n/*\\n * Remove tree from chunks. If \\'tagged\\' is set, remove tree only from tagged\\n * chunks. The function expects tagged chunks are all at the beginning of the\\n * chunks list.\\n */\\nstatic void prune_tree_chunks(struct audit_tree *victim, bool tagged)\\n{\\n\\tspin_lock(&hash_lock);\\n\\twhile (!list_empty(&victim->chunks)) {\\n\\t\\tstruct audit_node *p;\\n\\t\\tstruct audit_chunk *chunk;\\n\\t\\tstruct fsnotify_mark *mark;\\n\\n\\t\\tp = list_first_entry(&victim->chunks, struct audit_node, list);\\n\\t\\t/* have we run out of marked? */\\n\\t\\tif (tagged && !(p->index & (1U<<31)))\\n\\t\\t\\tbreak;\\n\\t\\tchunk = find_chunk(p);\\n\\t\\tmark = chunk->mark;\\n\\t\\tremove_chunk_node(chunk, p);\\n\\t\\t/* Racing with audit_tree_freeing_mark()? */\\n\\t\\tif (!mark)\\n\\t\\t\\tcontinue;\\n\\t\\tfsnotify_get_mark(mark);\\n\\t\\tspin_unlock(&hash_lock);\\n\\n\\t\\tuntag_chunk(chunk, mark);\\n\\t\\tfsnotify_put_mark(mark);\\n\\n\\t\\tspin_lock(&hash_lock);\\n\\t}\\n\\tspin_unlock(&hash_lock);\\n}\\n\\n/*\\n * finish killing struct audit_tree\\n */\\nstatic void prune_one(struct audit_tree *victim)\\n{\\n\\tprune_tree_chunks(victim, false);\\n\\tput_tree(victim);\\n}\\n\\n/* trim the uncommitted chunks from tree */\\n\\nstatic void trim_marked(struct audit_tree *tree)\\n{\\n\\tstruct list_head *p, *q;\\n\\tspin_lock(&hash_lock);\\n\\tif (tree->goner) {\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\treturn;\\n\\t}\\n\\t/* reorder */\\n\\tfor (p = tree->chunks.next; p != &tree->chunks; p = q) {\\n\\t\\tstruct audit_node *node = list_entry(p, struct audit_node, list);\\n\\t\\tq = p->next;\\n\\t\\tif (node->index & (1U<<31)) {\\n\\t\\t\\tlist_del_init(p);\\n\\t\\t\\tlist_add(p, &tree->chunks);\\n\\t\\t}\\n\\t}\\n\\tspin_unlock(&hash_lock);\\n\\n\\tprune_tree_chunks(tree, true);\\n\\n\\tspin_lock(&hash_lock);\\n\\tif (!tree->root && !tree->goner) {\\n\\t\\ttree->goner = 1;\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\tkill_rules(audit_context(), tree);\\n\\t\\tlist_del_init(&tree->list);\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\tprune_one(tree);\\n\\t} else {\\n\\t\\tspin_unlock(&hash_lock);\\n\\t}\\n}\\n\\nstatic void audit_schedule_prune(void);\\n\\n/* called with audit_filter_mutex */\\nint audit_remove_tree_rule(struct audit_krule *rule)\\n{\\n\\tstruct audit_tree *tree;\\n\\ttree = rule->tree;\\n\\tif (tree) {\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tlist_del_init(&rule->rlist);\\n\\t\\tif (list_empty(&tree->rules) && !tree->goner) {\\n\\t\\t\\ttree->root = NULL;\\n\\t\\t\\tlist_del_init(&tree->same_root);\\n\\t\\t\\ttree->goner = 1;\\n\\t\\t\\tlist_move(&tree->list, &prune_list);\\n\\t\\t\\trule->tree = NULL;\\n\\t\\t\\tspin_unlock(&hash_lock);\\n\\t\\t\\taudit_schedule_prune();\\n\\t\\t\\treturn 1;\\n\\t\\t}\\n\\t\\trule->tree = NULL;\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\treturn 1;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int compare_root(struct vfsmount *mnt, void *arg)\\n{\\n\\treturn inode_to_key(d_backing_inode(mnt->mnt_root)) ==\\n\\t       (unsigned long)arg;\\n}\\n\\nvoid audit_trim_trees(void)\\n{\\n\\tstruct list_head cursor;\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tlist_add(&cursor, &tree_list);\\n\\twhile (cursor.next != &tree_list) {\\n\\t\\tstruct audit_tree *tree;\\n\\t\\tstruct path path;\\n\\t\\tstruct vfsmount *root_mnt;\\n\\t\\tstruct audit_node *node;\\n\\t\\tint err;\\n\\n\\t\\ttree = container_of(cursor.next, struct audit_tree, list);\\n\\t\\tget_tree(tree);\\n\\t\\tlist_move(&cursor, &tree->list);\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\terr = kern_path(tree->pathname, 0, &path);\\n\\t\\tif (err)\\n\\t\\t\\tgoto skip_it;\\n\\n\\t\\troot_mnt = collect_mounts(&path);\\n\\t\\tpath_put(&path);\\n\\t\\tif (IS_ERR(root_mnt))\\n\\t\\t\\tgoto skip_it;\\n\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tlist_for_each_entry(node, &tree->chunks, list) {\\n\\t\\t\\tstruct audit_chunk *chunk = find_chunk(node);\\n\\t\\t\\t/* this could be NULL if the watch is dying else where... */\\n\\t\\t\\tnode->index |= 1U<<31;\\n\\t\\t\\tif (iterate_mounts(compare_root,\\n\\t\\t\\t\\t\\t   (void *)(chunk->key),\\n\\t\\t\\t\\t\\t   root_mnt))\\n\\t\\t\\t\\tnode->index &= ~(1U<<31);\\n\\t\\t}\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\ttrim_marked(tree);\\n\\t\\tdrop_collected_mounts(root_mnt);\\nskip_it:\\n\\t\\tput_tree(tree);\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t}\\n\\tlist_del(&cursor);\\n\\tmutex_unlock(&audit_filter_mutex);\\n}\\n\\nint audit_make_tree(struct audit_krule *rule, char *pathname, u32 op)\\n{\\n\\n\\tif (pathname[0] != \\'/\\' ||\\n\\t    (rule->listnr != AUDIT_FILTER_EXIT &&\\n\\t     rule->listnr != AUDIT_FILTER_URING_EXIT) ||\\n\\t    op != Audit_equal ||\\n\\t    rule->inode_f || rule->watch || rule->tree)\\n\\t\\treturn -EINVAL;\\n\\trule->tree = alloc_tree(pathname);\\n\\tif (!rule->tree)\\n\\t\\treturn -ENOMEM;\\n\\treturn 0;\\n}\\n\\nvoid audit_put_tree(struct audit_tree *tree)\\n{\\n\\tput_tree(tree);\\n}\\n\\nstatic int tag_mount(struct vfsmount *mnt, void *arg)\\n{\\n\\treturn tag_chunk(d_backing_inode(mnt->mnt_root), arg);\\n}\\n\\n/*\\n * That gets run when evict_chunk() ends up needing to kill audit_tree.\\n * Runs from a separate thread.\\n */\\nstatic int prune_tree_thread(void *unused)\\n{\\n\\tfor (;;) {\\n\\t\\tif (list_empty(&prune_list)) {\\n\\t\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\t\\t\\tschedule();\\n\\t\\t}\\n\\n\\t\\taudit_ctl_lock();\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\n\\t\\twhile (!list_empty(&prune_list)) {\\n\\t\\t\\tstruct audit_tree *victim;\\n\\n\\t\\t\\tvictim = list_entry(prune_list.next,\\n\\t\\t\\t\\t\\tstruct audit_tree, list);\\n\\t\\t\\tlist_del_init(&victim->list);\\n\\n\\t\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\t\\tprune_one(victim);\\n\\n\\t\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\t}\\n\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\taudit_ctl_unlock();\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int audit_launch_prune(void)\\n{\\n\\tif (prune_thread)\\n\\t\\treturn 0;\\n\\tprune_thread = kthread_run(prune_tree_thread, NULL,\\n\\t\\t\\t\\t\"audit_prune_tree\");\\n\\tif (IS_ERR(prune_thread)) {\\n\\t\\tpr_err(\"cannot start thread audit_prune_tree\");\\n\\t\\tprune_thread = NULL;\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\treturn 0;\\n}\\n\\n/* called with audit_filter_mutex */\\nint audit_add_tree_rule(struct audit_krule *rule)\\n{\\n\\tstruct audit_tree *seed = rule->tree, *tree;\\n\\tstruct path path;\\n\\tstruct vfsmount *mnt;\\n\\tint err;\\n\\n\\trule->tree = NULL;\\n\\tlist_for_each_entry(tree, &tree_list, list) {\\n\\t\\tif (!strcmp(seed->pathname, tree->pathname)) {\\n\\t\\t\\tput_tree(seed);\\n\\t\\t\\trule->tree = tree;\\n\\t\\t\\tlist_add(&rule->rlist, &tree->rules);\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\t}\\n\\ttree = seed;\\n\\tlist_add(&tree->list, &tree_list);\\n\\tlist_add(&rule->rlist, &tree->rules);\\n\\t/* do not set rule->tree yet */\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\tif (unlikely(!prune_thread)) {\\n\\t\\terr = audit_launch_prune();\\n\\t\\tif (err)\\n\\t\\t\\tgoto Err;\\n\\t}\\n\\n\\terr = kern_path(tree->pathname, 0, &path);\\n\\tif (err)\\n\\t\\tgoto Err;\\n\\tmnt = collect_mounts(&path);\\n\\tpath_put(&path);\\n\\tif (IS_ERR(mnt)) {\\n\\t\\terr = PTR_ERR(mnt);\\n\\t\\tgoto Err;\\n\\t}\\n\\n\\tget_tree(tree);\\n\\terr = iterate_mounts(tag_mount, tree, mnt);\\n\\tdrop_collected_mounts(mnt);\\n\\n\\tif (!err) {\\n\\t\\tstruct audit_node *node;\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tlist_for_each_entry(node, &tree->chunks, list)\\n\\t\\t\\tnode->index &= ~(1U<<31);\\n\\t\\tspin_unlock(&hash_lock);\\n\\t} else {\\n\\t\\ttrim_marked(tree);\\n\\t\\tgoto Err;\\n\\t}\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tif (list_empty(&rule->rlist)) {\\n\\t\\tput_tree(tree);\\n\\t\\treturn -ENOENT;\\n\\t}\\n\\trule->tree = tree;\\n\\tput_tree(tree);\\n\\n\\treturn 0;\\nErr:\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tlist_del_init(&tree->list);\\n\\tlist_del_init(&tree->rules);\\n\\tput_tree(tree);\\n\\treturn err;\\n}\\n\\nint audit_tag_tree(char *old, char *new)\\n{\\n\\tstruct list_head cursor, barrier;\\n\\tint failed = 0;\\n\\tstruct path path1, path2;\\n\\tstruct vfsmount *tagged;\\n\\tint err;\\n\\n\\terr = kern_path(new, 0, &path2);\\n\\tif (err)\\n\\t\\treturn err;\\n\\ttagged = collect_mounts(&path2);\\n\\tpath_put(&path2);\\n\\tif (IS_ERR(tagged))\\n\\t\\treturn PTR_ERR(tagged);\\n\\n\\terr = kern_path(old, 0, &path1);\\n\\tif (err) {\\n\\t\\tdrop_collected_mounts(tagged);\\n\\t\\treturn err;\\n\\t}\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tlist_add(&barrier, &tree_list);\\n\\tlist_add(&cursor, &barrier);\\n\\n\\twhile (cursor.next != &tree_list) {\\n\\t\\tstruct audit_tree *tree;\\n\\t\\tint good_one = 0;\\n\\n\\t\\ttree = container_of(cursor.next, struct audit_tree, list);\\n\\t\\tget_tree(tree);\\n\\t\\tlist_move(&cursor, &tree->list);\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\terr = kern_path(tree->pathname, 0, &path2);\\n\\t\\tif (!err) {\\n\\t\\t\\tgood_one = path_is_under(&path1, &path2);\\n\\t\\t\\tpath_put(&path2);\\n\\t\\t}\\n\\n\\t\\tif (!good_one) {\\n\\t\\t\\tput_tree(tree);\\n\\t\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tfailed = iterate_mounts(tag_mount, tree, tagged);\\n\\t\\tif (failed) {\\n\\t\\t\\tput_tree(tree);\\n\\t\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tif (!tree->goner) {\\n\\t\\t\\tlist_move(&tree->list, &tree_list);\\n\\t\\t}\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tput_tree(tree);\\n\\t}\\n\\n\\twhile (barrier.prev != &tree_list) {\\n\\t\\tstruct audit_tree *tree;\\n\\n\\t\\ttree = container_of(barrier.prev, struct audit_tree, list);\\n\\t\\tget_tree(tree);\\n\\t\\tlist_move(&tree->list, &barrier);\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\tif (!failed) {\\n\\t\\t\\tstruct audit_node *node;\\n\\t\\t\\tspin_lock(&hash_lock);\\n\\t\\t\\tlist_for_each_entry(node, &tree->chunks, list)\\n\\t\\t\\t\\tnode->index &= ~(1U<<31);\\n\\t\\t\\tspin_unlock(&hash_lock);\\n\\t\\t} else {\\n\\t\\t\\ttrim_marked(tree);\\n\\t\\t}\\n\\n\\t\\tput_tree(tree);\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t}\\n\\tlist_del(&barrier);\\n\\tlist_del(&cursor);\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\tpath_put(&path1);\\n\\tdrop_collected_mounts(tagged);\\n\\treturn failed;\\n}\\n\\n\\nstatic void audit_schedule_prune(void)\\n{\\n\\twake_up_process(prune_thread);\\n}\\n\\n/*\\n * ... and that one is done if evict_chunk() decides to delay until the end\\n * of syscall.  Runs synchronously.\\n */\\nvoid audit_kill_trees(struct audit_context *context)\\n{\\n\\tstruct list_head *list = &context->killed_trees;\\n\\n\\taudit_ctl_lock();\\n\\tmutex_lock(&audit_filter_mutex);\\n\\n\\twhile (!list_empty(list)) {\\n\\t\\tstruct audit_tree *victim;\\n\\n\\t\\tvictim = list_entry(list->next, struct audit_tree, list);\\n\\t\\tkill_rules(context, victim);\\n\\t\\tlist_del_init(&victim->list);\\n\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\tprune_one(victim);\\n\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t}\\n\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\taudit_ctl_unlock();\\n}\\n\\n/*\\n *  Here comes the stuff asynchronous to auditctl operations\\n */\\n\\nstatic void evict_chunk(struct audit_chunk *chunk)\\n{\\n\\tstruct audit_tree *owner;\\n\\tstruct list_head *postponed = audit_killed_trees();\\n\\tint need_prune = 0;\\n\\tint n;\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tspin_lock(&hash_lock);\\n\\twhile (!list_empty(&chunk->trees)) {\\n\\t\\towner = list_entry(chunk->trees.next,\\n\\t\\t\\t\\t   struct audit_tree, same_root);\\n\\t\\towner->goner = 1;\\n\\t\\towner->root = NULL;\\n\\t\\tlist_del_init(&owner->same_root);\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tif (!postponed) {\\n\\t\\t\\tkill_rules(audit_context(), owner);\\n\\t\\t\\tlist_move(&owner->list, &prune_list);\\n\\t\\t\\tneed_prune = 1;\\n\\t\\t} else {\\n\\t\\t\\tlist_move(&owner->list, postponed);\\n\\t\\t}\\n\\t\\tspin_lock(&hash_lock);\\n\\t}\\n\\tlist_del_rcu(&chunk->hash);\\n\\tfor (n = 0; n < chunk->count; n++)\\n\\t\\tlist_del_init(&chunk->owners[n].list);\\n\\tspin_unlock(&hash_lock);\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\tif (need_prune)\\n\\t\\taudit_schedule_prune();\\n}\\n\\nstatic int audit_tree_handle_event(struct fsnotify_mark *mark, u32 mask,\\n\\t\\t\\t\\t   struct inode *inode, struct inode *dir,\\n\\t\\t\\t\\t   const struct qstr *file_name, u32 cookie)\\n{\\n\\treturn 0;\\n}\\n\\nstatic void audit_tree_freeing_mark(struct fsnotify_mark *mark,\\n\\t\\t\\t\\t    struct fsnotify_group *group)\\n{\\n\\tstruct audit_chunk *chunk;\\n\\n\\tfsnotify_group_lock(mark->group);\\n\\tspin_lock(&hash_lock);\\n\\tchunk = mark_chunk(mark);\\n\\treplace_mark_chunk(mark, NULL);\\n\\tspin_unlock(&hash_lock);\\n\\tfsnotify_group_unlock(mark->group);\\n\\tif (chunk) {\\n\\t\\tevict_chunk(chunk);\\n\\t\\taudit_mark_put_chunk(chunk);\\n\\t}\\n\\n\\t/*\\n\\t * We are guaranteed to have at least one reference to the mark from\\n\\t * either the inode or the caller of fsnotify_destroy_mark().\\n\\t */\\n\\tBUG_ON(refcount_read(&mark->refcnt) < 1);\\n}\\n\\nstatic const struct fsnotify_ops audit_tree_ops = {\\n\\t.handle_inode_event = audit_tree_handle_event,\\n\\t.freeing_mark = audit_tree_freeing_mark,\\n\\t.free_mark = audit_tree_destroy_watch,\\n};\\n\\nstatic int __init audit_tree_init(void)\\n{\\n\\tint i;\\n\\n\\taudit_tree_mark_cachep = KMEM_CACHE(audit_tree_mark, SLAB_PANIC);\\n\\n\\taudit_tree_group = fsnotify_alloc_group(&audit_tree_ops, 0);\\n\\tif (IS_ERR(audit_tree_group))\\n\\t\\taudit_panic(\"cannot initialize fsnotify group for rectree watches\");\\n\\n\\tfor (i = 0; i < HASH_SIZE; i++)\\n\\t\\tINIT_LIST_HEAD(&chunk_hash_heads[i]);\\n\\n\\treturn 0;\\n}\\n__initcall(audit_tree_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/* audit_fsnotify.c -- tracking inodes\\n *\\n * Copyright 2003-2009,2014-2015 Red Hat, Inc.\\n * Copyright 2005 Hewlett-Packard Development Company, L.P.\\n * Copyright 2005 IBM Corporation\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/audit.h>\\n#include <linux/kthread.h>\\n#include <linux/mutex.h>\\n#include <linux/fs.h>\\n#include <linux/fsnotify_backend.h>\\n#include <linux/namei.h>\\n#include <linux/netlink.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/security.h>\\n#include \"audit.h\"\\n\\n/*\\n * this mark lives on the parent directory of the inode in question.\\n * but dev, ino, and path are about the child\\n */\\nstruct audit_fsnotify_mark {\\n\\tdev_t dev;\\t\\t/* associated superblock device */\\n\\tunsigned long ino;\\t/* associated inode number */\\n\\tchar *path;\\t\\t/* insertion path */\\n\\tstruct fsnotify_mark mark; /* fsnotify mark on the inode */\\n\\tstruct audit_krule *rule;\\n};\\n\\n/* fsnotify handle. */\\nstatic struct fsnotify_group *audit_fsnotify_group;\\n\\n/* fsnotify events we care about. */\\n#define AUDIT_FS_EVENTS (FS_MOVE | FS_CREATE | FS_DELETE | FS_DELETE_SELF |\\\\\\n\\t\\t\\t FS_MOVE_SELF)\\n\\nstatic void audit_fsnotify_mark_free(struct audit_fsnotify_mark *audit_mark)\\n{\\n\\tkfree(audit_mark->path);\\n\\tkfree(audit_mark);\\n}\\n\\nstatic void audit_fsnotify_free_mark(struct fsnotify_mark *mark)\\n{\\n\\tstruct audit_fsnotify_mark *audit_mark;\\n\\n\\taudit_mark = container_of(mark, struct audit_fsnotify_mark, mark);\\n\\taudit_fsnotify_mark_free(audit_mark);\\n}\\n\\nchar *audit_mark_path(struct audit_fsnotify_mark *mark)\\n{\\n\\treturn mark->path;\\n}\\n\\nint audit_mark_compare(struct audit_fsnotify_mark *mark, unsigned long ino, dev_t dev)\\n{\\n\\tif (mark->ino == AUDIT_INO_UNSET)\\n\\t\\treturn 0;\\n\\treturn (mark->ino == ino) && (mark->dev == dev);\\n}\\n\\nstatic void audit_update_mark(struct audit_fsnotify_mark *audit_mark,\\n\\t\\t\\t     const struct inode *inode)\\n{\\n\\taudit_mark->dev = inode ? inode->i_sb->s_dev : AUDIT_DEV_UNSET;\\n\\taudit_mark->ino = inode ? inode->i_ino : AUDIT_INO_UNSET;\\n}\\n\\nstruct audit_fsnotify_mark *audit_alloc_mark(struct audit_krule *krule, char *pathname, int len)\\n{\\n\\tstruct audit_fsnotify_mark *audit_mark;\\n\\tstruct path path;\\n\\tstruct dentry *dentry;\\n\\tstruct inode *inode;\\n\\tint ret;\\n\\n\\tif (pathname[0] != \\'/\\' || pathname[len-1] == \\'/\\')\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\tdentry = kern_path_locked(pathname, &path);\\n\\tif (IS_ERR(dentry))\\n\\t\\treturn ERR_CAST(dentry); /* returning an error */\\n\\tinode = path.dentry->d_inode;\\n\\tinode_unlock(inode);\\n\\n\\taudit_mark = kzalloc(sizeof(*audit_mark), GFP_KERNEL);\\n\\tif (unlikely(!audit_mark)) {\\n\\t\\taudit_mark = ERR_PTR(-ENOMEM);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tfsnotify_init_mark(&audit_mark->mark, audit_fsnotify_group);\\n\\taudit_mark->mark.mask = AUDIT_FS_EVENTS;\\n\\taudit_mark->path = pathname;\\n\\taudit_update_mark(audit_mark, dentry->d_inode);\\n\\taudit_mark->rule = krule;\\n\\n\\tret = fsnotify_add_inode_mark(&audit_mark->mark, inode, 0);\\n\\tif (ret < 0) {\\n\\t\\taudit_mark->path = NULL;\\n\\t\\tfsnotify_put_mark(&audit_mark->mark);\\n\\t\\taudit_mark = ERR_PTR(ret);\\n\\t}\\nout:\\n\\tdput(dentry);\\n\\tpath_put(&path);\\n\\treturn audit_mark;\\n}\\n\\nstatic void audit_mark_log_rule_change(struct audit_fsnotify_mark *audit_mark, char *op)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tstruct audit_krule *rule = audit_mark->rule;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\tab = audit_log_start(audit_context(), GFP_NOFS, AUDIT_CONFIG_CHANGE);\\n\\tif (unlikely(!ab))\\n\\t\\treturn;\\n\\taudit_log_session_info(ab);\\n\\taudit_log_format(ab, \" op=%s path=\", op);\\n\\taudit_log_untrustedstring(ab, audit_mark->path);\\n\\taudit_log_key(ab, rule->filterkey);\\n\\taudit_log_format(ab, \" list=%d res=1\", rule->listnr);\\n\\taudit_log_end(ab);\\n}\\n\\nvoid audit_remove_mark(struct audit_fsnotify_mark *audit_mark)\\n{\\n\\tfsnotify_destroy_mark(&audit_mark->mark, audit_fsnotify_group);\\n\\tfsnotify_put_mark(&audit_mark->mark);\\n}\\n\\nvoid audit_remove_mark_rule(struct audit_krule *krule)\\n{\\n\\tstruct audit_fsnotify_mark *mark = krule->exe;\\n\\n\\taudit_remove_mark(mark);\\n}\\n\\nstatic void audit_autoremove_mark_rule(struct audit_fsnotify_mark *audit_mark)\\n{\\n\\tstruct audit_krule *rule = audit_mark->rule;\\n\\tstruct audit_entry *entry = container_of(rule, struct audit_entry, rule);\\n\\n\\taudit_mark_log_rule_change(audit_mark, \"autoremove_rule\");\\n\\taudit_del_rule(entry);\\n}\\n\\n/* Update mark data in audit rules based on fsnotify events. */\\nstatic int audit_mark_handle_event(struct fsnotify_mark *inode_mark, u32 mask,\\n\\t\\t\\t\\t   struct inode *inode, struct inode *dir,\\n\\t\\t\\t\\t   const struct qstr *dname, u32 cookie)\\n{\\n\\tstruct audit_fsnotify_mark *audit_mark;\\n\\n\\taudit_mark = container_of(inode_mark, struct audit_fsnotify_mark, mark);\\n\\n\\tif (WARN_ON_ONCE(inode_mark->group != audit_fsnotify_group))\\n\\t\\treturn 0;\\n\\n\\tif (mask & (FS_CREATE|FS_MOVED_TO|FS_DELETE|FS_MOVED_FROM)) {\\n\\t\\tif (audit_compare_dname_path(dname, audit_mark->path, AUDIT_NAME_FULL))\\n\\t\\t\\treturn 0;\\n\\t\\taudit_update_mark(audit_mark, inode);\\n\\t} else if (mask & (FS_DELETE_SELF|FS_UNMOUNT|FS_MOVE_SELF)) {\\n\\t\\taudit_autoremove_mark_rule(audit_mark);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic const struct fsnotify_ops audit_mark_fsnotify_ops = {\\n\\t.handle_inode_event = audit_mark_handle_event,\\n\\t.free_mark = audit_fsnotify_free_mark,\\n};\\n\\nstatic int __init audit_fsnotify_init(void)\\n{\\n\\taudit_fsnotify_group = fsnotify_alloc_group(&audit_mark_fsnotify_ops,\\n\\t\\t\\t\\t\\t\\t    FSNOTIFY_GROUP_DUPS);\\n\\tif (IS_ERR(audit_fsnotify_group)) {\\n\\t\\taudit_fsnotify_group = NULL;\\n\\t\\taudit_panic(\"cannot create audit fsnotify group\");\\n\\t}\\n\\treturn 0;\\n}\\ndevice_initcall(audit_fsnotify_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * kexec.c - kexec_load system call\\n * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/capability.h>\\n#include <linux/mm.h>\\n#include <linux/file.h>\\n#include <linux/security.h>\\n#include <linux/kexec.h>\\n#include <linux/mutex.h>\\n#include <linux/list.h>\\n#include <linux/syscalls.h>\\n#include <linux/vmalloc.h>\\n#include <linux/slab.h>\\n\\n#include \"kexec_internal.h\"\\n\\nstatic int kimage_alloc_init(struct kimage **rimage, unsigned long entry,\\n\\t\\t\\t     unsigned long nr_segments,\\n\\t\\t\\t     struct kexec_segment *segments,\\n\\t\\t\\t     unsigned long flags)\\n{\\n\\tint ret;\\n\\tstruct kimage *image;\\n\\tbool kexec_on_panic = flags & KEXEC_ON_CRASH;\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif (kexec_on_panic) {\\n\\t\\t/* Verify we have a valid entry point */\\n\\t\\tif ((entry < phys_to_boot_phys(crashk_res.start)) ||\\n\\t\\t    (entry > phys_to_boot_phys(crashk_res.end)))\\n\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t}\\n#endif\\n\\n\\t/* Allocate and initialize a controlling structure */\\n\\timage = do_kimage_alloc_init();\\n\\tif (!image)\\n\\t\\treturn -ENOMEM;\\n\\n\\timage->start = entry;\\n\\timage->nr_segments = nr_segments;\\n\\tmemcpy(image->segment, segments, nr_segments * sizeof(*segments));\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif (kexec_on_panic) {\\n\\t\\t/* Enable special crash kernel control page alloc policy. */\\n\\t\\timage->control_page = crashk_res.start;\\n\\t\\timage->type = KEXEC_TYPE_CRASH;\\n\\t}\\n#endif\\n\\n\\tret = sanity_check_segment_list(image);\\n\\tif (ret)\\n\\t\\tgoto out_free_image;\\n\\n\\t/*\\n\\t * Find a location for the control code buffer, and add it\\n\\t * the vector of segments so that it\\'s pages will also be\\n\\t * counted as destination pages.\\n\\t */\\n\\tret = -ENOMEM;\\n\\timage->control_code_page = kimage_alloc_control_pages(image,\\n\\t\\t\\t\\t\\t   get_order(KEXEC_CONTROL_PAGE_SIZE));\\n\\tif (!image->control_code_page) {\\n\\t\\tpr_err(\"Could not allocate control_code_buffer\\\\n\");\\n\\t\\tgoto out_free_image;\\n\\t}\\n\\n\\tif (!kexec_on_panic) {\\n\\t\\timage->swap_page = kimage_alloc_control_pages(image, 0);\\n\\t\\tif (!image->swap_page) {\\n\\t\\t\\tpr_err(\"Could not allocate swap buffer\\\\n\");\\n\\t\\t\\tgoto out_free_control_pages;\\n\\t\\t}\\n\\t}\\n\\n\\t*rimage = image;\\n\\treturn 0;\\nout_free_control_pages:\\n\\tkimage_free_page_list(&image->control_pages);\\nout_free_image:\\n\\tkfree(image);\\n\\treturn ret;\\n}\\n\\nstatic int do_kexec_load(unsigned long entry, unsigned long nr_segments,\\n\\t\\tstruct kexec_segment *segments, unsigned long flags)\\n{\\n\\tstruct kimage **dest_image, *image;\\n\\tunsigned long i;\\n\\tint ret;\\n\\n\\t/*\\n\\t * Because we write directly to the reserved memory region when loading\\n\\t * crash kernels we need a serialization here to prevent multiple crash\\n\\t * kernels from attempting to load simultaneously.\\n\\t */\\n\\tif (!kexec_trylock())\\n\\t\\treturn -EBUSY;\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif (flags & KEXEC_ON_CRASH) {\\n\\t\\tdest_image = &kexec_crash_image;\\n\\t\\tif (kexec_crash_image)\\n\\t\\t\\tarch_kexec_unprotect_crashkres();\\n\\t} else\\n#endif\\n\\t\\tdest_image = &kexec_image;\\n\\n\\tif (nr_segments == 0) {\\n\\t\\t/* Uninstall image */\\n\\t\\tkimage_free(xchg(dest_image, NULL));\\n\\t\\tret = 0;\\n\\t\\tgoto out_unlock;\\n\\t}\\n\\tif (flags & KEXEC_ON_CRASH) {\\n\\t\\t/*\\n\\t\\t * Loading another kernel to switch to if this one\\n\\t\\t * crashes.  Free any current crash dump kernel before\\n\\t\\t * we corrupt it.\\n\\t\\t */\\n\\t\\tkimage_free(xchg(&kexec_crash_image, NULL));\\n\\t}\\n\\n\\tret = kimage_alloc_init(&image, entry, nr_segments, segments, flags);\\n\\tif (ret)\\n\\t\\tgoto out_unlock;\\n\\n\\tif (flags & KEXEC_PRESERVE_CONTEXT)\\n\\t\\timage->preserve_context = 1;\\n\\n#ifdef CONFIG_CRASH_HOTPLUG\\n\\tif ((flags & KEXEC_ON_CRASH) && arch_crash_hotplug_support(image, flags))\\n\\t\\timage->hotplug_support = 1;\\n#endif\\n\\n\\tret = machine_kexec_prepare(image);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Some architecture(like S390) may touch the crash memory before\\n\\t * machine_kexec_prepare(), we must copy vmcoreinfo data after it.\\n\\t */\\n\\tret = kimage_crash_copy_vmcoreinfo(image);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tret = kimage_load_segment(image, &image->segment[i]);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\tkimage_terminate(image);\\n\\n\\tret = machine_kexec_post_load(image);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\t/* Install the new kernel and uninstall the old */\\n\\timage = xchg(dest_image, image);\\n\\nout:\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif ((flags & KEXEC_ON_CRASH) && kexec_crash_image)\\n\\t\\tarch_kexec_protect_crashkres();\\n#endif\\n\\n\\tkimage_free(image);\\nout_unlock:\\n\\tkexec_unlock();\\n\\treturn ret;\\n}\\n\\n/*\\n * Exec Kernel system call: for obvious reasons only root may call it.\\n *\\n * This call breaks up into three pieces.\\n * - A generic part which loads the new kernel from the current\\n *   address space, and very carefully places the data in the\\n *   allocated pages.\\n *\\n * - A generic part that interacts with the kernel and tells all of\\n *   the devices to shut down.  Preventing on-going dmas, and placing\\n *   the devices in a consistent state so a later kernel can\\n *   reinitialize them.\\n *\\n * - A machine specific part that includes the syscall number\\n *   and then copies the image to it\\'s final destination.  And\\n *   jumps into the image at entry.\\n *\\n * kexec does not sync, or unmount filesystems so if you need\\n * that to happen you need to do that yourself.\\n */\\n\\nstatic inline int kexec_load_check(unsigned long nr_segments,\\n\\t\\t\\t\\t   unsigned long flags)\\n{\\n\\tint image_type = (flags & KEXEC_ON_CRASH) ?\\n\\t\\t\\t KEXEC_TYPE_CRASH : KEXEC_TYPE_DEFAULT;\\n\\tint result;\\n\\n\\t/* We only trust the superuser with rebooting the system. */\\n\\tif (!kexec_load_permitted(image_type))\\n\\t\\treturn -EPERM;\\n\\n\\t/* Permit LSMs and IMA to fail the kexec */\\n\\tresult = security_kernel_load_data(LOADING_KEXEC_IMAGE, false);\\n\\tif (result < 0)\\n\\t\\treturn result;\\n\\n\\t/*\\n\\t * kexec can be used to circumvent module loading restrictions, so\\n\\t * prevent loading in that case\\n\\t */\\n\\tresult = security_locked_down(LOCKDOWN_KEXEC);\\n\\tif (result)\\n\\t\\treturn result;\\n\\n\\t/*\\n\\t * Verify we have a legal set of flags\\n\\t * This leaves us room for future extensions.\\n\\t */\\n\\tif ((flags & KEXEC_FLAGS) != (flags & ~KEXEC_ARCH_MASK))\\n\\t\\treturn -EINVAL;\\n\\n\\t/* Put an artificial cap on the number\\n\\t * of segments passed to kexec_load.\\n\\t */\\n\\tif (nr_segments > KEXEC_SEGMENT_MAX)\\n\\t\\treturn -EINVAL;\\n\\n\\treturn 0;\\n}\\n\\nSYSCALL_DEFINE4(kexec_load, unsigned long, entry, unsigned long, nr_segments,\\n\\t\\tstruct kexec_segment __user *, segments, unsigned long, flags)\\n{\\n\\tstruct kexec_segment *ksegments;\\n\\tunsigned long result;\\n\\n\\tresult = kexec_load_check(nr_segments, flags);\\n\\tif (result)\\n\\t\\treturn result;\\n\\n\\t/* Verify we are on the appropriate architecture */\\n\\tif (((flags & KEXEC_ARCH_MASK) != KEXEC_ARCH) &&\\n\\t\\t((flags & KEXEC_ARCH_MASK) != KEXEC_ARCH_DEFAULT))\\n\\t\\treturn -EINVAL;\\n\\n\\tksegments = memdup_array_user(segments, nr_segments, sizeof(ksegments[0]));\\n\\tif (IS_ERR(ksegments))\\n\\t\\treturn PTR_ERR(ksegments);\\n\\n\\tresult = do_kexec_load(entry, nr_segments, ksegments, flags);\\n\\tkfree(ksegments);\\n\\n\\treturn result;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE4(kexec_load, compat_ulong_t, entry,\\n\\t\\t       compat_ulong_t, nr_segments,\\n\\t\\t       struct compat_kexec_segment __user *, segments,\\n\\t\\t       compat_ulong_t, flags)\\n{\\n\\tstruct compat_kexec_segment in;\\n\\tstruct kexec_segment *ksegments;\\n\\tunsigned long i, result;\\n\\n\\tresult = kexec_load_check(nr_segments, flags);\\n\\tif (result)\\n\\t\\treturn result;\\n\\n\\t/* Don\\'t allow clients that don\\'t understand the native\\n\\t * architecture to do anything.\\n\\t */\\n\\tif ((flags & KEXEC_ARCH_MASK) == KEXEC_ARCH_DEFAULT)\\n\\t\\treturn -EINVAL;\\n\\n\\tksegments = kmalloc_array(nr_segments, sizeof(ksegments[0]),\\n\\t\\t\\tGFP_KERNEL);\\n\\tif (!ksegments)\\n\\t\\treturn -ENOMEM;\\n\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tresult = copy_from_user(&in, &segments[i], sizeof(in));\\n\\t\\tif (result)\\n\\t\\t\\tgoto fail;\\n\\n\\t\\tksegments[i].buf   = compat_ptr(in.buf);\\n\\t\\tksegments[i].bufsz = in.bufsz;\\n\\t\\tksegments[i].mem   = in.mem;\\n\\t\\tksegments[i].memsz = in.memsz;\\n\\t}\\n\\n\\tresult = do_kexec_load(entry, nr_segments, ksegments, flags);\\n\\nfail:\\n\\tkfree(ksegments);\\n\\treturn result;\\n}\\n#endif\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * kernel/freezer.c - Function to freeze a process\\n *\\n * Originally from kernel/power/process.c\\n */\\n\\n#include <linux/interrupt.h>\\n#include <linux/suspend.h>\\n#include <linux/export.h>\\n#include <linux/syscalls.h>\\n#include <linux/freezer.h>\\n#include <linux/kthread.h>\\n\\n/* total number of freezing conditions in effect */\\nDEFINE_STATIC_KEY_FALSE(freezer_active);\\nEXPORT_SYMBOL(freezer_active);\\n\\n/*\\n * indicate whether PM freezing is in effect, protected by\\n * system_transition_mutex\\n */\\nbool pm_freezing;\\nbool pm_nosig_freezing;\\n\\n/* protects freezing and frozen transitions */\\nstatic DEFINE_SPINLOCK(freezer_lock);\\n\\n/**\\n * freezing_slow_path - slow path for testing whether a task needs to be frozen\\n * @p: task to be tested\\n *\\n * This function is called by freezing() if freezer_active isn\\'t zero\\n * and tests whether @p needs to enter and stay in frozen state.  Can be\\n * called under any context.  The freezers are responsible for ensuring the\\n * target tasks see the updated state.\\n */\\nbool freezing_slow_path(struct task_struct *p)\\n{\\n\\tif (p->flags & (PF_NOFREEZE | PF_SUSPEND_TASK))\\n\\t\\treturn false;\\n\\n\\tif (test_tsk_thread_flag(p, TIF_MEMDIE))\\n\\t\\treturn false;\\n\\n\\tif (pm_nosig_freezing || cgroup_freezing(p))\\n\\t\\treturn true;\\n\\n\\tif (pm_freezing && !(p->flags & PF_KTHREAD))\\n\\t\\treturn true;\\n\\n\\treturn false;\\n}\\nEXPORT_SYMBOL(freezing_slow_path);\\n\\nbool frozen(struct task_struct *p)\\n{\\n\\treturn READ_ONCE(p->__state) & TASK_FROZEN;\\n}\\n\\n/* Refrigerator is place where frozen processes are stored :-). */\\nbool __refrigerator(bool check_kthr_stop)\\n{\\n\\tunsigned int state = get_current_state();\\n\\tbool was_frozen = false;\\n\\n\\tpr_debug(\"%s entered refrigerator\\\\n\", current->comm);\\n\\n\\tWARN_ON_ONCE(state && !(state & TASK_NORMAL));\\n\\n\\tfor (;;) {\\n\\t\\tbool freeze;\\n\\n\\t\\traw_spin_lock_irq(&current->pi_lock);\\n\\t\\tWRITE_ONCE(current->__state, TASK_FROZEN);\\n\\t\\t/* unstale saved_state so that __thaw_task() will wake us up */\\n\\t\\tcurrent->saved_state = TASK_RUNNING;\\n\\t\\traw_spin_unlock_irq(&current->pi_lock);\\n\\n\\t\\tspin_lock_irq(&freezer_lock);\\n\\t\\tfreeze = freezing(current) && !(check_kthr_stop && kthread_should_stop());\\n\\t\\tspin_unlock_irq(&freezer_lock);\\n\\n\\t\\tif (!freeze)\\n\\t\\t\\tbreak;\\n\\n\\t\\twas_frozen = true;\\n\\t\\tschedule();\\n\\t}\\n\\t__set_current_state(TASK_RUNNING);\\n\\n\\tpr_debug(\"%s left refrigerator\\\\n\", current->comm);\\n\\n\\treturn was_frozen;\\n}\\nEXPORT_SYMBOL(__refrigerator);\\n\\nstatic void fake_signal_wake_up(struct task_struct *p)\\n{\\n\\tunsigned long flags;\\n\\n\\tif (lock_task_sighand(p, &flags)) {\\n\\t\\tsignal_wake_up(p, 0);\\n\\t\\tunlock_task_sighand(p, &flags);\\n\\t}\\n}\\n\\nstatic int __set_task_frozen(struct task_struct *p, void *arg)\\n{\\n\\tunsigned int state = READ_ONCE(p->__state);\\n\\n\\t/*\\n\\t * Allow freezing the sched_delayed tasks; they will not execute until\\n\\t * ttwu() fixes them up, so it is safe to swap their state now, instead\\n\\t * of waiting for them to get fully dequeued.\\n\\t */\\n\\tif (task_is_runnable(p))\\n\\t\\treturn 0;\\n\\n\\tif (p != current && task_curr(p))\\n\\t\\treturn 0;\\n\\n\\tif (!(state & (TASK_FREEZABLE | __TASK_STOPPED | __TASK_TRACED)))\\n\\t\\treturn 0;\\n\\n\\t/*\\n\\t * Only TASK_NORMAL can be augmented with TASK_FREEZABLE, since they\\n\\t * can suffer spurious wakeups.\\n\\t */\\n\\tif (state & TASK_FREEZABLE)\\n\\t\\tWARN_ON_ONCE(!(state & TASK_NORMAL));\\n\\n#ifdef CONFIG_LOCKDEP\\n\\t/*\\n\\t * It\\'s dangerous to freeze with locks held; there be dragons there.\\n\\t */\\n\\tif (!(state & __TASK_FREEZABLE_UNSAFE))\\n\\t\\tWARN_ON_ONCE(debug_locks && p->lockdep_depth);\\n#endif\\n\\n\\tp->saved_state = p->__state;\\n\\tWRITE_ONCE(p->__state, TASK_FROZEN);\\n\\treturn TASK_FROZEN;\\n}\\n\\nstatic bool __freeze_task(struct task_struct *p)\\n{\\n\\t/* TASK_FREEZABLE|TASK_STOPPED|TASK_TRACED -> TASK_FROZEN */\\n\\treturn task_call_func(p, __set_task_frozen, NULL);\\n}\\n\\n/**\\n * freeze_task - send a freeze request to given task\\n * @p: task to send the request to\\n *\\n * If @p is freezing, the freeze request is sent either by sending a fake\\n * signal (if it\\'s not a kernel thread) or waking it up (if it\\'s a kernel\\n * thread).\\n *\\n * RETURNS:\\n * %false, if @p is not freezing or already frozen; %true, otherwise\\n */\\nbool freeze_task(struct task_struct *p)\\n{\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&freezer_lock, flags);\\n\\tif (!freezing(p) || frozen(p) || __freeze_task(p)) {\\n\\t\\tspin_unlock_irqrestore(&freezer_lock, flags);\\n\\t\\treturn false;\\n\\t}\\n\\n\\tif (!(p->flags & PF_KTHREAD))\\n\\t\\tfake_signal_wake_up(p);\\n\\telse\\n\\t\\twake_up_state(p, TASK_NORMAL);\\n\\n\\tspin_unlock_irqrestore(&freezer_lock, flags);\\n\\treturn true;\\n}\\n\\n/*\\n * Restore the saved_state before the task entered freezer. For typical task\\n * in the __refrigerator(), saved_state == TASK_RUNNING so nothing happens\\n * here. For tasks which were TASK_NORMAL | TASK_FREEZABLE, their initial state\\n * is restored unless they got an expected wakeup (see ttwu_state_match()).\\n * Returns 1 if the task state was restored.\\n */\\nstatic int __restore_freezer_state(struct task_struct *p, void *arg)\\n{\\n\\tunsigned int state = p->saved_state;\\n\\n\\tif (state != TASK_RUNNING) {\\n\\t\\tWRITE_ONCE(p->__state, state);\\n\\t\\tp->saved_state = TASK_RUNNING;\\n\\t\\treturn 1;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nvoid __thaw_task(struct task_struct *p)\\n{\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&freezer_lock, flags);\\n\\tif (WARN_ON_ONCE(freezing(p)))\\n\\t\\tgoto unlock;\\n\\n\\tif (!frozen(p) || task_call_func(p, __restore_freezer_state, NULL))\\n\\t\\tgoto unlock;\\n\\n\\twake_up_state(p, TASK_FROZEN);\\nunlock:\\n\\tspin_unlock_irqrestore(&freezer_lock, flags);\\n}\\n\\n/**\\n * set_freezable - make %current freezable\\n *\\n * Mark %current freezable and enter refrigerator if necessary.\\n */\\nbool set_freezable(void)\\n{\\n\\tmight_sleep();\\n\\n\\t/*\\n\\t * Modify flags while holding freezer_lock.  This ensures the\\n\\t * freezer notices that we aren\\'t frozen yet or the freezing\\n\\t * condition is visible to try_to_freeze() below.\\n\\t */\\n\\tspin_lock_irq(&freezer_lock);\\n\\tcurrent->flags &= ~PF_NOFREEZE;\\n\\tspin_unlock_irq(&freezer_lock);\\n\\n\\treturn try_to_freeze();\\n}\\nEXPORT_SYMBOL(set_freezable);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  linux/kernel/fork.c\\n *\\n *  Copyright (C) 1991, 1992  Linus Torvalds\\n */\\n\\n/*\\n *  \\'fork.c\\' contains the help-routines for the \\'fork\\' system call\\n * (see also entry.S and others).\\n * Fork is rather simple, once you get the hang of it, but the memory\\n * management can be a bitch. See \\'mm/memory.c\\': \\'copy_page_range()\\'\\n */\\n\\n#include <linux/anon_inodes.h>\\n#include <linux/slab.h>\\n#include <linux/sched/autogroup.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/user.h>\\n#include <linux/sched/numa_balancing.h>\\n#include <linux/sched/stat.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/task_stack.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/sched/ext.h>\\n#include <linux/seq_file.h>\\n#include <linux/rtmutex.h>\\n#include <linux/init.h>\\n#include <linux/unistd.h>\\n#include <linux/module.h>\\n#include <linux/vmalloc.h>\\n#include <linux/completion.h>\\n#include <linux/personality.h>\\n#include <linux/mempolicy.h>\\n#include <linux/sem.h>\\n#include <linux/file.h>\\n#include <linux/fdtable.h>\\n#include <linux/iocontext.h>\\n#include <linux/key.h>\\n#include <linux/kmsan.h>\\n#include <linux/binfmts.h>\\n#include <linux/mman.h>\\n#include <linux/mmu_notifier.h>\\n#include <linux/fs.h>\\n#include <linux/mm.h>\\n#include <linux/mm_inline.h>\\n#include <linux/memblock.h>\\n#include <linux/nsproxy.h>\\n#include <linux/capability.h>\\n#include <linux/cpu.h>\\n#include <linux/cgroup.h>\\n#include <linux/security.h>\\n#include <linux/hugetlb.h>\\n#include <linux/seccomp.h>\\n#include <linux/swap.h>\\n#include <linux/syscalls.h>\\n#include <linux/syscall_user_dispatch.h>\\n#include <linux/jiffies.h>\\n#include <linux/futex.h>\\n#include <linux/compat.h>\\n#include <linux/kthread.h>\\n#include <linux/task_io_accounting_ops.h>\\n#include <linux/rcupdate.h>\\n#include <linux/ptrace.h>\\n#include <linux/mount.h>\\n#include <linux/audit.h>\\n#include <linux/memcontrol.h>\\n#include <linux/ftrace.h>\\n#include <linux/proc_fs.h>\\n#include <linux/profile.h>\\n#include <linux/rmap.h>\\n#include <linux/ksm.h>\\n#include <linux/acct.h>\\n#include <linux/userfaultfd_k.h>\\n#include <linux/tsacct_kern.h>\\n#include <linux/cn_proc.h>\\n#include <linux/freezer.h>\\n#include <linux/delayacct.h>\\n#include <linux/taskstats_kern.h>\\n#include <linux/tty.h>\\n#include <linux/fs_struct.h>\\n#include <linux/magic.h>\\n#include <linux/perf_event.h>\\n#include <linux/posix-timers.h>\\n#include <linux/user-return-notifier.h>\\n#include <linux/oom.h>\\n#include <linux/khugepaged.h>\\n#include <linux/signalfd.h>\\n#include <linux/uprobes.h>\\n#include <linux/aio.h>\\n#include <linux/compiler.h>\\n#include <linux/sysctl.h>\\n#include <linux/kcov.h>\\n#include <linux/livepatch.h>\\n#include <linux/thread_info.h>\\n#include <linux/stackleak.h>\\n#include <linux/kasan.h>\\n#include <linux/scs.h>\\n#include <linux/io_uring.h>\\n#include <linux/bpf.h>\\n#include <linux/stackprotector.h>\\n#include <linux/user_events.h>\\n#include <linux/iommu.h>\\n#include <linux/rseq.h>\\n#include <uapi/linux/pidfd.h>\\n#include <linux/pidfs.h>\\n#include <linux/tick.h>\\n\\n#include <asm/pgalloc.h>\\n#include <linux/uaccess.h>\\n#include <asm/mmu_context.h>\\n#include <asm/cacheflush.h>\\n#include <asm/tlbflush.h>\\n\\n#include <trace/events/sched.h>\\n\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/task.h>\\n\\n#include <kunit/visibility.h>\\n\\n/*\\n * Minimum number of threads to boot the kernel\\n */\\n#define MIN_THREADS 20\\n\\n/*\\n * Maximum number of threads\\n */\\n#define MAX_THREADS FUTEX_TID_MASK\\n\\n/*\\n * Protected counters by write_lock_irq(&tasklist_lock)\\n */\\nunsigned long total_forks;\\t/* Handle normal Linux uptimes. */\\nint nr_threads;\\t\\t\\t/* The idle threads do not count.. */\\n\\nstatic int max_threads;\\t\\t/* tunable limit on nr_threads */\\n\\n#define NAMED_ARRAY_INDEX(x)\\t[x] = __stringify(x)\\n\\nstatic const char * const resident_page_types[] = {\\n\\tNAMED_ARRAY_INDEX(MM_FILEPAGES),\\n\\tNAMED_ARRAY_INDEX(MM_ANONPAGES),\\n\\tNAMED_ARRAY_INDEX(MM_SWAPENTS),\\n\\tNAMED_ARRAY_INDEX(MM_SHMEMPAGES),\\n};\\n\\nDEFINE_PER_CPU(unsigned long, process_counts) = 0;\\n\\n__cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */\\n\\n#ifdef CONFIG_PROVE_RCU\\nint lockdep_tasklist_lock_is_held(void)\\n{\\n\\treturn lockdep_is_held(&tasklist_lock);\\n}\\nEXPORT_SYMBOL_GPL(lockdep_tasklist_lock_is_held);\\n#endif /* #ifdef CONFIG_PROVE_RCU */\\n\\nint nr_processes(void)\\n{\\n\\tint cpu;\\n\\tint total = 0;\\n\\n\\tfor_each_possible_cpu(cpu)\\n\\t\\ttotal += per_cpu(process_counts, cpu);\\n\\n\\treturn total;\\n}\\n\\nvoid __weak arch_release_task_struct(struct task_struct *tsk)\\n{\\n}\\n\\nstatic struct kmem_cache *task_struct_cachep;\\n\\nstatic inline struct task_struct *alloc_task_struct_node(int node)\\n{\\n\\treturn kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node);\\n}\\n\\nstatic inline void free_task_struct(struct task_struct *tsk)\\n{\\n\\tkmem_cache_free(task_struct_cachep, tsk);\\n}\\n\\n/*\\n * Allocate pages if THREAD_SIZE is >= PAGE_SIZE, otherwise use a\\n * kmemcache based allocator.\\n */\\n# if THREAD_SIZE >= PAGE_SIZE || defined(CONFIG_VMAP_STACK)\\n\\n#  ifdef CONFIG_VMAP_STACK\\n/*\\n * vmalloc() is a bit slow, and calling vfree() enough times will force a TLB\\n * flush.  Try to minimize the number of calls by caching stacks.\\n */\\n#define NR_CACHED_STACKS 2\\nstatic DEFINE_PER_CPU(struct vm_struct *, cached_stacks[NR_CACHED_STACKS]);\\n\\nstruct vm_stack {\\n\\tstruct rcu_head rcu;\\n\\tstruct vm_struct *stack_vm_area;\\n};\\n\\nstatic bool try_release_thread_stack_to_cache(struct vm_struct *vm)\\n{\\n\\tunsigned int i;\\n\\n\\tfor (i = 0; i < NR_CACHED_STACKS; i++) {\\n\\t\\tstruct vm_struct *tmp = NULL;\\n\\n\\t\\tif (this_cpu_try_cmpxchg(cached_stacks[i], &tmp, vm))\\n\\t\\t\\treturn true;\\n\\t}\\n\\treturn false;\\n}\\n\\nstatic void thread_stack_free_rcu(struct rcu_head *rh)\\n{\\n\\tstruct vm_stack *vm_stack = container_of(rh, struct vm_stack, rcu);\\n\\n\\tif (try_release_thread_stack_to_cache(vm_stack->stack_vm_area))\\n\\t\\treturn;\\n\\n\\tvfree(vm_stack);\\n}\\n\\nstatic void thread_stack_delayed_free(struct task_struct *tsk)\\n{\\n\\tstruct vm_stack *vm_stack = tsk->stack;\\n\\n\\tvm_stack->stack_vm_area = tsk->stack_vm_area;\\n\\tcall_rcu(&vm_stack->rcu, thread_stack_free_rcu);\\n}\\n\\nstatic int free_vm_stack_cache(unsigned int cpu)\\n{\\n\\tstruct vm_struct **cached_vm_stacks = per_cpu_ptr(cached_stacks, cpu);\\n\\tint i;\\n\\n\\tfor (i = 0; i < NR_CACHED_STACKS; i++) {\\n\\t\\tstruct vm_struct *vm_stack = cached_vm_stacks[i];\\n\\n\\t\\tif (!vm_stack)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tvfree(vm_stack->addr);\\n\\t\\tcached_vm_stacks[i] = NULL;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic int memcg_charge_kernel_stack(struct vm_struct *vm)\\n{\\n\\tint i;\\n\\tint ret;\\n\\tint nr_charged = 0;\\n\\n\\tBUG_ON(vm->nr_pages != THREAD_SIZE / PAGE_SIZE);\\n\\n\\tfor (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {\\n\\t\\tret = memcg_kmem_charge_page(vm->pages[i], GFP_KERNEL, 0);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto err;\\n\\t\\tnr_charged++;\\n\\t}\\n\\treturn 0;\\nerr:\\n\\tfor (i = 0; i < nr_charged; i++)\\n\\t\\tmemcg_kmem_uncharge_page(vm->pages[i], 0);\\n\\treturn ret;\\n}\\n\\nstatic int alloc_thread_stack_node(struct task_struct *tsk, int node)\\n{\\n\\tstruct vm_struct *vm;\\n\\tvoid *stack;\\n\\tint i;\\n\\n\\tfor (i = 0; i < NR_CACHED_STACKS; i++) {\\n\\t\\tstruct vm_struct *s;\\n\\n\\t\\ts = this_cpu_xchg(cached_stacks[i], NULL);\\n\\n\\t\\tif (!s)\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* Reset stack metadata. */\\n\\t\\tkasan_unpoison_range(s->addr, THREAD_SIZE);\\n\\n\\t\\tstack = kasan_reset_tag(s->addr);\\n\\n\\t\\t/* Clear stale pointers from reused stack. */\\n\\t\\tmemset(stack, 0, THREAD_SIZE);\\n\\n\\t\\tif (memcg_charge_kernel_stack(s)) {\\n\\t\\t\\tvfree(s->addr);\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\t}\\n\\n\\t\\ttsk->stack_vm_area = s;\\n\\t\\ttsk->stack = stack;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\t/*\\n\\t * Allocated stacks are cached and later reused by new threads,\\n\\t * so memcg accounting is performed manually on assigning/releasing\\n\\t * stacks to tasks. Drop __GFP_ACCOUNT.\\n\\t */\\n\\tstack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN,\\n\\t\\t\\t\\t     VMALLOC_START, VMALLOC_END,\\n\\t\\t\\t\\t     THREADINFO_GFP & ~__GFP_ACCOUNT,\\n\\t\\t\\t\\t     PAGE_KERNEL,\\n\\t\\t\\t\\t     0, node, __builtin_return_address(0));\\n\\tif (!stack)\\n\\t\\treturn -ENOMEM;\\n\\n\\tvm = find_vm_area(stack);\\n\\tif (memcg_charge_kernel_stack(vm)) {\\n\\t\\tvfree(stack);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\t/*\\n\\t * We can\\'t call find_vm_area() in interrupt context, and\\n\\t * free_thread_stack() can be called in interrupt context,\\n\\t * so cache the vm_struct.\\n\\t */\\n\\ttsk->stack_vm_area = vm;\\n\\tstack = kasan_reset_tag(stack);\\n\\ttsk->stack = stack;\\n\\treturn 0;\\n}\\n\\nstatic void free_thread_stack(struct task_struct *tsk)\\n{\\n\\tif (!try_release_thread_stack_to_cache(tsk->stack_vm_area))\\n\\t\\tthread_stack_delayed_free(tsk);\\n\\n\\ttsk->stack = NULL;\\n\\ttsk->stack_vm_area = NULL;\\n}\\n\\n#  else /* !CONFIG_VMAP_STACK */\\n\\nstatic void thread_stack_free_rcu(struct rcu_head *rh)\\n{\\n\\t__free_pages(virt_to_page(rh), THREAD_SIZE_ORDER);\\n}\\n\\nstatic void thread_stack_delayed_free(struct task_struct *tsk)\\n{\\n\\tstruct rcu_head *rh = tsk->stack;\\n\\n\\tcall_rcu(rh, thread_stack_free_rcu);\\n}\\n\\nstatic int alloc_thread_stack_node(struct task_struct *tsk, int node)\\n{\\n\\tstruct page *page = alloc_pages_node(node, THREADINFO_GFP,\\n\\t\\t\\t\\t\\t     THREAD_SIZE_ORDER);\\n\\n\\tif (likely(page)) {\\n\\t\\ttsk->stack = kasan_reset_tag(page_address(page));\\n\\t\\treturn 0;\\n\\t}\\n\\treturn -ENOMEM;\\n}\\n\\nstatic void free_thread_stack(struct task_struct *tsk)\\n{\\n\\tthread_stack_delayed_free(tsk);\\n\\ttsk->stack = NULL;\\n}\\n\\n#  endif /* CONFIG_VMAP_STACK */\\n# else /* !(THREAD_SIZE >= PAGE_SIZE || defined(CONFIG_VMAP_STACK)) */\\n\\nstatic struct kmem_cache *thread_stack_cache;\\n\\nstatic void thread_stack_free_rcu(struct rcu_head *rh)\\n{\\n\\tkmem_cache_free(thread_stack_cache, rh);\\n}\\n\\nstatic void thread_stack_delayed_free(struct task_struct *tsk)\\n{\\n\\tstruct rcu_head *rh = tsk->stack;\\n\\n\\tcall_rcu(rh, thread_stack_free_rcu);\\n}\\n\\nstatic int alloc_thread_stack_node(struct task_struct *tsk, int node)\\n{\\n\\tunsigned long *stack;\\n\\tstack = kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);\\n\\tstack = kasan_reset_tag(stack);\\n\\ttsk->stack = stack;\\n\\treturn stack ? 0 : -ENOMEM;\\n}\\n\\nstatic void free_thread_stack(struct task_struct *tsk)\\n{\\n\\tthread_stack_delayed_free(tsk);\\n\\ttsk->stack = NULL;\\n}\\n\\nvoid thread_stack_cache_init(void)\\n{\\n\\tthread_stack_cache = kmem_cache_create_usercopy(\"thread_stack\",\\n\\t\\t\\t\\t\\tTHREAD_SIZE, THREAD_SIZE, 0, 0,\\n\\t\\t\\t\\t\\tTHREAD_SIZE, NULL);\\n\\tBUG_ON(thread_stack_cache == NULL);\\n}\\n\\n# endif /* THREAD_SIZE >= PAGE_SIZE || defined(CONFIG_VMAP_STACK) */\\n\\n/* SLAB cache for signal_struct structures (tsk->signal) */\\nstatic struct kmem_cache *signal_cachep;\\n\\n/* SLAB cache for sighand_struct structures (tsk->sighand) */\\nstruct kmem_cache *sighand_cachep;\\n\\n/* SLAB cache for files_struct structures (tsk->files) */\\nstruct kmem_cache *files_cachep;\\n\\n/* SLAB cache for fs_struct structures (tsk->fs) */\\nstruct kmem_cache *fs_cachep;\\n\\n/* SLAB cache for vm_area_struct structures */\\nstatic struct kmem_cache *vm_area_cachep;\\n\\n/* SLAB cache for mm_struct structures (tsk->mm) */\\nstatic struct kmem_cache *mm_cachep;\\n\\n#ifdef CONFIG_PER_VMA_LOCK\\n\\n/* SLAB cache for vm_area_struct.lock */\\nstatic struct kmem_cache *vma_lock_cachep;\\n\\nstatic bool vma_lock_alloc(struct vm_area_struct *vma)\\n{\\n\\tvma->vm_lock = kmem_cache_alloc(vma_lock_cachep, GFP_KERNEL);\\n\\tif (!vma->vm_lock)\\n\\t\\treturn false;\\n\\n\\tinit_rwsem(&vma->vm_lock->lock);\\n\\tvma->vm_lock_seq = -1;\\n\\n\\treturn true;\\n}\\n\\nstatic inline void vma_lock_free(struct vm_area_struct *vma)\\n{\\n\\tkmem_cache_free(vma_lock_cachep, vma->vm_lock);\\n}\\n\\n#else /* CONFIG_PER_VMA_LOCK */\\n\\nstatic inline bool vma_lock_alloc(struct vm_area_struct *vma) { return true; }\\nstatic inline void vma_lock_free(struct vm_area_struct *vma) {}\\n\\n#endif /* CONFIG_PER_VMA_LOCK */\\n\\nstruct vm_area_struct *vm_area_alloc(struct mm_struct *mm)\\n{\\n\\tstruct vm_area_struct *vma;\\n\\n\\tvma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);\\n\\tif (!vma)\\n\\t\\treturn NULL;\\n\\n\\tvma_init(vma, mm);\\n\\tif (!vma_lock_alloc(vma)) {\\n\\t\\tkmem_cache_free(vm_area_cachep, vma);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\treturn vma;\\n}\\n\\nstruct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)\\n{\\n\\tstruct vm_area_struct *new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);\\n\\n\\tif (!new)\\n\\t\\treturn NULL;\\n\\n\\tASSERT_EXCLUSIVE_WRITER(orig->vm_flags);\\n\\tASSERT_EXCLUSIVE_WRITER(orig->vm_file);\\n\\t/*\\n\\t * orig->shared.rb may be modified concurrently, but the clone\\n\\t * will be reinitialized.\\n\\t */\\n\\tdata_race(memcpy(new, orig, sizeof(*new)));\\n\\tif (!vma_lock_alloc(new)) {\\n\\t\\tkmem_cache_free(vm_area_cachep, new);\\n\\t\\treturn NULL;\\n\\t}\\n\\tINIT_LIST_HEAD(&new->anon_vma_chain);\\n\\tvma_numab_state_init(new);\\n\\tdup_anon_vma_name(orig, new);\\n\\n\\treturn new;\\n}\\n\\nvoid __vm_area_free(struct vm_area_struct *vma)\\n{\\n\\tvma_numab_state_free(vma);\\n\\tfree_anon_vma_name(vma);\\n\\tvma_lock_free(vma);\\n\\tkmem_cache_free(vm_area_cachep, vma);\\n}\\n\\n#ifdef CONFIG_PER_VMA_LOCK\\nstatic void vm_area_free_rcu_cb(struct rcu_head *head)\\n{\\n\\tstruct vm_area_struct *vma = container_of(head, struct vm_area_struct,\\n\\t\\t\\t\\t\\t\\t  vm_rcu);\\n\\n\\t/* The vma should not be locked while being destroyed. */\\n\\tVM_BUG_ON_VMA(rwsem_is_locked(&vma->vm_lock->lock), vma);\\n\\t__vm_area_free(vma);\\n}\\n#endif\\n\\nvoid vm_area_free(struct vm_area_struct *vma)\\n{\\n#ifdef CONFIG_PER_VMA_LOCK\\n\\tcall_rcu(&vma->vm_rcu, vm_area_free_rcu_cb);\\n#else\\n\\t__vm_area_free(vma);\\n#endif\\n}\\n\\nstatic void account_kernel_stack(struct task_struct *tsk, int account)\\n{\\n\\tif (IS_ENABLED(CONFIG_VMAP_STACK)) {\\n\\t\\tstruct vm_struct *vm = task_stack_vm_area(tsk);\\n\\t\\tint i;\\n\\n\\t\\tfor (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++)\\n\\t\\t\\tmod_lruvec_page_state(vm->pages[i], NR_KERNEL_STACK_KB,\\n\\t\\t\\t\\t\\t      account * (PAGE_SIZE / 1024));\\n\\t} else {\\n\\t\\tvoid *stack = task_stack_page(tsk);\\n\\n\\t\\t/* All stack pages are in the same node. */\\n\\t\\tmod_lruvec_kmem_state(stack, NR_KERNEL_STACK_KB,\\n\\t\\t\\t\\t      account * (THREAD_SIZE / 1024));\\n\\t}\\n}\\n\\nvoid exit_task_stack_account(struct task_struct *tsk)\\n{\\n\\taccount_kernel_stack(tsk, -1);\\n\\n\\tif (IS_ENABLED(CONFIG_VMAP_STACK)) {\\n\\t\\tstruct vm_struct *vm;\\n\\t\\tint i;\\n\\n\\t\\tvm = task_stack_vm_area(tsk);\\n\\t\\tfor (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++)\\n\\t\\t\\tmemcg_kmem_uncharge_page(vm->pages[i], 0);\\n\\t}\\n}\\n\\nstatic void release_task_stack(struct task_struct *tsk)\\n{\\n\\tif (WARN_ON(READ_ONCE(tsk->__state) != TASK_DEAD))\\n\\t\\treturn;  /* Better to leak the stack than to free prematurely */\\n\\n\\tfree_thread_stack(tsk);\\n}\\n\\n#ifdef CONFIG_THREAD_INFO_IN_TASK\\nvoid put_task_stack(struct task_struct *tsk)\\n{\\n\\tif (refcount_dec_and_test(&tsk->stack_refcount))\\n\\t\\trelease_task_stack(tsk);\\n}\\n#endif\\n\\nvoid free_task(struct task_struct *tsk)\\n{\\n#ifdef CONFIG_SECCOMP\\n\\tWARN_ON_ONCE(tsk->seccomp.filter);\\n#endif\\n\\trelease_user_cpus_ptr(tsk);\\n\\tscs_release(tsk);\\n\\n#ifndef CONFIG_THREAD_INFO_IN_TASK\\n\\t/*\\n\\t * The task is finally done with both the stack and thread_info,\\n\\t * so free both.\\n\\t */\\n\\trelease_task_stack(tsk);\\n#else\\n\\t/*\\n\\t * If the task had a separate stack allocation, it should be gone\\n\\t * by now.\\n\\t */\\n\\tWARN_ON_ONCE(refcount_read(&tsk->stack_refcount) != 0);\\n#endif\\n\\trt_mutex_debug_task_free(tsk);\\n\\tftrace_graph_exit_task(tsk);\\n\\tarch_release_task_struct(tsk);\\n\\tif (tsk->flags & PF_KTHREAD)\\n\\t\\tfree_kthread_struct(tsk);\\n\\tbpf_task_storage_free(tsk);\\n\\tfree_task_struct(tsk);\\n}\\nEXPORT_SYMBOL(free_task);\\n\\nstatic void dup_mm_exe_file(struct mm_struct *mm, struct mm_struct *oldmm)\\n{\\n\\tstruct file *exe_file;\\n\\n\\texe_file = get_mm_exe_file(oldmm);\\n\\tRCU_INIT_POINTER(mm->exe_file, exe_file);\\n\\t/*\\n\\t * We depend on the oldmm having properly denied write access to the\\n\\t * exe_file already.\\n\\t */\\n\\tif (exe_file && deny_write_access(exe_file))\\n\\t\\tpr_warn_once(\"deny_write_access() failed in %s\\\\n\", __func__);\\n}\\n\\n#ifdef CONFIG_MMU\\nstatic __latent_entropy int dup_mmap(struct mm_struct *mm,\\n\\t\\t\\t\\t\\tstruct mm_struct *oldmm)\\n{\\n\\tstruct vm_area_struct *mpnt, *tmp;\\n\\tint retval;\\n\\tunsigned long charge = 0;\\n\\tLIST_HEAD(uf);\\n\\tVMA_ITERATOR(vmi, mm, 0);\\n\\n\\tif (mmap_write_lock_killable(oldmm))\\n\\t\\treturn -EINTR;\\n\\tflush_cache_dup_mm(oldmm);\\n\\tuprobe_dup_mmap(oldmm, mm);\\n\\t/*\\n\\t * Not linked in yet - no deadlock potential:\\n\\t */\\n\\tmmap_write_lock_nested(mm, SINGLE_DEPTH_NESTING);\\n\\n\\t/* No ordering required: file already has been exposed. */\\n\\tdup_mm_exe_file(mm, oldmm);\\n\\n\\tmm->total_vm = oldmm->total_vm;\\n\\tmm->data_vm = oldmm->data_vm;\\n\\tmm->exec_vm = oldmm->exec_vm;\\n\\tmm->stack_vm = oldmm->stack_vm;\\n\\n\\t/* Use __mt_dup() to efficiently build an identical maple tree. */\\n\\tretval = __mt_dup(&oldmm->mm_mt, &mm->mm_mt, GFP_KERNEL);\\n\\tif (unlikely(retval))\\n\\t\\tgoto out;\\n\\n\\tmt_clear_in_rcu(vmi.mas.tree);\\n\\tfor_each_vma(vmi, mpnt) {\\n\\t\\tstruct file *file;\\n\\n\\t\\tvma_start_write(mpnt);\\n\\t\\tif (mpnt->vm_flags & VM_DONTCOPY) {\\n\\t\\t\\tretval = vma_iter_clear_gfp(&vmi, mpnt->vm_start,\\n\\t\\t\\t\\t\\t\\t    mpnt->vm_end, GFP_KERNEL);\\n\\t\\t\\tif (retval)\\n\\t\\t\\t\\tgoto loop_out;\\n\\n\\t\\t\\tvm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tcharge = 0;\\n\\t\\t/*\\n\\t\\t * Don\\'t duplicate many vmas if we\\'ve been oom-killed (for\\n\\t\\t * example)\\n\\t\\t */\\n\\t\\tif (fatal_signal_pending(current)) {\\n\\t\\t\\tretval = -EINTR;\\n\\t\\t\\tgoto loop_out;\\n\\t\\t}\\n\\t\\tif (mpnt->vm_flags & VM_ACCOUNT) {\\n\\t\\t\\tunsigned long len = vma_pages(mpnt);\\n\\n\\t\\t\\tif (security_vm_enough_memory_mm(oldmm, len)) /* sic */\\n\\t\\t\\t\\tgoto fail_nomem;\\n\\t\\t\\tcharge = len;\\n\\t\\t}\\n\\t\\ttmp = vm_area_dup(mpnt);\\n\\t\\tif (!tmp)\\n\\t\\t\\tgoto fail_nomem;\\n\\t\\tretval = vma_dup_policy(mpnt, tmp);\\n\\t\\tif (retval)\\n\\t\\t\\tgoto fail_nomem_policy;\\n\\t\\ttmp->vm_mm = mm;\\n\\t\\tretval = dup_userfaultfd(tmp, &uf);\\n\\t\\tif (retval)\\n\\t\\t\\tgoto fail_nomem_anon_vma_fork;\\n\\t\\tif (tmp->vm_flags & VM_WIPEONFORK) {\\n\\t\\t\\t/*\\n\\t\\t\\t * VM_WIPEONFORK gets a clean slate in the child.\\n\\t\\t\\t * Don\\'t prepare anon_vma until fault since we don\\'t\\n\\t\\t\\t * copy page for current vma.\\n\\t\\t\\t */\\n\\t\\t\\ttmp->anon_vma = NULL;\\n\\t\\t} else if (anon_vma_fork(tmp, mpnt))\\n\\t\\t\\tgoto fail_nomem_anon_vma_fork;\\n\\t\\tvm_flags_clear(tmp, VM_LOCKED_MASK);\\n\\t\\t/*\\n\\t\\t * Copy/update hugetlb private vma information.\\n\\t\\t */\\n\\t\\tif (is_vm_hugetlb_page(tmp))\\n\\t\\t\\thugetlb_dup_vma_private(tmp);\\n\\n\\t\\t/*\\n\\t\\t * Link the vma into the MT. After using __mt_dup(), memory\\n\\t\\t * allocation is not necessary here, so it cannot fail.\\n\\t\\t */\\n\\t\\tvma_iter_bulk_store(&vmi, tmp);\\n\\n\\t\\tmm->map_count++;\\n\\n\\t\\tif (tmp->vm_ops && tmp->vm_ops->open)\\n\\t\\t\\ttmp->vm_ops->open(tmp);\\n\\n\\t\\tfile = tmp->vm_file;\\n\\t\\tif (file) {\\n\\t\\t\\tstruct address_space *mapping = file->f_mapping;\\n\\n\\t\\t\\tget_file(file);\\n\\t\\t\\ti_mmap_lock_write(mapping);\\n\\t\\t\\tif (vma_is_shared_maywrite(tmp))\\n\\t\\t\\t\\tmapping_allow_writable(mapping);\\n\\t\\t\\tflush_dcache_mmap_lock(mapping);\\n\\t\\t\\t/* insert tmp into the share list, just after mpnt */\\n\\t\\t\\tvma_interval_tree_insert_after(tmp, mpnt,\\n\\t\\t\\t\\t\\t&mapping->i_mmap);\\n\\t\\t\\tflush_dcache_mmap_unlock(mapping);\\n\\t\\t\\ti_mmap_unlock_write(mapping);\\n\\t\\t}\\n\\n\\t\\tif (!(tmp->vm_flags & VM_WIPEONFORK))\\n\\t\\t\\tretval = copy_page_range(tmp, mpnt);\\n\\n\\t\\tif (retval) {\\n\\t\\t\\tmpnt = vma_next(&vmi);\\n\\t\\t\\tgoto loop_out;\\n\\t\\t}\\n\\t}\\n\\t/* a new mm has just been created */\\n\\tretval = arch_dup_mmap(oldmm, mm);\\nloop_out:\\n\\tvma_iter_free(&vmi);\\n\\tif (!retval) {\\n\\t\\tmt_set_in_rcu(vmi.mas.tree);\\n\\t\\tksm_fork(mm, oldmm);\\n\\t\\tkhugepaged_fork(mm, oldmm);\\n\\t} else if (mpnt) {\\n\\t\\t/*\\n\\t\\t * The entire maple tree has already been duplicated. If the\\n\\t\\t * mmap duplication fails, mark the failure point with\\n\\t\\t * XA_ZERO_ENTRY. In exit_mmap(), if this marker is encountered,\\n\\t\\t * stop releasing VMAs that have not been duplicated after this\\n\\t\\t * point.\\n\\t\\t */\\n\\t\\tmas_set_range(&vmi.mas, mpnt->vm_start, mpnt->vm_end - 1);\\n\\t\\tmas_store(&vmi.mas, XA_ZERO_ENTRY);\\n\\t}\\nout:\\n\\tmmap_write_unlock(mm);\\n\\tflush_tlb_mm(oldmm);\\n\\tmmap_write_unlock(oldmm);\\n\\tif (!retval)\\n\\t\\tdup_userfaultfd_complete(&uf);\\n\\telse\\n\\t\\tdup_userfaultfd_fail(&uf);\\n\\treturn retval;\\n\\nfail_nomem_anon_vma_fork:\\n\\tmpol_put(vma_policy(tmp));\\nfail_nomem_policy:\\n\\tvm_area_free(tmp);\\nfail_nomem:\\n\\tretval = -ENOMEM;\\n\\tvm_unacct_memory(charge);\\n\\tgoto loop_out;\\n}\\n\\nstatic inline int mm_alloc_pgd(struct mm_struct *mm)\\n{\\n\\tmm->pgd = pgd_alloc(mm);\\n\\tif (unlikely(!mm->pgd))\\n\\t\\treturn -ENOMEM;\\n\\treturn 0;\\n}\\n\\nstatic inline void mm_free_pgd(struct mm_struct *mm)\\n{\\n\\tpgd_free(mm, mm->pgd);\\n}\\n#else\\nstatic int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)\\n{\\n\\tmmap_write_lock(oldmm);\\n\\tdup_mm_exe_file(mm, oldmm);\\n\\tmmap_write_unlock(oldmm);\\n\\treturn 0;\\n}\\n#define mm_alloc_pgd(mm)\\t(0)\\n#define mm_free_pgd(mm)\\n#endif /* CONFIG_MMU */\\n\\nstatic void check_mm(struct mm_struct *mm)\\n{\\n\\tint i;\\n\\n\\tBUILD_BUG_ON_MSG(ARRAY_SIZE(resident_page_types) != NR_MM_COUNTERS,\\n\\t\\t\\t \"Please make sure \\'struct resident_page_types[]\\' is updated as well\");\\n\\n\\tfor (i = 0; i < NR_MM_COUNTERS; i++) {\\n\\t\\tlong x = percpu_counter_sum(&mm->rss_stat[i]);\\n\\n\\t\\tif (unlikely(x))\\n\\t\\t\\tpr_alert(\"BUG: Bad rss-counter state mm:%p type:%s val:%ld\\\\n\",\\n\\t\\t\\t\\t mm, resident_page_types[i], x);\\n\\t}\\n\\n\\tif (mm_pgtables_bytes(mm))\\n\\t\\tpr_alert(\"BUG: non-zero pgtables_bytes on freeing mm: %ld\\\\n\",\\n\\t\\t\\t\\tmm_pgtables_bytes(mm));\\n\\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !defined(CONFIG_SPLIT_PMD_PTLOCKS)\\n\\tVM_BUG_ON_MM(mm->pmd_huge_pte, mm);\\n#endif\\n}\\n\\n#define allocate_mm()\\t(kmem_cache_alloc(mm_cachep, GFP_KERNEL))\\n#define free_mm(mm)\\t(kmem_cache_free(mm_cachep, (mm)))\\n\\nstatic void do_check_lazy_tlb(void *arg)\\n{\\n\\tstruct mm_struct *mm = arg;\\n\\n\\tWARN_ON_ONCE(current->active_mm == mm);\\n}\\n\\nstatic void do_shoot_lazy_tlb(void *arg)\\n{\\n\\tstruct mm_struct *mm = arg;\\n\\n\\tif (current->active_mm == mm) {\\n\\t\\tWARN_ON_ONCE(current->mm);\\n\\t\\tcurrent->active_mm = &init_mm;\\n\\t\\tswitch_mm(mm, &init_mm, current);\\n\\t}\\n}\\n\\nstatic void cleanup_lazy_tlbs(struct mm_struct *mm)\\n{\\n\\tif (!IS_ENABLED(CONFIG_MMU_LAZY_TLB_SHOOTDOWN)) {\\n\\t\\t/*\\n\\t\\t * In this case, lazy tlb mms are refounted and would not reach\\n\\t\\t * __mmdrop until all CPUs have switched away and mmdrop()ed.\\n\\t\\t */\\n\\t\\treturn;\\n\\t}\\n\\n\\t/*\\n\\t * Lazy mm shootdown does not refcount \"lazy tlb mm\" usage, rather it\\n\\t * requires lazy mm users to switch to another mm when the refcount\\n\\t * drops to zero, before the mm is freed. This requires IPIs here to\\n\\t * switch kernel threads to init_mm.\\n\\t *\\n\\t * archs that use IPIs to flush TLBs can piggy-back that lazy tlb mm\\n\\t * switch with the final userspace teardown TLB flush which leaves the\\n\\t * mm lazy on this CPU but no others, reducing the need for additional\\n\\t * IPIs here. There are cases where a final IPI is still required here,\\n\\t * such as the final mmdrop being performed on a different CPU than the\\n\\t * one exiting, or kernel threads using the mm when userspace exits.\\n\\t *\\n\\t * IPI overheads have not found to be expensive, but they could be\\n\\t * reduced in a number of possible ways, for example (roughly\\n\\t * increasing order of complexity):\\n\\t * - The last lazy reference created by exit_mm() could instead switch\\n\\t *   to init_mm, however it\\'s probable this will run on the same CPU\\n\\t *   immediately afterwards, so this may not reduce IPIs much.\\n\\t * - A batch of mms requiring IPIs could be gathered and freed at once.\\n\\t * - CPUs store active_mm where it can be remotely checked without a\\n\\t *   lock, to filter out false-positives in the cpumask.\\n\\t * - After mm_users or mm_count reaches zero, switching away from the\\n\\t *   mm could clear mm_cpumask to reduce some IPIs, perhaps together\\n\\t *   with some batching or delaying of the final IPIs.\\n\\t * - A delayed freeing and RCU-like quiescing sequence based on mm\\n\\t *   switching to avoid IPIs completely.\\n\\t */\\n\\ton_each_cpu_mask(mm_cpumask(mm), do_shoot_lazy_tlb, (void *)mm, 1);\\n\\tif (IS_ENABLED(CONFIG_DEBUG_VM_SHOOT_LAZIES))\\n\\t\\ton_each_cpu(do_check_lazy_tlb, (void *)mm, 1);\\n}\\n\\n/*\\n * Called when the last reference to the mm\\n * is dropped: either by a lazy thread or by\\n * mmput. Free the page directory and the mm.\\n */\\nvoid __mmdrop(struct mm_struct *mm)\\n{\\n\\tBUG_ON(mm == &init_mm);\\n\\tWARN_ON_ONCE(mm == current->mm);\\n\\n\\t/* Ensure no CPUs are using this as their lazy tlb mm */\\n\\tcleanup_lazy_tlbs(mm);\\n\\n\\tWARN_ON_ONCE(mm == current->active_mm);\\n\\tmm_free_pgd(mm);\\n\\tdestroy_context(mm);\\n\\tmmu_notifier_subscriptions_destroy(mm);\\n\\tcheck_mm(mm);\\n\\tput_user_ns(mm->user_ns);\\n\\tmm_pasid_drop(mm);\\n\\tmm_destroy_cid(mm);\\n\\tpercpu_counter_destroy_many(mm->rss_stat, NR_MM_COUNTERS);\\n\\n\\tfree_mm(mm);\\n}\\nEXPORT_SYMBOL_GPL(__mmdrop);\\n\\nstatic void mmdrop_async_fn(struct work_struct *work)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\tmm = container_of(work, struct mm_struct, async_put_work);\\n\\t__mmdrop(mm);\\n}\\n\\nstatic void mmdrop_async(struct mm_struct *mm)\\n{\\n\\tif (unlikely(atomic_dec_and_test(&mm->mm_count))) {\\n\\t\\tINIT_WORK(&mm->async_put_work, mmdrop_async_fn);\\n\\t\\tschedule_work(&mm->async_put_work);\\n\\t}\\n}\\n\\nstatic inline void free_signal_struct(struct signal_struct *sig)\\n{\\n\\ttaskstats_tgid_free(sig);\\n\\tsched_autogroup_exit(sig);\\n\\t/*\\n\\t * __mmdrop is not safe to call from softirq context on x86 due to\\n\\t * pgd_dtor so postpone it to the async context\\n\\t */\\n\\tif (sig->oom_mm)\\n\\t\\tmmdrop_async(sig->oom_mm);\\n\\tkmem_cache_free(signal_cachep, sig);\\n}\\n\\nstatic inline void put_signal_struct(struct signal_struct *sig)\\n{\\n\\tif (refcount_dec_and_test(&sig->sigcnt))\\n\\t\\tfree_signal_struct(sig);\\n}\\n\\nvoid __put_task_struct(struct task_struct *tsk)\\n{\\n\\tWARN_ON(!tsk->exit_state);\\n\\tWARN_ON(refcount_read(&tsk->usage));\\n\\tWARN_ON(tsk == current);\\n\\n\\tsched_ext_free(tsk);\\n\\tio_uring_free(tsk);\\n\\tcgroup_free(tsk);\\n\\ttask_numa_free(tsk, true);\\n\\tsecurity_task_free(tsk);\\n\\texit_creds(tsk);\\n\\tdelayacct_tsk_free(tsk);\\n\\tput_signal_struct(tsk->signal);\\n\\tsched_core_free(tsk);\\n\\tfree_task(tsk);\\n}\\nEXPORT_SYMBOL_GPL(__put_task_struct);\\n\\nvoid __put_task_struct_rcu_cb(struct rcu_head *rhp)\\n{\\n\\tstruct task_struct *task = container_of(rhp, struct task_struct, rcu);\\n\\n\\t__put_task_struct(task);\\n}\\nEXPORT_SYMBOL_GPL(__put_task_struct_rcu_cb);\\n\\nvoid __init __weak arch_task_cache_init(void) { }\\n\\n/*\\n * set_max_threads\\n */\\nstatic void __init set_max_threads(unsigned int max_threads_suggested)\\n{\\n\\tu64 threads;\\n\\tunsigned long nr_pages = memblock_estimated_nr_free_pages();\\n\\n\\t/*\\n\\t * The number of threads shall be limited such that the thread\\n\\t * structures may only consume a small part of the available memory.\\n\\t */\\n\\tif (fls64(nr_pages) + fls64(PAGE_SIZE) > 64)\\n\\t\\tthreads = MAX_THREADS;\\n\\telse\\n\\t\\tthreads = div64_u64((u64) nr_pages * (u64) PAGE_SIZE,\\n\\t\\t\\t\\t    (u64) THREAD_SIZE * 8UL);\\n\\n\\tif (threads > max_threads_suggested)\\n\\t\\tthreads = max_threads_suggested;\\n\\n\\tmax_threads = clamp_t(u64, threads, MIN_THREADS, MAX_THREADS);\\n}\\n\\n#ifdef CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT\\n/* Initialized by the architecture: */\\nint arch_task_struct_size __read_mostly;\\n#endif\\n\\nstatic void __init task_struct_whitelist(unsigned long *offset, unsigned long *size)\\n{\\n\\t/* Fetch thread_struct whitelist for the architecture. */\\n\\tarch_thread_struct_whitelist(offset, size);\\n\\n\\t/*\\n\\t * Handle zero-sized whitelist or empty thread_struct, otherwise\\n\\t * adjust offset to position of thread_struct in task_struct.\\n\\t */\\n\\tif (unlikely(*size == 0))\\n\\t\\t*offset = 0;\\n\\telse\\n\\t\\t*offset += offsetof(struct task_struct, thread);\\n}\\n\\nvoid __init fork_init(void)\\n{\\n\\tint i;\\n#ifndef ARCH_MIN_TASKALIGN\\n#define ARCH_MIN_TASKALIGN\\t0\\n#endif\\n\\tint align = max_t(int, L1_CACHE_BYTES, ARCH_MIN_TASKALIGN);\\n\\tunsigned long useroffset, usersize;\\n\\n\\t/* create a slab on which task_structs can be allocated */\\n\\ttask_struct_whitelist(&useroffset, &usersize);\\n\\ttask_struct_cachep = kmem_cache_create_usercopy(\"task_struct\",\\n\\t\\t\\tarch_task_struct_size, align,\\n\\t\\t\\tSLAB_PANIC|SLAB_ACCOUNT,\\n\\t\\t\\tuseroffset, usersize, NULL);\\n\\n\\t/* do the arch specific task caches init */\\n\\tarch_task_cache_init();\\n\\n\\tset_max_threads(MAX_THREADS);\\n\\n\\tinit_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;\\n\\tinit_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;\\n\\tinit_task.signal->rlim[RLIMIT_SIGPENDING] =\\n\\t\\tinit_task.signal->rlim[RLIMIT_NPROC];\\n\\n\\tfor (i = 0; i < UCOUNT_COUNTS; i++)\\n\\t\\tinit_user_ns.ucount_max[i] = max_threads/2;\\n\\n\\tset_userns_rlimit_max(&init_user_ns, UCOUNT_RLIMIT_NPROC,      RLIM_INFINITY);\\n\\tset_userns_rlimit_max(&init_user_ns, UCOUNT_RLIMIT_MSGQUEUE,   RLIM_INFINITY);\\n\\tset_userns_rlimit_max(&init_user_ns, UCOUNT_RLIMIT_SIGPENDING, RLIM_INFINITY);\\n\\tset_userns_rlimit_max(&init_user_ns, UCOUNT_RLIMIT_MEMLOCK,    RLIM_INFINITY);\\n\\n#ifdef CONFIG_VMAP_STACK\\n\\tcpuhp_setup_state(CPUHP_BP_PREPARE_DYN, \"fork:vm_stack_cache\",\\n\\t\\t\\t  NULL, free_vm_stack_cache);\\n#endif\\n\\n\\tscs_init();\\n\\n\\tlockdep_init_task(&init_task);\\n\\tuprobes_init();\\n}\\n\\nint __weak arch_dup_task_struct(struct task_struct *dst,\\n\\t\\t\\t\\t\\t       struct task_struct *src)\\n{\\n\\t*dst = *src;\\n\\treturn 0;\\n}\\n\\nvoid set_task_stack_end_magic(struct task_struct *tsk)\\n{\\n\\tunsigned long *stackend;\\n\\n\\tstackend = end_of_stack(tsk);\\n\\t*stackend = STACK_END_MAGIC;\\t/* for overflow detection */\\n}\\n\\nstatic struct task_struct *dup_task_struct(struct task_struct *orig, int node)\\n{\\n\\tstruct task_struct *tsk;\\n\\tint err;\\n\\n\\tif (node == NUMA_NO_NODE)\\n\\t\\tnode = tsk_fork_get_node(orig);\\n\\ttsk = alloc_task_struct_node(node);\\n\\tif (!tsk)\\n\\t\\treturn NULL;\\n\\n\\terr = arch_dup_task_struct(tsk, orig);\\n\\tif (err)\\n\\t\\tgoto free_tsk;\\n\\n\\terr = alloc_thread_stack_node(tsk, node);\\n\\tif (err)\\n\\t\\tgoto free_tsk;\\n\\n#ifdef CONFIG_THREAD_INFO_IN_TASK\\n\\trefcount_set(&tsk->stack_refcount, 1);\\n#endif\\n\\taccount_kernel_stack(tsk, 1);\\n\\n\\terr = scs_prepare(tsk, node);\\n\\tif (err)\\n\\t\\tgoto free_stack;\\n\\n#ifdef CONFIG_SECCOMP\\n\\t/*\\n\\t * We must handle setting up seccomp filters once we\\'re under\\n\\t * the sighand lock in case orig has changed between now and\\n\\t * then. Until then, filter must be NULL to avoid messing up\\n\\t * the usage counts on the error path calling free_task.\\n\\t */\\n\\ttsk->seccomp.filter = NULL;\\n#endif\\n\\n\\tsetup_thread_stack(tsk, orig);\\n\\tclear_user_return_notifier(tsk);\\n\\tclear_tsk_need_resched(tsk);\\n\\tset_task_stack_end_magic(tsk);\\n\\tclear_syscall_work_syscall_user_dispatch(tsk);\\n\\n#ifdef CONFIG_STACKPROTECTOR\\n\\ttsk->stack_canary = get_random_canary();\\n#endif\\n\\tif (orig->cpus_ptr == &orig->cpus_mask)\\n\\t\\ttsk->cpus_ptr = &tsk->cpus_mask;\\n\\tdup_user_cpus_ptr(tsk, orig, node);\\n\\n\\t/*\\n\\t * One for the user space visible state that goes away when reaped.\\n\\t * One for the scheduler.\\n\\t */\\n\\trefcount_set(&tsk->rcu_users, 2);\\n\\t/* One for the rcu users */\\n\\trefcount_set(&tsk->usage, 1);\\n#ifdef CONFIG_BLK_DEV_IO_TRACE\\n\\ttsk->btrace_seq = 0;\\n#endif\\n\\ttsk->splice_pipe = NULL;\\n\\ttsk->task_frag.page = NULL;\\n\\ttsk->wake_q.next = NULL;\\n\\ttsk->worker_private = NULL;\\n\\n\\tkcov_task_init(tsk);\\n\\tkmsan_task_create(tsk);\\n\\tkmap_local_fork(tsk);\\n\\n#ifdef CONFIG_FAULT_INJECTION\\n\\ttsk->fail_nth = 0;\\n#endif\\n\\n#ifdef CONFIG_BLK_CGROUP\\n\\ttsk->throttle_disk = NULL;\\n\\ttsk->use_memdelay = 0;\\n#endif\\n\\n#ifdef CONFIG_ARCH_HAS_CPU_PASID\\n\\ttsk->pasid_activated = 0;\\n#endif\\n\\n#ifdef CONFIG_MEMCG\\n\\ttsk->active_memcg = NULL;\\n#endif\\n\\n#ifdef CONFIG_X86_BUS_LOCK_DETECT\\n\\ttsk->reported_split_lock = 0;\\n#endif\\n\\n#ifdef CONFIG_SCHED_MM_CID\\n\\ttsk->mm_cid = -1;\\n\\ttsk->last_mm_cid = -1;\\n\\ttsk->mm_cid_active = 0;\\n\\ttsk->migrate_from_cpu = -1;\\n#endif\\n\\treturn tsk;\\n\\nfree_stack:\\n\\texit_task_stack_account(tsk);\\n\\tfree_thread_stack(tsk);\\nfree_tsk:\\n\\tfree_task_struct(tsk);\\n\\treturn NULL;\\n}\\n\\n__cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);\\n\\nstatic unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;\\n\\nstatic int __init coredump_filter_setup(char *s)\\n{\\n\\tdefault_dump_filter =\\n\\t\\t(simple_strtoul(s, NULL, 0) << MMF_DUMP_FILTER_SHIFT) &\\n\\t\\tMMF_DUMP_FILTER_MASK;\\n\\treturn 1;\\n}\\n\\n__setup(\"coredump_filter=\", coredump_filter_setup);\\n\\n#include <linux/init_task.h>\\n\\nstatic void mm_init_aio(struct mm_struct *mm)\\n{\\n#ifdef CONFIG_AIO\\n\\tspin_lock_init(&mm->ioctx_lock);\\n\\tmm->ioctx_table = NULL;\\n#endif\\n}\\n\\nstatic __always_inline void mm_clear_owner(struct mm_struct *mm,\\n\\t\\t\\t\\t\\t   struct task_struct *p)\\n{\\n#ifdef CONFIG_MEMCG\\n\\tif (mm->owner == p)\\n\\t\\tWRITE_ONCE(mm->owner, NULL);\\n#endif\\n}\\n\\nstatic void mm_init_owner(struct mm_struct *mm, struct task_struct *p)\\n{\\n#ifdef CONFIG_MEMCG\\n\\tmm->owner = p;\\n#endif\\n}\\n\\nstatic void mm_init_uprobes_state(struct mm_struct *mm)\\n{\\n#ifdef CONFIG_UPROBES\\n\\tmm->uprobes_state.xol_area = NULL;\\n#endif\\n}\\n\\nstatic struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,\\n\\tstruct user_namespace *user_ns)\\n{\\n\\tmt_init_flags(&mm->mm_mt, MM_MT_FLAGS);\\n\\tmt_set_external_lock(&mm->mm_mt, &mm->mmap_lock);\\n\\tatomic_set(&mm->mm_users, 1);\\n\\tatomic_set(&mm->mm_count, 1);\\n\\tseqcount_init(&mm->write_protect_seq);\\n\\tmmap_init_lock(mm);\\n\\tINIT_LIST_HEAD(&mm->mmlist);\\n#ifdef CONFIG_PER_VMA_LOCK\\n\\tmm->mm_lock_seq = 0;\\n#endif\\n\\tmm_pgtables_bytes_init(mm);\\n\\tmm->map_count = 0;\\n\\tmm->locked_vm = 0;\\n\\tatomic64_set(&mm->pinned_vm, 0);\\n\\tmemset(&mm->rss_stat, 0, sizeof(mm->rss_stat));\\n\\tspin_lock_init(&mm->page_table_lock);\\n\\tspin_lock_init(&mm->arg_lock);\\n\\tmm_init_cpumask(mm);\\n\\tmm_init_aio(mm);\\n\\tmm_init_owner(mm, p);\\n\\tmm_pasid_init(mm);\\n\\tRCU_INIT_POINTER(mm->exe_file, NULL);\\n\\tmmu_notifier_subscriptions_init(mm);\\n\\tinit_tlb_flush_pending(mm);\\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !defined(CONFIG_SPLIT_PMD_PTLOCKS)\\n\\tmm->pmd_huge_pte = NULL;\\n#endif\\n\\tmm_init_uprobes_state(mm);\\n\\thugetlb_count_init(mm);\\n\\n\\tif (current->mm) {\\n\\t\\tmm->flags = mmf_init_flags(current->mm->flags);\\n\\t\\tmm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;\\n\\t} else {\\n\\t\\tmm->flags = default_dump_filter;\\n\\t\\tmm->def_flags = 0;\\n\\t}\\n\\n\\tif (mm_alloc_pgd(mm))\\n\\t\\tgoto fail_nopgd;\\n\\n\\tif (init_new_context(p, mm))\\n\\t\\tgoto fail_nocontext;\\n\\n\\tif (mm_alloc_cid(mm, p))\\n\\t\\tgoto fail_cid;\\n\\n\\tif (percpu_counter_init_many(mm->rss_stat, 0, GFP_KERNEL_ACCOUNT,\\n\\t\\t\\t\\t     NR_MM_COUNTERS))\\n\\t\\tgoto fail_pcpu;\\n\\n\\tmm->user_ns = get_user_ns(user_ns);\\n\\tlru_gen_init_mm(mm);\\n\\treturn mm;\\n\\nfail_pcpu:\\n\\tmm_destroy_cid(mm);\\nfail_cid:\\n\\tdestroy_context(mm);\\nfail_nocontext:\\n\\tmm_free_pgd(mm);\\nfail_nopgd:\\n\\tfree_mm(mm);\\n\\treturn NULL;\\n}\\n\\n/*\\n * Allocate and initialize an mm_struct.\\n */\\nstruct mm_struct *mm_alloc(void)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\tmm = allocate_mm();\\n\\tif (!mm)\\n\\t\\treturn NULL;\\n\\n\\tmemset(mm, 0, sizeof(*mm));\\n\\treturn mm_init(mm, current, current_user_ns());\\n}\\nEXPORT_SYMBOL_IF_KUNIT(mm_alloc);\\n\\nstatic inline void __mmput(struct mm_struct *mm)\\n{\\n\\tVM_BUG_ON(atomic_read(&mm->mm_users));\\n\\n\\tuprobe_clear_state(mm);\\n\\texit_aio(mm);\\n\\tksm_exit(mm);\\n\\tkhugepaged_exit(mm); /* must run before exit_mmap */\\n\\texit_mmap(mm);\\n\\tmm_put_huge_zero_folio(mm);\\n\\tset_mm_exe_file(mm, NULL);\\n\\tif (!list_empty(&mm->mmlist)) {\\n\\t\\tspin_lock(&mmlist_lock);\\n\\t\\tlist_del(&mm->mmlist);\\n\\t\\tspin_unlock(&mmlist_lock);\\n\\t}\\n\\tif (mm->binfmt)\\n\\t\\tmodule_put(mm->binfmt->module);\\n\\tlru_gen_del_mm(mm);\\n\\tmmdrop(mm);\\n}\\n\\n/*\\n * Decrement the use count and release all resources for an mm.\\n */\\nvoid mmput(struct mm_struct *mm)\\n{\\n\\tmight_sleep();\\n\\n\\tif (atomic_dec_and_test(&mm->mm_users))\\n\\t\\t__mmput(mm);\\n}\\nEXPORT_SYMBOL_GPL(mmput);\\n\\n#ifdef CONFIG_MMU\\nstatic void mmput_async_fn(struct work_struct *work)\\n{\\n\\tstruct mm_struct *mm = container_of(work, struct mm_struct,\\n\\t\\t\\t\\t\\t    async_put_work);\\n\\n\\t__mmput(mm);\\n}\\n\\nvoid mmput_async(struct mm_struct *mm)\\n{\\n\\tif (atomic_dec_and_test(&mm->mm_users)) {\\n\\t\\tINIT_WORK(&mm->async_put_work, mmput_async_fn);\\n\\t\\tschedule_work(&mm->async_put_work);\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(mmput_async);\\n#endif\\n\\n/**\\n * set_mm_exe_file - change a reference to the mm\\'s executable file\\n * @mm: The mm to change.\\n * @new_exe_file: The new file to use.\\n *\\n * This changes mm\\'s executable file (shown as symlink /proc/[pid]/exe).\\n *\\n * Main users are mmput() and sys_execve(). Callers prevent concurrent\\n * invocations: in mmput() nobody alive left, in execve it happens before\\n * the new mm is made visible to anyone.\\n *\\n * Can only fail if new_exe_file != NULL.\\n */\\nint set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file)\\n{\\n\\tstruct file *old_exe_file;\\n\\n\\t/*\\n\\t * It is safe to dereference the exe_file without RCU as\\n\\t * this function is only called if nobody else can access\\n\\t * this mm -- see comment above for justification.\\n\\t */\\n\\told_exe_file = rcu_dereference_raw(mm->exe_file);\\n\\n\\tif (new_exe_file) {\\n\\t\\t/*\\n\\t\\t * We expect the caller (i.e., sys_execve) to already denied\\n\\t\\t * write access, so this is unlikely to fail.\\n\\t\\t */\\n\\t\\tif (unlikely(deny_write_access(new_exe_file)))\\n\\t\\t\\treturn -EACCES;\\n\\t\\tget_file(new_exe_file);\\n\\t}\\n\\trcu_assign_pointer(mm->exe_file, new_exe_file);\\n\\tif (old_exe_file) {\\n\\t\\tallow_write_access(old_exe_file);\\n\\t\\tfput(old_exe_file);\\n\\t}\\n\\treturn 0;\\n}\\n\\n/**\\n * replace_mm_exe_file - replace a reference to the mm\\'s executable file\\n * @mm: The mm to change.\\n * @new_exe_file: The new file to use.\\n *\\n * This changes mm\\'s executable file (shown as symlink /proc/[pid]/exe).\\n *\\n * Main user is sys_prctl(PR_SET_MM_MAP/EXE_FILE).\\n */\\nint replace_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file)\\n{\\n\\tstruct vm_area_struct *vma;\\n\\tstruct file *old_exe_file;\\n\\tint ret = 0;\\n\\n\\t/* Forbid mm->exe_file change if old file still mapped. */\\n\\told_exe_file = get_mm_exe_file(mm);\\n\\tif (old_exe_file) {\\n\\t\\tVMA_ITERATOR(vmi, mm, 0);\\n\\t\\tmmap_read_lock(mm);\\n\\t\\tfor_each_vma(vmi, vma) {\\n\\t\\t\\tif (!vma->vm_file)\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\tif (path_equal(&vma->vm_file->f_path,\\n\\t\\t\\t\\t       &old_exe_file->f_path)) {\\n\\t\\t\\t\\tret = -EBUSY;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tmmap_read_unlock(mm);\\n\\t\\tfput(old_exe_file);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\n\\tret = deny_write_access(new_exe_file);\\n\\tif (ret)\\n\\t\\treturn -EACCES;\\n\\tget_file(new_exe_file);\\n\\n\\t/* set the new file */\\n\\tmmap_write_lock(mm);\\n\\told_exe_file = rcu_dereference_raw(mm->exe_file);\\n\\trcu_assign_pointer(mm->exe_file, new_exe_file);\\n\\tmmap_write_unlock(mm);\\n\\n\\tif (old_exe_file) {\\n\\t\\tallow_write_access(old_exe_file);\\n\\t\\tfput(old_exe_file);\\n\\t}\\n\\treturn 0;\\n}\\n\\n/**\\n * get_mm_exe_file - acquire a reference to the mm\\'s executable file\\n * @mm: The mm of interest.\\n *\\n * Returns %NULL if mm has no associated executable file.\\n * User must release file via fput().\\n */\\nstruct file *get_mm_exe_file(struct mm_struct *mm)\\n{\\n\\tstruct file *exe_file;\\n\\n\\trcu_read_lock();\\n\\texe_file = get_file_rcu(&mm->exe_file);\\n\\trcu_read_unlock();\\n\\treturn exe_file;\\n}\\n\\n/**\\n * get_task_exe_file - acquire a reference to the task\\'s executable file\\n * @task: The task.\\n *\\n * Returns %NULL if task\\'s mm (if any) has no associated executable file or\\n * this is a kernel thread with borrowed mm (see the comment above get_task_mm).\\n * User must release file via fput().\\n */\\nstruct file *get_task_exe_file(struct task_struct *task)\\n{\\n\\tstruct file *exe_file = NULL;\\n\\tstruct mm_struct *mm;\\n\\n\\ttask_lock(task);\\n\\tmm = task->mm;\\n\\tif (mm) {\\n\\t\\tif (!(task->flags & PF_KTHREAD))\\n\\t\\t\\texe_file = get_mm_exe_file(mm);\\n\\t}\\n\\ttask_unlock(task);\\n\\treturn exe_file;\\n}\\n\\n/**\\n * get_task_mm - acquire a reference to the task\\'s mm\\n * @task: The task.\\n *\\n * Returns %NULL if the task has no mm.  Checks PF_KTHREAD (meaning\\n * this kernel workthread has transiently adopted a user mm with use_mm,\\n * to do its AIO) is not set and if so returns a reference to it, after\\n * bumping up the use count.  User must release the mm via mmput()\\n * after use.  Typically used by /proc and ptrace.\\n */\\nstruct mm_struct *get_task_mm(struct task_struct *task)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\tif (task->flags & PF_KTHREAD)\\n\\t\\treturn NULL;\\n\\n\\ttask_lock(task);\\n\\tmm = task->mm;\\n\\tif (mm)\\n\\t\\tmmget(mm);\\n\\ttask_unlock(task);\\n\\treturn mm;\\n}\\nEXPORT_SYMBOL_GPL(get_task_mm);\\n\\nstruct mm_struct *mm_access(struct task_struct *task, unsigned int mode)\\n{\\n\\tstruct mm_struct *mm;\\n\\tint err;\\n\\n\\terr =  down_read_killable(&task->signal->exec_update_lock);\\n\\tif (err)\\n\\t\\treturn ERR_PTR(err);\\n\\n\\tmm = get_task_mm(task);\\n\\tif (!mm) {\\n\\t\\tmm = ERR_PTR(-ESRCH);\\n\\t} else if (mm != current->mm && !ptrace_may_access(task, mode)) {\\n\\t\\tmmput(mm);\\n\\t\\tmm = ERR_PTR(-EACCES);\\n\\t}\\n\\tup_read(&task->signal->exec_update_lock);\\n\\n\\treturn mm;\\n}\\n\\nstatic void complete_vfork_done(struct task_struct *tsk)\\n{\\n\\tstruct completion *vfork;\\n\\n\\ttask_lock(tsk);\\n\\tvfork = tsk->vfork_done;\\n\\tif (likely(vfork)) {\\n\\t\\ttsk->vfork_done = NULL;\\n\\t\\tcomplete(vfork);\\n\\t}\\n\\ttask_unlock(tsk);\\n}\\n\\nstatic int wait_for_vfork_done(struct task_struct *child,\\n\\t\\t\\t\\tstruct completion *vfork)\\n{\\n\\tunsigned int state = TASK_KILLABLE|TASK_FREEZABLE;\\n\\tint killed;\\n\\n\\tcgroup_enter_frozen();\\n\\tkilled = wait_for_completion_state(vfork, state);\\n\\tcgroup_leave_frozen(false);\\n\\n\\tif (killed) {\\n\\t\\ttask_lock(child);\\n\\t\\tchild->vfork_done = NULL;\\n\\t\\ttask_unlock(child);\\n\\t}\\n\\n\\tput_task_struct(child);\\n\\treturn killed;\\n}\\n\\n/* Please note the differences between mmput and mm_release.\\n * mmput is called whenever we stop holding onto a mm_struct,\\n * error success whatever.\\n *\\n * mm_release is called after a mm_struct has been removed\\n * from the current process.\\n *\\n * This difference is important for error handling, when we\\n * only half set up a mm_struct for a new process and need to restore\\n * the old one.  Because we mmput the new mm_struct before\\n * restoring the old one. . .\\n * Eric Biederman 10 January 1998\\n */\\nstatic void mm_release(struct task_struct *tsk, struct mm_struct *mm)\\n{\\n\\tuprobe_free_utask(tsk);\\n\\n\\t/* Get rid of any cached register state */\\n\\tdeactivate_mm(tsk, mm);\\n\\n\\t/*\\n\\t * Signal userspace if we\\'re not exiting with a core dump\\n\\t * because we want to leave the value intact for debugging\\n\\t * purposes.\\n\\t */\\n\\tif (tsk->clear_child_tid) {\\n\\t\\tif (atomic_read(&mm->mm_users) > 1) {\\n\\t\\t\\t/*\\n\\t\\t\\t * We don\\'t check the error code - if userspace has\\n\\t\\t\\t * not set up a proper pointer then tough luck.\\n\\t\\t\\t */\\n\\t\\t\\tput_user(0, tsk->clear_child_tid);\\n\\t\\t\\tdo_futex(tsk->clear_child_tid, FUTEX_WAKE,\\n\\t\\t\\t\\t\\t1, NULL, NULL, 0, 0);\\n\\t\\t}\\n\\t\\ttsk->clear_child_tid = NULL;\\n\\t}\\n\\n\\t/*\\n\\t * All done, finally we can wake up parent and return this mm to him.\\n\\t * Also kthread_stop() uses this completion for synchronization.\\n\\t */\\n\\tif (tsk->vfork_done)\\n\\t\\tcomplete_vfork_done(tsk);\\n}\\n\\nvoid exit_mm_release(struct task_struct *tsk, struct mm_struct *mm)\\n{\\n\\tfutex_exit_release(tsk);\\n\\tmm_release(tsk, mm);\\n}\\n\\nvoid exec_mm_release(struct task_struct *tsk, struct mm_struct *mm)\\n{\\n\\tfutex_exec_release(tsk);\\n\\tmm_release(tsk, mm);\\n}\\n\\n/**\\n * dup_mm() - duplicates an existing mm structure\\n * @tsk: the task_struct with which the new mm will be associated.\\n * @oldmm: the mm to duplicate.\\n *\\n * Allocates a new mm structure and duplicates the provided @oldmm structure\\n * content into it.\\n *\\n * Return: the duplicated mm or NULL on failure.\\n */\\nstatic struct mm_struct *dup_mm(struct task_struct *tsk,\\n\\t\\t\\t\\tstruct mm_struct *oldmm)\\n{\\n\\tstruct mm_struct *mm;\\n\\tint err;\\n\\n\\tmm = allocate_mm();\\n\\tif (!mm)\\n\\t\\tgoto fail_nomem;\\n\\n\\tmemcpy(mm, oldmm, sizeof(*mm));\\n\\n\\tif (!mm_init(mm, tsk, mm->user_ns))\\n\\t\\tgoto fail_nomem;\\n\\n\\tuprobe_start_dup_mmap();\\n\\terr = dup_mmap(mm, oldmm);\\n\\tif (err)\\n\\t\\tgoto free_pt;\\n\\tuprobe_end_dup_mmap();\\n\\n\\tmm->hiwater_rss = get_mm_rss(mm);\\n\\tmm->hiwater_vm = mm->total_vm;\\n\\n\\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\\n\\t\\tgoto free_pt;\\n\\n\\treturn mm;\\n\\nfree_pt:\\n\\t/* don\\'t put binfmt in mmput, we haven\\'t got module yet */\\n\\tmm->binfmt = NULL;\\n\\tmm_init_owner(mm, NULL);\\n\\tmmput(mm);\\n\\tif (err)\\n\\t\\tuprobe_end_dup_mmap();\\n\\nfail_nomem:\\n\\treturn NULL;\\n}\\n\\nstatic int copy_mm(unsigned long clone_flags, struct task_struct *tsk)\\n{\\n\\tstruct mm_struct *mm, *oldmm;\\n\\n\\ttsk->min_flt = tsk->maj_flt = 0;\\n\\ttsk->nvcsw = tsk->nivcsw = 0;\\n#ifdef CONFIG_DETECT_HUNG_TASK\\n\\ttsk->last_switch_count = tsk->nvcsw + tsk->nivcsw;\\n\\ttsk->last_switch_time = 0;\\n#endif\\n\\n\\ttsk->mm = NULL;\\n\\ttsk->active_mm = NULL;\\n\\n\\t/*\\n\\t * Are we cloning a kernel thread?\\n\\t *\\n\\t * We need to steal a active VM for that..\\n\\t */\\n\\toldmm = current->mm;\\n\\tif (!oldmm)\\n\\t\\treturn 0;\\n\\n\\tif (clone_flags & CLONE_VM) {\\n\\t\\tmmget(oldmm);\\n\\t\\tmm = oldmm;\\n\\t} else {\\n\\t\\tmm = dup_mm(tsk, current->mm);\\n\\t\\tif (!mm)\\n\\t\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\ttsk->mm = mm;\\n\\ttsk->active_mm = mm;\\n\\tsched_mm_cid_fork(tsk);\\n\\treturn 0;\\n}\\n\\nstatic int copy_fs(unsigned long clone_flags, struct task_struct *tsk)\\n{\\n\\tstruct fs_struct *fs = current->fs;\\n\\tif (clone_flags & CLONE_FS) {\\n\\t\\t/* tsk->fs is already what we want */\\n\\t\\tspin_lock(&fs->lock);\\n\\t\\t/* \"users\" and \"in_exec\" locked for check_unsafe_exec() */\\n\\t\\tif (fs->in_exec) {\\n\\t\\t\\tspin_unlock(&fs->lock);\\n\\t\\t\\treturn -EAGAIN;\\n\\t\\t}\\n\\t\\tfs->users++;\\n\\t\\tspin_unlock(&fs->lock);\\n\\t\\treturn 0;\\n\\t}\\n\\ttsk->fs = copy_fs_struct(fs);\\n\\tif (!tsk->fs)\\n\\t\\treturn -ENOMEM;\\n\\treturn 0;\\n}\\n\\nstatic int copy_files(unsigned long clone_flags, struct task_struct *tsk,\\n\\t\\t      int no_files)\\n{\\n\\tstruct files_struct *oldf, *newf;\\n\\n\\t/*\\n\\t * A background process may not have any files ...\\n\\t */\\n\\toldf = current->files;\\n\\tif (!oldf)\\n\\t\\treturn 0;\\n\\n\\tif (no_files) {\\n\\t\\ttsk->files = NULL;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (clone_flags & CLONE_FILES) {\\n\\t\\tatomic_inc(&oldf->count);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tnewf = dup_fd(oldf, NULL);\\n\\tif (IS_ERR(newf))\\n\\t\\treturn PTR_ERR(newf);\\n\\n\\ttsk->files = newf;\\n\\treturn 0;\\n}\\n\\nstatic int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)\\n{\\n\\tstruct sighand_struct *sig;\\n\\n\\tif (clone_flags & CLONE_SIGHAND) {\\n\\t\\trefcount_inc(&current->sighand->count);\\n\\t\\treturn 0;\\n\\t}\\n\\tsig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);\\n\\tRCU_INIT_POINTER(tsk->sighand, sig);\\n\\tif (!sig)\\n\\t\\treturn -ENOMEM;\\n\\n\\trefcount_set(&sig->count, 1);\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\tmemcpy(sig->action, current->sighand->action, sizeof(sig->action));\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\n\\t/* Reset all signal handler not set to SIG_IGN to SIG_DFL. */\\n\\tif (clone_flags & CLONE_CLEAR_SIGHAND)\\n\\t\\tflush_signal_handlers(tsk, 0);\\n\\n\\treturn 0;\\n}\\n\\nvoid __cleanup_sighand(struct sighand_struct *sighand)\\n{\\n\\tif (refcount_dec_and_test(&sighand->count)) {\\n\\t\\tsignalfd_cleanup(sighand);\\n\\t\\t/*\\n\\t\\t * sighand_cachep is SLAB_TYPESAFE_BY_RCU so we can free it\\n\\t\\t * without an RCU grace period, see __lock_task_sighand().\\n\\t\\t */\\n\\t\\tkmem_cache_free(sighand_cachep, sighand);\\n\\t}\\n}\\n\\n/*\\n * Initialize POSIX timer handling for a thread group.\\n */\\nstatic void posix_cpu_timers_init_group(struct signal_struct *sig)\\n{\\n\\tstruct posix_cputimers *pct = &sig->posix_cputimers;\\n\\tunsigned long cpu_limit;\\n\\n\\tcpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);\\n\\tposix_cputimers_group_init(pct, cpu_limit);\\n}\\n\\nstatic int copy_signal(unsigned long clone_flags, struct task_struct *tsk)\\n{\\n\\tstruct signal_struct *sig;\\n\\n\\tif (clone_flags & CLONE_THREAD)\\n\\t\\treturn 0;\\n\\n\\tsig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);\\n\\ttsk->signal = sig;\\n\\tif (!sig)\\n\\t\\treturn -ENOMEM;\\n\\n\\tsig->nr_threads = 1;\\n\\tsig->quick_threads = 1;\\n\\tatomic_set(&sig->live, 1);\\n\\trefcount_set(&sig->sigcnt, 1);\\n\\n\\t/* list_add(thread_node, thread_head) without INIT_LIST_HEAD() */\\n\\tsig->thread_head = (struct list_head)LIST_HEAD_INIT(tsk->thread_node);\\n\\ttsk->thread_node = (struct list_head)LIST_HEAD_INIT(sig->thread_head);\\n\\n\\tinit_waitqueue_head(&sig->wait_chldexit);\\n\\tsig->curr_target = tsk;\\n\\tinit_sigpending(&sig->shared_pending);\\n\\tINIT_HLIST_HEAD(&sig->multiprocess);\\n\\tseqlock_init(&sig->stats_lock);\\n\\tprev_cputime_init(&sig->prev_cputime);\\n\\n#ifdef CONFIG_POSIX_TIMERS\\n\\tINIT_HLIST_HEAD(&sig->posix_timers);\\n\\tINIT_HLIST_HEAD(&sig->ignored_posix_timers);\\n\\thrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\\n\\tsig->real_timer.function = it_real_fn;\\n#endif\\n\\n\\ttask_lock(current->group_leader);\\n\\tmemcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);\\n\\ttask_unlock(current->group_leader);\\n\\n\\tposix_cpu_timers_init_group(sig);\\n\\n\\ttty_audit_fork(sig);\\n\\tsched_autogroup_fork(sig);\\n\\n\\tsig->oom_score_adj = current->signal->oom_score_adj;\\n\\tsig->oom_score_adj_min = current->signal->oom_score_adj_min;\\n\\n\\tmutex_init(&sig->cred_guard_mutex);\\n\\tinit_rwsem(&sig->exec_update_lock);\\n\\n\\treturn 0;\\n}\\n\\nstatic void copy_seccomp(struct task_struct *p)\\n{\\n#ifdef CONFIG_SECCOMP\\n\\t/*\\n\\t * Must be called with sighand->lock held, which is common to\\n\\t * all threads in the group. Holding cred_guard_mutex is not\\n\\t * needed because this new task is not yet running and cannot\\n\\t * be racing exec.\\n\\t */\\n\\tassert_spin_locked(&current->sighand->siglock);\\n\\n\\t/* Ref-count the new filter user, and assign it. */\\n\\tget_seccomp_filter(current);\\n\\tp->seccomp = current->seccomp;\\n\\n\\t/*\\n\\t * Explicitly enable no_new_privs here in case it got set\\n\\t * between the task_struct being duplicated and holding the\\n\\t * sighand lock. The seccomp state and nnp must be in sync.\\n\\t */\\n\\tif (task_no_new_privs(current))\\n\\t\\ttask_set_no_new_privs(p);\\n\\n\\t/*\\n\\t * If the parent gained a seccomp mode after copying thread\\n\\t * flags and between before we held the sighand lock, we have\\n\\t * to manually enable the seccomp thread flag here.\\n\\t */\\n\\tif (p->seccomp.mode != SECCOMP_MODE_DISABLED)\\n\\t\\tset_task_syscall_work(p, SECCOMP);\\n#endif\\n}\\n\\nSYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)\\n{\\n\\tcurrent->clear_child_tid = tidptr;\\n\\n\\treturn task_pid_vnr(current);\\n}\\n\\nstatic void rt_mutex_init_task(struct task_struct *p)\\n{\\n\\traw_spin_lock_init(&p->pi_lock);\\n#ifdef CONFIG_RT_MUTEXES\\n\\tp->pi_waiters = RB_ROOT_CACHED;\\n\\tp->pi_top_task = NULL;\\n\\tp->pi_blocked_on = NULL;\\n#endif\\n}\\n\\nstatic inline void init_task_pid_links(struct task_struct *task)\\n{\\n\\tenum pid_type type;\\n\\n\\tfor (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type)\\n\\t\\tINIT_HLIST_NODE(&task->pid_links[type]);\\n}\\n\\nstatic inline void\\ninit_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)\\n{\\n\\tif (type == PIDTYPE_PID)\\n\\t\\ttask->thread_pid = pid;\\n\\telse\\n\\t\\ttask->signal->pids[type] = pid;\\n}\\n\\nstatic inline void rcu_copy_process(struct task_struct *p)\\n{\\n#ifdef CONFIG_PREEMPT_RCU\\n\\tp->rcu_read_lock_nesting = 0;\\n\\tp->rcu_read_unlock_special.s = 0;\\n\\tp->rcu_blocked_node = NULL;\\n\\tINIT_LIST_HEAD(&p->rcu_node_entry);\\n#endif /* #ifdef CONFIG_PREEMPT_RCU */\\n#ifdef CONFIG_TASKS_RCU\\n\\tp->rcu_tasks_holdout = false;\\n\\tINIT_LIST_HEAD(&p->rcu_tasks_holdout_list);\\n\\tp->rcu_tasks_idle_cpu = -1;\\n\\tINIT_LIST_HEAD(&p->rcu_tasks_exit_list);\\n#endif /* #ifdef CONFIG_TASKS_RCU */\\n#ifdef CONFIG_TASKS_TRACE_RCU\\n\\tp->trc_reader_nesting = 0;\\n\\tp->trc_reader_special.s = 0;\\n\\tINIT_LIST_HEAD(&p->trc_holdout_list);\\n\\tINIT_LIST_HEAD(&p->trc_blkd_node);\\n#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */\\n}\\n\\n/**\\n * __pidfd_prepare - allocate a new pidfd_file and reserve a pidfd\\n * @pid:   the struct pid for which to create a pidfd\\n * @flags: flags of the new @pidfd\\n * @ret: Where to return the file for the pidfd.\\n *\\n * Allocate a new file that stashes @pid and reserve a new pidfd number in the\\n * caller\\'s file descriptor table. The pidfd is reserved but not installed yet.\\n *\\n * The helper doesn\\'t perform checks on @pid which makes it useful for pidfds\\n * created via CLONE_PIDFD where @pid has no task attached when the pidfd and\\n * pidfd file are prepared.\\n *\\n * If this function returns successfully the caller is responsible to either\\n * call fd_install() passing the returned pidfd and pidfd file as arguments in\\n * order to install the pidfd into its file descriptor table or they must use\\n * put_unused_fd() and fput() on the returned pidfd and pidfd file\\n * respectively.\\n *\\n * This function is useful when a pidfd must already be reserved but there\\n * might still be points of failure afterwards and the caller wants to ensure\\n * that no pidfd is leaked into its file descriptor table.\\n *\\n * Return: On success, a reserved pidfd is returned from the function and a new\\n *         pidfd file is returned in the last argument to the function. On\\n *         error, a negative error code is returned from the function and the\\n *         last argument remains unchanged.\\n */\\nstatic int __pidfd_prepare(struct pid *pid, unsigned int flags, struct file **ret)\\n{\\n\\tint pidfd;\\n\\tstruct file *pidfd_file;\\n\\n\\tpidfd = get_unused_fd_flags(O_CLOEXEC);\\n\\tif (pidfd < 0)\\n\\t\\treturn pidfd;\\n\\n\\tpidfd_file = pidfs_alloc_file(pid, flags | O_RDWR);\\n\\tif (IS_ERR(pidfd_file)) {\\n\\t\\tput_unused_fd(pidfd);\\n\\t\\treturn PTR_ERR(pidfd_file);\\n\\t}\\n\\t/*\\n\\t * anon_inode_getfile() ignores everything outside of the\\n\\t * O_ACCMODE | O_NONBLOCK mask, set PIDFD_THREAD manually.\\n\\t */\\n\\tpidfd_file->f_flags |= (flags & PIDFD_THREAD);\\n\\t*ret = pidfd_file;\\n\\treturn pidfd;\\n}\\n\\n/**\\n * pidfd_prepare - allocate a new pidfd_file and reserve a pidfd\\n * @pid:   the struct pid for which to create a pidfd\\n * @flags: flags of the new @pidfd\\n * @ret: Where to return the pidfd.\\n *\\n * Allocate a new file that stashes @pid and reserve a new pidfd number in the\\n * caller\\'s file descriptor table. The pidfd is reserved but not installed yet.\\n *\\n * The helper verifies that @pid is still in use, without PIDFD_THREAD the\\n * task identified by @pid must be a thread-group leader.\\n *\\n * If this function returns successfully the caller is responsible to either\\n * call fd_install() passing the returned pidfd and pidfd file as arguments in\\n * order to install the pidfd into its file descriptor table or they must use\\n * put_unused_fd() and fput() on the returned pidfd and pidfd file\\n * respectively.\\n *\\n * This function is useful when a pidfd must already be reserved but there\\n * might still be points of failure afterwards and the caller wants to ensure\\n * that no pidfd is leaked into its file descriptor table.\\n *\\n * Return: On success, a reserved pidfd is returned from the function and a new\\n *         pidfd file is returned in the last argument to the function. On\\n *         error, a negative error code is returned from the function and the\\n *         last argument remains unchanged.\\n */\\nint pidfd_prepare(struct pid *pid, unsigned int flags, struct file **ret)\\n{\\n\\tbool thread = flags & PIDFD_THREAD;\\n\\n\\tif (!pid || !pid_has_task(pid, thread ? PIDTYPE_PID : PIDTYPE_TGID))\\n\\t\\treturn -EINVAL;\\n\\n\\treturn __pidfd_prepare(pid, flags, ret);\\n}\\n\\nstatic void __delayed_free_task(struct rcu_head *rhp)\\n{\\n\\tstruct task_struct *tsk = container_of(rhp, struct task_struct, rcu);\\n\\n\\tfree_task(tsk);\\n}\\n\\nstatic __always_inline void delayed_free_task(struct task_struct *tsk)\\n{\\n\\tif (IS_ENABLED(CONFIG_MEMCG))\\n\\t\\tcall_rcu(&tsk->rcu, __delayed_free_task);\\n\\telse\\n\\t\\tfree_task(tsk);\\n}\\n\\nstatic void copy_oom_score_adj(u64 clone_flags, struct task_struct *tsk)\\n{\\n\\t/* Skip if kernel thread */\\n\\tif (!tsk->mm)\\n\\t\\treturn;\\n\\n\\t/* Skip if spawning a thread or using vfork */\\n\\tif ((clone_flags & (CLONE_VM | CLONE_THREAD | CLONE_VFORK)) != CLONE_VM)\\n\\t\\treturn;\\n\\n\\t/* We need to synchronize with __set_oom_adj */\\n\\tmutex_lock(&oom_adj_mutex);\\n\\tset_bit(MMF_MULTIPROCESS, &tsk->mm->flags);\\n\\t/* Update the values in case they were changed after copy_signal */\\n\\ttsk->signal->oom_score_adj = current->signal->oom_score_adj;\\n\\ttsk->signal->oom_score_adj_min = current->signal->oom_score_adj_min;\\n\\tmutex_unlock(&oom_adj_mutex);\\n}\\n\\n#ifdef CONFIG_RV\\nstatic void rv_task_fork(struct task_struct *p)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < RV_PER_TASK_MONITORS; i++)\\n\\t\\tp->rv[i].da_mon.monitoring = false;\\n}\\n#else\\n#define rv_task_fork(p) do {} while (0)\\n#endif\\n\\n/*\\n * This creates a new process as a copy of the old one,\\n * but does not actually start it yet.\\n *\\n * It copies the registers, and all the appropriate\\n * parts of the process environment (as per the clone\\n * flags). The actual kick-off is left to the caller.\\n */\\n__latent_entropy struct task_struct *copy_process(\\n\\t\\t\\t\\t\\tstruct pid *pid,\\n\\t\\t\\t\\t\\tint trace,\\n\\t\\t\\t\\t\\tint node,\\n\\t\\t\\t\\t\\tstruct kernel_clone_args *args)\\n{\\n\\tint pidfd = -1, retval;\\n\\tstruct task_struct *p;\\n\\tstruct multiprocess_signals delayed;\\n\\tstruct file *pidfile = NULL;\\n\\tconst u64 clone_flags = args->flags;\\n\\tstruct nsproxy *nsp = current->nsproxy;\\n\\n\\t/*\\n\\t * Don\\'t allow sharing the root directory with processes in a different\\n\\t * namespace\\n\\t */\\n\\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\t/*\\n\\t * Thread groups must share signals as well, and detached threads\\n\\t * can only be started up within the thread group.\\n\\t */\\n\\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\t/*\\n\\t * Shared signal handlers imply shared VM. By way of the above,\\n\\t * thread groups also imply shared VM. Blocking this case allows\\n\\t * for various simplifications in other code.\\n\\t */\\n\\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\t/*\\n\\t * Siblings of global init remain as zombies on exit since they are\\n\\t * not reaped by their parent (swapper). To solve this and to avoid\\n\\t * multi-rooted process trees, prevent global and container-inits\\n\\t * from creating siblings.\\n\\t */\\n\\tif ((clone_flags & CLONE_PARENT) &&\\n\\t\\t\\t\\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\t/*\\n\\t * If the new process will be in a different pid or user namespace\\n\\t * do not allow it to share a thread group with the forking task.\\n\\t */\\n\\tif (clone_flags & CLONE_THREAD) {\\n\\t\\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\\n\\t\\t    (task_active_pid_ns(current) != nsp->pid_ns_for_children))\\n\\t\\t\\treturn ERR_PTR(-EINVAL);\\n\\t}\\n\\n\\tif (clone_flags & CLONE_PIDFD) {\\n\\t\\t/*\\n\\t\\t * - CLONE_DETACHED is blocked so that we can potentially\\n\\t\\t *   reuse it later for CLONE_PIDFD.\\n\\t\\t */\\n\\t\\tif (clone_flags & CLONE_DETACHED)\\n\\t\\t\\treturn ERR_PTR(-EINVAL);\\n\\t}\\n\\n\\t/*\\n\\t * Force any signals received before this point to be delivered\\n\\t * before the fork happens.  Collect up signals sent to multiple\\n\\t * processes that happen during the fork and delay them so that\\n\\t * they appear to happen after the fork.\\n\\t */\\n\\tsigemptyset(&delayed.signal);\\n\\tINIT_HLIST_NODE(&delayed.node);\\n\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\tif (!(clone_flags & CLONE_THREAD))\\n\\t\\thlist_add_head(&delayed.node, &current->signal->multiprocess);\\n\\trecalc_sigpending();\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\tretval = -ERESTARTNOINTR;\\n\\tif (task_sigpending(current))\\n\\t\\tgoto fork_out;\\n\\n\\tretval = -ENOMEM;\\n\\tp = dup_task_struct(current, node);\\n\\tif (!p)\\n\\t\\tgoto fork_out;\\n\\tp->flags &= ~PF_KTHREAD;\\n\\tif (args->kthread)\\n\\t\\tp->flags |= PF_KTHREAD;\\n\\tif (args->user_worker) {\\n\\t\\t/*\\n\\t\\t * Mark us a user worker, and block any signal that isn\\'t\\n\\t\\t * fatal or STOP\\n\\t\\t */\\n\\t\\tp->flags |= PF_USER_WORKER;\\n\\t\\tsiginitsetinv(&p->blocked, sigmask(SIGKILL)|sigmask(SIGSTOP));\\n\\t}\\n\\tif (args->io_thread)\\n\\t\\tp->flags |= PF_IO_WORKER;\\n\\n\\tif (args->name)\\n\\t\\tstrscpy_pad(p->comm, args->name, sizeof(p->comm));\\n\\n\\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;\\n\\t/*\\n\\t * Clear TID on mm_release()?\\n\\t */\\n\\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;\\n\\n\\tftrace_graph_init_task(p);\\n\\n\\trt_mutex_init_task(p);\\n\\n\\tlockdep_assert_irqs_enabled();\\n#ifdef CONFIG_PROVE_LOCKING\\n\\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\\n#endif\\n\\tretval = copy_creds(p, clone_flags);\\n\\tif (retval < 0)\\n\\t\\tgoto bad_fork_free;\\n\\n\\tretval = -EAGAIN;\\n\\tif (is_rlimit_overlimit(task_ucounts(p), UCOUNT_RLIMIT_NPROC, rlimit(RLIMIT_NPROC))) {\\n\\t\\tif (p->real_cred->user != INIT_USER &&\\n\\t\\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\\n\\t\\t\\tgoto bad_fork_cleanup_count;\\n\\t}\\n\\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\\n\\n\\t/*\\n\\t * If multiple threads are within copy_process(), then this check\\n\\t * triggers too late. This doesn\\'t hurt, the check is only there\\n\\t * to stop root fork bombs.\\n\\t */\\n\\tretval = -EAGAIN;\\n\\tif (data_race(nr_threads >= max_threads))\\n\\t\\tgoto bad_fork_cleanup_count;\\n\\n\\tdelayacct_tsk_init(p);\\t/* Must remain after dup_task_struct() */\\n\\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE | PF_NO_SETAFFINITY);\\n\\tp->flags |= PF_FORKNOEXEC;\\n\\tINIT_LIST_HEAD(&p->children);\\n\\tINIT_LIST_HEAD(&p->sibling);\\n\\trcu_copy_process(p);\\n\\tp->vfork_done = NULL;\\n\\tspin_lock_init(&p->alloc_lock);\\n\\n\\tinit_sigpending(&p->pending);\\n\\n\\tp->utime = p->stime = p->gtime = 0;\\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\\n\\tp->utimescaled = p->stimescaled = 0;\\n#endif\\n\\tprev_cputime_init(&p->prev_cputime);\\n\\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\\n\\tseqcount_init(&p->vtime.seqcount);\\n\\tp->vtime.starttime = 0;\\n\\tp->vtime.state = VTIME_INACTIVE;\\n#endif\\n\\n#ifdef CONFIG_IO_URING\\n\\tp->io_uring = NULL;\\n#endif\\n\\n\\tp->default_timer_slack_ns = current->timer_slack_ns;\\n\\n#ifdef CONFIG_PSI\\n\\tp->psi_flags = 0;\\n#endif\\n\\n\\ttask_io_accounting_init(&p->ioac);\\n\\tacct_clear_integrals(p);\\n\\n\\tposix_cputimers_init(&p->posix_cputimers);\\n\\ttick_dep_init_task(p);\\n\\n\\tp->io_context = NULL;\\n\\taudit_set_context(p, NULL);\\n\\tcgroup_fork(p);\\n\\tif (args->kthread) {\\n\\t\\tif (!set_kthread_struct(p))\\n\\t\\t\\tgoto bad_fork_cleanup_delayacct;\\n\\t}\\n#ifdef CONFIG_NUMA\\n\\tp->mempolicy = mpol_dup(p->mempolicy);\\n\\tif (IS_ERR(p->mempolicy)) {\\n\\t\\tretval = PTR_ERR(p->mempolicy);\\n\\t\\tp->mempolicy = NULL;\\n\\t\\tgoto bad_fork_cleanup_delayacct;\\n\\t}\\n#endif\\n#ifdef CONFIG_CPUSETS\\n\\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\\n\\tseqcount_spinlock_init(&p->mems_allowed_seq, &p->alloc_lock);\\n#endif\\n#ifdef CONFIG_TRACE_IRQFLAGS\\n\\tmemset(&p->irqtrace, 0, sizeof(p->irqtrace));\\n\\tp->irqtrace.hardirq_disable_ip\\t= _THIS_IP_;\\n\\tp->irqtrace.softirq_enable_ip\\t= _THIS_IP_;\\n\\tp->softirqs_enabled\\t\\t= 1;\\n\\tp->softirq_context\\t\\t= 0;\\n#endif\\n\\n\\tp->pagefault_disabled = 0;\\n\\n#ifdef CONFIG_LOCKDEP\\n\\tlockdep_init_task(p);\\n#endif\\n\\n#ifdef CONFIG_DEBUG_MUTEXES\\n\\tp->blocked_on = NULL; /* not blocked yet */\\n#endif\\n#ifdef CONFIG_BCACHE\\n\\tp->sequential_io\\t= 0;\\n\\tp->sequential_io_avg\\t= 0;\\n#endif\\n#ifdef CONFIG_BPF_SYSCALL\\n\\tRCU_INIT_POINTER(p->bpf_storage, NULL);\\n\\tp->bpf_ctx = NULL;\\n#endif\\n\\n\\t/* Perform scheduler related setup. Assign this task to a CPU. */\\n\\tretval = sched_fork(clone_flags, p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_policy;\\n\\n\\tretval = perf_event_init_task(p, clone_flags);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_sched_cancel_fork;\\n\\tretval = audit_alloc(p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_perf;\\n\\t/* copy all the process information */\\n\\tshm_init_task(p);\\n\\tretval = security_task_alloc(p, clone_flags);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_audit;\\n\\tretval = copy_semundo(clone_flags, p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_security;\\n\\tretval = copy_files(clone_flags, p, args->no_files);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_semundo;\\n\\tretval = copy_fs(clone_flags, p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_files;\\n\\tretval = copy_sighand(clone_flags, p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_fs;\\n\\tretval = copy_signal(clone_flags, p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_sighand;\\n\\tretval = copy_mm(clone_flags, p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_signal;\\n\\tretval = copy_namespaces(clone_flags, p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_mm;\\n\\tretval = copy_io(clone_flags, p);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_namespaces;\\n\\tretval = copy_thread(p, args);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cleanup_io;\\n\\n\\tstackleak_task_init(p);\\n\\n\\tif (pid != &init_struct_pid) {\\n\\t\\tpid = alloc_pid(p->nsproxy->pid_ns_for_children, args->set_tid,\\n\\t\\t\\t\\targs->set_tid_size);\\n\\t\\tif (IS_ERR(pid)) {\\n\\t\\t\\tretval = PTR_ERR(pid);\\n\\t\\t\\tgoto bad_fork_cleanup_thread;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * This has to happen after we\\'ve potentially unshared the file\\n\\t * descriptor table (so that the pidfd doesn\\'t leak into the child\\n\\t * if the fd table isn\\'t shared).\\n\\t */\\n\\tif (clone_flags & CLONE_PIDFD) {\\n\\t\\tint flags = (clone_flags & CLONE_THREAD) ? PIDFD_THREAD : 0;\\n\\n\\t\\t/* Note that no task has been attached to @pid yet. */\\n\\t\\tretval = __pidfd_prepare(pid, flags, &pidfile);\\n\\t\\tif (retval < 0)\\n\\t\\t\\tgoto bad_fork_free_pid;\\n\\t\\tpidfd = retval;\\n\\n\\t\\tretval = put_user(pidfd, args->pidfd);\\n\\t\\tif (retval)\\n\\t\\t\\tgoto bad_fork_put_pidfd;\\n\\t}\\n\\n#ifdef CONFIG_BLOCK\\n\\tp->plug = NULL;\\n#endif\\n\\tfutex_init_task(p);\\n\\n\\t/*\\n\\t * sigaltstack should be cleared when sharing the same VM\\n\\t */\\n\\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\\n\\t\\tsas_ss_reset(p);\\n\\n\\t/*\\n\\t * Syscall tracing and stepping should be turned off in the\\n\\t * child regardless of CLONE_PTRACE.\\n\\t */\\n\\tuser_disable_single_step(p);\\n\\tclear_task_syscall_work(p, SYSCALL_TRACE);\\n#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)\\n\\tclear_task_syscall_work(p, SYSCALL_EMU);\\n#endif\\n\\tclear_tsk_latency_tracing(p);\\n\\n\\t/* ok, now we should be set up.. */\\n\\tp->pid = pid_nr(pid);\\n\\tif (clone_flags & CLONE_THREAD) {\\n\\t\\tp->group_leader = current->group_leader;\\n\\t\\tp->tgid = current->tgid;\\n\\t} else {\\n\\t\\tp->group_leader = p;\\n\\t\\tp->tgid = p->pid;\\n\\t}\\n\\n\\tp->nr_dirtied = 0;\\n\\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\\n\\tp->dirty_paused_when = 0;\\n\\n\\tp->pdeath_signal = 0;\\n\\tp->task_works = NULL;\\n\\tclear_posix_cputimers_work(p);\\n\\n#ifdef CONFIG_KRETPROBES\\n\\tp->kretprobe_instances.first = NULL;\\n#endif\\n#ifdef CONFIG_RETHOOK\\n\\tp->rethooks.first = NULL;\\n#endif\\n\\n\\t/*\\n\\t * Ensure that the cgroup subsystem policies allow the new process to be\\n\\t * forked. It should be noted that the new process\\'s css_set can be changed\\n\\t * between here and cgroup_post_fork() if an organisation operation is in\\n\\t * progress.\\n\\t */\\n\\tretval = cgroup_can_fork(p, args);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_put_pidfd;\\n\\n\\t/*\\n\\t * Now that the cgroups are pinned, re-clone the parent cgroup and put\\n\\t * the new task on the correct runqueue. All this *before* the task\\n\\t * becomes visible.\\n\\t *\\n\\t * This isn\\'t part of ->can_fork() because while the re-cloning is\\n\\t * cgroup specific, it unconditionally needs to place the task on a\\n\\t * runqueue.\\n\\t */\\n\\tretval = sched_cgroup_fork(p, args);\\n\\tif (retval)\\n\\t\\tgoto bad_fork_cancel_cgroup;\\n\\n\\t/*\\n\\t * From this point on we must avoid any synchronous user-space\\n\\t * communication until we take the tasklist-lock. In particular, we do\\n\\t * not want user-space to be able to predict the process start-time by\\n\\t * stalling fork(2) after we recorded the start_time but before it is\\n\\t * visible to the system.\\n\\t */\\n\\n\\tp->start_time = ktime_get_ns();\\n\\tp->start_boottime = ktime_get_boottime_ns();\\n\\n\\t/*\\n\\t * Make it visible to the rest of the system, but dont wake it up yet.\\n\\t * Need tasklist lock for parent etc handling!\\n\\t */\\n\\twrite_lock_irq(&tasklist_lock);\\n\\n\\t/* CLONE_PARENT re-uses the old parent */\\n\\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\\n\\t\\tp->real_parent = current->real_parent;\\n\\t\\tp->parent_exec_id = current->parent_exec_id;\\n\\t\\tif (clone_flags & CLONE_THREAD)\\n\\t\\t\\tp->exit_signal = -1;\\n\\t\\telse\\n\\t\\t\\tp->exit_signal = current->group_leader->exit_signal;\\n\\t} else {\\n\\t\\tp->real_parent = current;\\n\\t\\tp->parent_exec_id = current->self_exec_id;\\n\\t\\tp->exit_signal = args->exit_signal;\\n\\t}\\n\\n\\tklp_copy_process(p);\\n\\n\\tsched_core_fork(p);\\n\\n\\tspin_lock(&current->sighand->siglock);\\n\\n\\trv_task_fork(p);\\n\\n\\trseq_fork(p, clone_flags);\\n\\n\\t/* Don\\'t start children in a dying pid namespace */\\n\\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\\n\\t\\tretval = -ENOMEM;\\n\\t\\tgoto bad_fork_core_free;\\n\\t}\\n\\n\\t/* Let kill terminate clone/fork in the middle */\\n\\tif (fatal_signal_pending(current)) {\\n\\t\\tretval = -EINTR;\\n\\t\\tgoto bad_fork_core_free;\\n\\t}\\n\\n\\t/* No more failure paths after this point. */\\n\\n\\t/*\\n\\t * Copy seccomp details explicitly here, in case they were changed\\n\\t * before holding sighand lock.\\n\\t */\\n\\tcopy_seccomp(p);\\n\\n\\tinit_task_pid_links(p);\\n\\tif (likely(p->pid)) {\\n\\t\\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\\n\\n\\t\\tinit_task_pid(p, PIDTYPE_PID, pid);\\n\\t\\tif (thread_group_leader(p)) {\\n\\t\\t\\tinit_task_pid(p, PIDTYPE_TGID, pid);\\n\\t\\t\\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\\n\\t\\t\\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\\n\\n\\t\\t\\tif (is_child_reaper(pid)) {\\n\\t\\t\\t\\tns_of_pid(pid)->child_reaper = p;\\n\\t\\t\\t\\tp->signal->flags |= SIGNAL_UNKILLABLE;\\n\\t\\t\\t}\\n\\t\\t\\tp->signal->shared_pending.signal = delayed.signal;\\n\\t\\t\\tp->signal->tty = tty_kref_get(current->signal->tty);\\n\\t\\t\\t/*\\n\\t\\t\\t * Inherit has_child_subreaper flag under the same\\n\\t\\t\\t * tasklist_lock with adding child to the process tree\\n\\t\\t\\t * for propagate_has_child_subreaper optimization.\\n\\t\\t\\t */\\n\\t\\t\\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\\n\\t\\t\\t\\t\\t\\t\\t p->real_parent->signal->is_child_subreaper;\\n\\t\\t\\tlist_add_tail(&p->sibling, &p->real_parent->children);\\n\\t\\t\\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\\n\\t\\t\\tattach_pid(p, PIDTYPE_TGID);\\n\\t\\t\\tattach_pid(p, PIDTYPE_PGID);\\n\\t\\t\\tattach_pid(p, PIDTYPE_SID);\\n\\t\\t\\t__this_cpu_inc(process_counts);\\n\\t\\t} else {\\n\\t\\t\\tcurrent->signal->nr_threads++;\\n\\t\\t\\tcurrent->signal->quick_threads++;\\n\\t\\t\\tatomic_inc(&current->signal->live);\\n\\t\\t\\trefcount_inc(&current->signal->sigcnt);\\n\\t\\t\\ttask_join_group_stop(p);\\n\\t\\t\\tlist_add_tail_rcu(&p->thread_node,\\n\\t\\t\\t\\t\\t  &p->signal->thread_head);\\n\\t\\t}\\n\\t\\tattach_pid(p, PIDTYPE_PID);\\n\\t\\tnr_threads++;\\n\\t}\\n\\ttotal_forks++;\\n\\thlist_del_init(&delayed.node);\\n\\tspin_unlock(&current->sighand->siglock);\\n\\tsyscall_tracepoint_update(p);\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\n\\tif (pidfile)\\n\\t\\tfd_install(pidfd, pidfile);\\n\\n\\tproc_fork_connector(p);\\n\\tsched_post_fork(p);\\n\\tcgroup_post_fork(p, args);\\n\\tperf_event_fork(p);\\n\\n\\ttrace_task_newtask(p, clone_flags);\\n\\tuprobe_copy_process(p, clone_flags);\\n\\tuser_events_fork(p, clone_flags);\\n\\n\\tcopy_oom_score_adj(clone_flags, p);\\n\\n\\treturn p;\\n\\nbad_fork_core_free:\\n\\tsched_core_free(p);\\n\\tspin_unlock(&current->sighand->siglock);\\n\\twrite_unlock_irq(&tasklist_lock);\\nbad_fork_cancel_cgroup:\\n\\tcgroup_cancel_fork(p, args);\\nbad_fork_put_pidfd:\\n\\tif (clone_flags & CLONE_PIDFD) {\\n\\t\\tfput(pidfile);\\n\\t\\tput_unused_fd(pidfd);\\n\\t}\\nbad_fork_free_pid:\\n\\tif (pid != &init_struct_pid)\\n\\t\\tfree_pid(pid);\\nbad_fork_cleanup_thread:\\n\\texit_thread(p);\\nbad_fork_cleanup_io:\\n\\tif (p->io_context)\\n\\t\\texit_io_context(p);\\nbad_fork_cleanup_namespaces:\\n\\texit_task_namespaces(p);\\nbad_fork_cleanup_mm:\\n\\tif (p->mm) {\\n\\t\\tmm_clear_owner(p->mm, p);\\n\\t\\tmmput(p->mm);\\n\\t}\\nbad_fork_cleanup_signal:\\n\\tif (!(clone_flags & CLONE_THREAD))\\n\\t\\tfree_signal_struct(p->signal);\\nbad_fork_cleanup_sighand:\\n\\t__cleanup_sighand(p->sighand);\\nbad_fork_cleanup_fs:\\n\\texit_fs(p); /* blocking */\\nbad_fork_cleanup_files:\\n\\texit_files(p); /* blocking */\\nbad_fork_cleanup_semundo:\\n\\texit_sem(p);\\nbad_fork_cleanup_security:\\n\\tsecurity_task_free(p);\\nbad_fork_cleanup_audit:\\n\\taudit_free(p);\\nbad_fork_cleanup_perf:\\n\\tperf_event_free_task(p);\\nbad_fork_sched_cancel_fork:\\n\\tsched_cancel_fork(p);\\nbad_fork_cleanup_policy:\\n\\tlockdep_free_task(p);\\n#ifdef CONFIG_NUMA\\n\\tmpol_put(p->mempolicy);\\n#endif\\nbad_fork_cleanup_delayacct:\\n\\tdelayacct_tsk_free(p);\\nbad_fork_cleanup_count:\\n\\tdec_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);\\n\\texit_creds(p);\\nbad_fork_free:\\n\\tWRITE_ONCE(p->__state, TASK_DEAD);\\n\\texit_task_stack_account(p);\\n\\tput_task_stack(p);\\n\\tdelayed_free_task(p);\\nfork_out:\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\thlist_del_init(&delayed.node);\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\treturn ERR_PTR(retval);\\n}\\n\\nstatic inline void init_idle_pids(struct task_struct *idle)\\n{\\n\\tenum pid_type type;\\n\\n\\tfor (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {\\n\\t\\tINIT_HLIST_NODE(&idle->pid_links[type]); /* not really needed */\\n\\t\\tinit_task_pid(idle, type, &init_struct_pid);\\n\\t}\\n}\\n\\nstatic int idle_dummy(void *dummy)\\n{\\n\\t/* This function is never called */\\n\\treturn 0;\\n}\\n\\nstruct task_struct * __init fork_idle(int cpu)\\n{\\n\\tstruct task_struct *task;\\n\\tstruct kernel_clone_args args = {\\n\\t\\t.flags\\t\\t= CLONE_VM,\\n\\t\\t.fn\\t\\t= &idle_dummy,\\n\\t\\t.fn_arg\\t\\t= NULL,\\n\\t\\t.kthread\\t= 1,\\n\\t\\t.idle\\t\\t= 1,\\n\\t};\\n\\n\\ttask = copy_process(&init_struct_pid, 0, cpu_to_node(cpu), &args);\\n\\tif (!IS_ERR(task)) {\\n\\t\\tinit_idle_pids(task);\\n\\t\\tinit_idle(task, cpu);\\n\\t}\\n\\n\\treturn task;\\n}\\n\\n/*\\n * This is like kernel_clone(), but shaved down and tailored to just\\n * creating io_uring workers. It returns a created task, or an error pointer.\\n * The returned task is inactive, and the caller must fire it up through\\n * wake_up_new_task(p). All signals are blocked in the created task.\\n */\\nstruct task_struct *create_io_thread(int (*fn)(void *), void *arg, int node)\\n{\\n\\tunsigned long flags = CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|\\n\\t\\t\\t\\tCLONE_IO;\\n\\tstruct kernel_clone_args args = {\\n\\t\\t.flags\\t\\t= ((lower_32_bits(flags) | CLONE_VM |\\n\\t\\t\\t\\t    CLONE_UNTRACED) & ~CSIGNAL),\\n\\t\\t.exit_signal\\t= (lower_32_bits(flags) & CSIGNAL),\\n\\t\\t.fn\\t\\t= fn,\\n\\t\\t.fn_arg\\t\\t= arg,\\n\\t\\t.io_thread\\t= 1,\\n\\t\\t.user_worker\\t= 1,\\n\\t};\\n\\n\\treturn copy_process(NULL, 0, node, &args);\\n}\\n\\n/*\\n *  Ok, this is the main fork-routine.\\n *\\n * It copies the process, and if successful kick-starts\\n * it and waits for it to finish using the VM if required.\\n *\\n * args->exit_signal is expected to be checked for sanity by the caller.\\n */\\npid_t kernel_clone(struct kernel_clone_args *args)\\n{\\n\\tu64 clone_flags = args->flags;\\n\\tstruct completion vfork;\\n\\tstruct pid *pid;\\n\\tstruct task_struct *p;\\n\\tint trace = 0;\\n\\tpid_t nr;\\n\\n\\t/*\\n\\t * For legacy clone() calls, CLONE_PIDFD uses the parent_tid argument\\n\\t * to return the pidfd. Hence, CLONE_PIDFD and CLONE_PARENT_SETTID are\\n\\t * mutually exclusive. With clone3() CLONE_PIDFD has grown a separate\\n\\t * field in struct clone_args and it still doesn\\'t make sense to have\\n\\t * them both point at the same memory location. Performing this check\\n\\t * here has the advantage that we don\\'t need to have a separate helper\\n\\t * to check for legacy clone().\\n\\t */\\n\\tif ((clone_flags & CLONE_PIDFD) &&\\n\\t    (clone_flags & CLONE_PARENT_SETTID) &&\\n\\t    (args->pidfd == args->parent_tid))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Determine whether and which event to report to ptracer.  When\\n\\t * called from kernel_thread or CLONE_UNTRACED is explicitly\\n\\t * requested, no event is reported; otherwise, report if the event\\n\\t * for the type of forking is enabled.\\n\\t */\\n\\tif (!(clone_flags & CLONE_UNTRACED)) {\\n\\t\\tif (clone_flags & CLONE_VFORK)\\n\\t\\t\\ttrace = PTRACE_EVENT_VFORK;\\n\\t\\telse if (args->exit_signal != SIGCHLD)\\n\\t\\t\\ttrace = PTRACE_EVENT_CLONE;\\n\\t\\telse\\n\\t\\t\\ttrace = PTRACE_EVENT_FORK;\\n\\n\\t\\tif (likely(!ptrace_event_enabled(current, trace)))\\n\\t\\t\\ttrace = 0;\\n\\t}\\n\\n\\tp = copy_process(NULL, trace, NUMA_NO_NODE, args);\\n\\tadd_latent_entropy();\\n\\n\\tif (IS_ERR(p))\\n\\t\\treturn PTR_ERR(p);\\n\\n\\t/*\\n\\t * Do this prior waking up the new thread - the thread pointer\\n\\t * might get invalid after that point, if the thread exits quickly.\\n\\t */\\n\\ttrace_sched_process_fork(current, p);\\n\\n\\tpid = get_task_pid(p, PIDTYPE_PID);\\n\\tnr = pid_vnr(pid);\\n\\n\\tif (clone_flags & CLONE_PARENT_SETTID)\\n\\t\\tput_user(nr, args->parent_tid);\\n\\n\\tif (clone_flags & CLONE_VFORK) {\\n\\t\\tp->vfork_done = &vfork;\\n\\t\\tinit_completion(&vfork);\\n\\t\\tget_task_struct(p);\\n\\t}\\n\\n\\tif (IS_ENABLED(CONFIG_LRU_GEN_WALKS_MMU) && !(clone_flags & CLONE_VM)) {\\n\\t\\t/* lock the task to synchronize with memcg migration */\\n\\t\\ttask_lock(p);\\n\\t\\tlru_gen_add_mm(p->mm);\\n\\t\\ttask_unlock(p);\\n\\t}\\n\\n\\twake_up_new_task(p);\\n\\n\\t/* forking complete and child started to run, tell ptracer */\\n\\tif (unlikely(trace))\\n\\t\\tptrace_event_pid(trace, pid);\\n\\n\\tif (clone_flags & CLONE_VFORK) {\\n\\t\\tif (!wait_for_vfork_done(p, &vfork))\\n\\t\\t\\tptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);\\n\\t}\\n\\n\\tput_pid(pid);\\n\\treturn nr;\\n}\\n\\n/*\\n * Create a kernel thread.\\n */\\npid_t kernel_thread(int (*fn)(void *), void *arg, const char *name,\\n\\t\\t    unsigned long flags)\\n{\\n\\tstruct kernel_clone_args args = {\\n\\t\\t.flags\\t\\t= ((lower_32_bits(flags) | CLONE_VM |\\n\\t\\t\\t\\t    CLONE_UNTRACED) & ~CSIGNAL),\\n\\t\\t.exit_signal\\t= (lower_32_bits(flags) & CSIGNAL),\\n\\t\\t.fn\\t\\t= fn,\\n\\t\\t.fn_arg\\t\\t= arg,\\n\\t\\t.name\\t\\t= name,\\n\\t\\t.kthread\\t= 1,\\n\\t};\\n\\n\\treturn kernel_clone(&args);\\n}\\n\\n/*\\n * Create a user mode thread.\\n */\\npid_t user_mode_thread(int (*fn)(void *), void *arg, unsigned long flags)\\n{\\n\\tstruct kernel_clone_args args = {\\n\\t\\t.flags\\t\\t= ((lower_32_bits(flags) | CLONE_VM |\\n\\t\\t\\t\\t    CLONE_UNTRACED) & ~CSIGNAL),\\n\\t\\t.exit_signal\\t= (lower_32_bits(flags) & CSIGNAL),\\n\\t\\t.fn\\t\\t= fn,\\n\\t\\t.fn_arg\\t\\t= arg,\\n\\t};\\n\\n\\treturn kernel_clone(&args);\\n}\\n\\n#ifdef __ARCH_WANT_SYS_FORK\\nSYSCALL_DEFINE0(fork)\\n{\\n#ifdef CONFIG_MMU\\n\\tstruct kernel_clone_args args = {\\n\\t\\t.exit_signal = SIGCHLD,\\n\\t};\\n\\n\\treturn kernel_clone(&args);\\n#else\\n\\t/* can not support in nommu mode */\\n\\treturn -EINVAL;\\n#endif\\n}\\n#endif\\n\\n#ifdef __ARCH_WANT_SYS_VFORK\\nSYSCALL_DEFINE0(vfork)\\n{\\n\\tstruct kernel_clone_args args = {\\n\\t\\t.flags\\t\\t= CLONE_VFORK | CLONE_VM,\\n\\t\\t.exit_signal\\t= SIGCHLD,\\n\\t};\\n\\n\\treturn kernel_clone(&args);\\n}\\n#endif\\n\\n#ifdef __ARCH_WANT_SYS_CLONE\\n#ifdef CONFIG_CLONE_BACKWARDS\\nSYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,\\n\\t\\t int __user *, parent_tidptr,\\n\\t\\t unsigned long, tls,\\n\\t\\t int __user *, child_tidptr)\\n#elif defined(CONFIG_CLONE_BACKWARDS2)\\nSYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags,\\n\\t\\t int __user *, parent_tidptr,\\n\\t\\t int __user *, child_tidptr,\\n\\t\\t unsigned long, tls)\\n#elif defined(CONFIG_CLONE_BACKWARDS3)\\nSYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp,\\n\\t\\tint, stack_size,\\n\\t\\tint __user *, parent_tidptr,\\n\\t\\tint __user *, child_tidptr,\\n\\t\\tunsigned long, tls)\\n#else\\nSYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,\\n\\t\\t int __user *, parent_tidptr,\\n\\t\\t int __user *, child_tidptr,\\n\\t\\t unsigned long, tls)\\n#endif\\n{\\n\\tstruct kernel_clone_args args = {\\n\\t\\t.flags\\t\\t= (lower_32_bits(clone_flags) & ~CSIGNAL),\\n\\t\\t.pidfd\\t\\t= parent_tidptr,\\n\\t\\t.child_tid\\t= child_tidptr,\\n\\t\\t.parent_tid\\t= parent_tidptr,\\n\\t\\t.exit_signal\\t= (lower_32_bits(clone_flags) & CSIGNAL),\\n\\t\\t.stack\\t\\t= newsp,\\n\\t\\t.tls\\t\\t= tls,\\n\\t};\\n\\n\\treturn kernel_clone(&args);\\n}\\n#endif\\n\\nnoinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,\\n\\t\\t\\t\\t\\t      struct clone_args __user *uargs,\\n\\t\\t\\t\\t\\t      size_t usize)\\n{\\n\\tint err;\\n\\tstruct clone_args args;\\n\\tpid_t *kset_tid = kargs->set_tid;\\n\\n\\tBUILD_BUG_ON(offsetofend(struct clone_args, tls) !=\\n\\t\\t     CLONE_ARGS_SIZE_VER0);\\n\\tBUILD_BUG_ON(offsetofend(struct clone_args, set_tid_size) !=\\n\\t\\t     CLONE_ARGS_SIZE_VER1);\\n\\tBUILD_BUG_ON(offsetofend(struct clone_args, cgroup) !=\\n\\t\\t     CLONE_ARGS_SIZE_VER2);\\n\\tBUILD_BUG_ON(sizeof(struct clone_args) != CLONE_ARGS_SIZE_VER2);\\n\\n\\tif (unlikely(usize > PAGE_SIZE))\\n\\t\\treturn -E2BIG;\\n\\tif (unlikely(usize < CLONE_ARGS_SIZE_VER0))\\n\\t\\treturn -EINVAL;\\n\\n\\terr = copy_struct_from_user(&args, sizeof(args), uargs, usize);\\n\\tif (err)\\n\\t\\treturn err;\\n\\n\\tif (unlikely(args.set_tid_size > MAX_PID_NS_LEVEL))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (unlikely(!args.set_tid && args.set_tid_size > 0))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (unlikely(args.set_tid && args.set_tid_size == 0))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Verify that higher 32bits of exit_signal are unset and that\\n\\t * it is a valid signal\\n\\t */\\n\\tif (unlikely((args.exit_signal & ~((u64)CSIGNAL)) ||\\n\\t\\t     !valid_signal(args.exit_signal)))\\n\\t\\treturn -EINVAL;\\n\\n\\tif ((args.flags & CLONE_INTO_CGROUP) &&\\n\\t    (args.cgroup > INT_MAX || usize < CLONE_ARGS_SIZE_VER2))\\n\\t\\treturn -EINVAL;\\n\\n\\t*kargs = (struct kernel_clone_args){\\n\\t\\t.flags\\t\\t= args.flags,\\n\\t\\t.pidfd\\t\\t= u64_to_user_ptr(args.pidfd),\\n\\t\\t.child_tid\\t= u64_to_user_ptr(args.child_tid),\\n\\t\\t.parent_tid\\t= u64_to_user_ptr(args.parent_tid),\\n\\t\\t.exit_signal\\t= args.exit_signal,\\n\\t\\t.stack\\t\\t= args.stack,\\n\\t\\t.stack_size\\t= args.stack_size,\\n\\t\\t.tls\\t\\t= args.tls,\\n\\t\\t.set_tid_size\\t= args.set_tid_size,\\n\\t\\t.cgroup\\t\\t= args.cgroup,\\n\\t};\\n\\n\\tif (args.set_tid &&\\n\\t\\tcopy_from_user(kset_tid, u64_to_user_ptr(args.set_tid),\\n\\t\\t\\t(kargs->set_tid_size * sizeof(pid_t))))\\n\\t\\treturn -EFAULT;\\n\\n\\tkargs->set_tid = kset_tid;\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * clone3_stack_valid - check and prepare stack\\n * @kargs: kernel clone args\\n *\\n * Verify that the stack arguments userspace gave us are sane.\\n * In addition, set the stack direction for userspace since it\\'s easy for us to\\n * determine.\\n */\\nstatic inline bool clone3_stack_valid(struct kernel_clone_args *kargs)\\n{\\n\\tif (kargs->stack == 0) {\\n\\t\\tif (kargs->stack_size > 0)\\n\\t\\t\\treturn false;\\n\\t} else {\\n\\t\\tif (kargs->stack_size == 0)\\n\\t\\t\\treturn false;\\n\\n\\t\\tif (!access_ok((void __user *)kargs->stack, kargs->stack_size))\\n\\t\\t\\treturn false;\\n\\n#if !defined(CONFIG_STACK_GROWSUP)\\n\\t\\tkargs->stack += kargs->stack_size;\\n#endif\\n\\t}\\n\\n\\treturn true;\\n}\\n\\nstatic bool clone3_args_valid(struct kernel_clone_args *kargs)\\n{\\n\\t/* Verify that no unknown flags are passed along. */\\n\\tif (kargs->flags &\\n\\t    ~(CLONE_LEGACY_FLAGS | CLONE_CLEAR_SIGHAND | CLONE_INTO_CGROUP))\\n\\t\\treturn false;\\n\\n\\t/*\\n\\t * - make the CLONE_DETACHED bit reusable for clone3\\n\\t * - make the CSIGNAL bits reusable for clone3\\n\\t */\\n\\tif (kargs->flags & (CLONE_DETACHED | (CSIGNAL & (~CLONE_NEWTIME))))\\n\\t\\treturn false;\\n\\n\\tif ((kargs->flags & (CLONE_SIGHAND | CLONE_CLEAR_SIGHAND)) ==\\n\\t    (CLONE_SIGHAND | CLONE_CLEAR_SIGHAND))\\n\\t\\treturn false;\\n\\n\\tif ((kargs->flags & (CLONE_THREAD | CLONE_PARENT)) &&\\n\\t    kargs->exit_signal)\\n\\t\\treturn false;\\n\\n\\tif (!clone3_stack_valid(kargs))\\n\\t\\treturn false;\\n\\n\\treturn true;\\n}\\n\\n/**\\n * sys_clone3 - create a new process with specific properties\\n * @uargs: argument structure\\n * @size:  size of @uargs\\n *\\n * clone3() is the extensible successor to clone()/clone2().\\n * It takes a struct as argument that is versioned by its size.\\n *\\n * Return: On success, a positive PID for the child process.\\n *         On error, a negative errno number.\\n */\\nSYSCALL_DEFINE2(clone3, struct clone_args __user *, uargs, size_t, size)\\n{\\n\\tint err;\\n\\n\\tstruct kernel_clone_args kargs;\\n\\tpid_t set_tid[MAX_PID_NS_LEVEL];\\n\\n#ifdef __ARCH_BROKEN_SYS_CLONE3\\n#warning clone3() entry point is missing, please fix\\n\\treturn -ENOSYS;\\n#endif\\n\\n\\tkargs.set_tid = set_tid;\\n\\n\\terr = copy_clone_args_from_user(&kargs, uargs, size);\\n\\tif (err)\\n\\t\\treturn err;\\n\\n\\tif (!clone3_args_valid(&kargs))\\n\\t\\treturn -EINVAL;\\n\\n\\treturn kernel_clone(&kargs);\\n}\\n\\nvoid walk_process_tree(struct task_struct *top, proc_visitor visitor, void *data)\\n{\\n\\tstruct task_struct *leader, *parent, *child;\\n\\tint res;\\n\\n\\tread_lock(&tasklist_lock);\\n\\tleader = top = top->group_leader;\\ndown:\\n\\tfor_each_thread(leader, parent) {\\n\\t\\tlist_for_each_entry(child, &parent->children, sibling) {\\n\\t\\t\\tres = visitor(child, data);\\n\\t\\t\\tif (res) {\\n\\t\\t\\t\\tif (res < 0)\\n\\t\\t\\t\\t\\tgoto out;\\n\\t\\t\\t\\tleader = child;\\n\\t\\t\\t\\tgoto down;\\n\\t\\t\\t}\\nup:\\n\\t\\t\\t;\\n\\t\\t}\\n\\t}\\n\\n\\tif (leader != top) {\\n\\t\\tchild = leader;\\n\\t\\tparent = child->real_parent;\\n\\t\\tleader = parent->group_leader;\\n\\t\\tgoto up;\\n\\t}\\nout:\\n\\tread_unlock(&tasklist_lock);\\n}\\n\\n#ifndef ARCH_MIN_MMSTRUCT_ALIGN\\n#define ARCH_MIN_MMSTRUCT_ALIGN 0\\n#endif\\n\\nstatic void sighand_ctor(void *data)\\n{\\n\\tstruct sighand_struct *sighand = data;\\n\\n\\tspin_lock_init(&sighand->siglock);\\n\\tinit_waitqueue_head(&sighand->signalfd_wqh);\\n}\\n\\nvoid __init mm_cache_init(void)\\n{\\n\\tunsigned int mm_size;\\n\\n\\t/*\\n\\t * The mm_cpumask is located at the end of mm_struct, and is\\n\\t * dynamically sized based on the maximum CPU number this system\\n\\t * can have, taking hotplug into account (nr_cpu_ids).\\n\\t */\\n\\tmm_size = sizeof(struct mm_struct) + cpumask_size() + mm_cid_size();\\n\\n\\tmm_cachep = kmem_cache_create_usercopy(\"mm_struct\",\\n\\t\\t\\tmm_size, ARCH_MIN_MMSTRUCT_ALIGN,\\n\\t\\t\\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,\\n\\t\\t\\toffsetof(struct mm_struct, saved_auxv),\\n\\t\\t\\tsizeof_field(struct mm_struct, saved_auxv),\\n\\t\\t\\tNULL);\\n}\\n\\nvoid __init proc_caches_init(void)\\n{\\n\\tsighand_cachep = kmem_cache_create(\"sighand_cache\",\\n\\t\\t\\tsizeof(struct sighand_struct), 0,\\n\\t\\t\\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_TYPESAFE_BY_RCU|\\n\\t\\t\\tSLAB_ACCOUNT, sighand_ctor);\\n\\tsignal_cachep = kmem_cache_create(\"signal_cache\",\\n\\t\\t\\tsizeof(struct signal_struct), 0,\\n\\t\\t\\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,\\n\\t\\t\\tNULL);\\n\\tfiles_cachep = kmem_cache_create(\"files_cache\",\\n\\t\\t\\tsizeof(struct files_struct), 0,\\n\\t\\t\\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,\\n\\t\\t\\tNULL);\\n\\tfs_cachep = kmem_cache_create(\"fs_cache\",\\n\\t\\t\\tsizeof(struct fs_struct), 0,\\n\\t\\t\\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,\\n\\t\\t\\tNULL);\\n\\n\\tvm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_ACCOUNT);\\n#ifdef CONFIG_PER_VMA_LOCK\\n\\tvma_lock_cachep = KMEM_CACHE(vma_lock, SLAB_PANIC|SLAB_ACCOUNT);\\n#endif\\n\\tmmap_init();\\n\\tnsproxy_cache_init();\\n}\\n\\n/*\\n * Check constraints on flags passed to the unshare system call.\\n */\\nstatic int check_unshare_flags(unsigned long unshare_flags)\\n{\\n\\tif (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|\\n\\t\\t\\t\\tCLONE_VM|CLONE_FILES|CLONE_SYSVSEM|\\n\\t\\t\\t\\tCLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|\\n\\t\\t\\t\\tCLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP|\\n\\t\\t\\t\\tCLONE_NEWTIME))\\n\\t\\treturn -EINVAL;\\n\\t/*\\n\\t * Not implemented, but pretend it works if there is nothing\\n\\t * to unshare.  Note that unsharing the address space or the\\n\\t * signal handlers also need to unshare the signal queues (aka\\n\\t * CLONE_THREAD).\\n\\t */\\n\\tif (unshare_flags & (CLONE_THREAD | CLONE_SIGHAND | CLONE_VM)) {\\n\\t\\tif (!thread_group_empty(current))\\n\\t\\t\\treturn -EINVAL;\\n\\t}\\n\\tif (unshare_flags & (CLONE_SIGHAND | CLONE_VM)) {\\n\\t\\tif (refcount_read(&current->sighand->count) > 1)\\n\\t\\t\\treturn -EINVAL;\\n\\t}\\n\\tif (unshare_flags & CLONE_VM) {\\n\\t\\tif (!current_is_single_threaded())\\n\\t\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * Unshare the filesystem structure if it is being shared\\n */\\nstatic int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)\\n{\\n\\tstruct fs_struct *fs = current->fs;\\n\\n\\tif (!(unshare_flags & CLONE_FS) || !fs)\\n\\t\\treturn 0;\\n\\n\\t/* don\\'t need lock here; in the worst case we\\'ll do useless copy */\\n\\tif (fs->users == 1)\\n\\t\\treturn 0;\\n\\n\\t*new_fsp = copy_fs_struct(fs);\\n\\tif (!*new_fsp)\\n\\t\\treturn -ENOMEM;\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * Unshare file descriptor table if it is being shared\\n */\\nstatic int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp)\\n{\\n\\tstruct files_struct *fd = current->files;\\n\\n\\tif ((unshare_flags & CLONE_FILES) &&\\n\\t    (fd && atomic_read(&fd->count) > 1)) {\\n\\t\\tfd = dup_fd(fd, NULL);\\n\\t\\tif (IS_ERR(fd))\\n\\t\\t\\treturn PTR_ERR(fd);\\n\\t\\t*new_fdp = fd;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * unshare allows a process to \\'unshare\\' part of the process\\n * context which was originally shared using clone.  copy_*\\n * functions used by kernel_clone() cannot be used here directly\\n * because they modify an inactive task_struct that is being\\n * constructed. Here we are modifying the current, active,\\n * task_struct.\\n */\\nint ksys_unshare(unsigned long unshare_flags)\\n{\\n\\tstruct fs_struct *fs, *new_fs = NULL;\\n\\tstruct files_struct *new_fd = NULL;\\n\\tstruct cred *new_cred = NULL;\\n\\tstruct nsproxy *new_nsproxy = NULL;\\n\\tint do_sysvsem = 0;\\n\\tint err;\\n\\n\\t/*\\n\\t * If unsharing a user namespace must also unshare the thread group\\n\\t * and unshare the filesystem root and working directories.\\n\\t */\\n\\tif (unshare_flags & CLONE_NEWUSER)\\n\\t\\tunshare_flags |= CLONE_THREAD | CLONE_FS;\\n\\t/*\\n\\t * If unsharing vm, must also unshare signal handlers.\\n\\t */\\n\\tif (unshare_flags & CLONE_VM)\\n\\t\\tunshare_flags |= CLONE_SIGHAND;\\n\\t/*\\n\\t * If unsharing a signal handlers, must also unshare the signal queues.\\n\\t */\\n\\tif (unshare_flags & CLONE_SIGHAND)\\n\\t\\tunshare_flags |= CLONE_THREAD;\\n\\t/*\\n\\t * If unsharing namespace, must also unshare filesystem information.\\n\\t */\\n\\tif (unshare_flags & CLONE_NEWNS)\\n\\t\\tunshare_flags |= CLONE_FS;\\n\\n\\terr = check_unshare_flags(unshare_flags);\\n\\tif (err)\\n\\t\\tgoto bad_unshare_out;\\n\\t/*\\n\\t * CLONE_NEWIPC must also detach from the undolist: after switching\\n\\t * to a new ipc namespace, the semaphore arrays from the old\\n\\t * namespace are unreachable.\\n\\t */\\n\\tif (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))\\n\\t\\tdo_sysvsem = 1;\\n\\terr = unshare_fs(unshare_flags, &new_fs);\\n\\tif (err)\\n\\t\\tgoto bad_unshare_out;\\n\\terr = unshare_fd(unshare_flags, &new_fd);\\n\\tif (err)\\n\\t\\tgoto bad_unshare_cleanup_fs;\\n\\terr = unshare_userns(unshare_flags, &new_cred);\\n\\tif (err)\\n\\t\\tgoto bad_unshare_cleanup_fd;\\n\\terr = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,\\n\\t\\t\\t\\t\\t new_cred, new_fs);\\n\\tif (err)\\n\\t\\tgoto bad_unshare_cleanup_cred;\\n\\n\\tif (new_cred) {\\n\\t\\terr = set_cred_ucounts(new_cred);\\n\\t\\tif (err)\\n\\t\\t\\tgoto bad_unshare_cleanup_cred;\\n\\t}\\n\\n\\tif (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {\\n\\t\\tif (do_sysvsem) {\\n\\t\\t\\t/*\\n\\t\\t\\t * CLONE_SYSVSEM is equivalent to sys_exit().\\n\\t\\t\\t */\\n\\t\\t\\texit_sem(current);\\n\\t\\t}\\n\\t\\tif (unshare_flags & CLONE_NEWIPC) {\\n\\t\\t\\t/* Orphan segments in old ns (see sem above). */\\n\\t\\t\\texit_shm(current);\\n\\t\\t\\tshm_init_task(current);\\n\\t\\t}\\n\\n\\t\\tif (new_nsproxy)\\n\\t\\t\\tswitch_task_namespaces(current, new_nsproxy);\\n\\n\\t\\ttask_lock(current);\\n\\n\\t\\tif (new_fs) {\\n\\t\\t\\tfs = current->fs;\\n\\t\\t\\tspin_lock(&fs->lock);\\n\\t\\t\\tcurrent->fs = new_fs;\\n\\t\\t\\tif (--fs->users)\\n\\t\\t\\t\\tnew_fs = NULL;\\n\\t\\t\\telse\\n\\t\\t\\t\\tnew_fs = fs;\\n\\t\\t\\tspin_unlock(&fs->lock);\\n\\t\\t}\\n\\n\\t\\tif (new_fd)\\n\\t\\t\\tswap(current->files, new_fd);\\n\\n\\t\\ttask_unlock(current);\\n\\n\\t\\tif (new_cred) {\\n\\t\\t\\t/* Install the new user namespace */\\n\\t\\t\\tcommit_creds(new_cred);\\n\\t\\t\\tnew_cred = NULL;\\n\\t\\t}\\n\\t}\\n\\n\\tperf_event_namespaces(current);\\n\\nbad_unshare_cleanup_cred:\\n\\tif (new_cred)\\n\\t\\tput_cred(new_cred);\\nbad_unshare_cleanup_fd:\\n\\tif (new_fd)\\n\\t\\tput_files_struct(new_fd);\\n\\nbad_unshare_cleanup_fs:\\n\\tif (new_fs)\\n\\t\\tfree_fs_struct(new_fs);\\n\\nbad_unshare_out:\\n\\treturn err;\\n}\\n\\nSYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)\\n{\\n\\treturn ksys_unshare(unshare_flags);\\n}\\n\\n/*\\n *\\tHelper to unshare the files of the current task.\\n *\\tWe don\\'t want to expose copy_files internals to\\n *\\tthe exec layer of the kernel.\\n */\\n\\nint unshare_files(void)\\n{\\n\\tstruct task_struct *task = current;\\n\\tstruct files_struct *old, *copy = NULL;\\n\\tint error;\\n\\n\\terror = unshare_fd(CLONE_FILES, &copy);\\n\\tif (error || !copy)\\n\\t\\treturn error;\\n\\n\\told = task->files;\\n\\ttask_lock(task);\\n\\ttask->files = copy;\\n\\ttask_unlock(task);\\n\\tput_files_struct(old);\\n\\treturn 0;\\n}\\n\\nint sysctl_max_threads(const struct ctl_table *table, int write,\\n\\t\\t       void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct ctl_table t;\\n\\tint ret;\\n\\tint threads = max_threads;\\n\\tint min = 1;\\n\\tint max = MAX_THREADS;\\n\\n\\tt = *table;\\n\\tt.data = &threads;\\n\\tt.extra1 = &min;\\n\\tt.extra2 = &max;\\n\\n\\tret = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);\\n\\tif (ret || !write)\\n\\t\\treturn ret;\\n\\n\\tmax_threads = threads;\\n\\n\\treturn 0;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/* Task credentials management - see Documentation/security/credentials.rst\\n *\\n * Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.\\n * Written by David Howells (dhowells@redhat.com)\\n */\\n\\n#define pr_fmt(fmt) \"CRED: \" fmt\\n\\n#include <linux/export.h>\\n#include <linux/cred.h>\\n#include <linux/slab.h>\\n#include <linux/sched.h>\\n#include <linux/sched/coredump.h>\\n#include <linux/key.h>\\n#include <linux/keyctl.h>\\n#include <linux/init_task.h>\\n#include <linux/security.h>\\n#include <linux/binfmts.h>\\n#include <linux/cn_proc.h>\\n#include <linux/uidgid.h>\\n\\n#if 0\\n#define kdebug(FMT, ...)\\t\\t\\t\\t\\t\\t\\\\\\n\\tprintk(\"[%-5.5s%5u] \" FMT \"\\\\n\",\\t\\t\\t\\t\\t\\\\\\n\\t       current->comm, current->pid, ##__VA_ARGS__)\\n#else\\n#define kdebug(FMT, ...)\\t\\t\\t\\t\\t\\t\\\\\\ndo {\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\tif (0)\\t\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\t\\tno_printk(\"[%-5.5s%5u] \" FMT \"\\\\n\",\\t\\t\\t\\\\\\n\\t\\t\\t  current->comm, current->pid, ##__VA_ARGS__);\\t\\\\\\n} while (0)\\n#endif\\n\\nstatic struct kmem_cache *cred_jar;\\n\\n/* init to 2 - one for init_task, one to ensure it is never freed */\\nstatic struct group_info init_groups = { .usage = REFCOUNT_INIT(2) };\\n\\n/*\\n * The initial credentials for the initial task\\n */\\nstruct cred init_cred = {\\n\\t.usage\\t\\t\\t= ATOMIC_INIT(4),\\n\\t.uid\\t\\t\\t= GLOBAL_ROOT_UID,\\n\\t.gid\\t\\t\\t= GLOBAL_ROOT_GID,\\n\\t.suid\\t\\t\\t= GLOBAL_ROOT_UID,\\n\\t.sgid\\t\\t\\t= GLOBAL_ROOT_GID,\\n\\t.euid\\t\\t\\t= GLOBAL_ROOT_UID,\\n\\t.egid\\t\\t\\t= GLOBAL_ROOT_GID,\\n\\t.fsuid\\t\\t\\t= GLOBAL_ROOT_UID,\\n\\t.fsgid\\t\\t\\t= GLOBAL_ROOT_GID,\\n\\t.securebits\\t\\t= SECUREBITS_DEFAULT,\\n\\t.cap_inheritable\\t= CAP_EMPTY_SET,\\n\\t.cap_permitted\\t\\t= CAP_FULL_SET,\\n\\t.cap_effective\\t\\t= CAP_FULL_SET,\\n\\t.cap_bset\\t\\t= CAP_FULL_SET,\\n\\t.user\\t\\t\\t= INIT_USER,\\n\\t.user_ns\\t\\t= &init_user_ns,\\n\\t.group_info\\t\\t= &init_groups,\\n\\t.ucounts\\t\\t= &init_ucounts,\\n};\\n\\n/*\\n * The RCU callback to actually dispose of a set of credentials\\n */\\nstatic void put_cred_rcu(struct rcu_head *rcu)\\n{\\n\\tstruct cred *cred = container_of(rcu, struct cred, rcu);\\n\\n\\tkdebug(\"put_cred_rcu(%p)\", cred);\\n\\n\\tif (atomic_long_read(&cred->usage) != 0)\\n\\t\\tpanic(\"CRED: put_cred_rcu() sees %p with usage %ld\\\\n\",\\n\\t\\t      cred, atomic_long_read(&cred->usage));\\n\\n\\tsecurity_cred_free(cred);\\n\\tkey_put(cred->session_keyring);\\n\\tkey_put(cred->process_keyring);\\n\\tkey_put(cred->thread_keyring);\\n\\tkey_put(cred->request_key_auth);\\n\\tif (cred->group_info)\\n\\t\\tput_group_info(cred->group_info);\\n\\tfree_uid(cred->user);\\n\\tif (cred->ucounts)\\n\\t\\tput_ucounts(cred->ucounts);\\n\\tput_user_ns(cred->user_ns);\\n\\tkmem_cache_free(cred_jar, cred);\\n}\\n\\n/**\\n * __put_cred - Destroy a set of credentials\\n * @cred: The record to release\\n *\\n * Destroy a set of credentials on which no references remain.\\n */\\nvoid __put_cred(struct cred *cred)\\n{\\n\\tkdebug(\"__put_cred(%p{%ld})\", cred,\\n\\t       atomic_long_read(&cred->usage));\\n\\n\\tBUG_ON(atomic_long_read(&cred->usage) != 0);\\n\\tBUG_ON(cred == current->cred);\\n\\tBUG_ON(cred == current->real_cred);\\n\\n\\tif (cred->non_rcu)\\n\\t\\tput_cred_rcu(&cred->rcu);\\n\\telse\\n\\t\\tcall_rcu(&cred->rcu, put_cred_rcu);\\n}\\nEXPORT_SYMBOL(__put_cred);\\n\\n/*\\n * Clean up a task\\'s credentials when it exits\\n */\\nvoid exit_creds(struct task_struct *tsk)\\n{\\n\\tstruct cred *real_cred, *cred;\\n\\n\\tkdebug(\"exit_creds(%u,%p,%p,{%ld})\", tsk->pid, tsk->real_cred, tsk->cred,\\n\\t       atomic_long_read(&tsk->cred->usage));\\n\\n\\treal_cred = (struct cred *) tsk->real_cred;\\n\\ttsk->real_cred = NULL;\\n\\n\\tcred = (struct cred *) tsk->cred;\\n\\ttsk->cred = NULL;\\n\\n\\tif (real_cred == cred) {\\n\\t\\tput_cred_many(cred, 2);\\n\\t} else {\\n\\t\\tput_cred(real_cred);\\n\\t\\tput_cred(cred);\\n\\t}\\n\\n#ifdef CONFIG_KEYS_REQUEST_CACHE\\n\\tkey_put(tsk->cached_requested_key);\\n\\ttsk->cached_requested_key = NULL;\\n#endif\\n}\\n\\n/**\\n * get_task_cred - Get another task\\'s objective credentials\\n * @task: The task to query\\n *\\n * Get the objective credentials of a task, pinning them so that they can\\'t go\\n * away.  Accessing a task\\'s credentials directly is not permitted.\\n *\\n * The caller must also make sure task doesn\\'t get deleted, either by holding a\\n * ref on task or by holding tasklist_lock to prevent it from being unlinked.\\n */\\nconst struct cred *get_task_cred(struct task_struct *task)\\n{\\n\\tconst struct cred *cred;\\n\\n\\trcu_read_lock();\\n\\n\\tdo {\\n\\t\\tcred = __task_cred((task));\\n\\t\\tBUG_ON(!cred);\\n\\t} while (!get_cred_rcu(cred));\\n\\n\\trcu_read_unlock();\\n\\treturn cred;\\n}\\nEXPORT_SYMBOL(get_task_cred);\\n\\n/*\\n * Allocate blank credentials, such that the credentials can be filled in at a\\n * later date without risk of ENOMEM.\\n */\\nstruct cred *cred_alloc_blank(void)\\n{\\n\\tstruct cred *new;\\n\\n\\tnew = kmem_cache_zalloc(cred_jar, GFP_KERNEL);\\n\\tif (!new)\\n\\t\\treturn NULL;\\n\\n\\tatomic_long_set(&new->usage, 1);\\n\\tif (security_cred_alloc_blank(new, GFP_KERNEL_ACCOUNT) < 0)\\n\\t\\tgoto error;\\n\\n\\treturn new;\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn NULL;\\n}\\n\\n/**\\n * prepare_creds - Prepare a new set of credentials for modification\\n *\\n * Prepare a new set of task credentials for modification.  A task\\'s creds\\n * shouldn\\'t generally be modified directly, therefore this function is used to\\n * prepare a new copy, which the caller then modifies and then commits by\\n * calling commit_creds().\\n *\\n * Preparation involves making a copy of the objective creds for modification.\\n *\\n * Returns a pointer to the new creds-to-be if successful, NULL otherwise.\\n *\\n * Call commit_creds() or abort_creds() to clean up.\\n */\\nstruct cred *prepare_creds(void)\\n{\\n\\tstruct task_struct *task = current;\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\n\\tnew = kmem_cache_alloc(cred_jar, GFP_KERNEL);\\n\\tif (!new)\\n\\t\\treturn NULL;\\n\\n\\tkdebug(\"prepare_creds() alloc %p\", new);\\n\\n\\told = task->cred;\\n\\tmemcpy(new, old, sizeof(struct cred));\\n\\n\\tnew->non_rcu = 0;\\n\\tatomic_long_set(&new->usage, 1);\\n\\tget_group_info(new->group_info);\\n\\tget_uid(new->user);\\n\\tget_user_ns(new->user_ns);\\n\\n#ifdef CONFIG_KEYS\\n\\tkey_get(new->session_keyring);\\n\\tkey_get(new->process_keyring);\\n\\tkey_get(new->thread_keyring);\\n\\tkey_get(new->request_key_auth);\\n#endif\\n\\n#ifdef CONFIG_SECURITY\\n\\tnew->security = NULL;\\n#endif\\n\\n\\tnew->ucounts = get_ucounts(new->ucounts);\\n\\tif (!new->ucounts)\\n\\t\\tgoto error;\\n\\n\\tif (security_prepare_creds(new, old, GFP_KERNEL_ACCOUNT) < 0)\\n\\t\\tgoto error;\\n\\n\\treturn new;\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn NULL;\\n}\\nEXPORT_SYMBOL(prepare_creds);\\n\\n/*\\n * Prepare credentials for current to perform an execve()\\n * - The caller must hold ->cred_guard_mutex\\n */\\nstruct cred *prepare_exec_creds(void)\\n{\\n\\tstruct cred *new;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn new;\\n\\n#ifdef CONFIG_KEYS\\n\\t/* newly exec\\'d tasks don\\'t get a thread keyring */\\n\\tkey_put(new->thread_keyring);\\n\\tnew->thread_keyring = NULL;\\n\\n\\t/* inherit the session keyring; new process keyring */\\n\\tkey_put(new->process_keyring);\\n\\tnew->process_keyring = NULL;\\n#endif\\n\\n\\tnew->suid = new->fsuid = new->euid;\\n\\tnew->sgid = new->fsgid = new->egid;\\n\\n\\treturn new;\\n}\\n\\n/*\\n * Copy credentials for the new process created by fork()\\n *\\n * We share if we can, but under some circumstances we have to generate a new\\n * set.\\n *\\n * The new process gets the current process\\'s subjective credentials as its\\n * objective and subjective credentials\\n */\\nint copy_creds(struct task_struct *p, unsigned long clone_flags)\\n{\\n\\tstruct cred *new;\\n\\tint ret;\\n\\n#ifdef CONFIG_KEYS_REQUEST_CACHE\\n\\tp->cached_requested_key = NULL;\\n#endif\\n\\n\\tif (\\n#ifdef CONFIG_KEYS\\n\\t\\t!p->cred->thread_keyring &&\\n#endif\\n\\t\\tclone_flags & CLONE_THREAD\\n\\t    ) {\\n\\t\\tp->real_cred = get_cred_many(p->cred, 2);\\n\\t\\tkdebug(\"share_creds(%p{%ld})\",\\n\\t\\t       p->cred, atomic_long_read(&p->cred->usage));\\n\\t\\tinc_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\n\\tif (clone_flags & CLONE_NEWUSER) {\\n\\t\\tret = create_user_ns(new);\\n\\t\\tif (ret < 0)\\n\\t\\t\\tgoto error_put;\\n\\t\\tret = set_cred_ucounts(new);\\n\\t\\tif (ret < 0)\\n\\t\\t\\tgoto error_put;\\n\\t}\\n\\n#ifdef CONFIG_KEYS\\n\\t/* new threads get their own thread keyrings if their parent already\\n\\t * had one */\\n\\tif (new->thread_keyring) {\\n\\t\\tkey_put(new->thread_keyring);\\n\\t\\tnew->thread_keyring = NULL;\\n\\t\\tif (clone_flags & CLONE_THREAD)\\n\\t\\t\\tinstall_thread_keyring_to_cred(new);\\n\\t}\\n\\n\\t/* The process keyring is only shared between the threads in a process;\\n\\t * anything outside of those threads doesn\\'t inherit.\\n\\t */\\n\\tif (!(clone_flags & CLONE_THREAD)) {\\n\\t\\tkey_put(new->process_keyring);\\n\\t\\tnew->process_keyring = NULL;\\n\\t}\\n#endif\\n\\n\\tp->cred = p->real_cred = get_cred(new);\\n\\tinc_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);\\n\\treturn 0;\\n\\nerror_put:\\n\\tput_cred(new);\\n\\treturn ret;\\n}\\n\\nstatic bool cred_cap_issubset(const struct cred *set, const struct cred *subset)\\n{\\n\\tconst struct user_namespace *set_ns = set->user_ns;\\n\\tconst struct user_namespace *subset_ns = subset->user_ns;\\n\\n\\t/* If the two credentials are in the same user namespace see if\\n\\t * the capabilities of subset are a subset of set.\\n\\t */\\n\\tif (set_ns == subset_ns)\\n\\t\\treturn cap_issubset(subset->cap_permitted, set->cap_permitted);\\n\\n\\t/* The credentials are in a different user namespaces\\n\\t * therefore one is a subset of the other only if a set is an\\n\\t * ancestor of subset and set->euid is owner of subset or one\\n\\t * of subsets ancestors.\\n\\t */\\n\\tfor (;subset_ns != &init_user_ns; subset_ns = subset_ns->parent) {\\n\\t\\tif ((set_ns == subset_ns->parent)  &&\\n\\t\\t    uid_eq(subset_ns->owner, set->euid))\\n\\t\\t\\treturn true;\\n\\t}\\n\\n\\treturn false;\\n}\\n\\n/**\\n * commit_creds - Install new credentials upon the current task\\n * @new: The credentials to be assigned\\n *\\n * Install a new set of credentials to the current task, using RCU to replace\\n * the old set.  Both the objective and the subjective credentials pointers are\\n * updated.  This function may not be called if the subjective credentials are\\n * in an overridden state.\\n *\\n * This function eats the caller\\'s reference to the new credentials.\\n *\\n * Always returns 0 thus allowing this function to be tail-called at the end\\n * of, say, sys_setgid().\\n */\\nint commit_creds(struct cred *new)\\n{\\n\\tstruct task_struct *task = current;\\n\\tconst struct cred *old = task->real_cred;\\n\\n\\tkdebug(\"commit_creds(%p{%ld})\", new,\\n\\t       atomic_long_read(&new->usage));\\n\\n\\tBUG_ON(task->cred != old);\\n\\tBUG_ON(atomic_long_read(&new->usage) < 1);\\n\\n\\tget_cred(new); /* we will require a ref for the subj creds too */\\n\\n\\t/* dumpability changes */\\n\\tif (!uid_eq(old->euid, new->euid) ||\\n\\t    !gid_eq(old->egid, new->egid) ||\\n\\t    !uid_eq(old->fsuid, new->fsuid) ||\\n\\t    !gid_eq(old->fsgid, new->fsgid) ||\\n\\t    !cred_cap_issubset(old, new)) {\\n\\t\\tif (task->mm)\\n\\t\\t\\tset_dumpable(task->mm, suid_dumpable);\\n\\t\\ttask->pdeath_signal = 0;\\n\\t\\t/*\\n\\t\\t * If a task drops privileges and becomes nondumpable,\\n\\t\\t * the dumpability change must become visible before\\n\\t\\t * the credential change; otherwise, a __ptrace_may_access()\\n\\t\\t * racing with this change may be able to attach to a task it\\n\\t\\t * shouldn\\'t be able to attach to (as if the task had dropped\\n\\t\\t * privileges without becoming nondumpable).\\n\\t\\t * Pairs with a read barrier in __ptrace_may_access().\\n\\t\\t */\\n\\t\\tsmp_wmb();\\n\\t}\\n\\n\\t/* alter the thread keyring */\\n\\tif (!uid_eq(new->fsuid, old->fsuid))\\n\\t\\tkey_fsuid_changed(new);\\n\\tif (!gid_eq(new->fsgid, old->fsgid))\\n\\t\\tkey_fsgid_changed(new);\\n\\n\\t/* do it\\n\\t * RLIMIT_NPROC limits on user->processes have already been checked\\n\\t * in set_user().\\n\\t */\\n\\tif (new->user != old->user || new->user_ns != old->user_ns)\\n\\t\\tinc_rlimit_ucounts(new->ucounts, UCOUNT_RLIMIT_NPROC, 1);\\n\\trcu_assign_pointer(task->real_cred, new);\\n\\trcu_assign_pointer(task->cred, new);\\n\\tif (new->user != old->user || new->user_ns != old->user_ns)\\n\\t\\tdec_rlimit_ucounts(old->ucounts, UCOUNT_RLIMIT_NPROC, 1);\\n\\n\\t/* send notifications */\\n\\tif (!uid_eq(new->uid,   old->uid)  ||\\n\\t    !uid_eq(new->euid,  old->euid) ||\\n\\t    !uid_eq(new->suid,  old->suid) ||\\n\\t    !uid_eq(new->fsuid, old->fsuid))\\n\\t\\tproc_id_connector(task, PROC_EVENT_UID);\\n\\n\\tif (!gid_eq(new->gid,   old->gid)  ||\\n\\t    !gid_eq(new->egid,  old->egid) ||\\n\\t    !gid_eq(new->sgid,  old->sgid) ||\\n\\t    !gid_eq(new->fsgid, old->fsgid))\\n\\t\\tproc_id_connector(task, PROC_EVENT_GID);\\n\\n\\t/* release the old obj and subj refs both */\\n\\tput_cred_many(old, 2);\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(commit_creds);\\n\\n/**\\n * abort_creds - Discard a set of credentials and unlock the current task\\n * @new: The credentials that were going to be applied\\n *\\n * Discard a set of credentials that were under construction and unlock the\\n * current task.\\n */\\nvoid abort_creds(struct cred *new)\\n{\\n\\tkdebug(\"abort_creds(%p{%ld})\", new,\\n\\t       atomic_long_read(&new->usage));\\n\\n\\tBUG_ON(atomic_long_read(&new->usage) < 1);\\n\\tput_cred(new);\\n}\\nEXPORT_SYMBOL(abort_creds);\\n\\n/**\\n * override_creds - Override the current process\\'s subjective credentials\\n * @new: The credentials to be assigned\\n *\\n * Install a set of temporary override subjective credentials on the current\\n * process, returning the old set for later reversion.\\n */\\nconst struct cred *override_creds(const struct cred *new)\\n{\\n\\tconst struct cred *old;\\n\\n\\tkdebug(\"override_creds(%p{%ld})\", new,\\n\\t       atomic_long_read(&new->usage));\\n\\n\\t/*\\n\\t * NOTE! This uses \\'get_new_cred()\\' rather than \\'get_cred()\\'.\\n\\t *\\n\\t * That means that we do not clear the \\'non_rcu\\' flag, since\\n\\t * we are only installing the cred into the thread-synchronous\\n\\t * \\'->cred\\' pointer, not the \\'->real_cred\\' pointer that is\\n\\t * visible to other threads under RCU.\\n\\t */\\n\\tget_new_cred((struct cred *)new);\\n\\told = override_creds_light(new);\\n\\n\\tkdebug(\"override_creds() = %p{%ld}\", old,\\n\\t       atomic_long_read(&old->usage));\\n\\treturn old;\\n}\\nEXPORT_SYMBOL(override_creds);\\n\\n/**\\n * revert_creds - Revert a temporary subjective credentials override\\n * @old: The credentials to be restored\\n *\\n * Revert a temporary set of override subjective credentials to an old set,\\n * discarding the override set.\\n */\\nvoid revert_creds(const struct cred *old)\\n{\\n\\tconst struct cred *override = current->cred;\\n\\n\\tkdebug(\"revert_creds(%p{%ld})\", old,\\n\\t       atomic_long_read(&old->usage));\\n\\n\\trevert_creds_light(old);\\n\\tput_cred(override);\\n}\\nEXPORT_SYMBOL(revert_creds);\\n\\n/**\\n * cred_fscmp - Compare two credentials with respect to filesystem access.\\n * @a: The first credential\\n * @b: The second credential\\n *\\n * cred_cmp() will return zero if both credentials have the same\\n * fsuid, fsgid, and supplementary groups.  That is, if they will both\\n * provide the same access to files based on mode/uid/gid.\\n * If the credentials are different, then either -1 or 1 will\\n * be returned depending on whether @a comes before or after @b\\n * respectively in an arbitrary, but stable, ordering of credentials.\\n *\\n * Return: -1, 0, or 1 depending on comparison\\n */\\nint cred_fscmp(const struct cred *a, const struct cred *b)\\n{\\n\\tstruct group_info *ga, *gb;\\n\\tint g;\\n\\n\\tif (a == b)\\n\\t\\treturn 0;\\n\\tif (uid_lt(a->fsuid, b->fsuid))\\n\\t\\treturn -1;\\n\\tif (uid_gt(a->fsuid, b->fsuid))\\n\\t\\treturn 1;\\n\\n\\tif (gid_lt(a->fsgid, b->fsgid))\\n\\t\\treturn -1;\\n\\tif (gid_gt(a->fsgid, b->fsgid))\\n\\t\\treturn 1;\\n\\n\\tga = a->group_info;\\n\\tgb = b->group_info;\\n\\tif (ga == gb)\\n\\t\\treturn 0;\\n\\tif (ga == NULL)\\n\\t\\treturn -1;\\n\\tif (gb == NULL)\\n\\t\\treturn 1;\\n\\tif (ga->ngroups < gb->ngroups)\\n\\t\\treturn -1;\\n\\tif (ga->ngroups > gb->ngroups)\\n\\t\\treturn 1;\\n\\n\\tfor (g = 0; g < ga->ngroups; g++) {\\n\\t\\tif (gid_lt(ga->gid[g], gb->gid[g]))\\n\\t\\t\\treturn -1;\\n\\t\\tif (gid_gt(ga->gid[g], gb->gid[g]))\\n\\t\\t\\treturn 1;\\n\\t}\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(cred_fscmp);\\n\\nint set_cred_ucounts(struct cred *new)\\n{\\n\\tstruct ucounts *new_ucounts, *old_ucounts = new->ucounts;\\n\\n\\t/*\\n\\t * This optimization is needed because alloc_ucounts() uses locks\\n\\t * for table lookups.\\n\\t */\\n\\tif (old_ucounts->ns == new->user_ns && uid_eq(old_ucounts->uid, new->uid))\\n\\t\\treturn 0;\\n\\n\\tif (!(new_ucounts = alloc_ucounts(new->user_ns, new->uid)))\\n\\t\\treturn -EAGAIN;\\n\\n\\tnew->ucounts = new_ucounts;\\n\\tput_ucounts(old_ucounts);\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * initialise the credentials stuff\\n */\\nvoid __init cred_init(void)\\n{\\n\\t/* allocate a slab in which we can store credentials */\\n\\tcred_jar = KMEM_CACHE(cred,\\n\\t\\t\\t      SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT);\\n}\\n\\n/**\\n * prepare_kernel_cred - Prepare a set of credentials for a kernel service\\n * @daemon: A userspace daemon to be used as a reference\\n *\\n * Prepare a set of credentials for a kernel service.  This can then be used to\\n * override a task\\'s own credentials so that work can be done on behalf of that\\n * task that requires a different subjective context.\\n *\\n * @daemon is used to provide a base cred, with the security data derived from\\n * that; if this is \"&init_task\", they\\'ll be set to 0, no groups, full\\n * capabilities, and no keys.\\n *\\n * The caller may change these controls afterwards if desired.\\n *\\n * Returns the new credentials or NULL if out of memory.\\n */\\nstruct cred *prepare_kernel_cred(struct task_struct *daemon)\\n{\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\n\\tif (WARN_ON_ONCE(!daemon))\\n\\t\\treturn NULL;\\n\\n\\tnew = kmem_cache_alloc(cred_jar, GFP_KERNEL);\\n\\tif (!new)\\n\\t\\treturn NULL;\\n\\n\\tkdebug(\"prepare_kernel_cred() alloc %p\", new);\\n\\n\\told = get_task_cred(daemon);\\n\\n\\t*new = *old;\\n\\tnew->non_rcu = 0;\\n\\tatomic_long_set(&new->usage, 1);\\n\\tget_uid(new->user);\\n\\tget_user_ns(new->user_ns);\\n\\tget_group_info(new->group_info);\\n\\n#ifdef CONFIG_KEYS\\n\\tnew->session_keyring = NULL;\\n\\tnew->process_keyring = NULL;\\n\\tnew->thread_keyring = NULL;\\n\\tnew->request_key_auth = NULL;\\n\\tnew->jit_keyring = KEY_REQKEY_DEFL_THREAD_KEYRING;\\n#endif\\n\\n#ifdef CONFIG_SECURITY\\n\\tnew->security = NULL;\\n#endif\\n\\tnew->ucounts = get_ucounts(new->ucounts);\\n\\tif (!new->ucounts)\\n\\t\\tgoto error;\\n\\n\\tif (security_prepare_creds(new, old, GFP_KERNEL_ACCOUNT) < 0)\\n\\t\\tgoto error;\\n\\n\\tput_cred(old);\\n\\treturn new;\\n\\nerror:\\n\\tput_cred(new);\\n\\tput_cred(old);\\n\\treturn NULL;\\n}\\nEXPORT_SYMBOL(prepare_kernel_cred);\\n\\n/**\\n * set_security_override - Set the security ID in a set of credentials\\n * @new: The credentials to alter\\n * @secid: The LSM security ID to set\\n *\\n * Set the LSM security ID in a set of credentials so that the subjective\\n * security is overridden when an alternative set of credentials is used.\\n */\\nint set_security_override(struct cred *new, u32 secid)\\n{\\n\\treturn security_kernel_act_as(new, secid);\\n}\\nEXPORT_SYMBOL(set_security_override);\\n\\n/**\\n * set_security_override_from_ctx - Set the security ID in a set of credentials\\n * @new: The credentials to alter\\n * @secctx: The LSM security context to generate the security ID from.\\n *\\n * Set the LSM security ID in a set of credentials so that the subjective\\n * security is overridden when an alternative set of credentials is used.  The\\n * security ID is specified in string form as a security context to be\\n * interpreted by the LSM.\\n */\\nint set_security_override_from_ctx(struct cred *new, const char *secctx)\\n{\\n\\tu32 secid;\\n\\tint ret;\\n\\n\\tret = security_secctx_to_secid(secctx, strlen(secctx), &secid);\\n\\tif (ret < 0)\\n\\t\\treturn ret;\\n\\n\\treturn set_security_override(new, secid);\\n}\\nEXPORT_SYMBOL(set_security_override_from_ctx);\\n\\n/**\\n * set_create_files_as - Set the LSM file create context in a set of credentials\\n * @new: The credentials to alter\\n * @inode: The inode to take the context from\\n *\\n * Change the LSM file creation context in a set of credentials to be the same\\n * as the object context of the specified inode, so that the new inodes have\\n * the same MAC context as that inode.\\n */\\nint set_create_files_as(struct cred *new, struct inode *inode)\\n{\\n\\tif (!uid_valid(inode->i_uid) || !gid_valid(inode->i_gid))\\n\\t\\treturn -EINVAL;\\n\\tnew->fsuid = inode->i_uid;\\n\\tnew->fsgid = inode->i_gid;\\n\\treturn security_kernel_create_files_as(new, inode);\\n}\\nEXPORT_SYMBOL(set_create_files_as);\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/* delayacct.c - per-task delay accounting\\n *\\n * Copyright (C) Shailabh Nagar, IBM Corp. 2006\\n */\\n\\n#include <linux/sched.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/sched/clock.h>\\n#include <linux/slab.h>\\n#include <linux/taskstats.h>\\n#include <linux/sysctl.h>\\n#include <linux/delayacct.h>\\n#include <linux/module.h>\\n\\nDEFINE_STATIC_KEY_FALSE(delayacct_key);\\nint delayacct_on __read_mostly;\\t/* Delay accounting turned on/off */\\nstruct kmem_cache *delayacct_cache;\\n\\nstatic void set_delayacct(bool enabled)\\n{\\n\\tif (enabled) {\\n\\t\\tstatic_branch_enable(&delayacct_key);\\n\\t\\tdelayacct_on = 1;\\n\\t} else {\\n\\t\\tdelayacct_on = 0;\\n\\t\\tstatic_branch_disable(&delayacct_key);\\n\\t}\\n}\\n\\nstatic int __init delayacct_setup_enable(char *str)\\n{\\n\\tdelayacct_on = 1;\\n\\treturn 1;\\n}\\n__setup(\"delayacct\", delayacct_setup_enable);\\n\\nvoid delayacct_init(void)\\n{\\n\\tdelayacct_cache = KMEM_CACHE(task_delay_info, SLAB_PANIC|SLAB_ACCOUNT);\\n\\tdelayacct_tsk_init(&init_task);\\n\\tset_delayacct(delayacct_on);\\n}\\n\\n#ifdef CONFIG_PROC_SYSCTL\\nstatic int sysctl_delayacct(const struct ctl_table *table, int write, void *buffer,\\n\\t\\t     size_t *lenp, loff_t *ppos)\\n{\\n\\tint state = delayacct_on;\\n\\tstruct ctl_table t;\\n\\tint err;\\n\\n\\tif (write && !capable(CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tt = *table;\\n\\tt.data = &state;\\n\\terr = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);\\n\\tif (err < 0)\\n\\t\\treturn err;\\n\\tif (write)\\n\\t\\tset_delayacct(state);\\n\\treturn err;\\n}\\n\\nstatic struct ctl_table kern_delayacct_table[] = {\\n\\t{\\n\\t\\t.procname       = \"task_delayacct\",\\n\\t\\t.data           = NULL,\\n\\t\\t.maxlen         = sizeof(unsigned int),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = sysctl_delayacct,\\n\\t\\t.extra1         = SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE,\\n\\t},\\n};\\n\\nstatic __init int kernel_delayacct_sysctls_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", kern_delayacct_table);\\n\\treturn 0;\\n}\\nlate_initcall(kernel_delayacct_sysctls_init);\\n#endif\\n\\nvoid __delayacct_tsk_init(struct task_struct *tsk)\\n{\\n\\ttsk->delays = kmem_cache_zalloc(delayacct_cache, GFP_KERNEL);\\n\\tif (tsk->delays)\\n\\t\\traw_spin_lock_init(&tsk->delays->lock);\\n}\\n\\n/*\\n * Finish delay accounting for a statistic using its timestamps (@start),\\n * accumalator (@total) and @count\\n */\\nstatic void delayacct_end(raw_spinlock_t *lock, u64 *start, u64 *total, u32 *count)\\n{\\n\\ts64 ns = local_clock() - *start;\\n\\tunsigned long flags;\\n\\n\\tif (ns > 0) {\\n\\t\\traw_spin_lock_irqsave(lock, flags);\\n\\t\\t*total += ns;\\n\\t\\t(*count)++;\\n\\t\\traw_spin_unlock_irqrestore(lock, flags);\\n\\t}\\n}\\n\\nvoid __delayacct_blkio_start(void)\\n{\\n\\tcurrent->delays->blkio_start = local_clock();\\n}\\n\\n/*\\n * We cannot rely on the `current` macro, as we haven\\'t yet switched back to\\n * the process being woken.\\n */\\nvoid __delayacct_blkio_end(struct task_struct *p)\\n{\\n\\tdelayacct_end(&p->delays->lock,\\n\\t\\t      &p->delays->blkio_start,\\n\\t\\t      &p->delays->blkio_delay,\\n\\t\\t      &p->delays->blkio_count);\\n}\\n\\nint delayacct_add_tsk(struct taskstats *d, struct task_struct *tsk)\\n{\\n\\tu64 utime, stime, stimescaled, utimescaled;\\n\\tunsigned long long t2, t3;\\n\\tunsigned long flags, t1;\\n\\ts64 tmp;\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\ttmp = (s64)d->cpu_run_real_total;\\n\\ttmp += utime + stime;\\n\\td->cpu_run_real_total = (tmp < (s64)d->cpu_run_real_total) ? 0 : tmp;\\n\\n\\ttask_cputime_scaled(tsk, &utimescaled, &stimescaled);\\n\\ttmp = (s64)d->cpu_scaled_run_real_total;\\n\\ttmp += utimescaled + stimescaled;\\n\\td->cpu_scaled_run_real_total =\\n\\t\\t(tmp < (s64)d->cpu_scaled_run_real_total) ? 0 : tmp;\\n\\n\\t/*\\n\\t * No locking available for sched_info (and too expensive to add one)\\n\\t * Mitigate by taking snapshot of values\\n\\t */\\n\\tt1 = tsk->sched_info.pcount;\\n\\tt2 = tsk->sched_info.run_delay;\\n\\tt3 = tsk->se.sum_exec_runtime;\\n\\n\\td->cpu_count += t1;\\n\\n\\ttmp = (s64)d->cpu_delay_total + t2;\\n\\td->cpu_delay_total = (tmp < (s64)d->cpu_delay_total) ? 0 : tmp;\\n\\n\\ttmp = (s64)d->cpu_run_virtual_total + t3;\\n\\td->cpu_run_virtual_total =\\n\\t\\t(tmp < (s64)d->cpu_run_virtual_total) ?\\t0 : tmp;\\n\\n\\tif (!tsk->delays)\\n\\t\\treturn 0;\\n\\n\\t/* zero XXX_total, non-zero XXX_count implies XXX stat overflowed */\\n\\n\\traw_spin_lock_irqsave(&tsk->delays->lock, flags);\\n\\ttmp = d->blkio_delay_total + tsk->delays->blkio_delay;\\n\\td->blkio_delay_total = (tmp < d->blkio_delay_total) ? 0 : tmp;\\n\\ttmp = d->swapin_delay_total + tsk->delays->swapin_delay;\\n\\td->swapin_delay_total = (tmp < d->swapin_delay_total) ? 0 : tmp;\\n\\ttmp = d->freepages_delay_total + tsk->delays->freepages_delay;\\n\\td->freepages_delay_total = (tmp < d->freepages_delay_total) ? 0 : tmp;\\n\\ttmp = d->thrashing_delay_total + tsk->delays->thrashing_delay;\\n\\td->thrashing_delay_total = (tmp < d->thrashing_delay_total) ? 0 : tmp;\\n\\ttmp = d->compact_delay_total + tsk->delays->compact_delay;\\n\\td->compact_delay_total = (tmp < d->compact_delay_total) ? 0 : tmp;\\n\\ttmp = d->wpcopy_delay_total + tsk->delays->wpcopy_delay;\\n\\td->wpcopy_delay_total = (tmp < d->wpcopy_delay_total) ? 0 : tmp;\\n\\ttmp = d->irq_delay_total + tsk->delays->irq_delay;\\n\\td->irq_delay_total = (tmp < d->irq_delay_total) ? 0 : tmp;\\n\\td->blkio_count += tsk->delays->blkio_count;\\n\\td->swapin_count += tsk->delays->swapin_count;\\n\\td->freepages_count += tsk->delays->freepages_count;\\n\\td->thrashing_count += tsk->delays->thrashing_count;\\n\\td->compact_count += tsk->delays->compact_count;\\n\\td->wpcopy_count += tsk->delays->wpcopy_count;\\n\\td->irq_count += tsk->delays->irq_count;\\n\\traw_spin_unlock_irqrestore(&tsk->delays->lock, flags);\\n\\n\\treturn 0;\\n}\\n\\n__u64 __delayacct_blkio_ticks(struct task_struct *tsk)\\n{\\n\\t__u64 ret;\\n\\tunsigned long flags;\\n\\n\\traw_spin_lock_irqsave(&tsk->delays->lock, flags);\\n\\tret = nsec_to_clock_t(tsk->delays->blkio_delay);\\n\\traw_spin_unlock_irqrestore(&tsk->delays->lock, flags);\\n\\treturn ret;\\n}\\n\\nvoid __delayacct_freepages_start(void)\\n{\\n\\tcurrent->delays->freepages_start = local_clock();\\n}\\n\\nvoid __delayacct_freepages_end(void)\\n{\\n\\tdelayacct_end(&current->delays->lock,\\n\\t\\t      &current->delays->freepages_start,\\n\\t\\t      &current->delays->freepages_delay,\\n\\t\\t      &current->delays->freepages_count);\\n}\\n\\nvoid __delayacct_thrashing_start(bool *in_thrashing)\\n{\\n\\t*in_thrashing = !!current->in_thrashing;\\n\\tif (*in_thrashing)\\n\\t\\treturn;\\n\\n\\tcurrent->in_thrashing = 1;\\n\\tcurrent->delays->thrashing_start = local_clock();\\n}\\n\\nvoid __delayacct_thrashing_end(bool *in_thrashing)\\n{\\n\\tif (*in_thrashing)\\n\\t\\treturn;\\n\\n\\tcurrent->in_thrashing = 0;\\n\\tdelayacct_end(&current->delays->lock,\\n\\t\\t      &current->delays->thrashing_start,\\n\\t\\t      &current->delays->thrashing_delay,\\n\\t\\t      &current->delays->thrashing_count);\\n}\\n\\nvoid __delayacct_swapin_start(void)\\n{\\n\\tcurrent->delays->swapin_start = local_clock();\\n}\\n\\nvoid __delayacct_swapin_end(void)\\n{\\n\\tdelayacct_end(&current->delays->lock,\\n\\t\\t      &current->delays->swapin_start,\\n\\t\\t      &current->delays->swapin_delay,\\n\\t\\t      &current->delays->swapin_count);\\n}\\n\\nvoid __delayacct_compact_start(void)\\n{\\n\\tcurrent->delays->compact_start = local_clock();\\n}\\n\\nvoid __delayacct_compact_end(void)\\n{\\n\\tdelayacct_end(&current->delays->lock,\\n\\t\\t      &current->delays->compact_start,\\n\\t\\t      &current->delays->compact_delay,\\n\\t\\t      &current->delays->compact_count);\\n}\\n\\nvoid __delayacct_wpcopy_start(void)\\n{\\n\\tcurrent->delays->wpcopy_start = local_clock();\\n}\\n\\nvoid __delayacct_wpcopy_end(void)\\n{\\n\\tdelayacct_end(&current->delays->lock,\\n\\t\\t      &current->delays->wpcopy_start,\\n\\t\\t      &current->delays->wpcopy_delay,\\n\\t\\t      &current->delays->wpcopy_count);\\n}\\n\\nvoid __delayacct_irq(struct task_struct *task, u32 delta)\\n{\\n\\tunsigned long flags;\\n\\n\\traw_spin_lock_irqsave(&task->delays->lock, flags);\\n\\ttask->delays->irq_delay += delta;\\n\\ttask->delays->irq_count++;\\n\\traw_spin_unlock_irqrestore(&task->delays->lock, flags);\\n}\\n\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/*\\n * Test the function and performance of kallsyms\\n *\\n * Copyright (C) Huawei Technologies Co., Ltd., 2022\\n *\\n * Authors: Zhen Lei <thunder.leizhen@huawei.com> Huawei\\n */\\n\\n#define pr_fmt(fmt) \"kallsyms_selftest: \" fmt\\n\\n#include <linux/init.h>\\n#include <linux/module.h>\\n#include <linux/kallsyms.h>\\n#include <linux/random.h>\\n#include <linux/sched/clock.h>\\n#include <linux/kthread.h>\\n#include <linux/vmalloc.h>\\n\\n#include \"kallsyms_internal.h\"\\n#include \"kallsyms_selftest.h\"\\n\\n\\n#define MAX_NUM_OF_RECORDS\\t\\t64\\n\\nstruct test_stat {\\n\\tint min;\\n\\tint max;\\n\\tint save_cnt;\\n\\tint real_cnt;\\n\\tint perf;\\n\\tu64 sum;\\n\\tchar *name;\\n\\tunsigned long addr;\\n\\tunsigned long addrs[MAX_NUM_OF_RECORDS];\\n};\\n\\nstruct test_item {\\n\\tchar *name;\\n\\tunsigned long addr;\\n};\\n\\n#define ITEM_FUNC(s)\\t\\t\\t\\t\\\\\\n\\t{\\t\\t\\t\\t\\t\\\\\\n\\t\\t.name = #s,\\t\\t\\t\\\\\\n\\t\\t.addr = (unsigned long)s,\\t\\\\\\n\\t}\\n\\n#define ITEM_DATA(s)\\t\\t\\t\\t\\\\\\n\\t{\\t\\t\\t\\t\\t\\\\\\n\\t\\t.name = #s,\\t\\t\\t\\\\\\n\\t\\t.addr = (unsigned long)&s,\\t\\\\\\n\\t}\\n\\n\\nstatic int kallsyms_test_var_bss_static;\\nstatic int kallsyms_test_var_data_static = 1;\\nint kallsyms_test_var_bss;\\nint kallsyms_test_var_data = 1;\\n\\nstatic int kallsyms_test_func_static(void)\\n{\\n\\tkallsyms_test_var_bss_static++;\\n\\tkallsyms_test_var_data_static++;\\n\\n\\treturn 0;\\n}\\n\\nint kallsyms_test_func(void)\\n{\\n\\treturn kallsyms_test_func_static();\\n}\\n\\n__weak int kallsyms_test_func_weak(void)\\n{\\n\\tkallsyms_test_var_bss++;\\n\\tkallsyms_test_var_data++;\\n\\treturn 0;\\n}\\n\\nstatic struct test_item test_items[] = {\\n\\tITEM_FUNC(kallsyms_test_func_static),\\n\\tITEM_FUNC(kallsyms_test_func),\\n\\tITEM_FUNC(kallsyms_test_func_weak),\\n\\tITEM_FUNC(vmalloc_noprof),\\n\\tITEM_FUNC(vfree),\\n#ifdef CONFIG_KALLSYMS_ALL\\n\\tITEM_DATA(kallsyms_test_var_bss_static),\\n\\tITEM_DATA(kallsyms_test_var_data_static),\\n\\tITEM_DATA(kallsyms_test_var_bss),\\n\\tITEM_DATA(kallsyms_test_var_data),\\n#endif\\n};\\n\\nstatic char stub_name[KSYM_NAME_LEN];\\n\\nstatic int stat_symbol_len(void *data, const char *name, unsigned long addr)\\n{\\n\\t*(u32 *)data += strlen(name);\\n\\n\\treturn 0;\\n}\\n\\nstatic void test_kallsyms_compression_ratio(void)\\n{\\n\\tu32 pos, off, len, num;\\n\\tu32 ratio, total_size, total_len = 0;\\n\\n\\tkallsyms_on_each_symbol(stat_symbol_len, &total_len);\\n\\n\\t/*\\n\\t * A symbol name cannot start with a number. This stub name helps us\\n\\t * traverse the entire symbol table without finding a match. It\\'s used\\n\\t * for subsequent performance tests, and its length is the average\\n\\t * length of all symbol names.\\n\\t */\\n\\tmemset(stub_name, \\'4\\', sizeof(stub_name));\\n\\tpos = total_len / kallsyms_num_syms;\\n\\tstub_name[pos] = 0;\\n\\n\\tpos = 0;\\n\\tnum = 0;\\n\\toff = 0;\\n\\twhile (pos < kallsyms_num_syms) {\\n\\t\\tlen = kallsyms_names[off];\\n\\t\\tnum++;\\n\\t\\toff++;\\n\\t\\tpos++;\\n\\t\\tif ((len & 0x80) != 0) {\\n\\t\\t\\tlen = (len & 0x7f) | (kallsyms_names[off] << 7);\\n\\t\\t\\tnum++;\\n\\t\\t\\toff++;\\n\\t\\t}\\n\\t\\toff += len;\\n\\t}\\n\\n\\t/*\\n\\t * 1. The length fields is not counted\\n\\t * 2. The memory occupied by array kallsyms_token_table[] and\\n\\t *    kallsyms_token_index[] needs to be counted.\\n\\t */\\n\\ttotal_size = off - num;\\n\\tpos = kallsyms_token_index[0xff];\\n\\ttotal_size += pos + strlen(&kallsyms_token_table[pos]) + 1;\\n\\ttotal_size += 0x100 * sizeof(u16);\\n\\n\\tpr_info(\" ---------------------------------------------------------\\\\n\");\\n\\tpr_info(\"| nr_symbols | compressed size | original size | ratio(%%) |\\\\n\");\\n\\tpr_info(\"|---------------------------------------------------------|\\\\n\");\\n\\tratio = (u32)div_u64(10000ULL * total_size, total_len);\\n\\tpr_info(\"| %10d |    %10d   |   %10d  |  %2d.%-2d   |\\\\n\",\\n\\t\\tkallsyms_num_syms, total_size, total_len, ratio / 100, ratio % 100);\\n\\tpr_info(\" ---------------------------------------------------------\\\\n\");\\n}\\n\\nstatic int lookup_name(void *data, const char *name, unsigned long addr)\\n{\\n\\tu64 t0, t1, t;\\n\\tstruct test_stat *stat = (struct test_stat *)data;\\n\\n\\tt0 = ktime_get_ns();\\n\\t(void)kallsyms_lookup_name(name);\\n\\tt1 = ktime_get_ns();\\n\\n\\tt = t1 - t0;\\n\\tif (t < stat->min)\\n\\t\\tstat->min = t;\\n\\n\\tif (t > stat->max)\\n\\t\\tstat->max = t;\\n\\n\\tstat->real_cnt++;\\n\\tstat->sum += t;\\n\\n\\treturn 0;\\n}\\n\\nstatic void test_perf_kallsyms_lookup_name(void)\\n{\\n\\tstruct test_stat stat;\\n\\n\\tmemset(&stat, 0, sizeof(stat));\\n\\tstat.min = INT_MAX;\\n\\tkallsyms_on_each_symbol(lookup_name, &stat);\\n\\tpr_info(\"kallsyms_lookup_name() looked up %d symbols\\\\n\", stat.real_cnt);\\n\\tpr_info(\"The time spent on each symbol is (ns): min=%d, max=%d, avg=%lld\\\\n\",\\n\\t\\tstat.min, stat.max, div_u64(stat.sum, stat.real_cnt));\\n}\\n\\nstatic int find_symbol(void *data, const char *name, unsigned long addr)\\n{\\n\\tstruct test_stat *stat = (struct test_stat *)data;\\n\\n\\tif (!strcmp(name, stat->name)) {\\n\\t\\tstat->real_cnt++;\\n\\t\\tstat->addr = addr;\\n\\n\\t\\tif (stat->save_cnt < MAX_NUM_OF_RECORDS) {\\n\\t\\t\\tstat->addrs[stat->save_cnt] = addr;\\n\\t\\t\\tstat->save_cnt++;\\n\\t\\t}\\n\\n\\t\\tif (stat->real_cnt == stat->max)\\n\\t\\t\\treturn 1;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic void test_perf_kallsyms_on_each_symbol(void)\\n{\\n\\tu64 t0, t1;\\n\\tstruct test_stat stat;\\n\\n\\tmemset(&stat, 0, sizeof(stat));\\n\\tstat.max = INT_MAX;\\n\\tstat.name = stub_name;\\n\\tstat.perf = 1;\\n\\tt0 = ktime_get_ns();\\n\\tkallsyms_on_each_symbol(find_symbol, &stat);\\n\\tt1 = ktime_get_ns();\\n\\tpr_info(\"kallsyms_on_each_symbol() traverse all: %lld ns\\\\n\", t1 - t0);\\n}\\n\\nstatic int match_symbol(void *data, unsigned long addr)\\n{\\n\\tstruct test_stat *stat = (struct test_stat *)data;\\n\\n\\tstat->real_cnt++;\\n\\tstat->addr = addr;\\n\\n\\tif (stat->save_cnt < MAX_NUM_OF_RECORDS) {\\n\\t\\tstat->addrs[stat->save_cnt] = addr;\\n\\t\\tstat->save_cnt++;\\n\\t}\\n\\n\\tif (stat->real_cnt == stat->max)\\n\\t\\treturn 1;\\n\\n\\treturn 0;\\n}\\n\\nstatic void test_perf_kallsyms_on_each_match_symbol(void)\\n{\\n\\tu64 t0, t1;\\n\\tstruct test_stat stat;\\n\\n\\tmemset(&stat, 0, sizeof(stat));\\n\\tstat.max = INT_MAX;\\n\\tstat.name = stub_name;\\n\\tt0 = ktime_get_ns();\\n\\tkallsyms_on_each_match_symbol(match_symbol, stat.name, &stat);\\n\\tt1 = ktime_get_ns();\\n\\tpr_info(\"kallsyms_on_each_match_symbol() traverse all: %lld ns\\\\n\", t1 - t0);\\n}\\n\\nstatic int test_kallsyms_basic_function(void)\\n{\\n\\tint i, j, ret;\\n\\tint next = 0, nr_failed = 0;\\n\\tchar *prefix;\\n\\tunsigned short rand;\\n\\tunsigned long addr, lookup_addr;\\n\\tchar namebuf[KSYM_NAME_LEN];\\n\\tstruct test_stat *stat, *stat2;\\n\\n\\tstat = kmalloc(sizeof(*stat) * 2, GFP_KERNEL);\\n\\tif (!stat)\\n\\t\\treturn -ENOMEM;\\n\\tstat2 = stat + 1;\\n\\n\\tprefix = \"kallsyms_lookup_name() for\";\\n\\tfor (i = 0; i < ARRAY_SIZE(test_items); i++) {\\n\\t\\taddr = kallsyms_lookup_name(test_items[i].name);\\n\\t\\tif (addr != test_items[i].addr) {\\n\\t\\t\\tnr_failed++;\\n\\t\\t\\tpr_info(\"%s %s failed: addr=%lx, expect %lx\\\\n\",\\n\\t\\t\\t\\tprefix, test_items[i].name, addr, test_items[i].addr);\\n\\t\\t}\\n\\t}\\n\\n\\tprefix = \"kallsyms_on_each_symbol() for\";\\n\\tfor (i = 0; i < ARRAY_SIZE(test_items); i++) {\\n\\t\\tmemset(stat, 0, sizeof(*stat));\\n\\t\\tstat->max = INT_MAX;\\n\\t\\tstat->name = test_items[i].name;\\n\\t\\tkallsyms_on_each_symbol(find_symbol, stat);\\n\\t\\tif (stat->addr != test_items[i].addr || stat->real_cnt != 1) {\\n\\t\\t\\tnr_failed++;\\n\\t\\t\\tpr_info(\"%s %s failed: count=%d, addr=%lx, expect %lx\\\\n\",\\n\\t\\t\\t\\tprefix, test_items[i].name,\\n\\t\\t\\t\\tstat->real_cnt, stat->addr, test_items[i].addr);\\n\\t\\t}\\n\\t}\\n\\n\\tprefix = \"kallsyms_on_each_match_symbol() for\";\\n\\tfor (i = 0; i < ARRAY_SIZE(test_items); i++) {\\n\\t\\tmemset(stat, 0, sizeof(*stat));\\n\\t\\tstat->max = INT_MAX;\\n\\t\\tstat->name = test_items[i].name;\\n\\t\\tkallsyms_on_each_match_symbol(match_symbol, test_items[i].name, stat);\\n\\t\\tif (stat->addr != test_items[i].addr || stat->real_cnt != 1) {\\n\\t\\t\\tnr_failed++;\\n\\t\\t\\tpr_info(\"%s %s failed: count=%d, addr=%lx, expect %lx\\\\n\",\\n\\t\\t\\t\\tprefix, test_items[i].name,\\n\\t\\t\\t\\tstat->real_cnt, stat->addr, test_items[i].addr);\\n\\t\\t}\\n\\t}\\n\\n\\tif (nr_failed) {\\n\\t\\tkfree(stat);\\n\\t\\treturn -ESRCH;\\n\\t}\\n\\n\\tfor (i = 0; i < kallsyms_num_syms; i++) {\\n\\t\\taddr = kallsyms_sym_address(i);\\n\\t\\tif (!is_ksym_addr(addr))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tret = lookup_symbol_name(addr, namebuf);\\n\\t\\tif (unlikely(ret)) {\\n\\t\\t\\tnamebuf[0] = 0;\\n\\t\\t\\tpr_info(\"%d: lookup_symbol_name(%lx) failed\\\\n\", i, addr);\\n\\t\\t\\tgoto failed;\\n\\t\\t}\\n\\n\\t\\tlookup_addr = kallsyms_lookup_name(namebuf);\\n\\n\\t\\tmemset(stat, 0, sizeof(*stat));\\n\\t\\tstat->max = INT_MAX;\\n\\t\\tkallsyms_on_each_match_symbol(match_symbol, namebuf, stat);\\n\\n\\t\\t/*\\n\\t\\t * kallsyms_on_each_symbol() is too slow, randomly select some\\n\\t\\t * symbols for test.\\n\\t\\t */\\n\\t\\tif (i >= next) {\\n\\t\\t\\tmemset(stat2, 0, sizeof(*stat2));\\n\\t\\t\\tstat2->max = INT_MAX;\\n\\t\\t\\tstat2->name = namebuf;\\n\\t\\t\\tkallsyms_on_each_symbol(find_symbol, stat2);\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * kallsyms_on_each_symbol() and kallsyms_on_each_match_symbol()\\n\\t\\t\\t * need to get the same traversal result.\\n\\t\\t\\t */\\n\\t\\t\\tif (stat->addr != stat2->addr ||\\n\\t\\t\\t    stat->real_cnt != stat2->real_cnt ||\\n\\t\\t\\t    memcmp(stat->addrs, stat2->addrs,\\n\\t\\t\\t\\t   stat->save_cnt * sizeof(stat->addrs[0]))) {\\n\\t\\t\\t\\tpr_info(\"%s: mismatch between kallsyms_on_each_symbol() and kallsyms_on_each_match_symbol()\\\\n\",\\n\\t\\t\\t\\t\\tnamebuf);\\n\\t\\t\\t\\tgoto failed;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * The average of random increments is 128, that is, one of\\n\\t\\t\\t * them is tested every 128 symbols.\\n\\t\\t\\t */\\n\\t\\t\\tget_random_bytes(&rand, sizeof(rand));\\n\\t\\t\\tnext = i + (rand & 0xff) + 1;\\n\\t\\t}\\n\\n\\t\\t/* Need to be found at least once */\\n\\t\\tif (!stat->real_cnt) {\\n\\t\\t\\tpr_info(\"%s: Never found\\\\n\", namebuf);\\n\\t\\t\\tgoto failed;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * kallsyms_lookup_name() returns the address of the first\\n\\t\\t * symbol found and cannot be NULL.\\n\\t\\t */\\n\\t\\tif (!lookup_addr) {\\n\\t\\t\\tpr_info(\"%s: NULL lookup_addr?!\\\\n\", namebuf);\\n\\t\\t\\tgoto failed;\\n\\t\\t}\\n\\t\\tif (lookup_addr != stat->addrs[0]) {\\n\\t\\t\\tpr_info(\"%s: lookup_addr != stat->addrs[0]\\\\n\", namebuf);\\n\\t\\t\\tgoto failed;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * If the addresses of all matching symbols are recorded, the\\n\\t\\t * target address needs to be exist.\\n\\t\\t */\\n\\t\\tif (stat->real_cnt <= MAX_NUM_OF_RECORDS) {\\n\\t\\t\\tfor (j = 0; j < stat->save_cnt; j++) {\\n\\t\\t\\t\\tif (stat->addrs[j] == addr)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\tif (j == stat->save_cnt) {\\n\\t\\t\\t\\tpr_info(\"%s: j == save_cnt?!\\\\n\", namebuf);\\n\\t\\t\\t\\tgoto failed;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\tkfree(stat);\\n\\n\\treturn 0;\\n\\nfailed:\\n\\tpr_info(\"Test for %dth symbol failed: (%s) addr=%lx\", i, namebuf, addr);\\n\\tkfree(stat);\\n\\treturn -ESRCH;\\n}\\n\\nstatic int test_entry(void *p)\\n{\\n\\tint ret;\\n\\n\\tdo {\\n\\t\\tschedule_timeout(5 * HZ);\\n\\t} while (system_state != SYSTEM_RUNNING);\\n\\n\\tpr_info(\"start\\\\n\");\\n\\tret = test_kallsyms_basic_function();\\n\\tif (ret) {\\n\\t\\tpr_info(\"abort\\\\n\");\\n\\t\\treturn 0;\\n\\t}\\n\\n\\ttest_kallsyms_compression_ratio();\\n\\ttest_perf_kallsyms_lookup_name();\\n\\ttest_perf_kallsyms_on_each_symbol();\\n\\ttest_perf_kallsyms_on_each_match_symbol();\\n\\tpr_info(\"finish\\\\n\");\\n\\n\\treturn 0;\\n}\\n\\nstatic int __init kallsyms_test_init(void)\\n{\\n\\tstruct task_struct *t;\\n\\n\\tt = kthread_create(test_entry, NULL, \"kallsyms_test\");\\n\\tif (IS_ERR(t)) {\\n\\t\\tpr_info(\"Create kallsyms selftest task failed\\\\n\");\\n\\t\\treturn PTR_ERR(t);\\n\\t}\\n\\tkthread_bind(t, 0);\\n\\twake_up_process(t);\\n\\n\\treturn 0;\\n}\\nlate_initcall(kallsyms_test_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Load ELF vmlinux file for the kexec_file_load syscall.\\n *\\n * Copyright (C) 2004  Adam Litke (agl@us.ibm.com)\\n * Copyright (C) 2004  IBM Corp.\\n * Copyright (C) 2005  R Sharada (sharada@in.ibm.com)\\n * Copyright (C) 2006  Mohan Kumar M (mohan@in.ibm.com)\\n * Copyright (C) 2016  IBM Corporation\\n *\\n * Based on kexec-tools\\' kexec-elf-exec.c and kexec-elf-ppc64.c.\\n * Heavily modified for the kernel by\\n * Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>.\\n */\\n\\n#define pr_fmt(fmt)\\t\"kexec_elf: \" fmt\\n\\n#include <linux/elf.h>\\n#include <linux/kexec.h>\\n#include <linux/module.h>\\n#include <linux/slab.h>\\n#include <linux/types.h>\\n\\nstatic inline bool elf_is_elf_file(const struct elfhdr *ehdr)\\n{\\n\\treturn memcmp(ehdr->e_ident, ELFMAG, SELFMAG) == 0;\\n}\\n\\nstatic uint64_t elf64_to_cpu(const struct elfhdr *ehdr, uint64_t value)\\n{\\n\\tif (ehdr->e_ident[EI_DATA] == ELFDATA2LSB)\\n\\t\\tvalue = le64_to_cpu(value);\\n\\telse if (ehdr->e_ident[EI_DATA] == ELFDATA2MSB)\\n\\t\\tvalue = be64_to_cpu(value);\\n\\n\\treturn value;\\n}\\n\\nstatic uint32_t elf32_to_cpu(const struct elfhdr *ehdr, uint32_t value)\\n{\\n\\tif (ehdr->e_ident[EI_DATA] == ELFDATA2LSB)\\n\\t\\tvalue = le32_to_cpu(value);\\n\\telse if (ehdr->e_ident[EI_DATA] == ELFDATA2MSB)\\n\\t\\tvalue = be32_to_cpu(value);\\n\\n\\treturn value;\\n}\\n\\nstatic uint16_t elf16_to_cpu(const struct elfhdr *ehdr, uint16_t value)\\n{\\n\\tif (ehdr->e_ident[EI_DATA] == ELFDATA2LSB)\\n\\t\\tvalue = le16_to_cpu(value);\\n\\telse if (ehdr->e_ident[EI_DATA] == ELFDATA2MSB)\\n\\t\\tvalue = be16_to_cpu(value);\\n\\n\\treturn value;\\n}\\n\\n/**\\n * elf_is_ehdr_sane - check that it is safe to use the ELF header\\n * @buf_len:\\tsize of the buffer in which the ELF file is loaded.\\n */\\nstatic bool elf_is_ehdr_sane(const struct elfhdr *ehdr, size_t buf_len)\\n{\\n\\tif (ehdr->e_phnum > 0 && ehdr->e_phentsize != sizeof(struct elf_phdr)) {\\n\\t\\tpr_debug(\"Bad program header size.\\\\n\");\\n\\t\\treturn false;\\n\\t} else if (ehdr->e_shnum > 0 &&\\n\\t\\t   ehdr->e_shentsize != sizeof(struct elf_shdr)) {\\n\\t\\tpr_debug(\"Bad section header size.\\\\n\");\\n\\t\\treturn false;\\n\\t} else if (ehdr->e_ident[EI_VERSION] != EV_CURRENT ||\\n\\t\\t   ehdr->e_version != EV_CURRENT) {\\n\\t\\tpr_debug(\"Unknown ELF version.\\\\n\");\\n\\t\\treturn false;\\n\\t}\\n\\n\\tif (ehdr->e_phoff > 0 && ehdr->e_phnum > 0) {\\n\\t\\tsize_t phdr_size;\\n\\n\\t\\t/*\\n\\t\\t * e_phnum is at most 65535 so calculating the size of the\\n\\t\\t * program header cannot overflow.\\n\\t\\t */\\n\\t\\tphdr_size = sizeof(struct elf_phdr) * ehdr->e_phnum;\\n\\n\\t\\t/* Sanity check the program header table location. */\\n\\t\\tif (ehdr->e_phoff + phdr_size < ehdr->e_phoff) {\\n\\t\\t\\tpr_debug(\"Program headers at invalid location.\\\\n\");\\n\\t\\t\\treturn false;\\n\\t\\t} else if (ehdr->e_phoff + phdr_size > buf_len) {\\n\\t\\t\\tpr_debug(\"Program headers truncated.\\\\n\");\\n\\t\\t\\treturn false;\\n\\t\\t}\\n\\t}\\n\\n\\tif (ehdr->e_shoff > 0 && ehdr->e_shnum > 0) {\\n\\t\\tsize_t shdr_size;\\n\\n\\t\\t/*\\n\\t\\t * e_shnum is at most 65536 so calculating\\n\\t\\t * the size of the section header cannot overflow.\\n\\t\\t */\\n\\t\\tshdr_size = sizeof(struct elf_shdr) * ehdr->e_shnum;\\n\\n\\t\\t/* Sanity check the section header table location. */\\n\\t\\tif (ehdr->e_shoff + shdr_size < ehdr->e_shoff) {\\n\\t\\t\\tpr_debug(\"Section headers at invalid location.\\\\n\");\\n\\t\\t\\treturn false;\\n\\t\\t} else if (ehdr->e_shoff + shdr_size > buf_len) {\\n\\t\\t\\tpr_debug(\"Section headers truncated.\\\\n\");\\n\\t\\t\\treturn false;\\n\\t\\t}\\n\\t}\\n\\n\\treturn true;\\n}\\n\\nstatic int elf_read_ehdr(const char *buf, size_t len, struct elfhdr *ehdr)\\n{\\n\\tstruct elfhdr *buf_ehdr;\\n\\n\\tif (len < sizeof(*buf_ehdr)) {\\n\\t\\tpr_debug(\"Buffer is too small to hold ELF header.\\\\n\");\\n\\t\\treturn -ENOEXEC;\\n\\t}\\n\\n\\tmemset(ehdr, 0, sizeof(*ehdr));\\n\\tmemcpy(ehdr->e_ident, buf, sizeof(ehdr->e_ident));\\n\\tif (!elf_is_elf_file(ehdr)) {\\n\\t\\tpr_debug(\"No ELF header magic.\\\\n\");\\n\\t\\treturn -ENOEXEC;\\n\\t}\\n\\n\\tif (ehdr->e_ident[EI_CLASS] != ELF_CLASS) {\\n\\t\\tpr_debug(\"Not a supported ELF class.\\\\n\");\\n\\t\\treturn -ENOEXEC;\\n\\t} else  if (ehdr->e_ident[EI_DATA] != ELFDATA2LSB &&\\n\\t\\tehdr->e_ident[EI_DATA] != ELFDATA2MSB) {\\n\\t\\tpr_debug(\"Not a supported ELF data format.\\\\n\");\\n\\t\\treturn -ENOEXEC;\\n\\t}\\n\\n\\tbuf_ehdr = (struct elfhdr *) buf;\\n\\tif (elf16_to_cpu(ehdr, buf_ehdr->e_ehsize) != sizeof(*buf_ehdr)) {\\n\\t\\tpr_debug(\"Bad ELF header size.\\\\n\");\\n\\t\\treturn -ENOEXEC;\\n\\t}\\n\\n\\tehdr->e_type      = elf16_to_cpu(ehdr, buf_ehdr->e_type);\\n\\tehdr->e_machine   = elf16_to_cpu(ehdr, buf_ehdr->e_machine);\\n\\tehdr->e_version   = elf32_to_cpu(ehdr, buf_ehdr->e_version);\\n\\tehdr->e_flags     = elf32_to_cpu(ehdr, buf_ehdr->e_flags);\\n\\tehdr->e_phentsize = elf16_to_cpu(ehdr, buf_ehdr->e_phentsize);\\n\\tehdr->e_phnum     = elf16_to_cpu(ehdr, buf_ehdr->e_phnum);\\n\\tehdr->e_shentsize = elf16_to_cpu(ehdr, buf_ehdr->e_shentsize);\\n\\tehdr->e_shnum     = elf16_to_cpu(ehdr, buf_ehdr->e_shnum);\\n\\tehdr->e_shstrndx  = elf16_to_cpu(ehdr, buf_ehdr->e_shstrndx);\\n\\n\\tswitch (ehdr->e_ident[EI_CLASS]) {\\n\\tcase ELFCLASS64:\\n\\t\\tehdr->e_entry = elf64_to_cpu(ehdr, buf_ehdr->e_entry);\\n\\t\\tehdr->e_phoff = elf64_to_cpu(ehdr, buf_ehdr->e_phoff);\\n\\t\\tehdr->e_shoff = elf64_to_cpu(ehdr, buf_ehdr->e_shoff);\\n\\t\\tbreak;\\n\\n\\tcase ELFCLASS32:\\n\\t\\tehdr->e_entry = elf32_to_cpu(ehdr, buf_ehdr->e_entry);\\n\\t\\tehdr->e_phoff = elf32_to_cpu(ehdr, buf_ehdr->e_phoff);\\n\\t\\tehdr->e_shoff = elf32_to_cpu(ehdr, buf_ehdr->e_shoff);\\n\\t\\tbreak;\\n\\n\\tdefault:\\n\\t\\tpr_debug(\"Unknown ELF class.\\\\n\");\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\treturn elf_is_ehdr_sane(ehdr, len) ? 0 : -ENOEXEC;\\n}\\n\\n/**\\n * elf_is_phdr_sane - check that it is safe to use the program header\\n * @buf_len:\\tsize of the buffer in which the ELF file is loaded.\\n */\\nstatic bool elf_is_phdr_sane(const struct elf_phdr *phdr, size_t buf_len)\\n{\\n\\n\\tif (phdr->p_offset + phdr->p_filesz < phdr->p_offset) {\\n\\t\\tpr_debug(\"ELF segment location wraps around.\\\\n\");\\n\\t\\treturn false;\\n\\t} else if (phdr->p_offset + phdr->p_filesz > buf_len) {\\n\\t\\tpr_debug(\"ELF segment not in file.\\\\n\");\\n\\t\\treturn false;\\n\\t} else if (phdr->p_paddr + phdr->p_memsz < phdr->p_paddr) {\\n\\t\\tpr_debug(\"ELF segment address wraps around.\\\\n\");\\n\\t\\treturn false;\\n\\t}\\n\\n\\treturn true;\\n}\\n\\nstatic int elf_read_phdr(const char *buf, size_t len,\\n\\t\\t\\t struct kexec_elf_info *elf_info,\\n\\t\\t\\t int idx)\\n{\\n\\t/* Override the const in proghdrs, we are the ones doing the loading. */\\n\\tstruct elf_phdr *phdr = (struct elf_phdr *) &elf_info->proghdrs[idx];\\n\\tconst struct elfhdr *ehdr = elf_info->ehdr;\\n\\tconst char *pbuf;\\n\\tstruct elf_phdr *buf_phdr;\\n\\n\\tpbuf = buf + elf_info->ehdr->e_phoff + (idx * sizeof(*buf_phdr));\\n\\tbuf_phdr = (struct elf_phdr *) pbuf;\\n\\n\\tphdr->p_type   = elf32_to_cpu(elf_info->ehdr, buf_phdr->p_type);\\n\\tphdr->p_flags  = elf32_to_cpu(elf_info->ehdr, buf_phdr->p_flags);\\n\\n\\tswitch (ehdr->e_ident[EI_CLASS]) {\\n\\tcase ELFCLASS64:\\n\\t\\tphdr->p_offset = elf64_to_cpu(ehdr, buf_phdr->p_offset);\\n\\t\\tphdr->p_paddr  = elf64_to_cpu(ehdr, buf_phdr->p_paddr);\\n\\t\\tphdr->p_vaddr  = elf64_to_cpu(ehdr, buf_phdr->p_vaddr);\\n\\t\\tphdr->p_filesz = elf64_to_cpu(ehdr, buf_phdr->p_filesz);\\n\\t\\tphdr->p_memsz  = elf64_to_cpu(ehdr, buf_phdr->p_memsz);\\n\\t\\tphdr->p_align  = elf64_to_cpu(ehdr, buf_phdr->p_align);\\n\\t\\tbreak;\\n\\n\\tcase ELFCLASS32:\\n\\t\\tphdr->p_offset = elf32_to_cpu(ehdr, buf_phdr->p_offset);\\n\\t\\tphdr->p_paddr  = elf32_to_cpu(ehdr, buf_phdr->p_paddr);\\n\\t\\tphdr->p_vaddr  = elf32_to_cpu(ehdr, buf_phdr->p_vaddr);\\n\\t\\tphdr->p_filesz = elf32_to_cpu(ehdr, buf_phdr->p_filesz);\\n\\t\\tphdr->p_memsz  = elf32_to_cpu(ehdr, buf_phdr->p_memsz);\\n\\t\\tphdr->p_align  = elf32_to_cpu(ehdr, buf_phdr->p_align);\\n\\t\\tbreak;\\n\\n\\tdefault:\\n\\t\\tpr_debug(\"Unknown ELF class.\\\\n\");\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\treturn elf_is_phdr_sane(phdr, len) ? 0 : -ENOEXEC;\\n}\\n\\n/**\\n * elf_read_phdrs - read the program headers from the buffer\\n *\\n * This function assumes that the program header table was checked for sanity.\\n * Use elf_is_ehdr_sane() if it wasn\\'t.\\n */\\nstatic int elf_read_phdrs(const char *buf, size_t len,\\n\\t\\t\\t  struct kexec_elf_info *elf_info)\\n{\\n\\tsize_t phdr_size, i;\\n\\tconst struct elfhdr *ehdr = elf_info->ehdr;\\n\\n\\t/*\\n\\t * e_phnum is at most 65535 so calculating the size of the\\n\\t * program header cannot overflow.\\n\\t */\\n\\tphdr_size = sizeof(struct elf_phdr) * ehdr->e_phnum;\\n\\n\\telf_info->proghdrs = kzalloc(phdr_size, GFP_KERNEL);\\n\\tif (!elf_info->proghdrs)\\n\\t\\treturn -ENOMEM;\\n\\n\\tfor (i = 0; i < ehdr->e_phnum; i++) {\\n\\t\\tint ret;\\n\\n\\t\\tret = elf_read_phdr(buf, len, elf_info, i);\\n\\t\\tif (ret) {\\n\\t\\t\\tkfree(elf_info->proghdrs);\\n\\t\\t\\telf_info->proghdrs = NULL;\\n\\t\\t\\treturn ret;\\n\\t\\t}\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * elf_read_from_buffer - read ELF file and sets up ELF header and ELF info\\n * @buf:\\tBuffer to read ELF file from.\\n * @len:\\tSize of @buf.\\n * @ehdr:\\tPointer to existing struct which will be populated.\\n * @elf_info:\\tPointer to existing struct which will be populated.\\n *\\n * This function allows reading ELF files with different byte order than\\n * the kernel, byte-swapping the fields as needed.\\n *\\n * Return:\\n * On success returns 0, and the caller should call\\n * kexec_free_elf_info(elf_info) to free the memory allocated for the section\\n * and program headers.\\n */\\nstatic int elf_read_from_buffer(const char *buf, size_t len,\\n\\t\\t\\t\\tstruct elfhdr *ehdr,\\n\\t\\t\\t\\tstruct kexec_elf_info *elf_info)\\n{\\n\\tint ret;\\n\\n\\tret = elf_read_ehdr(buf, len, ehdr);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\telf_info->buffer = buf;\\n\\telf_info->ehdr = ehdr;\\n\\tif (ehdr->e_phoff > 0 && ehdr->e_phnum > 0) {\\n\\t\\tret = elf_read_phdrs(buf, len, elf_info);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\treturn 0;\\n}\\n\\n/**\\n * kexec_free_elf_info - free memory allocated by elf_read_from_buffer\\n */\\nvoid kexec_free_elf_info(struct kexec_elf_info *elf_info)\\n{\\n\\tkfree(elf_info->proghdrs);\\n\\tmemset(elf_info, 0, sizeof(*elf_info));\\n}\\n/**\\n * kexec_build_elf_info - read ELF executable and check that we can use it\\n */\\nint kexec_build_elf_info(const char *buf, size_t len, struct elfhdr *ehdr,\\n\\t\\t\\t       struct kexec_elf_info *elf_info)\\n{\\n\\tint i;\\n\\tint ret;\\n\\n\\tret = elf_read_from_buffer(buf, len, ehdr, elf_info);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/* Big endian vmlinux has type ET_DYN. */\\n\\tif (ehdr->e_type != ET_EXEC && ehdr->e_type != ET_DYN) {\\n\\t\\tpr_err(\"Not an ELF executable.\\\\n\");\\n\\t\\tgoto error;\\n\\t} else if (!elf_info->proghdrs) {\\n\\t\\tpr_err(\"No ELF program header.\\\\n\");\\n\\t\\tgoto error;\\n\\t}\\n\\n\\tfor (i = 0; i < ehdr->e_phnum; i++) {\\n\\t\\t/*\\n\\t\\t * Kexec does not support loading interpreters.\\n\\t\\t * In addition this check keeps us from attempting\\n\\t\\t * to kexec ordinay executables.\\n\\t\\t */\\n\\t\\tif (elf_info->proghdrs[i].p_type == PT_INTERP) {\\n\\t\\t\\tpr_err(\"Requires an ELF interpreter.\\\\n\");\\n\\t\\t\\tgoto error;\\n\\t\\t}\\n\\t}\\n\\n\\treturn 0;\\nerror:\\n\\tkexec_free_elf_info(elf_info);\\n\\treturn -ENOEXEC;\\n}\\n\\n\\nint kexec_elf_probe(const char *buf, unsigned long len)\\n{\\n\\tstruct elfhdr ehdr;\\n\\tstruct kexec_elf_info elf_info;\\n\\tint ret;\\n\\n\\tret = kexec_build_elf_info(buf, len, &ehdr, &elf_info);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tkexec_free_elf_info(&elf_info);\\n\\n\\treturn elf_check_arch(&ehdr) ? 0 : -ENOEXEC;\\n}\\n\\n/**\\n * kexec_elf_load - load ELF executable image\\n * @lowest_load_addr:\\tOn return, will be the address where the first PT_LOAD\\n *\\t\\t\\tsection will be loaded in memory.\\n *\\n * Return:\\n * 0 on success, negative value on failure.\\n */\\nint kexec_elf_load(struct kimage *image, struct elfhdr *ehdr,\\n\\t\\t\\t struct kexec_elf_info *elf_info,\\n\\t\\t\\t struct kexec_buf *kbuf,\\n\\t\\t\\t unsigned long *lowest_load_addr)\\n{\\n\\tunsigned long lowest_addr = UINT_MAX;\\n\\tint ret;\\n\\tsize_t i;\\n\\n\\t/* Read in the PT_LOAD segments. */\\n\\tfor (i = 0; i < ehdr->e_phnum; i++) {\\n\\t\\tunsigned long load_addr;\\n\\t\\tsize_t size;\\n\\t\\tconst struct elf_phdr *phdr;\\n\\n\\t\\tphdr = &elf_info->proghdrs[i];\\n\\t\\tif (phdr->p_type != PT_LOAD)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tsize = phdr->p_filesz;\\n\\t\\tif (size > phdr->p_memsz)\\n\\t\\t\\tsize = phdr->p_memsz;\\n\\n\\t\\tkbuf->buffer = (void *) elf_info->buffer + phdr->p_offset;\\n\\t\\tkbuf->bufsz = size;\\n\\t\\tkbuf->memsz = phdr->p_memsz;\\n\\t\\tkbuf->buf_align = phdr->p_align;\\n\\t\\tkbuf->buf_min = phdr->p_paddr;\\n\\t\\tkbuf->mem = KEXEC_BUF_MEM_UNKNOWN;\\n\\t\\tret = kexec_add_buffer(kbuf);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t\\tload_addr = kbuf->mem;\\n\\n\\t\\tif (load_addr < lowest_addr)\\n\\t\\t\\tlowest_addr = load_addr;\\n\\t}\\n\\n\\t*lowest_load_addr = lowest_addr;\\n\\tret = 0;\\n out:\\n\\treturn ret;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/* auditfilter.c -- filtering of audit events\\n *\\n * Copyright 2003-2004 Red Hat, Inc.\\n * Copyright 2005 Hewlett-Packard Development Company, L.P.\\n * Copyright 2005 IBM Corporation\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/kernel.h>\\n#include <linux/audit.h>\\n#include <linux/kthread.h>\\n#include <linux/mutex.h>\\n#include <linux/fs.h>\\n#include <linux/namei.h>\\n#include <linux/netlink.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/security.h>\\n#include <net/net_namespace.h>\\n#include <net/sock.h>\\n#include \"audit.h\"\\n\\n/*\\n * Locking model:\\n *\\n * audit_filter_mutex:\\n *\\t\\tSynchronizes writes and blocking reads of audit\\'s filterlist\\n *\\t\\tdata.  Rcu is used to traverse the filterlist and access\\n *\\t\\tcontents of structs audit_entry, audit_watch and opaque\\n *\\t\\tLSM rules during filtering.  If modified, these structures\\n *\\t\\tmust be copied and replace their counterparts in the filterlist.\\n *\\t\\tAn audit_parent struct is not accessed during filtering, so may\\n *\\t\\tbe written directly provided audit_filter_mutex is held.\\n */\\n\\n/* Audit filter lists, defined in <linux/audit.h> */\\nstruct list_head audit_filter_list[AUDIT_NR_FILTERS] = {\\n\\tLIST_HEAD_INIT(audit_filter_list[0]),\\n\\tLIST_HEAD_INIT(audit_filter_list[1]),\\n\\tLIST_HEAD_INIT(audit_filter_list[2]),\\n\\tLIST_HEAD_INIT(audit_filter_list[3]),\\n\\tLIST_HEAD_INIT(audit_filter_list[4]),\\n\\tLIST_HEAD_INIT(audit_filter_list[5]),\\n\\tLIST_HEAD_INIT(audit_filter_list[6]),\\n\\tLIST_HEAD_INIT(audit_filter_list[7]),\\n#if AUDIT_NR_FILTERS != 8\\n#error Fix audit_filter_list initialiser\\n#endif\\n};\\nstatic struct list_head audit_rules_list[AUDIT_NR_FILTERS] = {\\n\\tLIST_HEAD_INIT(audit_rules_list[0]),\\n\\tLIST_HEAD_INIT(audit_rules_list[1]),\\n\\tLIST_HEAD_INIT(audit_rules_list[2]),\\n\\tLIST_HEAD_INIT(audit_rules_list[3]),\\n\\tLIST_HEAD_INIT(audit_rules_list[4]),\\n\\tLIST_HEAD_INIT(audit_rules_list[5]),\\n\\tLIST_HEAD_INIT(audit_rules_list[6]),\\n\\tLIST_HEAD_INIT(audit_rules_list[7]),\\n};\\n\\nDEFINE_MUTEX(audit_filter_mutex);\\n\\nstatic void audit_free_lsm_field(struct audit_field *f)\\n{\\n\\tswitch (f->type) {\\n\\tcase AUDIT_SUBJ_USER:\\n\\tcase AUDIT_SUBJ_ROLE:\\n\\tcase AUDIT_SUBJ_TYPE:\\n\\tcase AUDIT_SUBJ_SEN:\\n\\tcase AUDIT_SUBJ_CLR:\\n\\tcase AUDIT_OBJ_USER:\\n\\tcase AUDIT_OBJ_ROLE:\\n\\tcase AUDIT_OBJ_TYPE:\\n\\tcase AUDIT_OBJ_LEV_LOW:\\n\\tcase AUDIT_OBJ_LEV_HIGH:\\n\\t\\tkfree(f->lsm_str);\\n\\t\\tsecurity_audit_rule_free(f->lsm_rule);\\n\\t}\\n}\\n\\nstatic inline void audit_free_rule(struct audit_entry *e)\\n{\\n\\tint i;\\n\\tstruct audit_krule *erule = &e->rule;\\n\\n\\t/* some rules don\\'t have associated watches */\\n\\tif (erule->watch)\\n\\t\\taudit_put_watch(erule->watch);\\n\\tif (erule->fields)\\n\\t\\tfor (i = 0; i < erule->field_count; i++)\\n\\t\\t\\taudit_free_lsm_field(&erule->fields[i]);\\n\\tkfree(erule->fields);\\n\\tkfree(erule->filterkey);\\n\\tkfree(e);\\n}\\n\\nvoid audit_free_rule_rcu(struct rcu_head *head)\\n{\\n\\tstruct audit_entry *e = container_of(head, struct audit_entry, rcu);\\n\\taudit_free_rule(e);\\n}\\n\\n/* Initialize an audit filterlist entry. */\\nstatic inline struct audit_entry *audit_init_entry(u32 field_count)\\n{\\n\\tstruct audit_entry *entry;\\n\\tstruct audit_field *fields;\\n\\n\\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\\n\\tif (unlikely(!entry))\\n\\t\\treturn NULL;\\n\\n\\tfields = kcalloc(field_count, sizeof(*fields), GFP_KERNEL);\\n\\tif (unlikely(!fields)) {\\n\\t\\tkfree(entry);\\n\\t\\treturn NULL;\\n\\t}\\n\\tentry->rule.fields = fields;\\n\\n\\treturn entry;\\n}\\n\\n/* Unpack a filter field\\'s string representation from user-space\\n * buffer. */\\nchar *audit_unpack_string(void **bufp, size_t *remain, size_t len)\\n{\\n\\tchar *str;\\n\\n\\tif (!*bufp || (len == 0) || (len > *remain))\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\t/* Of the currently implemented string fields, PATH_MAX\\n\\t * defines the longest valid length.\\n\\t */\\n\\tif (len > PATH_MAX)\\n\\t\\treturn ERR_PTR(-ENAMETOOLONG);\\n\\n\\tstr = kmalloc(len + 1, GFP_KERNEL);\\n\\tif (unlikely(!str))\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tmemcpy(str, *bufp, len);\\n\\tstr[len] = 0;\\n\\t*bufp += len;\\n\\t*remain -= len;\\n\\n\\treturn str;\\n}\\n\\n/* Translate an inode field to kernel representation. */\\nstatic inline int audit_to_inode(struct audit_krule *krule,\\n\\t\\t\\t\\t struct audit_field *f)\\n{\\n\\tif ((krule->listnr != AUDIT_FILTER_EXIT &&\\n\\t     krule->listnr != AUDIT_FILTER_URING_EXIT) ||\\n\\t    krule->inode_f || krule->watch || krule->tree ||\\n\\t    (f->op != Audit_equal && f->op != Audit_not_equal))\\n\\t\\treturn -EINVAL;\\n\\n\\tkrule->inode_f = f;\\n\\treturn 0;\\n}\\n\\nstatic __u32 *classes[AUDIT_SYSCALL_CLASSES];\\n\\nint __init audit_register_class(int class, unsigned *list)\\n{\\n\\t__u32 *p = kcalloc(AUDIT_BITMASK_SIZE, sizeof(__u32), GFP_KERNEL);\\n\\tif (!p)\\n\\t\\treturn -ENOMEM;\\n\\twhile (*list != ~0U) {\\n\\t\\tunsigned n = *list++;\\n\\t\\tif (n >= AUDIT_BITMASK_SIZE * 32 - AUDIT_SYSCALL_CLASSES) {\\n\\t\\t\\tkfree(p);\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\tp[AUDIT_WORD(n)] |= AUDIT_BIT(n);\\n\\t}\\n\\tif (class >= AUDIT_SYSCALL_CLASSES || classes[class]) {\\n\\t\\tkfree(p);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\tclasses[class] = p;\\n\\treturn 0;\\n}\\n\\nint audit_match_class(int class, unsigned syscall)\\n{\\n\\tif (unlikely(syscall >= AUDIT_BITMASK_SIZE * 32))\\n\\t\\treturn 0;\\n\\tif (unlikely(class >= AUDIT_SYSCALL_CLASSES || !classes[class]))\\n\\t\\treturn 0;\\n\\treturn classes[class][AUDIT_WORD(syscall)] & AUDIT_BIT(syscall);\\n}\\n\\n#ifdef CONFIG_AUDITSYSCALL\\nstatic inline int audit_match_class_bits(int class, u32 *mask)\\n{\\n\\tint i;\\n\\n\\tif (classes[class]) {\\n\\t\\tfor (i = 0; i < AUDIT_BITMASK_SIZE; i++)\\n\\t\\t\\tif (mask[i] & classes[class][i])\\n\\t\\t\\t\\treturn 0;\\n\\t}\\n\\treturn 1;\\n}\\n\\nstatic int audit_match_signal(struct audit_entry *entry)\\n{\\n\\tstruct audit_field *arch = entry->rule.arch_f;\\n\\n\\tif (!arch) {\\n\\t\\t/* When arch is unspecified, we must check both masks on biarch\\n\\t\\t * as syscall number alone is ambiguous. */\\n\\t\\treturn (audit_match_class_bits(AUDIT_CLASS_SIGNAL,\\n\\t\\t\\t\\t\\t       entry->rule.mask) &&\\n\\t\\t\\taudit_match_class_bits(AUDIT_CLASS_SIGNAL_32,\\n\\t\\t\\t\\t\\t       entry->rule.mask));\\n\\t}\\n\\n\\tswitch (audit_classify_arch(arch->val)) {\\n\\tcase 0: /* native */\\n\\t\\treturn (audit_match_class_bits(AUDIT_CLASS_SIGNAL,\\n\\t\\t\\t\\t\\t       entry->rule.mask));\\n\\tcase 1: /* 32bit on biarch */\\n\\t\\treturn (audit_match_class_bits(AUDIT_CLASS_SIGNAL_32,\\n\\t\\t\\t\\t\\t       entry->rule.mask));\\n\\tdefault:\\n\\t\\treturn 1;\\n\\t}\\n}\\n#endif\\n\\n/* Common user-space to kernel rule translation. */\\nstatic inline struct audit_entry *audit_to_entry_common(struct audit_rule_data *rule)\\n{\\n\\tunsigned listnr;\\n\\tstruct audit_entry *entry;\\n\\tint i, err;\\n\\n\\terr = -EINVAL;\\n\\tlistnr = rule->flags & ~AUDIT_FILTER_PREPEND;\\n\\tswitch (listnr) {\\n\\tdefault:\\n\\t\\tgoto exit_err;\\n#ifdef CONFIG_AUDITSYSCALL\\n\\tcase AUDIT_FILTER_ENTRY:\\n\\t\\tpr_err(\"AUDIT_FILTER_ENTRY is deprecated\\\\n\");\\n\\t\\tgoto exit_err;\\n\\tcase AUDIT_FILTER_EXIT:\\n\\tcase AUDIT_FILTER_URING_EXIT:\\n\\tcase AUDIT_FILTER_TASK:\\n#endif\\n\\tcase AUDIT_FILTER_USER:\\n\\tcase AUDIT_FILTER_EXCLUDE:\\n\\tcase AUDIT_FILTER_FS:\\n\\t\\t;\\n\\t}\\n\\tif (unlikely(rule->action == AUDIT_POSSIBLE)) {\\n\\t\\tpr_err(\"AUDIT_POSSIBLE is deprecated\\\\n\");\\n\\t\\tgoto exit_err;\\n\\t}\\n\\tif (rule->action != AUDIT_NEVER && rule->action != AUDIT_ALWAYS)\\n\\t\\tgoto exit_err;\\n\\tif (rule->field_count > AUDIT_MAX_FIELDS)\\n\\t\\tgoto exit_err;\\n\\n\\terr = -ENOMEM;\\n\\tentry = audit_init_entry(rule->field_count);\\n\\tif (!entry)\\n\\t\\tgoto exit_err;\\n\\n\\tentry->rule.flags = rule->flags & AUDIT_FILTER_PREPEND;\\n\\tentry->rule.listnr = listnr;\\n\\tentry->rule.action = rule->action;\\n\\tentry->rule.field_count = rule->field_count;\\n\\n\\tfor (i = 0; i < AUDIT_BITMASK_SIZE; i++)\\n\\t\\tentry->rule.mask[i] = rule->mask[i];\\n\\n\\tfor (i = 0; i < AUDIT_SYSCALL_CLASSES; i++) {\\n\\t\\tint bit = AUDIT_BITMASK_SIZE * 32 - i - 1;\\n\\t\\t__u32 *p = &entry->rule.mask[AUDIT_WORD(bit)];\\n\\t\\t__u32 *class;\\n\\n\\t\\tif (!(*p & AUDIT_BIT(bit)))\\n\\t\\t\\tcontinue;\\n\\t\\t*p &= ~AUDIT_BIT(bit);\\n\\t\\tclass = classes[i];\\n\\t\\tif (class) {\\n\\t\\t\\tint j;\\n\\t\\t\\tfor (j = 0; j < AUDIT_BITMASK_SIZE; j++)\\n\\t\\t\\t\\tentry->rule.mask[j] |= class[j];\\n\\t\\t}\\n\\t}\\n\\n\\treturn entry;\\n\\nexit_err:\\n\\treturn ERR_PTR(err);\\n}\\n\\nstatic u32 audit_ops[] =\\n{\\n\\t[Audit_equal] = AUDIT_EQUAL,\\n\\t[Audit_not_equal] = AUDIT_NOT_EQUAL,\\n\\t[Audit_bitmask] = AUDIT_BIT_MASK,\\n\\t[Audit_bittest] = AUDIT_BIT_TEST,\\n\\t[Audit_lt] = AUDIT_LESS_THAN,\\n\\t[Audit_gt] = AUDIT_GREATER_THAN,\\n\\t[Audit_le] = AUDIT_LESS_THAN_OR_EQUAL,\\n\\t[Audit_ge] = AUDIT_GREATER_THAN_OR_EQUAL,\\n};\\n\\nstatic u32 audit_to_op(u32 op)\\n{\\n\\tu32 n;\\n\\tfor (n = Audit_equal; n < Audit_bad && audit_ops[n] != op; n++)\\n\\t\\t;\\n\\treturn n;\\n}\\n\\n/* check if an audit field is valid */\\nstatic int audit_field_valid(struct audit_entry *entry, struct audit_field *f)\\n{\\n\\tswitch (f->type) {\\n\\tcase AUDIT_MSGTYPE:\\n\\t\\tif (entry->rule.listnr != AUDIT_FILTER_EXCLUDE &&\\n\\t\\t    entry->rule.listnr != AUDIT_FILTER_USER)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tcase AUDIT_FSTYPE:\\n\\t\\tif (entry->rule.listnr != AUDIT_FILTER_FS)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tcase AUDIT_PERM:\\n\\t\\tif (entry->rule.listnr == AUDIT_FILTER_URING_EXIT)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\t}\\n\\n\\tswitch (entry->rule.listnr) {\\n\\tcase AUDIT_FILTER_FS:\\n\\t\\tswitch (f->type) {\\n\\t\\tcase AUDIT_FSTYPE:\\n\\t\\tcase AUDIT_FILTERKEY:\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t}\\n\\n\\t/* Check for valid field type and op */\\n\\tswitch (f->type) {\\n\\tcase AUDIT_ARG0:\\n\\tcase AUDIT_ARG1:\\n\\tcase AUDIT_ARG2:\\n\\tcase AUDIT_ARG3:\\n\\tcase AUDIT_PERS: /* <uapi/linux/personality.h> */\\n\\tcase AUDIT_DEVMINOR:\\n\\t\\t/* all ops are valid */\\n\\t\\tbreak;\\n\\tcase AUDIT_UID:\\n\\tcase AUDIT_EUID:\\n\\tcase AUDIT_SUID:\\n\\tcase AUDIT_FSUID:\\n\\tcase AUDIT_LOGINUID:\\n\\tcase AUDIT_OBJ_UID:\\n\\tcase AUDIT_GID:\\n\\tcase AUDIT_EGID:\\n\\tcase AUDIT_SGID:\\n\\tcase AUDIT_FSGID:\\n\\tcase AUDIT_OBJ_GID:\\n\\tcase AUDIT_PID:\\n\\tcase AUDIT_MSGTYPE:\\n\\tcase AUDIT_PPID:\\n\\tcase AUDIT_DEVMAJOR:\\n\\tcase AUDIT_EXIT:\\n\\tcase AUDIT_SUCCESS:\\n\\tcase AUDIT_INODE:\\n\\tcase AUDIT_SESSIONID:\\n\\tcase AUDIT_SUBJ_SEN:\\n\\tcase AUDIT_SUBJ_CLR:\\n\\tcase AUDIT_OBJ_LEV_LOW:\\n\\tcase AUDIT_OBJ_LEV_HIGH:\\n\\tcase AUDIT_SADDR_FAM:\\n\\t\\t/* bit ops are only useful on syscall args */\\n\\t\\tif (f->op == Audit_bitmask || f->op == Audit_bittest)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tcase AUDIT_SUBJ_USER:\\n\\tcase AUDIT_SUBJ_ROLE:\\n\\tcase AUDIT_SUBJ_TYPE:\\n\\tcase AUDIT_OBJ_USER:\\n\\tcase AUDIT_OBJ_ROLE:\\n\\tcase AUDIT_OBJ_TYPE:\\n\\tcase AUDIT_WATCH:\\n\\tcase AUDIT_DIR:\\n\\tcase AUDIT_FILTERKEY:\\n\\tcase AUDIT_LOGINUID_SET:\\n\\tcase AUDIT_ARCH:\\n\\tcase AUDIT_FSTYPE:\\n\\tcase AUDIT_PERM:\\n\\tcase AUDIT_FILETYPE:\\n\\tcase AUDIT_FIELD_COMPARE:\\n\\tcase AUDIT_EXE:\\n\\t\\t/* only equal and not equal valid ops */\\n\\t\\tif (f->op != Audit_not_equal && f->op != Audit_equal)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\t/* field not recognized */\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\t/* Check for select valid field values */\\n\\tswitch (f->type) {\\n\\tcase AUDIT_LOGINUID_SET:\\n\\t\\tif ((f->val != 0) && (f->val != 1))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tcase AUDIT_PERM:\\n\\t\\tif (f->val & ~15)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tcase AUDIT_FILETYPE:\\n\\t\\tif (f->val & ~S_IFMT)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tcase AUDIT_FIELD_COMPARE:\\n\\t\\tif (f->val > AUDIT_MAX_FIELD_COMPARE)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tcase AUDIT_SADDR_FAM:\\n\\t\\tif (f->val >= AF_MAX)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tbreak;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/* Translate struct audit_rule_data to kernel\\'s rule representation. */\\nstatic struct audit_entry *audit_data_to_entry(struct audit_rule_data *data,\\n\\t\\t\\t\\t\\t       size_t datasz)\\n{\\n\\tint err = 0;\\n\\tstruct audit_entry *entry;\\n\\tvoid *bufp;\\n\\tsize_t remain = datasz - sizeof(struct audit_rule_data);\\n\\tint i;\\n\\tchar *str;\\n\\tstruct audit_fsnotify_mark *audit_mark;\\n\\n\\tentry = audit_to_entry_common(data);\\n\\tif (IS_ERR(entry))\\n\\t\\tgoto exit_nofree;\\n\\n\\tbufp = data->buf;\\n\\tfor (i = 0; i < data->field_count; i++) {\\n\\t\\tstruct audit_field *f = &entry->rule.fields[i];\\n\\t\\tu32 f_val;\\n\\n\\t\\terr = -EINVAL;\\n\\n\\t\\tf->op = audit_to_op(data->fieldflags[i]);\\n\\t\\tif (f->op == Audit_bad)\\n\\t\\t\\tgoto exit_free;\\n\\n\\t\\tf->type = data->fields[i];\\n\\t\\tf_val = data->values[i];\\n\\n\\t\\t/* Support legacy tests for a valid loginuid */\\n\\t\\tif ((f->type == AUDIT_LOGINUID) && (f_val == AUDIT_UID_UNSET)) {\\n\\t\\t\\tf->type = AUDIT_LOGINUID_SET;\\n\\t\\t\\tf_val = 0;\\n\\t\\t\\tentry->rule.pflags |= AUDIT_LOGINUID_LEGACY;\\n\\t\\t}\\n\\n\\t\\terr = audit_field_valid(entry, f);\\n\\t\\tif (err)\\n\\t\\t\\tgoto exit_free;\\n\\n\\t\\terr = -EINVAL;\\n\\t\\tswitch (f->type) {\\n\\t\\tcase AUDIT_LOGINUID:\\n\\t\\tcase AUDIT_UID:\\n\\t\\tcase AUDIT_EUID:\\n\\t\\tcase AUDIT_SUID:\\n\\t\\tcase AUDIT_FSUID:\\n\\t\\tcase AUDIT_OBJ_UID:\\n\\t\\t\\tf->uid = make_kuid(current_user_ns(), f_val);\\n\\t\\t\\tif (!uid_valid(f->uid))\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_GID:\\n\\t\\tcase AUDIT_EGID:\\n\\t\\tcase AUDIT_SGID:\\n\\t\\tcase AUDIT_FSGID:\\n\\t\\tcase AUDIT_OBJ_GID:\\n\\t\\t\\tf->gid = make_kgid(current_user_ns(), f_val);\\n\\t\\t\\tif (!gid_valid(f->gid))\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_ARCH:\\n\\t\\t\\tf->val = f_val;\\n\\t\\t\\tentry->rule.arch_f = f;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_SUBJ_USER:\\n\\t\\tcase AUDIT_SUBJ_ROLE:\\n\\t\\tcase AUDIT_SUBJ_TYPE:\\n\\t\\tcase AUDIT_SUBJ_SEN:\\n\\t\\tcase AUDIT_SUBJ_CLR:\\n\\t\\tcase AUDIT_OBJ_USER:\\n\\t\\tcase AUDIT_OBJ_ROLE:\\n\\t\\tcase AUDIT_OBJ_TYPE:\\n\\t\\tcase AUDIT_OBJ_LEV_LOW:\\n\\t\\tcase AUDIT_OBJ_LEV_HIGH:\\n\\t\\t\\tstr = audit_unpack_string(&bufp, &remain, f_val);\\n\\t\\t\\tif (IS_ERR(str)) {\\n\\t\\t\\t\\terr = PTR_ERR(str);\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\t}\\n\\t\\t\\tentry->rule.buflen += f_val;\\n\\t\\t\\tf->lsm_str = str;\\n\\t\\t\\terr = security_audit_rule_init(f->type, f->op, str,\\n\\t\\t\\t\\t\\t\\t       (void **)&f->lsm_rule,\\n\\t\\t\\t\\t\\t\\t       GFP_KERNEL);\\n\\t\\t\\t/* Keep currently invalid fields around in case they\\n\\t\\t\\t * become valid after a policy reload. */\\n\\t\\t\\tif (err == -EINVAL) {\\n\\t\\t\\t\\tpr_warn(\"audit rule for LSM \\\\\\'%s\\\\\\' is invalid\\\\n\",\\n\\t\\t\\t\\t\\tstr);\\n\\t\\t\\t\\terr = 0;\\n\\t\\t\\t} else if (err)\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_WATCH:\\n\\t\\t\\tstr = audit_unpack_string(&bufp, &remain, f_val);\\n\\t\\t\\tif (IS_ERR(str)) {\\n\\t\\t\\t\\terr = PTR_ERR(str);\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\t}\\n\\t\\t\\terr = audit_to_watch(&entry->rule, str, f_val, f->op);\\n\\t\\t\\tif (err) {\\n\\t\\t\\t\\tkfree(str);\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\t}\\n\\t\\t\\tentry->rule.buflen += f_val;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_DIR:\\n\\t\\t\\tstr = audit_unpack_string(&bufp, &remain, f_val);\\n\\t\\t\\tif (IS_ERR(str)) {\\n\\t\\t\\t\\terr = PTR_ERR(str);\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\t}\\n\\t\\t\\terr = audit_make_tree(&entry->rule, str, f->op);\\n\\t\\t\\tkfree(str);\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\tentry->rule.buflen += f_val;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_INODE:\\n\\t\\t\\tf->val = f_val;\\n\\t\\t\\terr = audit_to_inode(&entry->rule, f);\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FILTERKEY:\\n\\t\\t\\tif (entry->rule.filterkey || f_val > AUDIT_MAX_KEY_LEN)\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\tstr = audit_unpack_string(&bufp, &remain, f_val);\\n\\t\\t\\tif (IS_ERR(str)) {\\n\\t\\t\\t\\terr = PTR_ERR(str);\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\t}\\n\\t\\t\\tentry->rule.buflen += f_val;\\n\\t\\t\\tentry->rule.filterkey = str;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_EXE:\\n\\t\\t\\tif (entry->rule.exe || f_val > PATH_MAX)\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\tstr = audit_unpack_string(&bufp, &remain, f_val);\\n\\t\\t\\tif (IS_ERR(str)) {\\n\\t\\t\\t\\terr = PTR_ERR(str);\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\t}\\n\\t\\t\\taudit_mark = audit_alloc_mark(&entry->rule, str, f_val);\\n\\t\\t\\tif (IS_ERR(audit_mark)) {\\n\\t\\t\\t\\tkfree(str);\\n\\t\\t\\t\\terr = PTR_ERR(audit_mark);\\n\\t\\t\\t\\tgoto exit_free;\\n\\t\\t\\t}\\n\\t\\t\\tentry->rule.buflen += f_val;\\n\\t\\t\\tentry->rule.exe = audit_mark;\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\tf->val = f_val;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\tif (entry->rule.inode_f && entry->rule.inode_f->op == Audit_not_equal)\\n\\t\\tentry->rule.inode_f = NULL;\\n\\nexit_nofree:\\n\\treturn entry;\\n\\nexit_free:\\n\\tif (entry->rule.tree)\\n\\t\\taudit_put_tree(entry->rule.tree); /* that\\'s the temporary one */\\n\\tif (entry->rule.exe)\\n\\t\\taudit_remove_mark(entry->rule.exe); /* that\\'s the template one */\\n\\taudit_free_rule(entry);\\n\\treturn ERR_PTR(err);\\n}\\n\\n/* Pack a filter field\\'s string representation into data block. */\\nstatic inline size_t audit_pack_string(void **bufp, const char *str)\\n{\\n\\tsize_t len = strlen(str);\\n\\n\\tmemcpy(*bufp, str, len);\\n\\t*bufp += len;\\n\\n\\treturn len;\\n}\\n\\n/* Translate kernel rule representation to struct audit_rule_data. */\\nstatic struct audit_rule_data *audit_krule_to_data(struct audit_krule *krule)\\n{\\n\\tstruct audit_rule_data *data;\\n\\tvoid *bufp;\\n\\tint i;\\n\\n\\tdata = kmalloc(struct_size(data, buf, krule->buflen), GFP_KERNEL);\\n\\tif (unlikely(!data))\\n\\t\\treturn NULL;\\n\\tmemset(data, 0, sizeof(*data));\\n\\n\\tdata->flags = krule->flags | krule->listnr;\\n\\tdata->action = krule->action;\\n\\tdata->field_count = krule->field_count;\\n\\tbufp = data->buf;\\n\\tfor (i = 0; i < data->field_count; i++) {\\n\\t\\tstruct audit_field *f = &krule->fields[i];\\n\\n\\t\\tdata->fields[i] = f->type;\\n\\t\\tdata->fieldflags[i] = audit_ops[f->op];\\n\\t\\tswitch (f->type) {\\n\\t\\tcase AUDIT_SUBJ_USER:\\n\\t\\tcase AUDIT_SUBJ_ROLE:\\n\\t\\tcase AUDIT_SUBJ_TYPE:\\n\\t\\tcase AUDIT_SUBJ_SEN:\\n\\t\\tcase AUDIT_SUBJ_CLR:\\n\\t\\tcase AUDIT_OBJ_USER:\\n\\t\\tcase AUDIT_OBJ_ROLE:\\n\\t\\tcase AUDIT_OBJ_TYPE:\\n\\t\\tcase AUDIT_OBJ_LEV_LOW:\\n\\t\\tcase AUDIT_OBJ_LEV_HIGH:\\n\\t\\t\\tdata->buflen += data->values[i] =\\n\\t\\t\\t\\taudit_pack_string(&bufp, f->lsm_str);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_WATCH:\\n\\t\\t\\tdata->buflen += data->values[i] =\\n\\t\\t\\t\\taudit_pack_string(&bufp,\\n\\t\\t\\t\\t\\t\\t  audit_watch_path(krule->watch));\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_DIR:\\n\\t\\t\\tdata->buflen += data->values[i] =\\n\\t\\t\\t\\taudit_pack_string(&bufp,\\n\\t\\t\\t\\t\\t\\t  audit_tree_path(krule->tree));\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FILTERKEY:\\n\\t\\t\\tdata->buflen += data->values[i] =\\n\\t\\t\\t\\taudit_pack_string(&bufp, krule->filterkey);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_EXE:\\n\\t\\t\\tdata->buflen += data->values[i] =\\n\\t\\t\\t\\taudit_pack_string(&bufp, audit_mark_path(krule->exe));\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_LOGINUID_SET:\\n\\t\\t\\tif (krule->pflags & AUDIT_LOGINUID_LEGACY && !f->val) {\\n\\t\\t\\t\\tdata->fields[i] = AUDIT_LOGINUID;\\n\\t\\t\\t\\tdata->values[i] = AUDIT_UID_UNSET;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tfallthrough;\\t/* if set */\\n\\t\\tdefault:\\n\\t\\t\\tdata->values[i] = f->val;\\n\\t\\t}\\n\\t}\\n\\tfor (i = 0; i < AUDIT_BITMASK_SIZE; i++)\\n\\t\\tdata->mask[i] = krule->mask[i];\\n\\n\\treturn data;\\n}\\n\\n/* Compare two rules in kernel format.  Considered success if rules\\n * don\\'t match. */\\nstatic int audit_compare_rule(struct audit_krule *a, struct audit_krule *b)\\n{\\n\\tint i;\\n\\n\\tif (a->flags != b->flags ||\\n\\t    a->pflags != b->pflags ||\\n\\t    a->listnr != b->listnr ||\\n\\t    a->action != b->action ||\\n\\t    a->field_count != b->field_count)\\n\\t\\treturn 1;\\n\\n\\tfor (i = 0; i < a->field_count; i++) {\\n\\t\\tif (a->fields[i].type != b->fields[i].type ||\\n\\t\\t    a->fields[i].op != b->fields[i].op)\\n\\t\\t\\treturn 1;\\n\\n\\t\\tswitch (a->fields[i].type) {\\n\\t\\tcase AUDIT_SUBJ_USER:\\n\\t\\tcase AUDIT_SUBJ_ROLE:\\n\\t\\tcase AUDIT_SUBJ_TYPE:\\n\\t\\tcase AUDIT_SUBJ_SEN:\\n\\t\\tcase AUDIT_SUBJ_CLR:\\n\\t\\tcase AUDIT_OBJ_USER:\\n\\t\\tcase AUDIT_OBJ_ROLE:\\n\\t\\tcase AUDIT_OBJ_TYPE:\\n\\t\\tcase AUDIT_OBJ_LEV_LOW:\\n\\t\\tcase AUDIT_OBJ_LEV_HIGH:\\n\\t\\t\\tif (strcmp(a->fields[i].lsm_str, b->fields[i].lsm_str))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_WATCH:\\n\\t\\t\\tif (strcmp(audit_watch_path(a->watch),\\n\\t\\t\\t\\t   audit_watch_path(b->watch)))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_DIR:\\n\\t\\t\\tif (strcmp(audit_tree_path(a->tree),\\n\\t\\t\\t\\t   audit_tree_path(b->tree)))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FILTERKEY:\\n\\t\\t\\t/* both filterkeys exist based on above type compare */\\n\\t\\t\\tif (strcmp(a->filterkey, b->filterkey))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_EXE:\\n\\t\\t\\t/* both paths exist based on above type compare */\\n\\t\\t\\tif (strcmp(audit_mark_path(a->exe),\\n\\t\\t\\t\\t   audit_mark_path(b->exe)))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_UID:\\n\\t\\tcase AUDIT_EUID:\\n\\t\\tcase AUDIT_SUID:\\n\\t\\tcase AUDIT_FSUID:\\n\\t\\tcase AUDIT_LOGINUID:\\n\\t\\tcase AUDIT_OBJ_UID:\\n\\t\\t\\tif (!uid_eq(a->fields[i].uid, b->fields[i].uid))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_GID:\\n\\t\\tcase AUDIT_EGID:\\n\\t\\tcase AUDIT_SGID:\\n\\t\\tcase AUDIT_FSGID:\\n\\t\\tcase AUDIT_OBJ_GID:\\n\\t\\t\\tif (!gid_eq(a->fields[i].gid, b->fields[i].gid))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\tif (a->fields[i].val != b->fields[i].val)\\n\\t\\t\\t\\treturn 1;\\n\\t\\t}\\n\\t}\\n\\n\\tfor (i = 0; i < AUDIT_BITMASK_SIZE; i++)\\n\\t\\tif (a->mask[i] != b->mask[i])\\n\\t\\t\\treturn 1;\\n\\n\\treturn 0;\\n}\\n\\n/* Duplicate LSM field information.  The lsm_rule is opaque, so must be\\n * re-initialized. */\\nstatic inline int audit_dupe_lsm_field(struct audit_field *df,\\n\\t\\t\\t\\t\\t   struct audit_field *sf)\\n{\\n\\tint ret;\\n\\tchar *lsm_str;\\n\\n\\t/* our own copy of lsm_str */\\n\\tlsm_str = kstrdup(sf->lsm_str, GFP_KERNEL);\\n\\tif (unlikely(!lsm_str))\\n\\t\\treturn -ENOMEM;\\n\\tdf->lsm_str = lsm_str;\\n\\n\\t/* our own (refreshed) copy of lsm_rule */\\n\\tret = security_audit_rule_init(df->type, df->op, df->lsm_str,\\n\\t\\t\\t\\t       (void **)&df->lsm_rule, GFP_KERNEL);\\n\\t/* Keep currently invalid fields around in case they\\n\\t * become valid after a policy reload. */\\n\\tif (ret == -EINVAL) {\\n\\t\\tpr_warn(\"audit rule for LSM \\\\\\'%s\\\\\\' is invalid\\\\n\",\\n\\t\\t\\tdf->lsm_str);\\n\\t\\tret = 0;\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\n/* Duplicate an audit rule.  This will be a deep copy with the exception\\n * of the watch - that pointer is carried over.  The LSM specific fields\\n * will be updated in the copy.  The point is to be able to replace the old\\n * rule with the new rule in the filterlist, then free the old rule.\\n * The rlist element is undefined; list manipulations are handled apart from\\n * the initial copy. */\\nstruct audit_entry *audit_dupe_rule(struct audit_krule *old)\\n{\\n\\tu32 fcount = old->field_count;\\n\\tstruct audit_entry *entry;\\n\\tstruct audit_krule *new;\\n\\tchar *fk;\\n\\tint i, err = 0;\\n\\n\\tentry = audit_init_entry(fcount);\\n\\tif (unlikely(!entry))\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tnew = &entry->rule;\\n\\tnew->flags = old->flags;\\n\\tnew->pflags = old->pflags;\\n\\tnew->listnr = old->listnr;\\n\\tnew->action = old->action;\\n\\tfor (i = 0; i < AUDIT_BITMASK_SIZE; i++)\\n\\t\\tnew->mask[i] = old->mask[i];\\n\\tnew->prio = old->prio;\\n\\tnew->buflen = old->buflen;\\n\\tnew->inode_f = old->inode_f;\\n\\tnew->field_count = old->field_count;\\n\\n\\t/*\\n\\t * note that we are OK with not refcounting here; audit_match_tree()\\n\\t * never dereferences tree and we can\\'t get false positives there\\n\\t * since we\\'d have to have rule gone from the list *and* removed\\n\\t * before the chunks found by lookup had been allocated, i.e. before\\n\\t * the beginning of list scan.\\n\\t */\\n\\tnew->tree = old->tree;\\n\\tmemcpy(new->fields, old->fields, sizeof(struct audit_field) * fcount);\\n\\n\\t/* deep copy this information, updating the lsm_rule fields, because\\n\\t * the originals will all be freed when the old rule is freed. */\\n\\tfor (i = 0; i < fcount; i++) {\\n\\t\\tswitch (new->fields[i].type) {\\n\\t\\tcase AUDIT_SUBJ_USER:\\n\\t\\tcase AUDIT_SUBJ_ROLE:\\n\\t\\tcase AUDIT_SUBJ_TYPE:\\n\\t\\tcase AUDIT_SUBJ_SEN:\\n\\t\\tcase AUDIT_SUBJ_CLR:\\n\\t\\tcase AUDIT_OBJ_USER:\\n\\t\\tcase AUDIT_OBJ_ROLE:\\n\\t\\tcase AUDIT_OBJ_TYPE:\\n\\t\\tcase AUDIT_OBJ_LEV_LOW:\\n\\t\\tcase AUDIT_OBJ_LEV_HIGH:\\n\\t\\t\\terr = audit_dupe_lsm_field(&new->fields[i],\\n\\t\\t\\t\\t\\t\\t       &old->fields[i]);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FILTERKEY:\\n\\t\\t\\tfk = kstrdup(old->filterkey, GFP_KERNEL);\\n\\t\\t\\tif (unlikely(!fk))\\n\\t\\t\\t\\terr = -ENOMEM;\\n\\t\\t\\telse\\n\\t\\t\\t\\tnew->filterkey = fk;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_EXE:\\n\\t\\t\\terr = audit_dupe_exe(new, old);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tif (err) {\\n\\t\\t\\tif (new->exe)\\n\\t\\t\\t\\taudit_remove_mark(new->exe);\\n\\t\\t\\taudit_free_rule(entry);\\n\\t\\t\\treturn ERR_PTR(err);\\n\\t\\t}\\n\\t}\\n\\n\\tif (old->watch) {\\n\\t\\taudit_get_watch(old->watch);\\n\\t\\tnew->watch = old->watch;\\n\\t}\\n\\n\\treturn entry;\\n}\\n\\n/* Find an existing audit rule.\\n * Caller must hold audit_filter_mutex to prevent stale rule data. */\\nstatic struct audit_entry *audit_find_rule(struct audit_entry *entry,\\n\\t\\t\\t\\t\\t   struct list_head **p)\\n{\\n\\tstruct audit_entry *e, *found = NULL;\\n\\tstruct list_head *list;\\n\\tint h;\\n\\n\\tif (entry->rule.inode_f) {\\n\\t\\th = audit_hash_ino(entry->rule.inode_f->val);\\n\\t\\t*p = list = &audit_inode_hash[h];\\n\\t} else if (entry->rule.watch) {\\n\\t\\t/* we don\\'t know the inode number, so must walk entire hash */\\n\\t\\tfor (h = 0; h < AUDIT_INODE_BUCKETS; h++) {\\n\\t\\t\\tlist = &audit_inode_hash[h];\\n\\t\\t\\tlist_for_each_entry(e, list, list)\\n\\t\\t\\t\\tif (!audit_compare_rule(&entry->rule, &e->rule)) {\\n\\t\\t\\t\\t\\tfound = e;\\n\\t\\t\\t\\t\\tgoto out;\\n\\t\\t\\t\\t}\\n\\t\\t}\\n\\t\\tgoto out;\\n\\t} else {\\n\\t\\t*p = list = &audit_filter_list[entry->rule.listnr];\\n\\t}\\n\\n\\tlist_for_each_entry(e, list, list)\\n\\t\\tif (!audit_compare_rule(&entry->rule, &e->rule)) {\\n\\t\\t\\tfound = e;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\nout:\\n\\treturn found;\\n}\\n\\nstatic u64 prio_low = ~0ULL/2;\\nstatic u64 prio_high = ~0ULL/2 - 1;\\n\\n/* Add rule to given filterlist if not a duplicate. */\\nstatic inline int audit_add_rule(struct audit_entry *entry)\\n{\\n\\tstruct audit_entry *e;\\n\\tstruct audit_watch *watch = entry->rule.watch;\\n\\tstruct audit_tree *tree = entry->rule.tree;\\n\\tstruct list_head *list;\\n\\tint err = 0;\\n#ifdef CONFIG_AUDITSYSCALL\\n\\tint dont_count = 0;\\n\\n\\t/* If any of these, don\\'t count towards total */\\n\\tswitch (entry->rule.listnr) {\\n\\tcase AUDIT_FILTER_USER:\\n\\tcase AUDIT_FILTER_EXCLUDE:\\n\\tcase AUDIT_FILTER_FS:\\n\\t\\tdont_count = 1;\\n\\t}\\n#endif\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\te = audit_find_rule(entry, &list);\\n\\tif (e) {\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\terr = -EEXIST;\\n\\t\\t/* normally audit_add_tree_rule() will free it on failure */\\n\\t\\tif (tree)\\n\\t\\t\\taudit_put_tree(tree);\\n\\t\\treturn err;\\n\\t}\\n\\n\\tif (watch) {\\n\\t\\t/* audit_filter_mutex is dropped and re-taken during this call */\\n\\t\\terr = audit_add_watch(&entry->rule, &list);\\n\\t\\tif (err) {\\n\\t\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\t\\t/*\\n\\t\\t\\t * normally audit_add_tree_rule() will free it\\n\\t\\t\\t * on failure\\n\\t\\t\\t */\\n\\t\\t\\tif (tree)\\n\\t\\t\\t\\taudit_put_tree(tree);\\n\\t\\t\\treturn err;\\n\\t\\t}\\n\\t}\\n\\tif (tree) {\\n\\t\\terr = audit_add_tree_rule(&entry->rule);\\n\\t\\tif (err) {\\n\\t\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\t\\treturn err;\\n\\t\\t}\\n\\t}\\n\\n\\tentry->rule.prio = ~0ULL;\\n\\tif (entry->rule.listnr == AUDIT_FILTER_EXIT ||\\n\\t    entry->rule.listnr == AUDIT_FILTER_URING_EXIT) {\\n\\t\\tif (entry->rule.flags & AUDIT_FILTER_PREPEND)\\n\\t\\t\\tentry->rule.prio = ++prio_high;\\n\\t\\telse\\n\\t\\t\\tentry->rule.prio = --prio_low;\\n\\t}\\n\\n\\tif (entry->rule.flags & AUDIT_FILTER_PREPEND) {\\n\\t\\tlist_add(&entry->rule.list,\\n\\t\\t\\t &audit_rules_list[entry->rule.listnr]);\\n\\t\\tlist_add_rcu(&entry->list, list);\\n\\t\\tentry->rule.flags &= ~AUDIT_FILTER_PREPEND;\\n\\t} else {\\n\\t\\tlist_add_tail(&entry->rule.list,\\n\\t\\t\\t      &audit_rules_list[entry->rule.listnr]);\\n\\t\\tlist_add_tail_rcu(&entry->list, list);\\n\\t}\\n#ifdef CONFIG_AUDITSYSCALL\\n\\tif (!dont_count)\\n\\t\\taudit_n_rules++;\\n\\n\\tif (!audit_match_signal(entry))\\n\\t\\taudit_signals++;\\n#endif\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\treturn err;\\n}\\n\\n/* Remove an existing rule from filterlist. */\\nint audit_del_rule(struct audit_entry *entry)\\n{\\n\\tstruct audit_entry  *e;\\n\\tstruct audit_tree *tree = entry->rule.tree;\\n\\tstruct list_head *list;\\n\\tint ret = 0;\\n#ifdef CONFIG_AUDITSYSCALL\\n\\tint dont_count = 0;\\n\\n\\t/* If any of these, don\\'t count towards total */\\n\\tswitch (entry->rule.listnr) {\\n\\tcase AUDIT_FILTER_USER:\\n\\tcase AUDIT_FILTER_EXCLUDE:\\n\\tcase AUDIT_FILTER_FS:\\n\\t\\tdont_count = 1;\\n\\t}\\n#endif\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\te = audit_find_rule(entry, &list);\\n\\tif (!e) {\\n\\t\\tret = -ENOENT;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (e->rule.watch)\\n\\t\\taudit_remove_watch_rule(&e->rule);\\n\\n\\tif (e->rule.tree)\\n\\t\\taudit_remove_tree_rule(&e->rule);\\n\\n\\tif (e->rule.exe)\\n\\t\\taudit_remove_mark_rule(&e->rule);\\n\\n#ifdef CONFIG_AUDITSYSCALL\\n\\tif (!dont_count)\\n\\t\\taudit_n_rules--;\\n\\n\\tif (!audit_match_signal(entry))\\n\\t\\taudit_signals--;\\n#endif\\n\\n\\tlist_del_rcu(&e->list);\\n\\tlist_del(&e->rule.list);\\n\\tcall_rcu(&e->rcu, audit_free_rule_rcu);\\n\\nout:\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\tif (tree)\\n\\t\\taudit_put_tree(tree);\\t/* that\\'s the temporary one */\\n\\n\\treturn ret;\\n}\\n\\n/* List rules using struct audit_rule_data. */\\nstatic void audit_list_rules(int seq, struct sk_buff_head *q)\\n{\\n\\tstruct sk_buff *skb;\\n\\tstruct audit_krule *r;\\n\\tint i;\\n\\n\\t/* This is a blocking read, so use audit_filter_mutex instead of rcu\\n\\t * iterator to sync with list writers. */\\n\\tfor (i = 0; i < AUDIT_NR_FILTERS; i++) {\\n\\t\\tlist_for_each_entry(r, &audit_rules_list[i], list) {\\n\\t\\t\\tstruct audit_rule_data *data;\\n\\n\\t\\t\\tdata = audit_krule_to_data(r);\\n\\t\\t\\tif (unlikely(!data))\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tskb = audit_make_reply(seq, AUDIT_LIST_RULES, 0, 1,\\n\\t\\t\\t\\t\\t       data,\\n\\t\\t\\t\\t\\t       struct_size(data, buf, data->buflen));\\n\\t\\t\\tif (skb)\\n\\t\\t\\t\\tskb_queue_tail(q, skb);\\n\\t\\t\\tkfree(data);\\n\\t\\t}\\n\\t}\\n\\tskb = audit_make_reply(seq, AUDIT_LIST_RULES, 1, 1, NULL, 0);\\n\\tif (skb)\\n\\t\\tskb_queue_tail(q, skb);\\n}\\n\\n/* Log rule additions and removals */\\nstatic void audit_log_rule_change(char *action, struct audit_krule *rule, int res)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_CONFIG_CHANGE);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\taudit_log_session_info(ab);\\n\\taudit_log_task_context(ab);\\n\\taudit_log_format(ab, \" op=%s\", action);\\n\\taudit_log_key(ab, rule->filterkey);\\n\\taudit_log_format(ab, \" list=%d res=%d\", rule->listnr, res);\\n\\taudit_log_end(ab);\\n}\\n\\n/**\\n * audit_rule_change - apply all rules to the specified message type\\n * @type: audit message type\\n * @seq: netlink audit message sequence (serial) number\\n * @data: payload data\\n * @datasz: size of payload data\\n */\\nint audit_rule_change(int type, int seq, void *data, size_t datasz)\\n{\\n\\tint err = 0;\\n\\tstruct audit_entry *entry;\\n\\n\\tswitch (type) {\\n\\tcase AUDIT_ADD_RULE:\\n\\t\\tentry = audit_data_to_entry(data, datasz);\\n\\t\\tif (IS_ERR(entry))\\n\\t\\t\\treturn PTR_ERR(entry);\\n\\t\\terr = audit_add_rule(entry);\\n\\t\\taudit_log_rule_change(\"add_rule\", &entry->rule, !err);\\n\\t\\tbreak;\\n\\tcase AUDIT_DEL_RULE:\\n\\t\\tentry = audit_data_to_entry(data, datasz);\\n\\t\\tif (IS_ERR(entry))\\n\\t\\t\\treturn PTR_ERR(entry);\\n\\t\\terr = audit_del_rule(entry);\\n\\t\\taudit_log_rule_change(\"remove_rule\", &entry->rule, !err);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tWARN_ON(1);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tif (err || type == AUDIT_DEL_RULE) {\\n\\t\\tif (entry->rule.exe)\\n\\t\\t\\taudit_remove_mark(entry->rule.exe);\\n\\t\\taudit_free_rule(entry);\\n\\t}\\n\\n\\treturn err;\\n}\\n\\n/**\\n * audit_list_rules_send - list the audit rules\\n * @request_skb: skb of request we are replying to (used to target the reply)\\n * @seq: netlink audit message sequence (serial) number\\n */\\nint audit_list_rules_send(struct sk_buff *request_skb, int seq)\\n{\\n\\tstruct task_struct *tsk;\\n\\tstruct audit_netlink_list *dest;\\n\\n\\t/* We can\\'t just spew out the rules here because we might fill\\n\\t * the available socket buffer space and deadlock waiting for\\n\\t * auditctl to read from it... which isn\\'t ever going to\\n\\t * happen if we\\'re actually running in the context of auditctl\\n\\t * trying to _send_ the stuff */\\n\\n\\tdest = kmalloc(sizeof(*dest), GFP_KERNEL);\\n\\tif (!dest)\\n\\t\\treturn -ENOMEM;\\n\\tdest->net = get_net(sock_net(NETLINK_CB(request_skb).sk));\\n\\tdest->portid = NETLINK_CB(request_skb).portid;\\n\\tskb_queue_head_init(&dest->q);\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\taudit_list_rules(seq, &dest->q);\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\ttsk = kthread_run(audit_send_list_thread, dest, \"audit_send_list\");\\n\\tif (IS_ERR(tsk)) {\\n\\t\\tskb_queue_purge(&dest->q);\\n\\t\\tput_net(dest->net);\\n\\t\\tkfree(dest);\\n\\t\\treturn PTR_ERR(tsk);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nint audit_comparator(u32 left, u32 op, u32 right)\\n{\\n\\tswitch (op) {\\n\\tcase Audit_equal:\\n\\t\\treturn (left == right);\\n\\tcase Audit_not_equal:\\n\\t\\treturn (left != right);\\n\\tcase Audit_lt:\\n\\t\\treturn (left < right);\\n\\tcase Audit_le:\\n\\t\\treturn (left <= right);\\n\\tcase Audit_gt:\\n\\t\\treturn (left > right);\\n\\tcase Audit_ge:\\n\\t\\treturn (left >= right);\\n\\tcase Audit_bitmask:\\n\\t\\treturn (left & right);\\n\\tcase Audit_bittest:\\n\\t\\treturn ((left & right) == right);\\n\\tdefault:\\n\\t\\treturn 0;\\n\\t}\\n}\\n\\nint audit_uid_comparator(kuid_t left, u32 op, kuid_t right)\\n{\\n\\tswitch (op) {\\n\\tcase Audit_equal:\\n\\t\\treturn uid_eq(left, right);\\n\\tcase Audit_not_equal:\\n\\t\\treturn !uid_eq(left, right);\\n\\tcase Audit_lt:\\n\\t\\treturn uid_lt(left, right);\\n\\tcase Audit_le:\\n\\t\\treturn uid_lte(left, right);\\n\\tcase Audit_gt:\\n\\t\\treturn uid_gt(left, right);\\n\\tcase Audit_ge:\\n\\t\\treturn uid_gte(left, right);\\n\\tcase Audit_bitmask:\\n\\tcase Audit_bittest:\\n\\tdefault:\\n\\t\\treturn 0;\\n\\t}\\n}\\n\\nint audit_gid_comparator(kgid_t left, u32 op, kgid_t right)\\n{\\n\\tswitch (op) {\\n\\tcase Audit_equal:\\n\\t\\treturn gid_eq(left, right);\\n\\tcase Audit_not_equal:\\n\\t\\treturn !gid_eq(left, right);\\n\\tcase Audit_lt:\\n\\t\\treturn gid_lt(left, right);\\n\\tcase Audit_le:\\n\\t\\treturn gid_lte(left, right);\\n\\tcase Audit_gt:\\n\\t\\treturn gid_gt(left, right);\\n\\tcase Audit_ge:\\n\\t\\treturn gid_gte(left, right);\\n\\tcase Audit_bitmask:\\n\\tcase Audit_bittest:\\n\\tdefault:\\n\\t\\treturn 0;\\n\\t}\\n}\\n\\n/**\\n * parent_len - find the length of the parent portion of a pathname\\n * @path: pathname of which to determine length\\n */\\nint parent_len(const char *path)\\n{\\n\\tint plen;\\n\\tconst char *p;\\n\\n\\tplen = strlen(path);\\n\\n\\tif (plen == 0)\\n\\t\\treturn plen;\\n\\n\\t/* disregard trailing slashes */\\n\\tp = path + plen - 1;\\n\\twhile ((*p == \\'/\\') && (p > path))\\n\\t\\tp--;\\n\\n\\t/* walk backward until we find the next slash or hit beginning */\\n\\twhile ((*p != \\'/\\') && (p > path))\\n\\t\\tp--;\\n\\n\\t/* did we find a slash? Then increment to include it in path */\\n\\tif (*p == \\'/\\')\\n\\t\\tp++;\\n\\n\\treturn p - path;\\n}\\n\\n/**\\n * audit_compare_dname_path - compare given dentry name with last component in\\n * \\t\\t\\t      given path. Return of 0 indicates a match.\\n * @dname:\\tdentry name that we\\'re comparing\\n * @path:\\tfull pathname that we\\'re comparing\\n * @parentlen:\\tlength of the parent if known. Passing in AUDIT_NAME_FULL\\n * \\t\\there indicates that we must compute this value.\\n */\\nint audit_compare_dname_path(const struct qstr *dname, const char *path, int parentlen)\\n{\\n\\tint dlen, pathlen;\\n\\tconst char *p;\\n\\n\\tdlen = dname->len;\\n\\tpathlen = strlen(path);\\n\\tif (pathlen < dlen)\\n\\t\\treturn 1;\\n\\n\\tparentlen = parentlen == AUDIT_NAME_FULL ? parent_len(path) : parentlen;\\n\\tif (pathlen - parentlen != dlen)\\n\\t\\treturn 1;\\n\\n\\tp = path + parentlen;\\n\\n\\treturn strncmp(p, dname->name, dlen);\\n}\\n\\nint audit_filter(int msgtype, unsigned int listtype)\\n{\\n\\tstruct audit_entry *e;\\n\\tint ret = 1; /* Audit by default */\\n\\n\\trcu_read_lock();\\n\\tlist_for_each_entry_rcu(e, &audit_filter_list[listtype], list) {\\n\\t\\tint i, result = 0;\\n\\n\\t\\tfor (i = 0; i < e->rule.field_count; i++) {\\n\\t\\t\\tstruct audit_field *f = &e->rule.fields[i];\\n\\t\\t\\tstruct lsm_prop prop = { };\\n\\t\\t\\tpid_t pid;\\n\\n\\t\\t\\tswitch (f->type) {\\n\\t\\t\\tcase AUDIT_PID:\\n\\t\\t\\t\\tpid = task_tgid_nr(current);\\n\\t\\t\\t\\tresult = audit_comparator(pid, f->op, f->val);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase AUDIT_UID:\\n\\t\\t\\t\\tresult = audit_uid_comparator(current_uid(), f->op, f->uid);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase AUDIT_GID:\\n\\t\\t\\t\\tresult = audit_gid_comparator(current_gid(), f->op, f->gid);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase AUDIT_LOGINUID:\\n\\t\\t\\t\\tresult = audit_uid_comparator(audit_get_loginuid(current),\\n\\t\\t\\t\\t\\t\\t\\t      f->op, f->uid);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase AUDIT_LOGINUID_SET:\\n\\t\\t\\t\\tresult = audit_comparator(audit_loginuid_set(current),\\n\\t\\t\\t\\t\\t\\t\\t  f->op, f->val);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase AUDIT_MSGTYPE:\\n\\t\\t\\t\\tresult = audit_comparator(msgtype, f->op, f->val);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase AUDIT_SUBJ_USER:\\n\\t\\t\\tcase AUDIT_SUBJ_ROLE:\\n\\t\\t\\tcase AUDIT_SUBJ_TYPE:\\n\\t\\t\\tcase AUDIT_SUBJ_SEN:\\n\\t\\t\\tcase AUDIT_SUBJ_CLR:\\n\\t\\t\\t\\tif (f->lsm_rule) {\\n\\t\\t\\t\\t\\tsecurity_current_getlsmprop_subj(&prop);\\n\\t\\t\\t\\t\\tresult = security_audit_rule_match(\\n\\t\\t\\t\\t\\t\\t   &prop, f->type, f->op,\\n\\t\\t\\t\\t\\t\\t   f->lsm_rule);\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase AUDIT_EXE:\\n\\t\\t\\t\\tresult = audit_exe_compare(current, e->rule.exe);\\n\\t\\t\\t\\tif (f->op == Audit_not_equal)\\n\\t\\t\\t\\t\\tresult = !result;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tdefault:\\n\\t\\t\\t\\tgoto unlock_and_return;\\n\\t\\t\\t}\\n\\t\\t\\tif (result < 0) /* error */\\n\\t\\t\\t\\tgoto unlock_and_return;\\n\\t\\t\\tif (!result)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tif (result > 0) {\\n\\t\\t\\tif (e->rule.action == AUDIT_NEVER || listtype == AUDIT_FILTER_EXCLUDE)\\n\\t\\t\\t\\tret = 0;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\nunlock_and_return:\\n\\trcu_read_unlock();\\n\\treturn ret;\\n}\\n\\nstatic int update_lsm_rule(struct audit_krule *r)\\n{\\n\\tstruct audit_entry *entry = container_of(r, struct audit_entry, rule);\\n\\tstruct audit_entry *nentry;\\n\\tint err = 0;\\n\\n\\tif (!security_audit_rule_known(r))\\n\\t\\treturn 0;\\n\\n\\tnentry = audit_dupe_rule(r);\\n\\tif (entry->rule.exe)\\n\\t\\taudit_remove_mark(entry->rule.exe);\\n\\tif (IS_ERR(nentry)) {\\n\\t\\t/* save the first error encountered for the\\n\\t\\t * return value */\\n\\t\\terr = PTR_ERR(nentry);\\n\\t\\taudit_panic(\"error updating LSM filters\");\\n\\t\\tif (r->watch)\\n\\t\\t\\tlist_del(&r->rlist);\\n\\t\\tlist_del_rcu(&entry->list);\\n\\t\\tlist_del(&r->list);\\n\\t} else {\\n\\t\\tif (r->watch || r->tree)\\n\\t\\t\\tlist_replace_init(&r->rlist, &nentry->rule.rlist);\\n\\t\\tlist_replace_rcu(&entry->list, &nentry->list);\\n\\t\\tlist_replace(&r->list, &nentry->rule.list);\\n\\t}\\n\\tcall_rcu(&entry->rcu, audit_free_rule_rcu);\\n\\n\\treturn err;\\n}\\n\\n/* This function will re-initialize the lsm_rule field of all applicable rules.\\n * It will traverse the filter lists serarching for rules that contain LSM\\n * specific filter fields.  When such a rule is found, it is copied, the\\n * LSM field is re-initialized, and the old rule is replaced with the\\n * updated rule. */\\nint audit_update_lsm_rules(void)\\n{\\n\\tstruct audit_krule *r, *n;\\n\\tint i, err = 0;\\n\\n\\t/* audit_filter_mutex synchronizes the writers */\\n\\tmutex_lock(&audit_filter_mutex);\\n\\n\\tfor (i = 0; i < AUDIT_NR_FILTERS; i++) {\\n\\t\\tlist_for_each_entry_safe(r, n, &audit_rules_list[i], list) {\\n\\t\\t\\tint res = update_lsm_rule(r);\\n\\t\\t\\tif (!err)\\n\\t\\t\\t\\terr = res;\\n\\t\\t}\\n\\t}\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\treturn err;\\n}\\n\\n/* SPDX-License-Identifier: GPL-2.0 */\\n#include <linux/device.h>\\n#include <linux/types.h>\\n#include <linux/io.h>\\n#include <linux/mm.h>\\n#include <linux/ioremap.h>\\n\\n#ifndef arch_memremap_wb\\nstatic void *arch_memremap_wb(resource_size_t offset, unsigned long size)\\n{\\n#ifdef ioremap_cache\\n\\treturn (__force void *)ioremap_cache(offset, size);\\n#else\\n\\treturn (__force void *)ioremap(offset, size);\\n#endif\\n}\\n#endif\\n\\n#ifndef arch_memremap_can_ram_remap\\nstatic bool arch_memremap_can_ram_remap(resource_size_t offset, size_t size,\\n\\t\\t\\t\\t\\tunsigned long flags)\\n{\\n\\treturn true;\\n}\\n#endif\\n\\nstatic void *try_ram_remap(resource_size_t offset, size_t size,\\n\\t\\t\\t   unsigned long flags)\\n{\\n\\tunsigned long pfn = PHYS_PFN(offset);\\n\\n\\t/* In the simple case just return the existing linear address */\\n\\tif (pfn_valid(pfn) && !PageHighMem(pfn_to_page(pfn)) &&\\n\\t    arch_memremap_can_ram_remap(offset, size, flags))\\n\\t\\treturn __va(offset);\\n\\n\\treturn NULL; /* fallback to arch_memremap_wb */\\n}\\n\\n/**\\n * memremap() - remap an iomem_resource as cacheable memory\\n * @offset: iomem resource start address\\n * @size: size of remap\\n * @flags: any of MEMREMAP_WB, MEMREMAP_WT, MEMREMAP_WC,\\n *\\t\\t  MEMREMAP_ENC, MEMREMAP_DEC\\n *\\n * memremap() is \"ioremap\" for cases where it is known that the resource\\n * being mapped does not have i/o side effects and the __iomem\\n * annotation is not applicable. In the case of multiple flags, the different\\n * mapping types will be attempted in the order listed below until one of\\n * them succeeds.\\n *\\n * MEMREMAP_WB - matches the default mapping for System RAM on\\n * the architecture.  This is usually a read-allocate write-back cache.\\n * Moreover, if MEMREMAP_WB is specified and the requested remap region is RAM\\n * memremap() will bypass establishing a new mapping and instead return\\n * a pointer into the direct map.\\n *\\n * MEMREMAP_WT - establish a mapping whereby writes either bypass the\\n * cache or are written through to memory and never exist in a\\n * cache-dirty state with respect to program visibility.  Attempts to\\n * map System RAM with this mapping type will fail.\\n *\\n * MEMREMAP_WC - establish a writecombine mapping, whereby writes may\\n * be coalesced together (e.g. in the CPU\\'s write buffers), but is otherwise\\n * uncached. Attempts to map System RAM with this mapping type will fail.\\n */\\nvoid *memremap(resource_size_t offset, size_t size, unsigned long flags)\\n{\\n\\tint is_ram = region_intersects(offset, size,\\n\\t\\t\\t\\t       IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);\\n\\tvoid *addr = NULL;\\n\\n\\tif (!flags)\\n\\t\\treturn NULL;\\n\\n\\tif (is_ram == REGION_MIXED) {\\n\\t\\tWARN_ONCE(1, \"memremap attempted on mixed range %pa size: %#lx\\\\n\",\\n\\t\\t\\t\\t&offset, (unsigned long) size);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t/* Try all mapping types requested until one returns non-NULL */\\n\\tif (flags & MEMREMAP_WB) {\\n\\t\\t/*\\n\\t\\t * MEMREMAP_WB is special in that it can be satisfied\\n\\t\\t * from the direct map.  Some archs depend on the\\n\\t\\t * capability of memremap() to autodetect cases where\\n\\t\\t * the requested range is potentially in System RAM.\\n\\t\\t */\\n\\t\\tif (is_ram == REGION_INTERSECTS)\\n\\t\\t\\taddr = try_ram_remap(offset, size, flags);\\n\\t\\tif (!addr)\\n\\t\\t\\taddr = arch_memremap_wb(offset, size);\\n\\t}\\n\\n\\t/*\\n\\t * If we don\\'t have a mapping yet and other request flags are\\n\\t * present then we will be attempting to establish a new virtual\\n\\t * address mapping.  Enforce that this mapping is not aliasing\\n\\t * System RAM.\\n\\t */\\n\\tif (!addr && is_ram == REGION_INTERSECTS && flags != MEMREMAP_WB) {\\n\\t\\tWARN_ONCE(1, \"memremap attempted on ram %pa size: %#lx\\\\n\",\\n\\t\\t\\t\\t&offset, (unsigned long) size);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tif (!addr && (flags & MEMREMAP_WT))\\n\\t\\taddr = ioremap_wt(offset, size);\\n\\n\\tif (!addr && (flags & MEMREMAP_WC))\\n\\t\\taddr = ioremap_wc(offset, size);\\n\\n\\treturn addr;\\n}\\nEXPORT_SYMBOL(memremap);\\n\\nvoid memunmap(void *addr)\\n{\\n\\tif (is_ioremap_addr(addr))\\n\\t\\tiounmap((void __iomem *) addr);\\n}\\nEXPORT_SYMBOL(memunmap);\\n\\nstatic void devm_memremap_release(struct device *dev, void *res)\\n{\\n\\tmemunmap(*(void **)res);\\n}\\n\\nstatic int devm_memremap_match(struct device *dev, void *res, void *match_data)\\n{\\n\\treturn *(void **)res == match_data;\\n}\\n\\nvoid *devm_memremap(struct device *dev, resource_size_t offset,\\n\\t\\tsize_t size, unsigned long flags)\\n{\\n\\tvoid **ptr, *addr;\\n\\n\\tptr = devres_alloc_node(devm_memremap_release, sizeof(*ptr), GFP_KERNEL,\\n\\t\\t\\tdev_to_node(dev));\\n\\tif (!ptr)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\taddr = memremap(offset, size, flags);\\n\\tif (addr) {\\n\\t\\t*ptr = addr;\\n\\t\\tdevres_add(dev, ptr);\\n\\t} else {\\n\\t\\tdevres_free(ptr);\\n\\t\\treturn ERR_PTR(-ENXIO);\\n\\t}\\n\\n\\treturn addr;\\n}\\nEXPORT_SYMBOL(devm_memremap);\\n\\nvoid devm_memunmap(struct device *dev, void *addr)\\n{\\n\\tWARN_ON(devres_release(dev, devm_memremap_release,\\n\\t\\t\\t\\tdevm_memremap_match, addr));\\n}\\nEXPORT_SYMBOL(devm_memunmap);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * kexec: kexec_file_load system call\\n *\\n * Copyright (C) 2014 Red Hat Inc.\\n * Authors:\\n *      Vivek Goyal <vgoyal@redhat.com>\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/capability.h>\\n#include <linux/mm.h>\\n#include <linux/file.h>\\n#include <linux/slab.h>\\n#include <linux/kexec.h>\\n#include <linux/memblock.h>\\n#include <linux/mutex.h>\\n#include <linux/list.h>\\n#include <linux/fs.h>\\n#include <linux/ima.h>\\n#include <crypto/hash.h>\\n#include <crypto/sha2.h>\\n#include <linux/elf.h>\\n#include <linux/elfcore.h>\\n#include <linux/kernel.h>\\n#include <linux/kernel_read_file.h>\\n#include <linux/syscalls.h>\\n#include <linux/vmalloc.h>\\n#include \"kexec_internal.h\"\\n\\n#ifdef CONFIG_KEXEC_SIG\\nstatic bool sig_enforce = IS_ENABLED(CONFIG_KEXEC_SIG_FORCE);\\n\\nvoid set_kexec_sig_enforced(void)\\n{\\n\\tsig_enforce = true;\\n}\\n#endif\\n\\nstatic int kexec_calculate_store_digests(struct kimage *image);\\n\\n/* Maximum size in bytes for kernel/initrd files. */\\n#define KEXEC_FILE_SIZE_MAX\\tmin_t(s64, 4LL << 30, SSIZE_MAX)\\n\\n/*\\n * Currently this is the only default function that is exported as some\\n * architectures need it to do additional handlings.\\n * In the future, other default functions may be exported too if required.\\n */\\nint kexec_image_probe_default(struct kimage *image, void *buf,\\n\\t\\t\\t      unsigned long buf_len)\\n{\\n\\tconst struct kexec_file_ops * const *fops;\\n\\tint ret = -ENOEXEC;\\n\\n\\tfor (fops = &kexec_file_loaders[0]; *fops && (*fops)->probe; ++fops) {\\n\\t\\tret = (*fops)->probe(buf, buf_len);\\n\\t\\tif (!ret) {\\n\\t\\t\\timage->fops = *fops;\\n\\t\\t\\treturn ret;\\n\\t\\t}\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic void *kexec_image_load_default(struct kimage *image)\\n{\\n\\tif (!image->fops || !image->fops->load)\\n\\t\\treturn ERR_PTR(-ENOEXEC);\\n\\n\\treturn image->fops->load(image, image->kernel_buf,\\n\\t\\t\\t\\t image->kernel_buf_len, image->initrd_buf,\\n\\t\\t\\t\\t image->initrd_buf_len, image->cmdline_buf,\\n\\t\\t\\t\\t image->cmdline_buf_len);\\n}\\n\\nint kexec_image_post_load_cleanup_default(struct kimage *image)\\n{\\n\\tif (!image->fops || !image->fops->cleanup)\\n\\t\\treturn 0;\\n\\n\\treturn image->fops->cleanup(image->image_loader_data);\\n}\\n\\n/*\\n * Free up memory used by kernel, initrd, and command line. This is temporary\\n * memory allocation which is not needed any more after these buffers have\\n * been loaded into separate segments and have been copied elsewhere.\\n */\\nvoid kimage_file_post_load_cleanup(struct kimage *image)\\n{\\n\\tstruct purgatory_info *pi = &image->purgatory_info;\\n\\n\\tvfree(image->kernel_buf);\\n\\timage->kernel_buf = NULL;\\n\\n\\tvfree(image->initrd_buf);\\n\\timage->initrd_buf = NULL;\\n\\n\\tkfree(image->cmdline_buf);\\n\\timage->cmdline_buf = NULL;\\n\\n\\tvfree(pi->purgatory_buf);\\n\\tpi->purgatory_buf = NULL;\\n\\n\\tvfree(pi->sechdrs);\\n\\tpi->sechdrs = NULL;\\n\\n#ifdef CONFIG_IMA_KEXEC\\n\\tvfree(image->ima_buffer);\\n\\timage->ima_buffer = NULL;\\n#endif /* CONFIG_IMA_KEXEC */\\n\\n\\t/* See if architecture has anything to cleanup post load */\\n\\tarch_kimage_file_post_load_cleanup(image);\\n\\n\\t/*\\n\\t * Above call should have called into bootloader to free up\\n\\t * any data stored in kimage->image_loader_data. It should\\n\\t * be ok now to free it up.\\n\\t */\\n\\tkfree(image->image_loader_data);\\n\\timage->image_loader_data = NULL;\\n\\n\\tkexec_file_dbg_print = false;\\n}\\n\\n#ifdef CONFIG_KEXEC_SIG\\n#ifdef CONFIG_SIGNED_PE_FILE_VERIFICATION\\nint kexec_kernel_verify_pe_sig(const char *kernel, unsigned long kernel_len)\\n{\\n\\tint ret;\\n\\n\\tret = verify_pefile_signature(kernel, kernel_len,\\n\\t\\t\\t\\t      VERIFY_USE_SECONDARY_KEYRING,\\n\\t\\t\\t\\t      VERIFYING_KEXEC_PE_SIGNATURE);\\n\\tif (ret == -ENOKEY && IS_ENABLED(CONFIG_INTEGRITY_PLATFORM_KEYRING)) {\\n\\t\\tret = verify_pefile_signature(kernel, kernel_len,\\n\\t\\t\\t\\t\\t      VERIFY_USE_PLATFORM_KEYRING,\\n\\t\\t\\t\\t\\t      VERIFYING_KEXEC_PE_SIGNATURE);\\n\\t}\\n\\treturn ret;\\n}\\n#endif\\n\\nstatic int kexec_image_verify_sig(struct kimage *image, void *buf,\\n\\t\\t\\t\\t  unsigned long buf_len)\\n{\\n\\tif (!image->fops || !image->fops->verify_sig) {\\n\\t\\tpr_debug(\"kernel loader does not support signature verification.\\\\n\");\\n\\t\\treturn -EKEYREJECTED;\\n\\t}\\n\\n\\treturn image->fops->verify_sig(buf, buf_len);\\n}\\n\\nstatic int\\nkimage_validate_signature(struct kimage *image)\\n{\\n\\tint ret;\\n\\n\\tret = kexec_image_verify_sig(image, image->kernel_buf,\\n\\t\\t\\t\\t     image->kernel_buf_len);\\n\\tif (ret) {\\n\\n\\t\\tif (sig_enforce) {\\n\\t\\t\\tpr_notice(\"Enforced kernel signature verification failed (%d).\\\\n\", ret);\\n\\t\\t\\treturn ret;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * If IMA is guaranteed to appraise a signature on the kexec\\n\\t\\t * image, permit it even if the kernel is otherwise locked\\n\\t\\t * down.\\n\\t\\t */\\n\\t\\tif (!ima_appraise_signature(READING_KEXEC_IMAGE) &&\\n\\t\\t    security_locked_down(LOCKDOWN_KEXEC))\\n\\t\\t\\treturn -EPERM;\\n\\n\\t\\tpr_debug(\"kernel signature verification failed (%d).\\\\n\", ret);\\n\\t}\\n\\n\\treturn 0;\\n}\\n#endif\\n\\n/*\\n * In file mode list of segments is prepared by kernel. Copy relevant\\n * data from user space, do error checking, prepare segment list\\n */\\nstatic int\\nkimage_file_prepare_segments(struct kimage *image, int kernel_fd, int initrd_fd,\\n\\t\\t\\t     const char __user *cmdline_ptr,\\n\\t\\t\\t     unsigned long cmdline_len, unsigned flags)\\n{\\n\\tssize_t ret;\\n\\tvoid *ldata;\\n\\n\\tret = kernel_read_file_from_fd(kernel_fd, 0, &image->kernel_buf,\\n\\t\\t\\t\\t       KEXEC_FILE_SIZE_MAX, NULL,\\n\\t\\t\\t\\t       READING_KEXEC_IMAGE);\\n\\tif (ret < 0)\\n\\t\\treturn ret;\\n\\timage->kernel_buf_len = ret;\\n\\tkexec_dprintk(\"kernel: %p kernel_size: %#lx\\\\n\",\\n\\t\\t      image->kernel_buf, image->kernel_buf_len);\\n\\n\\t/* Call arch image probe handlers */\\n\\tret = arch_kexec_kernel_image_probe(image, image->kernel_buf,\\n\\t\\t\\t\\t\\t    image->kernel_buf_len);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n#ifdef CONFIG_KEXEC_SIG\\n\\tret = kimage_validate_signature(image);\\n\\n\\tif (ret)\\n\\t\\tgoto out;\\n#endif\\n\\t/* It is possible that there no initramfs is being loaded */\\n\\tif (!(flags & KEXEC_FILE_NO_INITRAMFS)) {\\n\\t\\tret = kernel_read_file_from_fd(initrd_fd, 0, &image->initrd_buf,\\n\\t\\t\\t\\t\\t       KEXEC_FILE_SIZE_MAX, NULL,\\n\\t\\t\\t\\t\\t       READING_KEXEC_INITRAMFS);\\n\\t\\tif (ret < 0)\\n\\t\\t\\tgoto out;\\n\\t\\timage->initrd_buf_len = ret;\\n\\t\\tret = 0;\\n\\t}\\n\\n\\tif (cmdline_len) {\\n\\t\\timage->cmdline_buf = memdup_user(cmdline_ptr, cmdline_len);\\n\\t\\tif (IS_ERR(image->cmdline_buf)) {\\n\\t\\t\\tret = PTR_ERR(image->cmdline_buf);\\n\\t\\t\\timage->cmdline_buf = NULL;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\n\\t\\timage->cmdline_buf_len = cmdline_len;\\n\\n\\t\\t/* command line should be a string with last byte null */\\n\\t\\tif (image->cmdline_buf[cmdline_len - 1] != \\'\\\\0\\') {\\n\\t\\t\\tret = -EINVAL;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\n\\t\\tima_kexec_cmdline(kernel_fd, image->cmdline_buf,\\n\\t\\t\\t\\t  image->cmdline_buf_len - 1);\\n\\t}\\n\\n\\t/* IMA needs to pass the measurement list to the next kernel. */\\n\\tima_add_kexec_buffer(image);\\n\\n\\t/* Call image load handler */\\n\\tldata = kexec_image_load_default(image);\\n\\n\\tif (IS_ERR(ldata)) {\\n\\t\\tret = PTR_ERR(ldata);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\timage->image_loader_data = ldata;\\nout:\\n\\t/* In case of error, free up all allocated memory in this function */\\n\\tif (ret)\\n\\t\\tkimage_file_post_load_cleanup(image);\\n\\treturn ret;\\n}\\n\\nstatic int\\nkimage_file_alloc_init(struct kimage **rimage, int kernel_fd,\\n\\t\\t       int initrd_fd, const char __user *cmdline_ptr,\\n\\t\\t       unsigned long cmdline_len, unsigned long flags)\\n{\\n\\tint ret;\\n\\tstruct kimage *image;\\n\\tbool kexec_on_panic = flags & KEXEC_FILE_ON_CRASH;\\n\\n\\timage = do_kimage_alloc_init();\\n\\tif (!image)\\n\\t\\treturn -ENOMEM;\\n\\n\\tkexec_file_dbg_print = !!(flags & KEXEC_FILE_DEBUG);\\n\\timage->file_mode = 1;\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif (kexec_on_panic) {\\n\\t\\t/* Enable special crash kernel control page alloc policy. */\\n\\t\\timage->control_page = crashk_res.start;\\n\\t\\timage->type = KEXEC_TYPE_CRASH;\\n\\t}\\n#endif\\n\\n\\tret = kimage_file_prepare_segments(image, kernel_fd, initrd_fd,\\n\\t\\t\\t\\t\\t   cmdline_ptr, cmdline_len, flags);\\n\\tif (ret)\\n\\t\\tgoto out_free_image;\\n\\n\\tret = sanity_check_segment_list(image);\\n\\tif (ret)\\n\\t\\tgoto out_free_post_load_bufs;\\n\\n\\tret = -ENOMEM;\\n\\timage->control_code_page = kimage_alloc_control_pages(image,\\n\\t\\t\\t\\t\\t   get_order(KEXEC_CONTROL_PAGE_SIZE));\\n\\tif (!image->control_code_page) {\\n\\t\\tpr_err(\"Could not allocate control_code_buffer\\\\n\");\\n\\t\\tgoto out_free_post_load_bufs;\\n\\t}\\n\\n\\tif (!kexec_on_panic) {\\n\\t\\timage->swap_page = kimage_alloc_control_pages(image, 0);\\n\\t\\tif (!image->swap_page) {\\n\\t\\t\\tpr_err(\"Could not allocate swap buffer\\\\n\");\\n\\t\\t\\tgoto out_free_control_pages;\\n\\t\\t}\\n\\t}\\n\\n\\t*rimage = image;\\n\\treturn 0;\\nout_free_control_pages:\\n\\tkimage_free_page_list(&image->control_pages);\\nout_free_post_load_bufs:\\n\\tkimage_file_post_load_cleanup(image);\\nout_free_image:\\n\\tkfree(image);\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE5(kexec_file_load, int, kernel_fd, int, initrd_fd,\\n\\t\\tunsigned long, cmdline_len, const char __user *, cmdline_ptr,\\n\\t\\tunsigned long, flags)\\n{\\n\\tint image_type = (flags & KEXEC_FILE_ON_CRASH) ?\\n\\t\\t\\t KEXEC_TYPE_CRASH : KEXEC_TYPE_DEFAULT;\\n\\tstruct kimage **dest_image, *image;\\n\\tint ret = 0, i;\\n\\n\\t/* We only trust the superuser with rebooting the system. */\\n\\tif (!kexec_load_permitted(image_type))\\n\\t\\treturn -EPERM;\\n\\n\\t/* Make sure we have a legal set of flags */\\n\\tif (flags != (flags & KEXEC_FILE_FLAGS))\\n\\t\\treturn -EINVAL;\\n\\n\\timage = NULL;\\n\\n\\tif (!kexec_trylock())\\n\\t\\treturn -EBUSY;\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif (image_type == KEXEC_TYPE_CRASH) {\\n\\t\\tdest_image = &kexec_crash_image;\\n\\t\\tif (kexec_crash_image)\\n\\t\\t\\tarch_kexec_unprotect_crashkres();\\n\\t} else\\n#endif\\n\\t\\tdest_image = &kexec_image;\\n\\n\\tif (flags & KEXEC_FILE_UNLOAD)\\n\\t\\tgoto exchange;\\n\\n\\t/*\\n\\t * In case of crash, new kernel gets loaded in reserved region. It is\\n\\t * same memory where old crash kernel might be loaded. Free any\\n\\t * current crash dump kernel before we corrupt it.\\n\\t */\\n\\tif (flags & KEXEC_FILE_ON_CRASH)\\n\\t\\tkimage_free(xchg(&kexec_crash_image, NULL));\\n\\n\\tret = kimage_file_alloc_init(&image, kernel_fd, initrd_fd, cmdline_ptr,\\n\\t\\t\\t\\t     cmdline_len, flags);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n#ifdef CONFIG_CRASH_HOTPLUG\\n\\tif ((flags & KEXEC_FILE_ON_CRASH) && arch_crash_hotplug_support(image, flags))\\n\\t\\timage->hotplug_support = 1;\\n#endif\\n\\n\\tret = machine_kexec_prepare(image);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Some architecture(like S390) may touch the crash memory before\\n\\t * machine_kexec_prepare(), we must copy vmcoreinfo data after it.\\n\\t */\\n\\tret = kimage_crash_copy_vmcoreinfo(image);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\tret = kexec_calculate_store_digests(image);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\tkexec_dprintk(\"nr_segments = %lu\\\\n\", image->nr_segments);\\n\\tfor (i = 0; i < image->nr_segments; i++) {\\n\\t\\tstruct kexec_segment *ksegment;\\n\\n\\t\\tksegment = &image->segment[i];\\n\\t\\tkexec_dprintk(\"segment[%d]: buf=0x%p bufsz=0x%zx mem=0x%lx memsz=0x%zx\\\\n\",\\n\\t\\t\\t      i, ksegment->buf, ksegment->bufsz, ksegment->mem,\\n\\t\\t\\t      ksegment->memsz);\\n\\n\\t\\tret = kimage_load_segment(image, &image->segment[i]);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\tkimage_terminate(image);\\n\\n\\tret = machine_kexec_post_load(image);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\tkexec_dprintk(\"kexec_file_load: type:%u, start:0x%lx head:0x%lx flags:0x%lx\\\\n\",\\n\\t\\t      image->type, image->start, image->head, flags);\\n\\t/*\\n\\t * Free up any temporary buffers allocated which are not needed\\n\\t * after image has been loaded\\n\\t */\\n\\tkimage_file_post_load_cleanup(image);\\nexchange:\\n\\timage = xchg(dest_image, image);\\nout:\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif ((flags & KEXEC_FILE_ON_CRASH) && kexec_crash_image)\\n\\t\\tarch_kexec_protect_crashkres();\\n#endif\\n\\n\\tkexec_unlock();\\n\\tkimage_free(image);\\n\\treturn ret;\\n}\\n\\nstatic int locate_mem_hole_top_down(unsigned long start, unsigned long end,\\n\\t\\t\\t\\t    struct kexec_buf *kbuf)\\n{\\n\\tstruct kimage *image = kbuf->image;\\n\\tunsigned long temp_start, temp_end;\\n\\n\\ttemp_end = min(end, kbuf->buf_max);\\n\\ttemp_start = temp_end - kbuf->memsz + 1;\\n\\n\\tdo {\\n\\t\\t/* align down start */\\n\\t\\ttemp_start = ALIGN_DOWN(temp_start, kbuf->buf_align);\\n\\n\\t\\tif (temp_start < start || temp_start < kbuf->buf_min)\\n\\t\\t\\treturn 0;\\n\\n\\t\\ttemp_end = temp_start + kbuf->memsz - 1;\\n\\n\\t\\t/*\\n\\t\\t * Make sure this does not conflict with any of existing\\n\\t\\t * segments\\n\\t\\t */\\n\\t\\tif (kimage_is_destination_range(image, temp_start, temp_end)) {\\n\\t\\t\\ttemp_start = temp_start - PAGE_SIZE;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\t/* We found a suitable memory range */\\n\\t\\tbreak;\\n\\t} while (1);\\n\\n\\t/* If we are here, we found a suitable memory range */\\n\\tkbuf->mem = temp_start;\\n\\n\\t/* Success, stop navigating through remaining System RAM ranges */\\n\\treturn 1;\\n}\\n\\nstatic int locate_mem_hole_bottom_up(unsigned long start, unsigned long end,\\n\\t\\t\\t\\t     struct kexec_buf *kbuf)\\n{\\n\\tstruct kimage *image = kbuf->image;\\n\\tunsigned long temp_start, temp_end;\\n\\n\\ttemp_start = max(start, kbuf->buf_min);\\n\\n\\tdo {\\n\\t\\ttemp_start = ALIGN(temp_start, kbuf->buf_align);\\n\\t\\ttemp_end = temp_start + kbuf->memsz - 1;\\n\\n\\t\\tif (temp_end > end || temp_end > kbuf->buf_max)\\n\\t\\t\\treturn 0;\\n\\t\\t/*\\n\\t\\t * Make sure this does not conflict with any of existing\\n\\t\\t * segments\\n\\t\\t */\\n\\t\\tif (kimage_is_destination_range(image, temp_start, temp_end)) {\\n\\t\\t\\ttemp_start = temp_start + PAGE_SIZE;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\t/* We found a suitable memory range */\\n\\t\\tbreak;\\n\\t} while (1);\\n\\n\\t/* If we are here, we found a suitable memory range */\\n\\tkbuf->mem = temp_start;\\n\\n\\t/* Success, stop navigating through remaining System RAM ranges */\\n\\treturn 1;\\n}\\n\\nstatic int locate_mem_hole_callback(struct resource *res, void *arg)\\n{\\n\\tstruct kexec_buf *kbuf = (struct kexec_buf *)arg;\\n\\tu64 start = res->start, end = res->end;\\n\\tunsigned long sz = end - start + 1;\\n\\n\\t/* Returning 0 will take to next memory range */\\n\\n\\t/* Don\\'t use memory that will be detected and handled by a driver. */\\n\\tif (res->flags & IORESOURCE_SYSRAM_DRIVER_MANAGED)\\n\\t\\treturn 0;\\n\\n\\tif (sz < kbuf->memsz)\\n\\t\\treturn 0;\\n\\n\\tif (end < kbuf->buf_min || start > kbuf->buf_max)\\n\\t\\treturn 0;\\n\\n\\t/*\\n\\t * Allocate memory top down with-in ram range. Otherwise bottom up\\n\\t * allocation.\\n\\t */\\n\\tif (kbuf->top_down)\\n\\t\\treturn locate_mem_hole_top_down(start, end, kbuf);\\n\\treturn locate_mem_hole_bottom_up(start, end, kbuf);\\n}\\n\\n#ifdef CONFIG_ARCH_KEEP_MEMBLOCK\\nstatic int kexec_walk_memblock(struct kexec_buf *kbuf,\\n\\t\\t\\t       int (*func)(struct resource *, void *))\\n{\\n\\tint ret = 0;\\n\\tu64 i;\\n\\tphys_addr_t mstart, mend;\\n\\tstruct resource res = { };\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif (kbuf->image->type == KEXEC_TYPE_CRASH)\\n\\t\\treturn func(&crashk_res, kbuf);\\n#endif\\n\\n\\t/*\\n\\t * Using MEMBLOCK_NONE will properly skip MEMBLOCK_DRIVER_MANAGED. See\\n\\t * IORESOURCE_SYSRAM_DRIVER_MANAGED handling in\\n\\t * locate_mem_hole_callback().\\n\\t */\\n\\tif (kbuf->top_down) {\\n\\t\\tfor_each_free_mem_range_reverse(i, NUMA_NO_NODE, MEMBLOCK_NONE,\\n\\t\\t\\t\\t\\t\\t&mstart, &mend, NULL) {\\n\\t\\t\\t/*\\n\\t\\t\\t * In memblock, end points to the first byte after the\\n\\t\\t\\t * range while in kexec, end points to the last byte\\n\\t\\t\\t * in the range.\\n\\t\\t\\t */\\n\\t\\t\\tres.start = mstart;\\n\\t\\t\\tres.end = mend - 1;\\n\\t\\t\\tret = func(&res, kbuf);\\n\\t\\t\\tif (ret)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t} else {\\n\\t\\tfor_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE,\\n\\t\\t\\t\\t\\t&mstart, &mend, NULL) {\\n\\t\\t\\t/*\\n\\t\\t\\t * In memblock, end points to the first byte after the\\n\\t\\t\\t * range while in kexec, end points to the last byte\\n\\t\\t\\t * in the range.\\n\\t\\t\\t */\\n\\t\\t\\tres.start = mstart;\\n\\t\\t\\tres.end = mend - 1;\\n\\t\\t\\tret = func(&res, kbuf);\\n\\t\\t\\tif (ret)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\treturn ret;\\n}\\n#else\\nstatic int kexec_walk_memblock(struct kexec_buf *kbuf,\\n\\t\\t\\t       int (*func)(struct resource *, void *))\\n{\\n\\treturn 0;\\n}\\n#endif\\n\\n/**\\n * kexec_walk_resources - call func(data) on free memory regions\\n * @kbuf:\\tContext info for the search. Also passed to @func.\\n * @func:\\tFunction to call for each memory region.\\n *\\n * Return: The memory walk will stop when func returns a non-zero value\\n * and that value will be returned. If all free regions are visited without\\n * func returning non-zero, then zero will be returned.\\n */\\nstatic int kexec_walk_resources(struct kexec_buf *kbuf,\\n\\t\\t\\t\\tint (*func)(struct resource *, void *))\\n{\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif (kbuf->image->type == KEXEC_TYPE_CRASH)\\n\\t\\treturn walk_iomem_res_desc(crashk_res.desc,\\n\\t\\t\\t\\t\\t   IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY,\\n\\t\\t\\t\\t\\t   crashk_res.start, crashk_res.end,\\n\\t\\t\\t\\t\\t   kbuf, func);\\n#endif\\n\\tif (kbuf->top_down)\\n\\t\\treturn walk_system_ram_res_rev(0, ULONG_MAX, kbuf, func);\\n\\telse\\n\\t\\treturn walk_system_ram_res(0, ULONG_MAX, kbuf, func);\\n}\\n\\n/**\\n * kexec_locate_mem_hole - find free memory for the purgatory or the next kernel\\n * @kbuf:\\tParameters for the memory search.\\n *\\n * On success, kbuf->mem will have the start address of the memory region found.\\n *\\n * Return: 0 on success, negative errno on error.\\n */\\nint kexec_locate_mem_hole(struct kexec_buf *kbuf)\\n{\\n\\tint ret;\\n\\n\\t/* Arch knows where to place */\\n\\tif (kbuf->mem != KEXEC_BUF_MEM_UNKNOWN)\\n\\t\\treturn 0;\\n\\n\\tif (!IS_ENABLED(CONFIG_ARCH_KEEP_MEMBLOCK))\\n\\t\\tret = kexec_walk_resources(kbuf, locate_mem_hole_callback);\\n\\telse\\n\\t\\tret = kexec_walk_memblock(kbuf, locate_mem_hole_callback);\\n\\n\\treturn ret == 1 ? 0 : -EADDRNOTAVAIL;\\n}\\n\\n/**\\n * kexec_add_buffer - place a buffer in a kexec segment\\n * @kbuf:\\tBuffer contents and memory parameters.\\n *\\n * This function assumes that kexec_lock is held.\\n * On successful return, @kbuf->mem will have the physical address of\\n * the buffer in memory.\\n *\\n * Return: 0 on success, negative errno on error.\\n */\\nint kexec_add_buffer(struct kexec_buf *kbuf)\\n{\\n\\tstruct kexec_segment *ksegment;\\n\\tint ret;\\n\\n\\t/* Currently adding segment this way is allowed only in file mode */\\n\\tif (!kbuf->image->file_mode)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (kbuf->image->nr_segments >= KEXEC_SEGMENT_MAX)\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Make sure we are not trying to add buffer after allocating\\n\\t * control pages. All segments need to be placed first before\\n\\t * any control pages are allocated. As control page allocation\\n\\t * logic goes through list of segments to make sure there are\\n\\t * no destination overlaps.\\n\\t */\\n\\tif (!list_empty(&kbuf->image->control_pages)) {\\n\\t\\tWARN_ON(1);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\t/* Ensure minimum alignment needed for segments. */\\n\\tkbuf->memsz = ALIGN(kbuf->memsz, PAGE_SIZE);\\n\\tkbuf->buf_align = max(kbuf->buf_align, PAGE_SIZE);\\n\\n\\t/* Walk the RAM ranges and allocate a suitable range for the buffer */\\n\\tret = arch_kexec_locate_mem_hole(kbuf);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/* Found a suitable memory range */\\n\\tksegment = &kbuf->image->segment[kbuf->image->nr_segments];\\n\\tksegment->kbuf = kbuf->buffer;\\n\\tksegment->bufsz = kbuf->bufsz;\\n\\tksegment->mem = kbuf->mem;\\n\\tksegment->memsz = kbuf->memsz;\\n\\tkbuf->image->nr_segments++;\\n\\treturn 0;\\n}\\n\\n/* Calculate and store the digest of segments */\\nstatic int kexec_calculate_store_digests(struct kimage *image)\\n{\\n\\tstruct crypto_shash *tfm;\\n\\tstruct shash_desc *desc;\\n\\tint ret = 0, i, j, zero_buf_sz, sha_region_sz;\\n\\tsize_t desc_size, nullsz;\\n\\tchar *digest;\\n\\tvoid *zero_buf;\\n\\tstruct kexec_sha_region *sha_regions;\\n\\tstruct purgatory_info *pi = &image->purgatory_info;\\n\\n\\tif (!IS_ENABLED(CONFIG_ARCH_SUPPORTS_KEXEC_PURGATORY))\\n\\t\\treturn 0;\\n\\n\\tzero_buf = __va(page_to_pfn(ZERO_PAGE(0)) << PAGE_SHIFT);\\n\\tzero_buf_sz = PAGE_SIZE;\\n\\n\\ttfm = crypto_alloc_shash(\"sha256\", 0, 0);\\n\\tif (IS_ERR(tfm)) {\\n\\t\\tret = PTR_ERR(tfm);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tdesc_size = crypto_shash_descsize(tfm) + sizeof(*desc);\\n\\tdesc = kzalloc(desc_size, GFP_KERNEL);\\n\\tif (!desc) {\\n\\t\\tret = -ENOMEM;\\n\\t\\tgoto out_free_tfm;\\n\\t}\\n\\n\\tsha_region_sz = KEXEC_SEGMENT_MAX * sizeof(struct kexec_sha_region);\\n\\tsha_regions = vzalloc(sha_region_sz);\\n\\tif (!sha_regions) {\\n\\t\\tret = -ENOMEM;\\n\\t\\tgoto out_free_desc;\\n\\t}\\n\\n\\tdesc->tfm   = tfm;\\n\\n\\tret = crypto_shash_init(desc);\\n\\tif (ret < 0)\\n\\t\\tgoto out_free_sha_regions;\\n\\n\\tdigest = kzalloc(SHA256_DIGEST_SIZE, GFP_KERNEL);\\n\\tif (!digest) {\\n\\t\\tret = -ENOMEM;\\n\\t\\tgoto out_free_sha_regions;\\n\\t}\\n\\n\\tfor (j = i = 0; i < image->nr_segments; i++) {\\n\\t\\tstruct kexec_segment *ksegment;\\n\\n#ifdef CONFIG_CRASH_HOTPLUG\\n\\t\\t/* Exclude elfcorehdr segment to allow future changes via hotplug */\\n\\t\\tif (i == image->elfcorehdr_index)\\n\\t\\t\\tcontinue;\\n#endif\\n\\n\\t\\tksegment = &image->segment[i];\\n\\t\\t/*\\n\\t\\t * Skip purgatory as it will be modified once we put digest\\n\\t\\t * info in purgatory.\\n\\t\\t */\\n\\t\\tif (ksegment->kbuf == pi->purgatory_buf)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tret = crypto_shash_update(desc, ksegment->kbuf,\\n\\t\\t\\t\\t\\t  ksegment->bufsz);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/*\\n\\t\\t * Assume rest of the buffer is filled with zero and\\n\\t\\t * update digest accordingly.\\n\\t\\t */\\n\\t\\tnullsz = ksegment->memsz - ksegment->bufsz;\\n\\t\\twhile (nullsz) {\\n\\t\\t\\tunsigned long bytes = nullsz;\\n\\n\\t\\t\\tif (bytes > zero_buf_sz)\\n\\t\\t\\t\\tbytes = zero_buf_sz;\\n\\t\\t\\tret = crypto_shash_update(desc, zero_buf, bytes);\\n\\t\\t\\tif (ret)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tnullsz -= bytes;\\n\\t\\t}\\n\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\n\\t\\tsha_regions[j].start = ksegment->mem;\\n\\t\\tsha_regions[j].len = ksegment->memsz;\\n\\t\\tj++;\\n\\t}\\n\\n\\tif (!ret) {\\n\\t\\tret = crypto_shash_final(desc, digest);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out_free_digest;\\n\\t\\tret = kexec_purgatory_get_set_symbol(image, \"purgatory_sha_regions\",\\n\\t\\t\\t\\t\\t\\t     sha_regions, sha_region_sz, 0);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out_free_digest;\\n\\n\\t\\tret = kexec_purgatory_get_set_symbol(image, \"purgatory_sha256_digest\",\\n\\t\\t\\t\\t\\t\\t     digest, SHA256_DIGEST_SIZE, 0);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out_free_digest;\\n\\t}\\n\\nout_free_digest:\\n\\tkfree(digest);\\nout_free_sha_regions:\\n\\tvfree(sha_regions);\\nout_free_desc:\\n\\tkfree(desc);\\nout_free_tfm:\\n\\tkfree(tfm);\\nout:\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_ARCH_SUPPORTS_KEXEC_PURGATORY\\n/*\\n * kexec_purgatory_setup_kbuf - prepare buffer to load purgatory.\\n * @pi:\\t\\tPurgatory to be loaded.\\n * @kbuf:\\tBuffer to setup.\\n *\\n * Allocates the memory needed for the buffer. Caller is responsible to free\\n * the memory after use.\\n *\\n * Return: 0 on success, negative errno on error.\\n */\\nstatic int kexec_purgatory_setup_kbuf(struct purgatory_info *pi,\\n\\t\\t\\t\\t      struct kexec_buf *kbuf)\\n{\\n\\tconst Elf_Shdr *sechdrs;\\n\\tunsigned long bss_align;\\n\\tunsigned long bss_sz;\\n\\tunsigned long align;\\n\\tint i, ret;\\n\\n\\tsechdrs = (void *)pi->ehdr + pi->ehdr->e_shoff;\\n\\tkbuf->buf_align = bss_align = 1;\\n\\tkbuf->bufsz = bss_sz = 0;\\n\\n\\tfor (i = 0; i < pi->ehdr->e_shnum; i++) {\\n\\t\\tif (!(sechdrs[i].sh_flags & SHF_ALLOC))\\n\\t\\t\\tcontinue;\\n\\n\\t\\talign = sechdrs[i].sh_addralign;\\n\\t\\tif (sechdrs[i].sh_type != SHT_NOBITS) {\\n\\t\\t\\tif (kbuf->buf_align < align)\\n\\t\\t\\t\\tkbuf->buf_align = align;\\n\\t\\t\\tkbuf->bufsz = ALIGN(kbuf->bufsz, align);\\n\\t\\t\\tkbuf->bufsz += sechdrs[i].sh_size;\\n\\t\\t} else {\\n\\t\\t\\tif (bss_align < align)\\n\\t\\t\\t\\tbss_align = align;\\n\\t\\t\\tbss_sz = ALIGN(bss_sz, align);\\n\\t\\t\\tbss_sz += sechdrs[i].sh_size;\\n\\t\\t}\\n\\t}\\n\\tkbuf->bufsz = ALIGN(kbuf->bufsz, bss_align);\\n\\tkbuf->memsz = kbuf->bufsz + bss_sz;\\n\\tif (kbuf->buf_align < bss_align)\\n\\t\\tkbuf->buf_align = bss_align;\\n\\n\\tkbuf->buffer = vzalloc(kbuf->bufsz);\\n\\tif (!kbuf->buffer)\\n\\t\\treturn -ENOMEM;\\n\\tpi->purgatory_buf = kbuf->buffer;\\n\\n\\tret = kexec_add_buffer(kbuf);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\treturn 0;\\nout:\\n\\tvfree(pi->purgatory_buf);\\n\\tpi->purgatory_buf = NULL;\\n\\treturn ret;\\n}\\n\\n/*\\n * kexec_purgatory_setup_sechdrs - prepares the pi->sechdrs buffer.\\n * @pi:\\t\\tPurgatory to be loaded.\\n * @kbuf:\\tBuffer prepared to store purgatory.\\n *\\n * Allocates the memory needed for the buffer. Caller is responsible to free\\n * the memory after use.\\n *\\n * Return: 0 on success, negative errno on error.\\n */\\nstatic int kexec_purgatory_setup_sechdrs(struct purgatory_info *pi,\\n\\t\\t\\t\\t\\t struct kexec_buf *kbuf)\\n{\\n\\tunsigned long bss_addr;\\n\\tunsigned long offset;\\n\\tsize_t sechdrs_size;\\n\\tElf_Shdr *sechdrs;\\n\\tint i;\\n\\n\\t/*\\n\\t * The section headers in kexec_purgatory are read-only. In order to\\n\\t * have them modifiable make a temporary copy.\\n\\t */\\n\\tsechdrs_size = array_size(sizeof(Elf_Shdr), pi->ehdr->e_shnum);\\n\\tsechdrs = vzalloc(sechdrs_size);\\n\\tif (!sechdrs)\\n\\t\\treturn -ENOMEM;\\n\\tmemcpy(sechdrs, (void *)pi->ehdr + pi->ehdr->e_shoff, sechdrs_size);\\n\\tpi->sechdrs = sechdrs;\\n\\n\\toffset = 0;\\n\\tbss_addr = kbuf->mem + kbuf->bufsz;\\n\\tkbuf->image->start = pi->ehdr->e_entry;\\n\\n\\tfor (i = 0; i < pi->ehdr->e_shnum; i++) {\\n\\t\\tunsigned long align;\\n\\t\\tvoid *src, *dst;\\n\\n\\t\\tif (!(sechdrs[i].sh_flags & SHF_ALLOC))\\n\\t\\t\\tcontinue;\\n\\n\\t\\talign = sechdrs[i].sh_addralign;\\n\\t\\tif (sechdrs[i].sh_type == SHT_NOBITS) {\\n\\t\\t\\tbss_addr = ALIGN(bss_addr, align);\\n\\t\\t\\tsechdrs[i].sh_addr = bss_addr;\\n\\t\\t\\tbss_addr += sechdrs[i].sh_size;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\toffset = ALIGN(offset, align);\\n\\n\\t\\t/*\\n\\t\\t * Check if the segment contains the entry point, if so,\\n\\t\\t * calculate the value of image->start based on it.\\n\\t\\t * If the compiler has produced more than one .text section\\n\\t\\t * (Eg: .text.hot), they are generally after the main .text\\n\\t\\t * section, and they shall not be used to calculate\\n\\t\\t * image->start. So do not re-calculate image->start if it\\n\\t\\t * is not set to the initial value, and warn the user so they\\n\\t\\t * have a chance to fix their purgatory\\'s linker script.\\n\\t\\t */\\n\\t\\tif (sechdrs[i].sh_flags & SHF_EXECINSTR &&\\n\\t\\t    pi->ehdr->e_entry >= sechdrs[i].sh_addr &&\\n\\t\\t    pi->ehdr->e_entry < (sechdrs[i].sh_addr\\n\\t\\t\\t\\t\\t + sechdrs[i].sh_size) &&\\n\\t\\t    !WARN_ON(kbuf->image->start != pi->ehdr->e_entry)) {\\n\\t\\t\\tkbuf->image->start -= sechdrs[i].sh_addr;\\n\\t\\t\\tkbuf->image->start += kbuf->mem + offset;\\n\\t\\t}\\n\\n\\t\\tsrc = (void *)pi->ehdr + sechdrs[i].sh_offset;\\n\\t\\tdst = pi->purgatory_buf + offset;\\n\\t\\tmemcpy(dst, src, sechdrs[i].sh_size);\\n\\n\\t\\tsechdrs[i].sh_addr = kbuf->mem + offset;\\n\\t\\tsechdrs[i].sh_offset = offset;\\n\\t\\toffset += sechdrs[i].sh_size;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic int kexec_apply_relocations(struct kimage *image)\\n{\\n\\tint i, ret;\\n\\tstruct purgatory_info *pi = &image->purgatory_info;\\n\\tconst Elf_Shdr *sechdrs;\\n\\n\\tsechdrs = (void *)pi->ehdr + pi->ehdr->e_shoff;\\n\\n\\tfor (i = 0; i < pi->ehdr->e_shnum; i++) {\\n\\t\\tconst Elf_Shdr *relsec;\\n\\t\\tconst Elf_Shdr *symtab;\\n\\t\\tElf_Shdr *section;\\n\\n\\t\\trelsec = sechdrs + i;\\n\\n\\t\\tif (relsec->sh_type != SHT_RELA &&\\n\\t\\t    relsec->sh_type != SHT_REL)\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * For section of type SHT_RELA/SHT_REL,\\n\\t\\t * ->sh_link contains section header index of associated\\n\\t\\t * symbol table. And ->sh_info contains section header\\n\\t\\t * index of section to which relocations apply.\\n\\t\\t */\\n\\t\\tif (relsec->sh_info >= pi->ehdr->e_shnum ||\\n\\t\\t    relsec->sh_link >= pi->ehdr->e_shnum)\\n\\t\\t\\treturn -ENOEXEC;\\n\\n\\t\\tsection = pi->sechdrs + relsec->sh_info;\\n\\t\\tsymtab = sechdrs + relsec->sh_link;\\n\\n\\t\\tif (!(section->sh_flags & SHF_ALLOC))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * symtab->sh_link contain section header index of associated\\n\\t\\t * string table.\\n\\t\\t */\\n\\t\\tif (symtab->sh_link >= pi->ehdr->e_shnum)\\n\\t\\t\\t/* Invalid section number? */\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * Respective architecture needs to provide support for applying\\n\\t\\t * relocations of type SHT_RELA/SHT_REL.\\n\\t\\t */\\n\\t\\tif (relsec->sh_type == SHT_RELA)\\n\\t\\t\\tret = arch_kexec_apply_relocations_add(pi, section,\\n\\t\\t\\t\\t\\t\\t\\t       relsec, symtab);\\n\\t\\telse if (relsec->sh_type == SHT_REL)\\n\\t\\t\\tret = arch_kexec_apply_relocations(pi, section,\\n\\t\\t\\t\\t\\t\\t\\t   relsec, symtab);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * kexec_load_purgatory - Load and relocate the purgatory object.\\n * @image:\\tImage to add the purgatory to.\\n * @kbuf:\\tMemory parameters to use.\\n *\\n * Allocates the memory needed for image->purgatory_info.sechdrs and\\n * image->purgatory_info.purgatory_buf/kbuf->buffer. Caller is responsible\\n * to free the memory after use.\\n *\\n * Return: 0 on success, negative errno on error.\\n */\\nint kexec_load_purgatory(struct kimage *image, struct kexec_buf *kbuf)\\n{\\n\\tstruct purgatory_info *pi = &image->purgatory_info;\\n\\tint ret;\\n\\n\\tif (kexec_purgatory_size <= 0)\\n\\t\\treturn -EINVAL;\\n\\n\\tpi->ehdr = (const Elf_Ehdr *)kexec_purgatory;\\n\\n\\tret = kexec_purgatory_setup_kbuf(pi, kbuf);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tret = kexec_purgatory_setup_sechdrs(pi, kbuf);\\n\\tif (ret)\\n\\t\\tgoto out_free_kbuf;\\n\\n\\tret = kexec_apply_relocations(image);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\treturn 0;\\nout:\\n\\tvfree(pi->sechdrs);\\n\\tpi->sechdrs = NULL;\\nout_free_kbuf:\\n\\tvfree(pi->purgatory_buf);\\n\\tpi->purgatory_buf = NULL;\\n\\treturn ret;\\n}\\n\\n/*\\n * kexec_purgatory_find_symbol - find a symbol in the purgatory\\n * @pi:\\t\\tPurgatory to search in.\\n * @name:\\tName of the symbol.\\n *\\n * Return: pointer to symbol in read-only symtab on success, NULL on error.\\n */\\nstatic const Elf_Sym *kexec_purgatory_find_symbol(struct purgatory_info *pi,\\n\\t\\t\\t\\t\\t\\t  const char *name)\\n{\\n\\tconst Elf_Shdr *sechdrs;\\n\\tconst Elf_Ehdr *ehdr;\\n\\tconst Elf_Sym *syms;\\n\\tconst char *strtab;\\n\\tint i, k;\\n\\n\\tif (!pi->ehdr)\\n\\t\\treturn NULL;\\n\\n\\tehdr = pi->ehdr;\\n\\tsechdrs = (void *)ehdr + ehdr->e_shoff;\\n\\n\\tfor (i = 0; i < ehdr->e_shnum; i++) {\\n\\t\\tif (sechdrs[i].sh_type != SHT_SYMTAB)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (sechdrs[i].sh_link >= ehdr->e_shnum)\\n\\t\\t\\t/* Invalid strtab section number */\\n\\t\\t\\tcontinue;\\n\\t\\tstrtab = (void *)ehdr + sechdrs[sechdrs[i].sh_link].sh_offset;\\n\\t\\tsyms = (void *)ehdr + sechdrs[i].sh_offset;\\n\\n\\t\\t/* Go through symbols for a match */\\n\\t\\tfor (k = 0; k < sechdrs[i].sh_size/sizeof(Elf_Sym); k++) {\\n\\t\\t\\tif (ELF_ST_BIND(syms[k].st_info) != STB_GLOBAL)\\n\\t\\t\\t\\tcontinue;\\n\\n\\t\\t\\tif (strcmp(strtab + syms[k].st_name, name) != 0)\\n\\t\\t\\t\\tcontinue;\\n\\n\\t\\t\\tif (syms[k].st_shndx == SHN_UNDEF ||\\n\\t\\t\\t    syms[k].st_shndx >= ehdr->e_shnum) {\\n\\t\\t\\t\\tpr_debug(\"Symbol: %s has bad section index %d.\\\\n\",\\n\\t\\t\\t\\t\\t\\tname, syms[k].st_shndx);\\n\\t\\t\\t\\treturn NULL;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/* Found the symbol we are looking for */\\n\\t\\t\\treturn &syms[k];\\n\\t\\t}\\n\\t}\\n\\n\\treturn NULL;\\n}\\n\\nvoid *kexec_purgatory_get_symbol_addr(struct kimage *image, const char *name)\\n{\\n\\tstruct purgatory_info *pi = &image->purgatory_info;\\n\\tconst Elf_Sym *sym;\\n\\tElf_Shdr *sechdr;\\n\\n\\tsym = kexec_purgatory_find_symbol(pi, name);\\n\\tif (!sym)\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\tsechdr = &pi->sechdrs[sym->st_shndx];\\n\\n\\t/*\\n\\t * Returns the address where symbol will finally be loaded after\\n\\t * kexec_load_segment()\\n\\t */\\n\\treturn (void *)(sechdr->sh_addr + sym->st_value);\\n}\\n\\n/*\\n * Get or set value of a symbol. If \"get_value\" is true, symbol value is\\n * returned in buf otherwise symbol value is set based on value in buf.\\n */\\nint kexec_purgatory_get_set_symbol(struct kimage *image, const char *name,\\n\\t\\t\\t\\t   void *buf, unsigned int size, bool get_value)\\n{\\n\\tstruct purgatory_info *pi = &image->purgatory_info;\\n\\tconst Elf_Sym *sym;\\n\\tElf_Shdr *sec;\\n\\tchar *sym_buf;\\n\\n\\tsym = kexec_purgatory_find_symbol(pi, name);\\n\\tif (!sym)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (sym->st_size != size) {\\n\\t\\tpr_err(\"symbol %s size mismatch: expected %lu actual %u\\\\n\",\\n\\t\\t       name, (unsigned long)sym->st_size, size);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tsec = pi->sechdrs + sym->st_shndx;\\n\\n\\tif (sec->sh_type == SHT_NOBITS) {\\n\\t\\tpr_err(\"symbol %s is in a bss section. Cannot %s\\\\n\", name,\\n\\t\\t       get_value ? \"get\" : \"set\");\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tsym_buf = (char *)pi->purgatory_buf + sec->sh_offset + sym->st_value;\\n\\n\\tif (get_value)\\n\\t\\tmemcpy((void *)buf, sym_buf, size);\\n\\telse\\n\\t\\tmemcpy((void *)sym_buf, buf, size);\\n\\n\\treturn 0;\\n}\\n#endif /* CONFIG_ARCH_SUPPORTS_KEXEC_PURGATORY */\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/* Rewritten by Rusty Russell, on the backs of many others...\\n   Copyright (C) 2001 Rusty Russell, 2002 Rusty Russell IBM.\\n\\n*/\\n#include <linux/elf.h>\\n#include <linux/ftrace.h>\\n#include <linux/memory.h>\\n#include <linux/extable.h>\\n#include <linux/module.h>\\n#include <linux/mutex.h>\\n#include <linux/init.h>\\n#include <linux/kprobes.h>\\n#include <linux/filter.h>\\n\\n#include <asm/sections.h>\\n#include <linux/uaccess.h>\\n\\n/*\\n * mutex protecting text section modification (dynamic code patching).\\n * some users need to sleep (allocating memory...) while they hold this lock.\\n *\\n * Note: Also protects SMP-alternatives modification on x86.\\n *\\n * NOT exported to modules - patching kernel text is a really delicate matter.\\n */\\nDEFINE_MUTEX(text_mutex);\\n\\nextern struct exception_table_entry __start___ex_table[];\\nextern struct exception_table_entry __stop___ex_table[];\\n\\n/* Cleared by build time tools if the table is already sorted. */\\nu32 __initdata __visible main_extable_sort_needed = 1;\\n\\n/* Sort the kernel\\'s built-in exception table */\\nvoid __init sort_main_extable(void)\\n{\\n\\tif (main_extable_sort_needed &&\\n\\t    &__stop___ex_table > &__start___ex_table) {\\n\\t\\tpr_notice(\"Sorting __ex_table...\\\\n\");\\n\\t\\tsort_extable(__start___ex_table, __stop___ex_table);\\n\\t}\\n}\\n\\n/* Given an address, look for it in the kernel exception table */\\nconst\\nstruct exception_table_entry *search_kernel_exception_table(unsigned long addr)\\n{\\n\\treturn search_extable(__start___ex_table,\\n\\t\\t\\t      __stop___ex_table - __start___ex_table, addr);\\n}\\n\\n/* Given an address, look for it in the exception tables. */\\nconst struct exception_table_entry *search_exception_tables(unsigned long addr)\\n{\\n\\tconst struct exception_table_entry *e;\\n\\n\\te = search_kernel_exception_table(addr);\\n\\tif (!e)\\n\\t\\te = search_module_extables(addr);\\n\\tif (!e)\\n\\t\\te = search_bpf_extables(addr);\\n\\treturn e;\\n}\\n\\nint notrace core_kernel_text(unsigned long addr)\\n{\\n\\tif (is_kernel_text(addr))\\n\\t\\treturn 1;\\n\\n\\tif (system_state < SYSTEM_FREEING_INITMEM &&\\n\\t    is_kernel_inittext(addr))\\n\\t\\treturn 1;\\n\\treturn 0;\\n}\\n\\nint __kernel_text_address(unsigned long addr)\\n{\\n\\tif (kernel_text_address(addr))\\n\\t\\treturn 1;\\n\\t/*\\n\\t * There might be init symbols in saved stacktraces.\\n\\t * Give those symbols a chance to be printed in\\n\\t * backtraces (such as lockdep traces).\\n\\t *\\n\\t * Since we are after the module-symbols check, there\\'s\\n\\t * no danger of address overlap:\\n\\t */\\n\\tif (is_kernel_inittext(addr))\\n\\t\\treturn 1;\\n\\treturn 0;\\n}\\n\\nint kernel_text_address(unsigned long addr)\\n{\\n\\tbool no_rcu;\\n\\tint ret = 1;\\n\\n\\tif (core_kernel_text(addr))\\n\\t\\treturn 1;\\n\\n\\t/*\\n\\t * If a stack dump happens while RCU is not watching, then\\n\\t * RCU needs to be notified that it requires to start\\n\\t * watching again. This can happen either by tracing that\\n\\t * triggers a stack trace, or a WARN() that happens during\\n\\t * coming back from idle, or cpu on or offlining.\\n\\t *\\n\\t * is_module_text_address() as well as the kprobe slots,\\n\\t * is_bpf_text_address() and is_bpf_image_address require\\n\\t * RCU to be watching.\\n\\t */\\n\\tno_rcu = !rcu_is_watching();\\n\\n\\t/* Treat this like an NMI as it can happen anywhere */\\n\\tif (no_rcu)\\n\\t\\tct_nmi_enter();\\n\\n\\tif (is_module_text_address(addr))\\n\\t\\tgoto out;\\n\\tif (is_ftrace_trampoline(addr))\\n\\t\\tgoto out;\\n\\tif (is_kprobe_optinsn_slot(addr) || is_kprobe_insn_slot(addr))\\n\\t\\tgoto out;\\n\\tif (is_bpf_text_address(addr))\\n\\t\\tgoto out;\\n\\tret = 0;\\nout:\\n\\tif (no_rcu)\\n\\t\\tct_nmi_exit();\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * On some architectures (PPC64, IA64, PARISC) function pointers\\n * are actually only tokens to some data that then holds the\\n * real function address. As a result, to find if a function\\n * pointer is part of the kernel text, we need to do some\\n * special dereferencing first.\\n */\\n#ifdef CONFIG_HAVE_FUNCTION_DESCRIPTORS\\nvoid *dereference_function_descriptor(void *ptr)\\n{\\n\\tfunc_desc_t *desc = ptr;\\n\\tvoid *p;\\n\\n\\tif (!get_kernel_nofault(p, (void *)&desc->addr))\\n\\t\\tptr = p;\\n\\treturn ptr;\\n}\\nEXPORT_SYMBOL_GPL(dereference_function_descriptor);\\n\\nvoid *dereference_kernel_function_descriptor(void *ptr)\\n{\\n\\tif (ptr < (void *)__start_opd || ptr >= (void *)__end_opd)\\n\\t\\treturn ptr;\\n\\n\\treturn dereference_function_descriptor(ptr);\\n}\\n#endif\\n\\nint func_ptr_is_kernel_text(void *ptr)\\n{\\n\\tunsigned long addr;\\n\\taddr = (unsigned long) dereference_function_descriptor(ptr);\\n\\tif (core_kernel_text(addr))\\n\\t\\treturn 1;\\n\\treturn is_module_text_address(addr);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/* auditsc.c -- System-call auditing support\\n * Handles all system-call specific auditing features.\\n *\\n * Copyright 2003-2004 Red Hat Inc., Durham, North Carolina.\\n * Copyright 2005 Hewlett-Packard Development Company, L.P.\\n * Copyright (C) 2005, 2006 IBM Corporation\\n * All Rights Reserved.\\n *\\n * Written by Rickard E. (Rik) Faith <faith@redhat.com>\\n *\\n * Many of the ideas implemented here are from Stephen C. Tweedie,\\n * especially the idea of avoiding a copy by using getname.\\n *\\n * The method for actual interception of syscall entry and exit (not in\\n * this file -- see entry.S) is based on a GPL\\'d patch written by\\n * okir@suse.de and Copyright 2003 SuSE Linux AG.\\n *\\n * POSIX message queue support added by George Wilson <ltcgcw@us.ibm.com>,\\n * 2006.\\n *\\n * The support of additional filter rules compares (>, <, >=, <=) was\\n * added by Dustin Kirkland <dustin.kirkland@us.ibm.com>, 2005.\\n *\\n * Modified by Amy Griffis <amy.griffis@hp.com> to collect additional\\n * filesystem information.\\n *\\n * Subject and object context labeling support added by <danjones@us.ibm.com>\\n * and <dustin.kirkland@us.ibm.com> for LSPP certification compliance.\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/init.h>\\n#include <asm/types.h>\\n#include <linux/atomic.h>\\n#include <linux/fs.h>\\n#include <linux/namei.h>\\n#include <linux/mm.h>\\n#include <linux/export.h>\\n#include <linux/slab.h>\\n#include <linux/mount.h>\\n#include <linux/socket.h>\\n#include <linux/mqueue.h>\\n#include <linux/audit.h>\\n#include <linux/personality.h>\\n#include <linux/time.h>\\n#include <linux/netlink.h>\\n#include <linux/compiler.h>\\n#include <asm/unistd.h>\\n#include <linux/security.h>\\n#include <linux/list.h>\\n#include <linux/binfmts.h>\\n#include <linux/highmem.h>\\n#include <linux/syscalls.h>\\n#include <asm/syscall.h>\\n#include <linux/capability.h>\\n#include <linux/fs_struct.h>\\n#include <linux/compat.h>\\n#include <linux/ctype.h>\\n#include <linux/string.h>\\n#include <linux/uaccess.h>\\n#include <linux/fsnotify_backend.h>\\n#include <uapi/linux/limits.h>\\n#include <uapi/linux/netfilter/nf_tables.h>\\n#include <uapi/linux/openat2.h> // struct open_how\\n#include <uapi/linux/fanotify.h>\\n\\n#include \"audit.h\"\\n\\n/* flags stating the success for a syscall */\\n#define AUDITSC_INVALID 0\\n#define AUDITSC_SUCCESS 1\\n#define AUDITSC_FAILURE 2\\n\\n/* no execve audit message should be longer than this (userspace limits),\\n * see the note near the top of audit_log_execve_info() about this value */\\n#define MAX_EXECVE_AUDIT_LEN 7500\\n\\n/* max length to print of cmdline/proctitle value during audit */\\n#define MAX_PROCTITLE_AUDIT_LEN 128\\n\\n/* number of audit rules */\\nint audit_n_rules;\\n\\n/* determines whether we collect data for signals sent */\\nint audit_signals;\\n\\nstruct audit_aux_data {\\n\\tstruct audit_aux_data\\t*next;\\n\\tint\\t\\t\\ttype;\\n};\\n\\n/* Number of target pids per aux struct. */\\n#define AUDIT_AUX_PIDS\\t16\\n\\nstruct audit_aux_data_pids {\\n\\tstruct audit_aux_data\\td;\\n\\tpid_t\\t\\t\\ttarget_pid[AUDIT_AUX_PIDS];\\n\\tkuid_t\\t\\t\\ttarget_auid[AUDIT_AUX_PIDS];\\n\\tkuid_t\\t\\t\\ttarget_uid[AUDIT_AUX_PIDS];\\n\\tunsigned int\\t\\ttarget_sessionid[AUDIT_AUX_PIDS];\\n\\tstruct lsm_prop\\t\\ttarget_ref[AUDIT_AUX_PIDS];\\n\\tchar \\t\\t\\ttarget_comm[AUDIT_AUX_PIDS][TASK_COMM_LEN];\\n\\tint\\t\\t\\tpid_count;\\n};\\n\\nstruct audit_aux_data_bprm_fcaps {\\n\\tstruct audit_aux_data\\td;\\n\\tstruct audit_cap_data\\tfcap;\\n\\tunsigned int\\t\\tfcap_ver;\\n\\tstruct audit_cap_data\\told_pcap;\\n\\tstruct audit_cap_data\\tnew_pcap;\\n};\\n\\nstruct audit_tree_refs {\\n\\tstruct audit_tree_refs *next;\\n\\tstruct audit_chunk *c[31];\\n};\\n\\nstruct audit_nfcfgop_tab {\\n\\tenum audit_nfcfgop\\top;\\n\\tconst char\\t\\t*s;\\n};\\n\\nstatic const struct audit_nfcfgop_tab audit_nfcfgs[] = {\\n\\t{ AUDIT_XT_OP_REGISTER,\\t\\t\\t\"xt_register\"\\t\\t   },\\n\\t{ AUDIT_XT_OP_REPLACE,\\t\\t\\t\"xt_replace\"\\t\\t   },\\n\\t{ AUDIT_XT_OP_UNREGISTER,\\t\\t\"xt_unregister\"\\t\\t   },\\n\\t{ AUDIT_NFT_OP_TABLE_REGISTER,\\t\\t\"nft_register_table\"\\t   },\\n\\t{ AUDIT_NFT_OP_TABLE_UNREGISTER,\\t\"nft_unregister_table\"\\t   },\\n\\t{ AUDIT_NFT_OP_CHAIN_REGISTER,\\t\\t\"nft_register_chain\"\\t   },\\n\\t{ AUDIT_NFT_OP_CHAIN_UNREGISTER,\\t\"nft_unregister_chain\"\\t   },\\n\\t{ AUDIT_NFT_OP_RULE_REGISTER,\\t\\t\"nft_register_rule\"\\t   },\\n\\t{ AUDIT_NFT_OP_RULE_UNREGISTER,\\t\\t\"nft_unregister_rule\"\\t   },\\n\\t{ AUDIT_NFT_OP_SET_REGISTER,\\t\\t\"nft_register_set\"\\t   },\\n\\t{ AUDIT_NFT_OP_SET_UNREGISTER,\\t\\t\"nft_unregister_set\"\\t   },\\n\\t{ AUDIT_NFT_OP_SETELEM_REGISTER,\\t\"nft_register_setelem\"\\t   },\\n\\t{ AUDIT_NFT_OP_SETELEM_UNREGISTER,\\t\"nft_unregister_setelem\"   },\\n\\t{ AUDIT_NFT_OP_GEN_REGISTER,\\t\\t\"nft_register_gen\"\\t   },\\n\\t{ AUDIT_NFT_OP_OBJ_REGISTER,\\t\\t\"nft_register_obj\"\\t   },\\n\\t{ AUDIT_NFT_OP_OBJ_UNREGISTER,\\t\\t\"nft_unregister_obj\"\\t   },\\n\\t{ AUDIT_NFT_OP_OBJ_RESET,\\t\\t\"nft_reset_obj\"\\t\\t   },\\n\\t{ AUDIT_NFT_OP_FLOWTABLE_REGISTER,\\t\"nft_register_flowtable\"   },\\n\\t{ AUDIT_NFT_OP_FLOWTABLE_UNREGISTER,\\t\"nft_unregister_flowtable\" },\\n\\t{ AUDIT_NFT_OP_SETELEM_RESET,\\t\\t\"nft_reset_setelem\"        },\\n\\t{ AUDIT_NFT_OP_RULE_RESET,\\t\\t\"nft_reset_rule\"           },\\n\\t{ AUDIT_NFT_OP_INVALID,\\t\\t\\t\"nft_invalid\"\\t\\t   },\\n};\\n\\nstatic int audit_match_perm(struct audit_context *ctx, int mask)\\n{\\n\\tunsigned n;\\n\\n\\tif (unlikely(!ctx))\\n\\t\\treturn 0;\\n\\tn = ctx->major;\\n\\n\\tswitch (audit_classify_syscall(ctx->arch, n)) {\\n\\tcase AUDITSC_NATIVE:\\n\\t\\tif ((mask & AUDIT_PERM_WRITE) &&\\n\\t\\t     audit_match_class(AUDIT_CLASS_WRITE, n))\\n\\t\\t\\treturn 1;\\n\\t\\tif ((mask & AUDIT_PERM_READ) &&\\n\\t\\t     audit_match_class(AUDIT_CLASS_READ, n))\\n\\t\\t\\treturn 1;\\n\\t\\tif ((mask & AUDIT_PERM_ATTR) &&\\n\\t\\t     audit_match_class(AUDIT_CLASS_CHATTR, n))\\n\\t\\t\\treturn 1;\\n\\t\\treturn 0;\\n\\tcase AUDITSC_COMPAT: /* 32bit on biarch */\\n\\t\\tif ((mask & AUDIT_PERM_WRITE) &&\\n\\t\\t     audit_match_class(AUDIT_CLASS_WRITE_32, n))\\n\\t\\t\\treturn 1;\\n\\t\\tif ((mask & AUDIT_PERM_READ) &&\\n\\t\\t     audit_match_class(AUDIT_CLASS_READ_32, n))\\n\\t\\t\\treturn 1;\\n\\t\\tif ((mask & AUDIT_PERM_ATTR) &&\\n\\t\\t     audit_match_class(AUDIT_CLASS_CHATTR_32, n))\\n\\t\\t\\treturn 1;\\n\\t\\treturn 0;\\n\\tcase AUDITSC_OPEN:\\n\\t\\treturn mask & ACC_MODE(ctx->argv[1]);\\n\\tcase AUDITSC_OPENAT:\\n\\t\\treturn mask & ACC_MODE(ctx->argv[2]);\\n\\tcase AUDITSC_SOCKETCALL:\\n\\t\\treturn ((mask & AUDIT_PERM_WRITE) && ctx->argv[0] == SYS_BIND);\\n\\tcase AUDITSC_EXECVE:\\n\\t\\treturn mask & AUDIT_PERM_EXEC;\\n\\tcase AUDITSC_OPENAT2:\\n\\t\\treturn mask & ACC_MODE((u32)ctx->openat2.flags);\\n\\tdefault:\\n\\t\\treturn 0;\\n\\t}\\n}\\n\\nstatic int audit_match_filetype(struct audit_context *ctx, int val)\\n{\\n\\tstruct audit_names *n;\\n\\tumode_t mode = (umode_t)val;\\n\\n\\tif (unlikely(!ctx))\\n\\t\\treturn 0;\\n\\n\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\tif ((n->ino != AUDIT_INO_UNSET) &&\\n\\t\\t    ((n->mode & S_IFMT) == mode))\\n\\t\\t\\treturn 1;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * We keep a linked list of fixed-sized (31 pointer) arrays of audit_chunk *;\\n * ->first_trees points to its beginning, ->trees - to the current end of data.\\n * ->tree_count is the number of free entries in array pointed to by ->trees.\\n * Original condition is (NULL, NULL, 0); as soon as it grows we never revert to NULL,\\n * \"empty\" becomes (p, p, 31) afterwards.  We don\\'t shrink the list (and seriously,\\n * it\\'s going to remain 1-element for almost any setup) until we free context itself.\\n * References in it _are_ dropped - at the same time we free/drop aux stuff.\\n */\\n\\nstatic void audit_set_auditable(struct audit_context *ctx)\\n{\\n\\tif (!ctx->prio) {\\n\\t\\tctx->prio = 1;\\n\\t\\tctx->current_state = AUDIT_STATE_RECORD;\\n\\t}\\n}\\n\\nstatic int put_tree_ref(struct audit_context *ctx, struct audit_chunk *chunk)\\n{\\n\\tstruct audit_tree_refs *p = ctx->trees;\\n\\tint left = ctx->tree_count;\\n\\n\\tif (likely(left)) {\\n\\t\\tp->c[--left] = chunk;\\n\\t\\tctx->tree_count = left;\\n\\t\\treturn 1;\\n\\t}\\n\\tif (!p)\\n\\t\\treturn 0;\\n\\tp = p->next;\\n\\tif (p) {\\n\\t\\tp->c[30] = chunk;\\n\\t\\tctx->trees = p;\\n\\t\\tctx->tree_count = 30;\\n\\t\\treturn 1;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int grow_tree_refs(struct audit_context *ctx)\\n{\\n\\tstruct audit_tree_refs *p = ctx->trees;\\n\\n\\tctx->trees = kzalloc(sizeof(struct audit_tree_refs), GFP_KERNEL);\\n\\tif (!ctx->trees) {\\n\\t\\tctx->trees = p;\\n\\t\\treturn 0;\\n\\t}\\n\\tif (p)\\n\\t\\tp->next = ctx->trees;\\n\\telse\\n\\t\\tctx->first_trees = ctx->trees;\\n\\tctx->tree_count = 31;\\n\\treturn 1;\\n}\\n\\nstatic void unroll_tree_refs(struct audit_context *ctx,\\n\\t\\t      struct audit_tree_refs *p, int count)\\n{\\n\\tstruct audit_tree_refs *q;\\n\\tint n;\\n\\n\\tif (!p) {\\n\\t\\t/* we started with empty chain */\\n\\t\\tp = ctx->first_trees;\\n\\t\\tcount = 31;\\n\\t\\t/* if the very first allocation has failed, nothing to do */\\n\\t\\tif (!p)\\n\\t\\t\\treturn;\\n\\t}\\n\\tn = count;\\n\\tfor (q = p; q != ctx->trees; q = q->next, n = 31) {\\n\\t\\twhile (n--) {\\n\\t\\t\\taudit_put_chunk(q->c[n]);\\n\\t\\t\\tq->c[n] = NULL;\\n\\t\\t}\\n\\t}\\n\\twhile (n-- > ctx->tree_count) {\\n\\t\\taudit_put_chunk(q->c[n]);\\n\\t\\tq->c[n] = NULL;\\n\\t}\\n\\tctx->trees = p;\\n\\tctx->tree_count = count;\\n}\\n\\nstatic void free_tree_refs(struct audit_context *ctx)\\n{\\n\\tstruct audit_tree_refs *p, *q;\\n\\n\\tfor (p = ctx->first_trees; p; p = q) {\\n\\t\\tq = p->next;\\n\\t\\tkfree(p);\\n\\t}\\n}\\n\\nstatic int match_tree_refs(struct audit_context *ctx, struct audit_tree *tree)\\n{\\n\\tstruct audit_tree_refs *p;\\n\\tint n;\\n\\n\\tif (!tree)\\n\\t\\treturn 0;\\n\\t/* full ones */\\n\\tfor (p = ctx->first_trees; p != ctx->trees; p = p->next) {\\n\\t\\tfor (n = 0; n < 31; n++)\\n\\t\\t\\tif (audit_tree_match(p->c[n], tree))\\n\\t\\t\\t\\treturn 1;\\n\\t}\\n\\t/* partial */\\n\\tif (p) {\\n\\t\\tfor (n = ctx->tree_count; n < 31; n++)\\n\\t\\t\\tif (audit_tree_match(p->c[n], tree))\\n\\t\\t\\t\\treturn 1;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int audit_compare_uid(kuid_t uid,\\n\\t\\t\\t     struct audit_names *name,\\n\\t\\t\\t     struct audit_field *f,\\n\\t\\t\\t     struct audit_context *ctx)\\n{\\n\\tstruct audit_names *n;\\n\\tint rc;\\n\\n\\tif (name) {\\n\\t\\trc = audit_uid_comparator(uid, f->op, name->uid);\\n\\t\\tif (rc)\\n\\t\\t\\treturn rc;\\n\\t}\\n\\n\\tif (ctx) {\\n\\t\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\t\\trc = audit_uid_comparator(uid, f->op, n->uid);\\n\\t\\t\\tif (rc)\\n\\t\\t\\t\\treturn rc;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int audit_compare_gid(kgid_t gid,\\n\\t\\t\\t     struct audit_names *name,\\n\\t\\t\\t     struct audit_field *f,\\n\\t\\t\\t     struct audit_context *ctx)\\n{\\n\\tstruct audit_names *n;\\n\\tint rc;\\n\\n\\tif (name) {\\n\\t\\trc = audit_gid_comparator(gid, f->op, name->gid);\\n\\t\\tif (rc)\\n\\t\\t\\treturn rc;\\n\\t}\\n\\n\\tif (ctx) {\\n\\t\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\t\\trc = audit_gid_comparator(gid, f->op, n->gid);\\n\\t\\t\\tif (rc)\\n\\t\\t\\t\\treturn rc;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int audit_field_compare(struct task_struct *tsk,\\n\\t\\t\\t       const struct cred *cred,\\n\\t\\t\\t       struct audit_field *f,\\n\\t\\t\\t       struct audit_context *ctx,\\n\\t\\t\\t       struct audit_names *name)\\n{\\n\\tswitch (f->val) {\\n\\t/* process to file object comparisons */\\n\\tcase AUDIT_COMPARE_UID_TO_OBJ_UID:\\n\\t\\treturn audit_compare_uid(cred->uid, name, f, ctx);\\n\\tcase AUDIT_COMPARE_GID_TO_OBJ_GID:\\n\\t\\treturn audit_compare_gid(cred->gid, name, f, ctx);\\n\\tcase AUDIT_COMPARE_EUID_TO_OBJ_UID:\\n\\t\\treturn audit_compare_uid(cred->euid, name, f, ctx);\\n\\tcase AUDIT_COMPARE_EGID_TO_OBJ_GID:\\n\\t\\treturn audit_compare_gid(cred->egid, name, f, ctx);\\n\\tcase AUDIT_COMPARE_AUID_TO_OBJ_UID:\\n\\t\\treturn audit_compare_uid(audit_get_loginuid(tsk), name, f, ctx);\\n\\tcase AUDIT_COMPARE_SUID_TO_OBJ_UID:\\n\\t\\treturn audit_compare_uid(cred->suid, name, f, ctx);\\n\\tcase AUDIT_COMPARE_SGID_TO_OBJ_GID:\\n\\t\\treturn audit_compare_gid(cred->sgid, name, f, ctx);\\n\\tcase AUDIT_COMPARE_FSUID_TO_OBJ_UID:\\n\\t\\treturn audit_compare_uid(cred->fsuid, name, f, ctx);\\n\\tcase AUDIT_COMPARE_FSGID_TO_OBJ_GID:\\n\\t\\treturn audit_compare_gid(cred->fsgid, name, f, ctx);\\n\\t/* uid comparisons */\\n\\tcase AUDIT_COMPARE_UID_TO_AUID:\\n\\t\\treturn audit_uid_comparator(cred->uid, f->op,\\n\\t\\t\\t\\t\\t    audit_get_loginuid(tsk));\\n\\tcase AUDIT_COMPARE_UID_TO_EUID:\\n\\t\\treturn audit_uid_comparator(cred->uid, f->op, cred->euid);\\n\\tcase AUDIT_COMPARE_UID_TO_SUID:\\n\\t\\treturn audit_uid_comparator(cred->uid, f->op, cred->suid);\\n\\tcase AUDIT_COMPARE_UID_TO_FSUID:\\n\\t\\treturn audit_uid_comparator(cred->uid, f->op, cred->fsuid);\\n\\t/* auid comparisons */\\n\\tcase AUDIT_COMPARE_AUID_TO_EUID:\\n\\t\\treturn audit_uid_comparator(audit_get_loginuid(tsk), f->op,\\n\\t\\t\\t\\t\\t    cred->euid);\\n\\tcase AUDIT_COMPARE_AUID_TO_SUID:\\n\\t\\treturn audit_uid_comparator(audit_get_loginuid(tsk), f->op,\\n\\t\\t\\t\\t\\t    cred->suid);\\n\\tcase AUDIT_COMPARE_AUID_TO_FSUID:\\n\\t\\treturn audit_uid_comparator(audit_get_loginuid(tsk), f->op,\\n\\t\\t\\t\\t\\t    cred->fsuid);\\n\\t/* euid comparisons */\\n\\tcase AUDIT_COMPARE_EUID_TO_SUID:\\n\\t\\treturn audit_uid_comparator(cred->euid, f->op, cred->suid);\\n\\tcase AUDIT_COMPARE_EUID_TO_FSUID:\\n\\t\\treturn audit_uid_comparator(cred->euid, f->op, cred->fsuid);\\n\\t/* suid comparisons */\\n\\tcase AUDIT_COMPARE_SUID_TO_FSUID:\\n\\t\\treturn audit_uid_comparator(cred->suid, f->op, cred->fsuid);\\n\\t/* gid comparisons */\\n\\tcase AUDIT_COMPARE_GID_TO_EGID:\\n\\t\\treturn audit_gid_comparator(cred->gid, f->op, cred->egid);\\n\\tcase AUDIT_COMPARE_GID_TO_SGID:\\n\\t\\treturn audit_gid_comparator(cred->gid, f->op, cred->sgid);\\n\\tcase AUDIT_COMPARE_GID_TO_FSGID:\\n\\t\\treturn audit_gid_comparator(cred->gid, f->op, cred->fsgid);\\n\\t/* egid comparisons */\\n\\tcase AUDIT_COMPARE_EGID_TO_SGID:\\n\\t\\treturn audit_gid_comparator(cred->egid, f->op, cred->sgid);\\n\\tcase AUDIT_COMPARE_EGID_TO_FSGID:\\n\\t\\treturn audit_gid_comparator(cred->egid, f->op, cred->fsgid);\\n\\t/* sgid comparison */\\n\\tcase AUDIT_COMPARE_SGID_TO_FSGID:\\n\\t\\treturn audit_gid_comparator(cred->sgid, f->op, cred->fsgid);\\n\\tdefault:\\n\\t\\tWARN(1, \"Missing AUDIT_COMPARE define.  Report as a bug\\\\n\");\\n\\t\\treturn 0;\\n\\t}\\n\\treturn 0;\\n}\\n\\n/* Determine if any context name data matches a rule\\'s watch data */\\n/* Compare a task_struct with an audit_rule.  Return 1 on match, 0\\n * otherwise.\\n *\\n * If task_creation is true, this is an explicit indication that we are\\n * filtering a task rule at task creation time.  This and tsk == current are\\n * the only situations where tsk->cred may be accessed without an rcu read lock.\\n */\\nstatic int audit_filter_rules(struct task_struct *tsk,\\n\\t\\t\\t      struct audit_krule *rule,\\n\\t\\t\\t      struct audit_context *ctx,\\n\\t\\t\\t      struct audit_names *name,\\n\\t\\t\\t      enum audit_state *state,\\n\\t\\t\\t      bool task_creation)\\n{\\n\\tconst struct cred *cred;\\n\\tint i, need_sid = 1;\\n\\tstruct lsm_prop prop = { };\\n\\tunsigned int sessionid;\\n\\n\\tif (ctx && rule->prio <= ctx->prio)\\n\\t\\treturn 0;\\n\\n\\tcred = rcu_dereference_check(tsk->cred, tsk == current || task_creation);\\n\\n\\tfor (i = 0; i < rule->field_count; i++) {\\n\\t\\tstruct audit_field *f = &rule->fields[i];\\n\\t\\tstruct audit_names *n;\\n\\t\\tint result = 0;\\n\\t\\tpid_t pid;\\n\\n\\t\\tswitch (f->type) {\\n\\t\\tcase AUDIT_PID:\\n\\t\\t\\tpid = task_tgid_nr(tsk);\\n\\t\\t\\tresult = audit_comparator(pid, f->op, f->val);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_PPID:\\n\\t\\t\\tif (ctx) {\\n\\t\\t\\t\\tif (!ctx->ppid)\\n\\t\\t\\t\\t\\tctx->ppid = task_ppid_nr(tsk);\\n\\t\\t\\t\\tresult = audit_comparator(ctx->ppid, f->op, f->val);\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_EXE:\\n\\t\\t\\tresult = audit_exe_compare(tsk, rule->exe);\\n\\t\\t\\tif (f->op == Audit_not_equal)\\n\\t\\t\\t\\tresult = !result;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_UID:\\n\\t\\t\\tresult = audit_uid_comparator(cred->uid, f->op, f->uid);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_EUID:\\n\\t\\t\\tresult = audit_uid_comparator(cred->euid, f->op, f->uid);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_SUID:\\n\\t\\t\\tresult = audit_uid_comparator(cred->suid, f->op, f->uid);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FSUID:\\n\\t\\t\\tresult = audit_uid_comparator(cred->fsuid, f->op, f->uid);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_GID:\\n\\t\\t\\tresult = audit_gid_comparator(cred->gid, f->op, f->gid);\\n\\t\\t\\tif (f->op == Audit_equal) {\\n\\t\\t\\t\\tif (!result)\\n\\t\\t\\t\\t\\tresult = groups_search(cred->group_info, f->gid);\\n\\t\\t\\t} else if (f->op == Audit_not_equal) {\\n\\t\\t\\t\\tif (result)\\n\\t\\t\\t\\t\\tresult = !groups_search(cred->group_info, f->gid);\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_EGID:\\n\\t\\t\\tresult = audit_gid_comparator(cred->egid, f->op, f->gid);\\n\\t\\t\\tif (f->op == Audit_equal) {\\n\\t\\t\\t\\tif (!result)\\n\\t\\t\\t\\t\\tresult = groups_search(cred->group_info, f->gid);\\n\\t\\t\\t} else if (f->op == Audit_not_equal) {\\n\\t\\t\\t\\tif (result)\\n\\t\\t\\t\\t\\tresult = !groups_search(cred->group_info, f->gid);\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_SGID:\\n\\t\\t\\tresult = audit_gid_comparator(cred->sgid, f->op, f->gid);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FSGID:\\n\\t\\t\\tresult = audit_gid_comparator(cred->fsgid, f->op, f->gid);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_SESSIONID:\\n\\t\\t\\tsessionid = audit_get_sessionid(tsk);\\n\\t\\t\\tresult = audit_comparator(sessionid, f->op, f->val);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_PERS:\\n\\t\\t\\tresult = audit_comparator(tsk->personality, f->op, f->val);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_ARCH:\\n\\t\\t\\tif (ctx)\\n\\t\\t\\t\\tresult = audit_comparator(ctx->arch, f->op, f->val);\\n\\t\\t\\tbreak;\\n\\n\\t\\tcase AUDIT_EXIT:\\n\\t\\t\\tif (ctx && ctx->return_valid != AUDITSC_INVALID)\\n\\t\\t\\t\\tresult = audit_comparator(ctx->return_code, f->op, f->val);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_SUCCESS:\\n\\t\\t\\tif (ctx && ctx->return_valid != AUDITSC_INVALID) {\\n\\t\\t\\t\\tif (f->val)\\n\\t\\t\\t\\t\\tresult = audit_comparator(ctx->return_valid, f->op, AUDITSC_SUCCESS);\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tresult = audit_comparator(ctx->return_valid, f->op, AUDITSC_FAILURE);\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_DEVMAJOR:\\n\\t\\t\\tif (name) {\\n\\t\\t\\t\\tif (audit_comparator(MAJOR(name->dev), f->op, f->val) ||\\n\\t\\t\\t\\t    audit_comparator(MAJOR(name->rdev), f->op, f->val))\\n\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t} else if (ctx) {\\n\\t\\t\\t\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\t\\t\\t\\tif (audit_comparator(MAJOR(n->dev), f->op, f->val) ||\\n\\t\\t\\t\\t\\t    audit_comparator(MAJOR(n->rdev), f->op, f->val)) {\\n\\t\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_DEVMINOR:\\n\\t\\t\\tif (name) {\\n\\t\\t\\t\\tif (audit_comparator(MINOR(name->dev), f->op, f->val) ||\\n\\t\\t\\t\\t    audit_comparator(MINOR(name->rdev), f->op, f->val))\\n\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t} else if (ctx) {\\n\\t\\t\\t\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\t\\t\\t\\tif (audit_comparator(MINOR(n->dev), f->op, f->val) ||\\n\\t\\t\\t\\t\\t    audit_comparator(MINOR(n->rdev), f->op, f->val)) {\\n\\t\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_INODE:\\n\\t\\t\\tif (name)\\n\\t\\t\\t\\tresult = audit_comparator(name->ino, f->op, f->val);\\n\\t\\t\\telse if (ctx) {\\n\\t\\t\\t\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\t\\t\\t\\tif (audit_comparator(n->ino, f->op, f->val)) {\\n\\t\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_OBJ_UID:\\n\\t\\t\\tif (name) {\\n\\t\\t\\t\\tresult = audit_uid_comparator(name->uid, f->op, f->uid);\\n\\t\\t\\t} else if (ctx) {\\n\\t\\t\\t\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\t\\t\\t\\tif (audit_uid_comparator(n->uid, f->op, f->uid)) {\\n\\t\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_OBJ_GID:\\n\\t\\t\\tif (name) {\\n\\t\\t\\t\\tresult = audit_gid_comparator(name->gid, f->op, f->gid);\\n\\t\\t\\t} else if (ctx) {\\n\\t\\t\\t\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\t\\t\\t\\tif (audit_gid_comparator(n->gid, f->op, f->gid)) {\\n\\t\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_WATCH:\\n\\t\\t\\tif (name) {\\n\\t\\t\\t\\tresult = audit_watch_compare(rule->watch,\\n\\t\\t\\t\\t\\t\\t\\t     name->ino,\\n\\t\\t\\t\\t\\t\\t\\t     name->dev);\\n\\t\\t\\t\\tif (f->op == Audit_not_equal)\\n\\t\\t\\t\\t\\tresult = !result;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_DIR:\\n\\t\\t\\tif (ctx) {\\n\\t\\t\\t\\tresult = match_tree_refs(ctx, rule->tree);\\n\\t\\t\\t\\tif (f->op == Audit_not_equal)\\n\\t\\t\\t\\t\\tresult = !result;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_LOGINUID:\\n\\t\\t\\tresult = audit_uid_comparator(audit_get_loginuid(tsk),\\n\\t\\t\\t\\t\\t\\t      f->op, f->uid);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_LOGINUID_SET:\\n\\t\\t\\tresult = audit_comparator(audit_loginuid_set(tsk), f->op, f->val);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_SADDR_FAM:\\n\\t\\t\\tif (ctx && ctx->sockaddr)\\n\\t\\t\\t\\tresult = audit_comparator(ctx->sockaddr->ss_family,\\n\\t\\t\\t\\t\\t\\t\\t  f->op, f->val);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_SUBJ_USER:\\n\\t\\tcase AUDIT_SUBJ_ROLE:\\n\\t\\tcase AUDIT_SUBJ_TYPE:\\n\\t\\tcase AUDIT_SUBJ_SEN:\\n\\t\\tcase AUDIT_SUBJ_CLR:\\n\\t\\t\\t/* NOTE: this may return negative values indicating\\n\\t\\t\\t   a temporary error.  We simply treat this as a\\n\\t\\t\\t   match for now to avoid losing information that\\n\\t\\t\\t   may be wanted.   An error message will also be\\n\\t\\t\\t   logged upon error */\\n\\t\\t\\tif (f->lsm_rule) {\\n\\t\\t\\t\\tif (need_sid) {\\n\\t\\t\\t\\t\\t/* @tsk should always be equal to\\n\\t\\t\\t\\t\\t * @current with the exception of\\n\\t\\t\\t\\t\\t * fork()/copy_process() in which case\\n\\t\\t\\t\\t\\t * the new @tsk creds are still a dup\\n\\t\\t\\t\\t\\t * of @current\\'s creds so we can still\\n\\t\\t\\t\\t\\t * use\\n\\t\\t\\t\\t\\t * security_current_getlsmprop_subj()\\n\\t\\t\\t\\t\\t * here even though it always refs\\n\\t\\t\\t\\t\\t * @current\\'s creds\\n\\t\\t\\t\\t\\t */\\n\\t\\t\\t\\t\\tsecurity_current_getlsmprop_subj(&prop);\\n\\t\\t\\t\\t\\tneed_sid = 0;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tresult = security_audit_rule_match(&prop,\\n\\t\\t\\t\\t\\t\\t\\t\\t   f->type,\\n\\t\\t\\t\\t\\t\\t\\t\\t   f->op,\\n\\t\\t\\t\\t\\t\\t\\t\\t   f->lsm_rule);\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_OBJ_USER:\\n\\t\\tcase AUDIT_OBJ_ROLE:\\n\\t\\tcase AUDIT_OBJ_TYPE:\\n\\t\\tcase AUDIT_OBJ_LEV_LOW:\\n\\t\\tcase AUDIT_OBJ_LEV_HIGH:\\n\\t\\t\\t/* The above note for AUDIT_SUBJ_USER...AUDIT_SUBJ_CLR\\n\\t\\t\\t   also applies here */\\n\\t\\t\\tif (f->lsm_rule) {\\n\\t\\t\\t\\t/* Find files that match */\\n\\t\\t\\t\\tif (name) {\\n\\t\\t\\t\\t\\tresult = security_audit_rule_match(\\n\\t\\t\\t\\t\\t\\t\\t\\t&name->oprop,\\n\\t\\t\\t\\t\\t\\t\\t\\tf->type,\\n\\t\\t\\t\\t\\t\\t\\t\\tf->op,\\n\\t\\t\\t\\t\\t\\t\\t\\tf->lsm_rule);\\n\\t\\t\\t\\t} else if (ctx) {\\n\\t\\t\\t\\t\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\t\\t\\t\\t\\tif (security_audit_rule_match(\\n\\t\\t\\t\\t\\t\\t\\t\\t&n->oprop,\\n\\t\\t\\t\\t\\t\\t\\t\\tf->type,\\n\\t\\t\\t\\t\\t\\t\\t\\tf->op,\\n\\t\\t\\t\\t\\t\\t\\t\\tf->lsm_rule)) {\\n\\t\\t\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t/* Find ipc objects that match */\\n\\t\\t\\t\\tif (!ctx || ctx->type != AUDIT_IPC)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tif (security_audit_rule_match(&ctx->ipc.oprop,\\n\\t\\t\\t\\t\\t\\t\\t      f->type, f->op,\\n\\t\\t\\t\\t\\t\\t\\t      f->lsm_rule))\\n\\t\\t\\t\\t\\t++result;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_ARG0:\\n\\t\\tcase AUDIT_ARG1:\\n\\t\\tcase AUDIT_ARG2:\\n\\t\\tcase AUDIT_ARG3:\\n\\t\\t\\tif (ctx)\\n\\t\\t\\t\\tresult = audit_comparator(ctx->argv[f->type-AUDIT_ARG0], f->op, f->val);\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FILTERKEY:\\n\\t\\t\\t/* ignore this field for filtering */\\n\\t\\t\\tresult = 1;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_PERM:\\n\\t\\t\\tresult = audit_match_perm(ctx, f->val);\\n\\t\\t\\tif (f->op == Audit_not_equal)\\n\\t\\t\\t\\tresult = !result;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FILETYPE:\\n\\t\\t\\tresult = audit_match_filetype(ctx, f->val);\\n\\t\\t\\tif (f->op == Audit_not_equal)\\n\\t\\t\\t\\tresult = !result;\\n\\t\\t\\tbreak;\\n\\t\\tcase AUDIT_FIELD_COMPARE:\\n\\t\\t\\tresult = audit_field_compare(tsk, cred, f, ctx, name);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tif (!result)\\n\\t\\t\\treturn 0;\\n\\t}\\n\\n\\tif (ctx) {\\n\\t\\tif (rule->filterkey) {\\n\\t\\t\\tkfree(ctx->filterkey);\\n\\t\\t\\tctx->filterkey = kstrdup(rule->filterkey, GFP_ATOMIC);\\n\\t\\t}\\n\\t\\tctx->prio = rule->prio;\\n\\t}\\n\\tswitch (rule->action) {\\n\\tcase AUDIT_NEVER:\\n\\t\\t*state = AUDIT_STATE_DISABLED;\\n\\t\\tbreak;\\n\\tcase AUDIT_ALWAYS:\\n\\t\\t*state = AUDIT_STATE_RECORD;\\n\\t\\tbreak;\\n\\t}\\n\\treturn 1;\\n}\\n\\n/* At process creation time, we can determine if system-call auditing is\\n * completely disabled for this task.  Since we only have the task\\n * structure at this point, we can only check uid and gid.\\n */\\nstatic enum audit_state audit_filter_task(struct task_struct *tsk, char **key)\\n{\\n\\tstruct audit_entry *e;\\n\\tenum audit_state   state;\\n\\n\\trcu_read_lock();\\n\\tlist_for_each_entry_rcu(e, &audit_filter_list[AUDIT_FILTER_TASK], list) {\\n\\t\\tif (audit_filter_rules(tsk, &e->rule, NULL, NULL,\\n\\t\\t\\t\\t       &state, true)) {\\n\\t\\t\\tif (state == AUDIT_STATE_RECORD)\\n\\t\\t\\t\\t*key = kstrdup(e->rule.filterkey, GFP_ATOMIC);\\n\\t\\t\\trcu_read_unlock();\\n\\t\\t\\treturn state;\\n\\t\\t}\\n\\t}\\n\\trcu_read_unlock();\\n\\treturn AUDIT_STATE_BUILD;\\n}\\n\\nstatic int audit_in_mask(const struct audit_krule *rule, unsigned long val)\\n{\\n\\tint word, bit;\\n\\n\\tif (val > 0xffffffff)\\n\\t\\treturn false;\\n\\n\\tword = AUDIT_WORD(val);\\n\\tif (word >= AUDIT_BITMASK_SIZE)\\n\\t\\treturn false;\\n\\n\\tbit = AUDIT_BIT(val);\\n\\n\\treturn rule->mask[word] & bit;\\n}\\n\\n/**\\n * __audit_filter_op - common filter helper for operations (syscall/uring/etc)\\n * @tsk: associated task\\n * @ctx: audit context\\n * @list: audit filter list\\n * @name: audit_name (can be NULL)\\n * @op: current syscall/uring_op\\n *\\n * Run the udit filters specified in @list against @tsk using @ctx,\\n * @name, and @op, as necessary; the caller is responsible for ensuring\\n * that the call is made while the RCU read lock is held. The @name\\n * parameter can be NULL, but all others must be specified.\\n * Returns 1/true if the filter finds a match, 0/false if none are found.\\n */\\nstatic int __audit_filter_op(struct task_struct *tsk,\\n\\t\\t\\t   struct audit_context *ctx,\\n\\t\\t\\t   struct list_head *list,\\n\\t\\t\\t   struct audit_names *name,\\n\\t\\t\\t   unsigned long op)\\n{\\n\\tstruct audit_entry *e;\\n\\tenum audit_state state;\\n\\n\\tlist_for_each_entry_rcu(e, list, list) {\\n\\t\\tif (audit_in_mask(&e->rule, op) &&\\n\\t\\t    audit_filter_rules(tsk, &e->rule, ctx, name,\\n\\t\\t\\t\\t       &state, false)) {\\n\\t\\t\\tctx->current_state = state;\\n\\t\\t\\treturn 1;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\n/**\\n * audit_filter_uring - apply filters to an io_uring operation\\n * @tsk: associated task\\n * @ctx: audit context\\n */\\nstatic void audit_filter_uring(struct task_struct *tsk,\\n\\t\\t\\t       struct audit_context *ctx)\\n{\\n\\tif (auditd_test_task(tsk))\\n\\t\\treturn;\\n\\n\\trcu_read_lock();\\n\\t__audit_filter_op(tsk, ctx, &audit_filter_list[AUDIT_FILTER_URING_EXIT],\\n\\t\\t\\tNULL, ctx->uring_op);\\n\\trcu_read_unlock();\\n}\\n\\n/* At syscall exit time, this filter is called if the audit_state is\\n * not low enough that auditing cannot take place, but is also not\\n * high enough that we already know we have to write an audit record\\n * (i.e., the state is AUDIT_STATE_BUILD).\\n */\\nstatic void audit_filter_syscall(struct task_struct *tsk,\\n\\t\\t\\t\\t struct audit_context *ctx)\\n{\\n\\tif (auditd_test_task(tsk))\\n\\t\\treturn;\\n\\n\\trcu_read_lock();\\n\\t__audit_filter_op(tsk, ctx, &audit_filter_list[AUDIT_FILTER_EXIT],\\n\\t\\t\\tNULL, ctx->major);\\n\\trcu_read_unlock();\\n}\\n\\n/*\\n * Given an audit_name check the inode hash table to see if they match.\\n * Called holding the rcu read lock to protect the use of audit_inode_hash\\n */\\nstatic int audit_filter_inode_name(struct task_struct *tsk,\\n\\t\\t\\t\\t   struct audit_names *n,\\n\\t\\t\\t\\t   struct audit_context *ctx)\\n{\\n\\tint h = audit_hash_ino((u32)n->ino);\\n\\tstruct list_head *list = &audit_inode_hash[h];\\n\\n\\treturn __audit_filter_op(tsk, ctx, list, n, ctx->major);\\n}\\n\\n/* At syscall exit time, this filter is called if any audit_names have been\\n * collected during syscall processing.  We only check rules in sublists at hash\\n * buckets applicable to the inode numbers in audit_names.\\n * Regarding audit_state, same rules apply as for audit_filter_syscall().\\n */\\nvoid audit_filter_inodes(struct task_struct *tsk, struct audit_context *ctx)\\n{\\n\\tstruct audit_names *n;\\n\\n\\tif (auditd_test_task(tsk))\\n\\t\\treturn;\\n\\n\\trcu_read_lock();\\n\\n\\tlist_for_each_entry(n, &ctx->names_list, list) {\\n\\t\\tif (audit_filter_inode_name(tsk, n, ctx))\\n\\t\\t\\tbreak;\\n\\t}\\n\\trcu_read_unlock();\\n}\\n\\nstatic inline void audit_proctitle_free(struct audit_context *context)\\n{\\n\\tkfree(context->proctitle.value);\\n\\tcontext->proctitle.value = NULL;\\n\\tcontext->proctitle.len = 0;\\n}\\n\\nstatic inline void audit_free_module(struct audit_context *context)\\n{\\n\\tif (context->type == AUDIT_KERN_MODULE) {\\n\\t\\tkfree(context->module.name);\\n\\t\\tcontext->module.name = NULL;\\n\\t}\\n}\\nstatic inline void audit_free_names(struct audit_context *context)\\n{\\n\\tstruct audit_names *n, *next;\\n\\n\\tlist_for_each_entry_safe(n, next, &context->names_list, list) {\\n\\t\\tlist_del(&n->list);\\n\\t\\tif (n->name)\\n\\t\\t\\tputname(n->name);\\n\\t\\tif (n->should_free)\\n\\t\\t\\tkfree(n);\\n\\t}\\n\\tcontext->name_count = 0;\\n\\tpath_put(&context->pwd);\\n\\tcontext->pwd.dentry = NULL;\\n\\tcontext->pwd.mnt = NULL;\\n}\\n\\nstatic inline void audit_free_aux(struct audit_context *context)\\n{\\n\\tstruct audit_aux_data *aux;\\n\\n\\twhile ((aux = context->aux)) {\\n\\t\\tcontext->aux = aux->next;\\n\\t\\tkfree(aux);\\n\\t}\\n\\tcontext->aux = NULL;\\n\\twhile ((aux = context->aux_pids)) {\\n\\t\\tcontext->aux_pids = aux->next;\\n\\t\\tkfree(aux);\\n\\t}\\n\\tcontext->aux_pids = NULL;\\n}\\n\\n/**\\n * audit_reset_context - reset a audit_context structure\\n * @ctx: the audit_context to reset\\n *\\n * All fields in the audit_context will be reset to an initial state, all\\n * references held by fields will be dropped, and private memory will be\\n * released.  When this function returns the audit_context will be suitable\\n * for reuse, so long as the passed context is not NULL or a dummy context.\\n */\\nstatic void audit_reset_context(struct audit_context *ctx)\\n{\\n\\tif (!ctx)\\n\\t\\treturn;\\n\\n\\t/* if ctx is non-null, reset the \"ctx->context\" regardless */\\n\\tctx->context = AUDIT_CTX_UNUSED;\\n\\tif (ctx->dummy)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * NOTE: It shouldn\\'t matter in what order we release the fields, so\\n\\t *       release them in the order in which they appear in the struct;\\n\\t *       this gives us some hope of quickly making sure we are\\n\\t *       resetting the audit_context properly.\\n\\t *\\n\\t *       Other things worth mentioning:\\n\\t *       - we don\\'t reset \"dummy\"\\n\\t *       - we don\\'t reset \"state\", we do reset \"current_state\"\\n\\t *       - we preserve \"filterkey\" if \"state\" is AUDIT_STATE_RECORD\\n\\t *       - much of this is likely overkill, but play it safe for now\\n\\t *       - we really need to work on improving the audit_context struct\\n\\t */\\n\\n\\tctx->current_state = ctx->state;\\n\\tctx->serial = 0;\\n\\tctx->major = 0;\\n\\tctx->uring_op = 0;\\n\\tctx->ctime = (struct timespec64){ .tv_sec = 0, .tv_nsec = 0 };\\n\\tmemset(ctx->argv, 0, sizeof(ctx->argv));\\n\\tctx->return_code = 0;\\n\\tctx->prio = (ctx->state == AUDIT_STATE_RECORD ? ~0ULL : 0);\\n\\tctx->return_valid = AUDITSC_INVALID;\\n\\taudit_free_names(ctx);\\n\\tif (ctx->state != AUDIT_STATE_RECORD) {\\n\\t\\tkfree(ctx->filterkey);\\n\\t\\tctx->filterkey = NULL;\\n\\t}\\n\\taudit_free_aux(ctx);\\n\\tkfree(ctx->sockaddr);\\n\\tctx->sockaddr = NULL;\\n\\tctx->sockaddr_len = 0;\\n\\tctx->ppid = 0;\\n\\tctx->uid = ctx->euid = ctx->suid = ctx->fsuid = KUIDT_INIT(0);\\n\\tctx->gid = ctx->egid = ctx->sgid = ctx->fsgid = KGIDT_INIT(0);\\n\\tctx->personality = 0;\\n\\tctx->arch = 0;\\n\\tctx->target_pid = 0;\\n\\tctx->target_auid = ctx->target_uid = KUIDT_INIT(0);\\n\\tctx->target_sessionid = 0;\\n\\tlsmprop_init(&ctx->target_ref);\\n\\tctx->target_comm[0] = \\'\\\\0\\';\\n\\tunroll_tree_refs(ctx, NULL, 0);\\n\\tWARN_ON(!list_empty(&ctx->killed_trees));\\n\\taudit_free_module(ctx);\\n\\tctx->fds[0] = -1;\\n\\tctx->type = 0; /* reset last for audit_free_*() */\\n}\\n\\nstatic inline struct audit_context *audit_alloc_context(enum audit_state state)\\n{\\n\\tstruct audit_context *context;\\n\\n\\tcontext = kzalloc(sizeof(*context), GFP_KERNEL);\\n\\tif (!context)\\n\\t\\treturn NULL;\\n\\tcontext->context = AUDIT_CTX_UNUSED;\\n\\tcontext->state = state;\\n\\tcontext->prio = state == AUDIT_STATE_RECORD ? ~0ULL : 0;\\n\\tINIT_LIST_HEAD(&context->killed_trees);\\n\\tINIT_LIST_HEAD(&context->names_list);\\n\\tcontext->fds[0] = -1;\\n\\tcontext->return_valid = AUDITSC_INVALID;\\n\\treturn context;\\n}\\n\\n/**\\n * audit_alloc - allocate an audit context block for a task\\n * @tsk: task\\n *\\n * Filter on the task information and allocate a per-task audit context\\n * if necessary.  Doing so turns on system call auditing for the\\n * specified task.  This is called from copy_process, so no lock is\\n * needed.\\n */\\nint audit_alloc(struct task_struct *tsk)\\n{\\n\\tstruct audit_context *context;\\n\\tenum audit_state     state;\\n\\tchar *key = NULL;\\n\\n\\tif (likely(!audit_ever_enabled))\\n\\t\\treturn 0;\\n\\n\\tstate = audit_filter_task(tsk, &key);\\n\\tif (state == AUDIT_STATE_DISABLED) {\\n\\t\\tclear_task_syscall_work(tsk, SYSCALL_AUDIT);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tcontext = audit_alloc_context(state);\\n\\tif (!context) {\\n\\t\\tkfree(key);\\n\\t\\taudit_log_lost(\"out of memory in audit_alloc\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\tcontext->filterkey = key;\\n\\n\\taudit_set_context(tsk, context);\\n\\tset_task_syscall_work(tsk, SYSCALL_AUDIT);\\n\\treturn 0;\\n}\\n\\nstatic inline void audit_free_context(struct audit_context *context)\\n{\\n\\t/* resetting is extra work, but it is likely just noise */\\n\\taudit_reset_context(context);\\n\\taudit_proctitle_free(context);\\n\\tfree_tree_refs(context);\\n\\tkfree(context->filterkey);\\n\\tkfree(context);\\n}\\n\\nstatic int audit_log_pid_context(struct audit_context *context, pid_t pid,\\n\\t\\t\\t\\t kuid_t auid, kuid_t uid,\\n\\t\\t\\t\\t unsigned int sessionid, struct lsm_prop *prop,\\n\\t\\t\\t\\t char *comm)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tchar *ctx = NULL;\\n\\tu32 len;\\n\\tint rc = 0;\\n\\n\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_OBJ_PID);\\n\\tif (!ab)\\n\\t\\treturn rc;\\n\\n\\taudit_log_format(ab, \"opid=%d oauid=%d ouid=%d oses=%d\", pid,\\n\\t\\t\\t from_kuid(&init_user_ns, auid),\\n\\t\\t\\t from_kuid(&init_user_ns, uid), sessionid);\\n\\tif (lsmprop_is_set(prop)) {\\n\\t\\tif (security_lsmprop_to_secctx(prop, &ctx, &len)) {\\n\\t\\t\\taudit_log_format(ab, \" obj=(none)\");\\n\\t\\t\\trc = 1;\\n\\t\\t} else {\\n\\t\\t\\taudit_log_format(ab, \" obj=%s\", ctx);\\n\\t\\t\\tsecurity_release_secctx(ctx, len);\\n\\t\\t}\\n\\t}\\n\\taudit_log_format(ab, \" ocomm=\");\\n\\taudit_log_untrustedstring(ab, comm);\\n\\taudit_log_end(ab);\\n\\n\\treturn rc;\\n}\\n\\nstatic void audit_log_execve_info(struct audit_context *context,\\n\\t\\t\\t\\t  struct audit_buffer **ab)\\n{\\n\\tlong len_max;\\n\\tlong len_rem;\\n\\tlong len_full;\\n\\tlong len_buf;\\n\\tlong len_abuf = 0;\\n\\tlong len_tmp;\\n\\tbool require_data;\\n\\tbool encode;\\n\\tunsigned int iter;\\n\\tunsigned int arg;\\n\\tchar *buf_head;\\n\\tchar *buf;\\n\\tconst char __user *p = (const char __user *)current->mm->arg_start;\\n\\n\\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\\n\\t *       data we put in the audit record for this argument (see the\\n\\t *       code below) ... at this point in time 96 is plenty */\\n\\tchar abuf[96];\\n\\n\\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\\n\\t *       current value of 7500 is not as important as the fact that it\\n\\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\\n\\t *       room if we go over a little bit in the logging below */\\n\\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\\n\\tlen_max = MAX_EXECVE_AUDIT_LEN;\\n\\n\\t/* scratch buffer to hold the userspace args */\\n\\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\\n\\tif (!buf_head) {\\n\\t\\taudit_panic(\"out of memory for argv string\");\\n\\t\\treturn;\\n\\t}\\n\\tbuf = buf_head;\\n\\n\\taudit_log_format(*ab, \"argc=%d\", context->execve.argc);\\n\\n\\tlen_rem = len_max;\\n\\tlen_buf = 0;\\n\\tlen_full = 0;\\n\\trequire_data = true;\\n\\tencode = false;\\n\\titer = 0;\\n\\targ = 0;\\n\\tdo {\\n\\t\\t/* NOTE: we don\\'t ever want to trust this value for anything\\n\\t\\t *       serious, but the audit record format insists we\\n\\t\\t *       provide an argument length for really long arguments,\\n\\t\\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\\n\\t\\t *       to use strncpy_from_user() to obtain this value for\\n\\t\\t *       recording in the log, although we don\\'t use it\\n\\t\\t *       anywhere here to avoid a double-fetch problem */\\n\\t\\tif (len_full == 0)\\n\\t\\t\\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\\n\\n\\t\\t/* read more data from userspace */\\n\\t\\tif (require_data) {\\n\\t\\t\\t/* can we make more room in the buffer? */\\n\\t\\t\\tif (buf != buf_head) {\\n\\t\\t\\t\\tmemmove(buf_head, buf, len_buf);\\n\\t\\t\\t\\tbuf = buf_head;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/* fetch as much as we can of the argument */\\n\\t\\t\\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\\n\\t\\t\\t\\t\\t\\t    len_max - len_buf);\\n\\t\\t\\tif (len_tmp == -EFAULT) {\\n\\t\\t\\t\\t/* unable to copy from userspace */\\n\\t\\t\\t\\tsend_sig(SIGKILL, current, 0);\\n\\t\\t\\t\\tgoto out;\\n\\t\\t\\t} else if (len_tmp == (len_max - len_buf)) {\\n\\t\\t\\t\\t/* buffer is not large enough */\\n\\t\\t\\t\\trequire_data = true;\\n\\t\\t\\t\\t/* NOTE: if we are going to span multiple\\n\\t\\t\\t\\t *       buffers force the encoding so we stand\\n\\t\\t\\t\\t *       a chance at a sane len_full value and\\n\\t\\t\\t\\t *       consistent record encoding */\\n\\t\\t\\t\\tencode = true;\\n\\t\\t\\t\\tlen_full = len_full * 2;\\n\\t\\t\\t\\tp += len_tmp;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\trequire_data = false;\\n\\t\\t\\t\\tif (!encode)\\n\\t\\t\\t\\t\\tencode = audit_string_contains_control(\\n\\t\\t\\t\\t\\t\\t\\t\\tbuf, len_tmp);\\n\\t\\t\\t\\t/* try to use a trusted value for len_full */\\n\\t\\t\\t\\tif (len_full < len_max)\\n\\t\\t\\t\\t\\tlen_full = (encode ?\\n\\t\\t\\t\\t\\t\\t    len_tmp * 2 : len_tmp);\\n\\t\\t\\t\\tp += len_tmp + 1;\\n\\t\\t\\t}\\n\\t\\t\\tlen_buf += len_tmp;\\n\\t\\t\\tbuf_head[len_buf] = \\'\\\\0\\';\\n\\n\\t\\t\\t/* length of the buffer in the audit record? */\\n\\t\\t\\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\\n\\t\\t}\\n\\n\\t\\t/* write as much as we can to the audit log */\\n\\t\\tif (len_buf >= 0) {\\n\\t\\t\\t/* NOTE: some magic numbers here - basically if we\\n\\t\\t\\t *       can\\'t fit a reasonable amount of data into the\\n\\t\\t\\t *       existing audit buffer, flush it and start with\\n\\t\\t\\t *       a new buffer */\\n\\t\\t\\tif ((sizeof(abuf) + 8) > len_rem) {\\n\\t\\t\\t\\tlen_rem = len_max;\\n\\t\\t\\t\\taudit_log_end(*ab);\\n\\t\\t\\t\\t*ab = audit_log_start(context,\\n\\t\\t\\t\\t\\t\\t      GFP_KERNEL, AUDIT_EXECVE);\\n\\t\\t\\t\\tif (!*ab)\\n\\t\\t\\t\\t\\tgoto out;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/* create the non-arg portion of the arg record */\\n\\t\\t\\tlen_tmp = 0;\\n\\t\\t\\tif (require_data || (iter > 0) ||\\n\\t\\t\\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\\n\\t\\t\\t\\tif (iter == 0) {\\n\\t\\t\\t\\t\\tlen_tmp += snprintf(&abuf[len_tmp],\\n\\t\\t\\t\\t\\t\\t\\tsizeof(abuf) - len_tmp,\\n\\t\\t\\t\\t\\t\\t\\t\" a%d_len=%lu\",\\n\\t\\t\\t\\t\\t\\t\\targ, len_full);\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tlen_tmp += snprintf(&abuf[len_tmp],\\n\\t\\t\\t\\t\\t\\t    sizeof(abuf) - len_tmp,\\n\\t\\t\\t\\t\\t\\t    \" a%d[%d]=\", arg, iter++);\\n\\t\\t\\t} else\\n\\t\\t\\t\\tlen_tmp += snprintf(&abuf[len_tmp],\\n\\t\\t\\t\\t\\t\\t    sizeof(abuf) - len_tmp,\\n\\t\\t\\t\\t\\t\\t    \" a%d=\", arg);\\n\\t\\t\\tWARN_ON(len_tmp >= sizeof(abuf));\\n\\t\\t\\tabuf[sizeof(abuf) - 1] = \\'\\\\0\\';\\n\\n\\t\\t\\t/* log the arg in the audit record */\\n\\t\\t\\taudit_log_format(*ab, \"%s\", abuf);\\n\\t\\t\\tlen_rem -= len_tmp;\\n\\t\\t\\tlen_tmp = len_buf;\\n\\t\\t\\tif (encode) {\\n\\t\\t\\t\\tif (len_abuf > len_rem)\\n\\t\\t\\t\\t\\tlen_tmp = len_rem / 2; /* encoding */\\n\\t\\t\\t\\taudit_log_n_hex(*ab, buf, len_tmp);\\n\\t\\t\\t\\tlen_rem -= len_tmp * 2;\\n\\t\\t\\t\\tlen_abuf -= len_tmp * 2;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tif (len_abuf > len_rem)\\n\\t\\t\\t\\t\\tlen_tmp = len_rem - 2; /* quotes */\\n\\t\\t\\t\\taudit_log_n_string(*ab, buf, len_tmp);\\n\\t\\t\\t\\tlen_rem -= len_tmp + 2;\\n\\t\\t\\t\\t/* don\\'t subtract the \"2\" because we still need\\n\\t\\t\\t\\t * to add quotes to the remaining string */\\n\\t\\t\\t\\tlen_abuf -= len_tmp;\\n\\t\\t\\t}\\n\\t\\t\\tlen_buf -= len_tmp;\\n\\t\\t\\tbuf += len_tmp;\\n\\t\\t}\\n\\n\\t\\t/* ready to move to the next argument? */\\n\\t\\tif ((len_buf == 0) && !require_data) {\\n\\t\\t\\targ++;\\n\\t\\t\\titer = 0;\\n\\t\\t\\tlen_full = 0;\\n\\t\\t\\trequire_data = true;\\n\\t\\t\\tencode = false;\\n\\t\\t}\\n\\t} while (arg < context->execve.argc);\\n\\n\\t/* NOTE: the caller handles the final audit_log_end() call */\\n\\nout:\\n\\tkfree(buf_head);\\n}\\n\\nstatic void audit_log_cap(struct audit_buffer *ab, char *prefix,\\n\\t\\t\\t  kernel_cap_t *cap)\\n{\\n\\tif (cap_isclear(*cap)) {\\n\\t\\taudit_log_format(ab, \" %s=0\", prefix);\\n\\t\\treturn;\\n\\t}\\n\\taudit_log_format(ab, \" %s=%016llx\", prefix, cap->val);\\n}\\n\\nstatic void audit_log_fcaps(struct audit_buffer *ab, struct audit_names *name)\\n{\\n\\tif (name->fcap_ver == -1) {\\n\\t\\taudit_log_format(ab, \" cap_fe=? cap_fver=? cap_fp=? cap_fi=?\");\\n\\t\\treturn;\\n\\t}\\n\\taudit_log_cap(ab, \"cap_fp\", &name->fcap.permitted);\\n\\taudit_log_cap(ab, \"cap_fi\", &name->fcap.inheritable);\\n\\taudit_log_format(ab, \" cap_fe=%d cap_fver=%x cap_frootid=%d\",\\n\\t\\t\\t name->fcap.fE, name->fcap_ver,\\n\\t\\t\\t from_kuid(&init_user_ns, name->fcap.rootid));\\n}\\n\\nstatic void audit_log_time(struct audit_context *context, struct audit_buffer **ab)\\n{\\n\\tconst struct audit_ntp_data *ntp = &context->time.ntp_data;\\n\\tconst struct timespec64 *tk = &context->time.tk_injoffset;\\n\\tstatic const char * const ntp_name[] = {\\n\\t\\t\"offset\",\\n\\t\\t\"freq\",\\n\\t\\t\"status\",\\n\\t\\t\"tai\",\\n\\t\\t\"tick\",\\n\\t\\t\"adjust\",\\n\\t};\\n\\tint type;\\n\\n\\tif (context->type == AUDIT_TIME_ADJNTPVAL) {\\n\\t\\tfor (type = 0; type < AUDIT_NTP_NVALS; type++) {\\n\\t\\t\\tif (ntp->vals[type].newval != ntp->vals[type].oldval) {\\n\\t\\t\\t\\tif (!*ab) {\\n\\t\\t\\t\\t\\t*ab = audit_log_start(context,\\n\\t\\t\\t\\t\\t\\t\\tGFP_KERNEL,\\n\\t\\t\\t\\t\\t\\t\\tAUDIT_TIME_ADJNTPVAL);\\n\\t\\t\\t\\t\\tif (!*ab)\\n\\t\\t\\t\\t\\t\\treturn;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\taudit_log_format(*ab, \"op=%s old=%lli new=%lli\",\\n\\t\\t\\t\\t\\t\\t ntp_name[type],\\n\\t\\t\\t\\t\\t\\t ntp->vals[type].oldval,\\n\\t\\t\\t\\t\\t\\t ntp->vals[type].newval);\\n\\t\\t\\t\\taudit_log_end(*ab);\\n\\t\\t\\t\\t*ab = NULL;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\tif (tk->tv_sec != 0 || tk->tv_nsec != 0) {\\n\\t\\tif (!*ab) {\\n\\t\\t\\t*ab = audit_log_start(context, GFP_KERNEL,\\n\\t\\t\\t\\t\\t      AUDIT_TIME_INJOFFSET);\\n\\t\\t\\tif (!*ab)\\n\\t\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\taudit_log_format(*ab, \"sec=%lli nsec=%li\",\\n\\t\\t\\t\\t (long long)tk->tv_sec, tk->tv_nsec);\\n\\t\\taudit_log_end(*ab);\\n\\t\\t*ab = NULL;\\n\\t}\\n}\\n\\nstatic void show_special(struct audit_context *context, int *call_panic)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tint i;\\n\\n\\tab = audit_log_start(context, GFP_KERNEL, context->type);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\tswitch (context->type) {\\n\\tcase AUDIT_SOCKETCALL: {\\n\\t\\tint nargs = context->socketcall.nargs;\\n\\n\\t\\taudit_log_format(ab, \"nargs=%d\", nargs);\\n\\t\\tfor (i = 0; i < nargs; i++)\\n\\t\\t\\taudit_log_format(ab, \" a%d=%lx\", i,\\n\\t\\t\\t\\tcontext->socketcall.args[i]);\\n\\t\\tbreak; }\\n\\tcase AUDIT_IPC:\\n\\t\\taudit_log_format(ab, \"ouid=%u ogid=%u mode=%#ho\",\\n\\t\\t\\t\\t from_kuid(&init_user_ns, context->ipc.uid),\\n\\t\\t\\t\\t from_kgid(&init_user_ns, context->ipc.gid),\\n\\t\\t\\t\\t context->ipc.mode);\\n\\t\\tif (lsmprop_is_set(&context->ipc.oprop)) {\\n\\t\\t\\tchar *ctx = NULL;\\n\\t\\t\\tu32 len;\\n\\n\\t\\t\\tif (security_lsmprop_to_secctx(&context->ipc.oprop,\\n\\t\\t\\t\\t\\t\\t       &ctx, &len)) {\\n\\t\\t\\t\\t*call_panic = 1;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\taudit_log_format(ab, \" obj=%s\", ctx);\\n\\t\\t\\t\\tsecurity_release_secctx(ctx, len);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif (context->ipc.has_perm) {\\n\\t\\t\\taudit_log_end(ab);\\n\\t\\t\\tab = audit_log_start(context, GFP_KERNEL,\\n\\t\\t\\t\\t\\t     AUDIT_IPC_SET_PERM);\\n\\t\\t\\tif (unlikely(!ab))\\n\\t\\t\\t\\treturn;\\n\\t\\t\\taudit_log_format(ab,\\n\\t\\t\\t\\t\"qbytes=%lx ouid=%u ogid=%u mode=%#ho\",\\n\\t\\t\\t\\tcontext->ipc.qbytes,\\n\\t\\t\\t\\tcontext->ipc.perm_uid,\\n\\t\\t\\t\\tcontext->ipc.perm_gid,\\n\\t\\t\\t\\tcontext->ipc.perm_mode);\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase AUDIT_MQ_OPEN:\\n\\t\\taudit_log_format(ab,\\n\\t\\t\\t\"oflag=0x%x mode=%#ho mq_flags=0x%lx mq_maxmsg=%ld \"\\n\\t\\t\\t\"mq_msgsize=%ld mq_curmsgs=%ld\",\\n\\t\\t\\tcontext->mq_open.oflag, context->mq_open.mode,\\n\\t\\t\\tcontext->mq_open.attr.mq_flags,\\n\\t\\t\\tcontext->mq_open.attr.mq_maxmsg,\\n\\t\\t\\tcontext->mq_open.attr.mq_msgsize,\\n\\t\\t\\tcontext->mq_open.attr.mq_curmsgs);\\n\\t\\tbreak;\\n\\tcase AUDIT_MQ_SENDRECV:\\n\\t\\taudit_log_format(ab,\\n\\t\\t\\t\"mqdes=%d msg_len=%zd msg_prio=%u \"\\n\\t\\t\\t\"abs_timeout_sec=%lld abs_timeout_nsec=%ld\",\\n\\t\\t\\tcontext->mq_sendrecv.mqdes,\\n\\t\\t\\tcontext->mq_sendrecv.msg_len,\\n\\t\\t\\tcontext->mq_sendrecv.msg_prio,\\n\\t\\t\\t(long long) context->mq_sendrecv.abs_timeout.tv_sec,\\n\\t\\t\\tcontext->mq_sendrecv.abs_timeout.tv_nsec);\\n\\t\\tbreak;\\n\\tcase AUDIT_MQ_NOTIFY:\\n\\t\\taudit_log_format(ab, \"mqdes=%d sigev_signo=%d\",\\n\\t\\t\\t\\tcontext->mq_notify.mqdes,\\n\\t\\t\\t\\tcontext->mq_notify.sigev_signo);\\n\\t\\tbreak;\\n\\tcase AUDIT_MQ_GETSETATTR: {\\n\\t\\tstruct mq_attr *attr = &context->mq_getsetattr.mqstat;\\n\\n\\t\\taudit_log_format(ab,\\n\\t\\t\\t\"mqdes=%d mq_flags=0x%lx mq_maxmsg=%ld mq_msgsize=%ld \"\\n\\t\\t\\t\"mq_curmsgs=%ld \",\\n\\t\\t\\tcontext->mq_getsetattr.mqdes,\\n\\t\\t\\tattr->mq_flags, attr->mq_maxmsg,\\n\\t\\t\\tattr->mq_msgsize, attr->mq_curmsgs);\\n\\t\\tbreak; }\\n\\tcase AUDIT_CAPSET:\\n\\t\\taudit_log_format(ab, \"pid=%d\", context->capset.pid);\\n\\t\\taudit_log_cap(ab, \"cap_pi\", &context->capset.cap.inheritable);\\n\\t\\taudit_log_cap(ab, \"cap_pp\", &context->capset.cap.permitted);\\n\\t\\taudit_log_cap(ab, \"cap_pe\", &context->capset.cap.effective);\\n\\t\\taudit_log_cap(ab, \"cap_pa\", &context->capset.cap.ambient);\\n\\t\\tbreak;\\n\\tcase AUDIT_MMAP:\\n\\t\\taudit_log_format(ab, \"fd=%d flags=0x%x\", context->mmap.fd,\\n\\t\\t\\t\\t context->mmap.flags);\\n\\t\\tbreak;\\n\\tcase AUDIT_OPENAT2:\\n\\t\\taudit_log_format(ab, \"oflag=0%llo mode=0%llo resolve=0x%llx\",\\n\\t\\t\\t\\t context->openat2.flags,\\n\\t\\t\\t\\t context->openat2.mode,\\n\\t\\t\\t\\t context->openat2.resolve);\\n\\t\\tbreak;\\n\\tcase AUDIT_EXECVE:\\n\\t\\taudit_log_execve_info(context, &ab);\\n\\t\\tbreak;\\n\\tcase AUDIT_KERN_MODULE:\\n\\t\\taudit_log_format(ab, \"name=\");\\n\\t\\tif (context->module.name) {\\n\\t\\t\\taudit_log_untrustedstring(ab, context->module.name);\\n\\t\\t} else\\n\\t\\t\\taudit_log_format(ab, \"(null)\");\\n\\n\\t\\tbreak;\\n\\tcase AUDIT_TIME_ADJNTPVAL:\\n\\tcase AUDIT_TIME_INJOFFSET:\\n\\t\\t/* this call deviates from the rest, eating the buffer */\\n\\t\\taudit_log_time(context, &ab);\\n\\t\\tbreak;\\n\\t}\\n\\taudit_log_end(ab);\\n}\\n\\nstatic inline int audit_proctitle_rtrim(char *proctitle, int len)\\n{\\n\\tchar *end = proctitle + len - 1;\\n\\n\\twhile (end > proctitle && !isprint(*end))\\n\\t\\tend--;\\n\\n\\t/* catch the case where proctitle is only 1 non-print character */\\n\\tlen = end - proctitle + 1;\\n\\tlen -= isprint(proctitle[len-1]) == 0;\\n\\treturn len;\\n}\\n\\n/*\\n * audit_log_name - produce AUDIT_PATH record from struct audit_names\\n * @context: audit_context for the task\\n * @n: audit_names structure with reportable details\\n * @path: optional path to report instead of audit_names->name\\n * @record_num: record number to report when handling a list of names\\n * @call_panic: optional pointer to int that will be updated if secid fails\\n */\\nstatic void audit_log_name(struct audit_context *context, struct audit_names *n,\\n\\t\\t    const struct path *path, int record_num, int *call_panic)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_PATH);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\n\\taudit_log_format(ab, \"item=%d\", record_num);\\n\\n\\tif (path)\\n\\t\\taudit_log_d_path(ab, \" name=\", path);\\n\\telse if (n->name) {\\n\\t\\tswitch (n->name_len) {\\n\\t\\tcase AUDIT_NAME_FULL:\\n\\t\\t\\t/* log the full path */\\n\\t\\t\\taudit_log_format(ab, \" name=\");\\n\\t\\t\\taudit_log_untrustedstring(ab, n->name->name);\\n\\t\\t\\tbreak;\\n\\t\\tcase 0:\\n\\t\\t\\t/* name was specified as a relative path and the\\n\\t\\t\\t * directory component is the cwd\\n\\t\\t\\t */\\n\\t\\t\\tif (context->pwd.dentry && context->pwd.mnt)\\n\\t\\t\\t\\taudit_log_d_path(ab, \" name=\", &context->pwd);\\n\\t\\t\\telse\\n\\t\\t\\t\\taudit_log_format(ab, \" name=(null)\");\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\t/* log the name\\'s directory component */\\n\\t\\t\\taudit_log_format(ab, \" name=\");\\n\\t\\t\\taudit_log_n_untrustedstring(ab, n->name->name,\\n\\t\\t\\t\\t\\t\\t    n->name_len);\\n\\t\\t}\\n\\t} else\\n\\t\\taudit_log_format(ab, \" name=(null)\");\\n\\n\\tif (n->ino != AUDIT_INO_UNSET)\\n\\t\\taudit_log_format(ab, \" inode=%lu dev=%02x:%02x mode=%#ho ouid=%u ogid=%u rdev=%02x:%02x\",\\n\\t\\t\\t\\t n->ino,\\n\\t\\t\\t\\t MAJOR(n->dev),\\n\\t\\t\\t\\t MINOR(n->dev),\\n\\t\\t\\t\\t n->mode,\\n\\t\\t\\t\\t from_kuid(&init_user_ns, n->uid),\\n\\t\\t\\t\\t from_kgid(&init_user_ns, n->gid),\\n\\t\\t\\t\\t MAJOR(n->rdev),\\n\\t\\t\\t\\t MINOR(n->rdev));\\n\\tif (lsmprop_is_set(&n->oprop)) {\\n\\t\\tchar *ctx = NULL;\\n\\t\\tu32 len;\\n\\n\\t\\tif (security_lsmprop_to_secctx(&n->oprop, &ctx, &len)) {\\n\\t\\t\\tif (call_panic)\\n\\t\\t\\t\\t*call_panic = 2;\\n\\t\\t} else {\\n\\t\\t\\taudit_log_format(ab, \" obj=%s\", ctx);\\n\\t\\t\\tsecurity_release_secctx(ctx, len);\\n\\t\\t}\\n\\t}\\n\\n\\t/* log the audit_names record type */\\n\\tswitch (n->type) {\\n\\tcase AUDIT_TYPE_NORMAL:\\n\\t\\taudit_log_format(ab, \" nametype=NORMAL\");\\n\\t\\tbreak;\\n\\tcase AUDIT_TYPE_PARENT:\\n\\t\\taudit_log_format(ab, \" nametype=PARENT\");\\n\\t\\tbreak;\\n\\tcase AUDIT_TYPE_CHILD_DELETE:\\n\\t\\taudit_log_format(ab, \" nametype=DELETE\");\\n\\t\\tbreak;\\n\\tcase AUDIT_TYPE_CHILD_CREATE:\\n\\t\\taudit_log_format(ab, \" nametype=CREATE\");\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\taudit_log_format(ab, \" nametype=UNKNOWN\");\\n\\t\\tbreak;\\n\\t}\\n\\n\\taudit_log_fcaps(ab, n);\\n\\taudit_log_end(ab);\\n}\\n\\nstatic void audit_log_proctitle(void)\\n{\\n\\tint res;\\n\\tchar *buf;\\n\\tchar *msg = \"(null)\";\\n\\tint len = strlen(msg);\\n\\tstruct audit_context *context = audit_context();\\n\\tstruct audit_buffer *ab;\\n\\n\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_PROCTITLE);\\n\\tif (!ab)\\n\\t\\treturn;\\t/* audit_panic or being filtered */\\n\\n\\taudit_log_format(ab, \"proctitle=\");\\n\\n\\t/* Not  cached */\\n\\tif (!context->proctitle.value) {\\n\\t\\tbuf = kmalloc(MAX_PROCTITLE_AUDIT_LEN, GFP_KERNEL);\\n\\t\\tif (!buf)\\n\\t\\t\\tgoto out;\\n\\t\\t/* Historically called this from procfs naming */\\n\\t\\tres = get_cmdline(current, buf, MAX_PROCTITLE_AUDIT_LEN);\\n\\t\\tif (res == 0) {\\n\\t\\t\\tkfree(buf);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tres = audit_proctitle_rtrim(buf, res);\\n\\t\\tif (res == 0) {\\n\\t\\t\\tkfree(buf);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tcontext->proctitle.value = buf;\\n\\t\\tcontext->proctitle.len = res;\\n\\t}\\n\\tmsg = context->proctitle.value;\\n\\tlen = context->proctitle.len;\\nout:\\n\\taudit_log_n_untrustedstring(ab, msg, len);\\n\\taudit_log_end(ab);\\n}\\n\\n/**\\n * audit_log_uring - generate a AUDIT_URINGOP record\\n * @ctx: the audit context\\n */\\nstatic void audit_log_uring(struct audit_context *ctx)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tconst struct cred *cred;\\n\\n\\tab = audit_log_start(ctx, GFP_ATOMIC, AUDIT_URINGOP);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\tcred = current_cred();\\n\\taudit_log_format(ab, \"uring_op=%d\", ctx->uring_op);\\n\\tif (ctx->return_valid != AUDITSC_INVALID)\\n\\t\\taudit_log_format(ab, \" success=%s exit=%ld\",\\n\\t\\t\\t\\t str_yes_no(ctx->return_valid ==\\n\\t\\t\\t\\t\\t    AUDITSC_SUCCESS),\\n\\t\\t\\t\\t ctx->return_code);\\n\\taudit_log_format(ab,\\n\\t\\t\\t \" items=%d\"\\n\\t\\t\\t \" ppid=%d pid=%d uid=%u gid=%u euid=%u suid=%u\"\\n\\t\\t\\t \" fsuid=%u egid=%u sgid=%u fsgid=%u\",\\n\\t\\t\\t ctx->name_count,\\n\\t\\t\\t task_ppid_nr(current), task_tgid_nr(current),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->uid),\\n\\t\\t\\t from_kgid(&init_user_ns, cred->gid),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->euid),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->suid),\\n\\t\\t\\t from_kuid(&init_user_ns, cred->fsuid),\\n\\t\\t\\t from_kgid(&init_user_ns, cred->egid),\\n\\t\\t\\t from_kgid(&init_user_ns, cred->sgid),\\n\\t\\t\\t from_kgid(&init_user_ns, cred->fsgid));\\n\\taudit_log_task_context(ab);\\n\\taudit_log_key(ab, ctx->filterkey);\\n\\taudit_log_end(ab);\\n}\\n\\nstatic void audit_log_exit(void)\\n{\\n\\tint i, call_panic = 0;\\n\\tstruct audit_context *context = audit_context();\\n\\tstruct audit_buffer *ab;\\n\\tstruct audit_aux_data *aux;\\n\\tstruct audit_names *n;\\n\\n\\tcontext->personality = current->personality;\\n\\n\\tswitch (context->context) {\\n\\tcase AUDIT_CTX_SYSCALL:\\n\\t\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_SYSCALL);\\n\\t\\tif (!ab)\\n\\t\\t\\treturn;\\n\\t\\taudit_log_format(ab, \"arch=%x syscall=%d\",\\n\\t\\t\\t\\t context->arch, context->major);\\n\\t\\tif (context->personality != PER_LINUX)\\n\\t\\t\\taudit_log_format(ab, \" per=%lx\", context->personality);\\n\\t\\tif (context->return_valid != AUDITSC_INVALID)\\n\\t\\t\\taudit_log_format(ab, \" success=%s exit=%ld\",\\n\\t\\t\\t\\t\\t str_yes_no(context->return_valid ==\\n\\t\\t\\t\\t\\t\\t    AUDITSC_SUCCESS),\\n\\t\\t\\t\\t\\t context->return_code);\\n\\t\\taudit_log_format(ab,\\n\\t\\t\\t\\t \" a0=%lx a1=%lx a2=%lx a3=%lx items=%d\",\\n\\t\\t\\t\\t context->argv[0],\\n\\t\\t\\t\\t context->argv[1],\\n\\t\\t\\t\\t context->argv[2],\\n\\t\\t\\t\\t context->argv[3],\\n\\t\\t\\t\\t context->name_count);\\n\\t\\taudit_log_task_info(ab);\\n\\t\\taudit_log_key(ab, context->filterkey);\\n\\t\\taudit_log_end(ab);\\n\\t\\tbreak;\\n\\tcase AUDIT_CTX_URING:\\n\\t\\taudit_log_uring(context);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tBUG();\\n\\t\\tbreak;\\n\\t}\\n\\n\\tfor (aux = context->aux; aux; aux = aux->next) {\\n\\n\\t\\tab = audit_log_start(context, GFP_KERNEL, aux->type);\\n\\t\\tif (!ab)\\n\\t\\t\\tcontinue; /* audit_panic has been called */\\n\\n\\t\\tswitch (aux->type) {\\n\\n\\t\\tcase AUDIT_BPRM_FCAPS: {\\n\\t\\t\\tstruct audit_aux_data_bprm_fcaps *axs = (void *)aux;\\n\\n\\t\\t\\taudit_log_format(ab, \"fver=%x\", axs->fcap_ver);\\n\\t\\t\\taudit_log_cap(ab, \"fp\", &axs->fcap.permitted);\\n\\t\\t\\taudit_log_cap(ab, \"fi\", &axs->fcap.inheritable);\\n\\t\\t\\taudit_log_format(ab, \" fe=%d\", axs->fcap.fE);\\n\\t\\t\\taudit_log_cap(ab, \"old_pp\", &axs->old_pcap.permitted);\\n\\t\\t\\taudit_log_cap(ab, \"old_pi\", &axs->old_pcap.inheritable);\\n\\t\\t\\taudit_log_cap(ab, \"old_pe\", &axs->old_pcap.effective);\\n\\t\\t\\taudit_log_cap(ab, \"old_pa\", &axs->old_pcap.ambient);\\n\\t\\t\\taudit_log_cap(ab, \"pp\", &axs->new_pcap.permitted);\\n\\t\\t\\taudit_log_cap(ab, \"pi\", &axs->new_pcap.inheritable);\\n\\t\\t\\taudit_log_cap(ab, \"pe\", &axs->new_pcap.effective);\\n\\t\\t\\taudit_log_cap(ab, \"pa\", &axs->new_pcap.ambient);\\n\\t\\t\\taudit_log_format(ab, \" frootid=%d\",\\n\\t\\t\\t\\t\\t from_kuid(&init_user_ns,\\n\\t\\t\\t\\t\\t\\t   axs->fcap.rootid));\\n\\t\\t\\tbreak; }\\n\\n\\t\\t}\\n\\t\\taudit_log_end(ab);\\n\\t}\\n\\n\\tif (context->type)\\n\\t\\tshow_special(context, &call_panic);\\n\\n\\tif (context->fds[0] >= 0) {\\n\\t\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_FD_PAIR);\\n\\t\\tif (ab) {\\n\\t\\t\\taudit_log_format(ab, \"fd0=%d fd1=%d\",\\n\\t\\t\\t\\t\\tcontext->fds[0], context->fds[1]);\\n\\t\\t\\taudit_log_end(ab);\\n\\t\\t}\\n\\t}\\n\\n\\tif (context->sockaddr_len) {\\n\\t\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_SOCKADDR);\\n\\t\\tif (ab) {\\n\\t\\t\\taudit_log_format(ab, \"saddr=\");\\n\\t\\t\\taudit_log_n_hex(ab, (void *)context->sockaddr,\\n\\t\\t\\t\\t\\tcontext->sockaddr_len);\\n\\t\\t\\taudit_log_end(ab);\\n\\t\\t}\\n\\t}\\n\\n\\tfor (aux = context->aux_pids; aux; aux = aux->next) {\\n\\t\\tstruct audit_aux_data_pids *axs = (void *)aux;\\n\\n\\t\\tfor (i = 0; i < axs->pid_count; i++)\\n\\t\\t\\tif (audit_log_pid_context(context, axs->target_pid[i],\\n\\t\\t\\t\\t\\t\\t  axs->target_auid[i],\\n\\t\\t\\t\\t\\t\\t  axs->target_uid[i],\\n\\t\\t\\t\\t\\t\\t  axs->target_sessionid[i],\\n\\t\\t\\t\\t\\t\\t  &axs->target_ref[i],\\n\\t\\t\\t\\t\\t\\t  axs->target_comm[i]))\\n\\t\\t\\t\\tcall_panic = 1;\\n\\t}\\n\\n\\tif (context->target_pid &&\\n\\t    audit_log_pid_context(context, context->target_pid,\\n\\t\\t\\t\\t  context->target_auid, context->target_uid,\\n\\t\\t\\t\\t  context->target_sessionid,\\n\\t\\t\\t\\t  &context->target_ref, context->target_comm))\\n\\t\\t\\tcall_panic = 1;\\n\\n\\tif (context->pwd.dentry && context->pwd.mnt) {\\n\\t\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_CWD);\\n\\t\\tif (ab) {\\n\\t\\t\\taudit_log_d_path(ab, \"cwd=\", &context->pwd);\\n\\t\\t\\taudit_log_end(ab);\\n\\t\\t}\\n\\t}\\n\\n\\ti = 0;\\n\\tlist_for_each_entry(n, &context->names_list, list) {\\n\\t\\tif (n->hidden)\\n\\t\\t\\tcontinue;\\n\\t\\taudit_log_name(context, n, NULL, i++, &call_panic);\\n\\t}\\n\\n\\tif (context->context == AUDIT_CTX_SYSCALL)\\n\\t\\taudit_log_proctitle();\\n\\n\\t/* Send end of event record to help user space know we are finished */\\n\\tab = audit_log_start(context, GFP_KERNEL, AUDIT_EOE);\\n\\tif (ab)\\n\\t\\taudit_log_end(ab);\\n\\tif (call_panic)\\n\\t\\taudit_panic(\"error in audit_log_exit()\");\\n}\\n\\n/**\\n * __audit_free - free a per-task audit context\\n * @tsk: task whose audit context block to free\\n *\\n * Called from copy_process, do_exit, and the io_uring code\\n */\\nvoid __audit_free(struct task_struct *tsk)\\n{\\n\\tstruct audit_context *context = tsk->audit_context;\\n\\n\\tif (!context)\\n\\t\\treturn;\\n\\n\\t/* this may generate CONFIG_CHANGE records */\\n\\tif (!list_empty(&context->killed_trees))\\n\\t\\taudit_kill_trees(context);\\n\\n\\t/* We are called either by do_exit() or the fork() error handling code;\\n\\t * in the former case tsk == current and in the latter tsk is a\\n\\t * random task_struct that doesn\\'t have any meaningful data we\\n\\t * need to log via audit_log_exit().\\n\\t */\\n\\tif (tsk == current && !context->dummy) {\\n\\t\\tcontext->return_valid = AUDITSC_INVALID;\\n\\t\\tcontext->return_code = 0;\\n\\t\\tif (context->context == AUDIT_CTX_SYSCALL) {\\n\\t\\t\\taudit_filter_syscall(tsk, context);\\n\\t\\t\\taudit_filter_inodes(tsk, context);\\n\\t\\t\\tif (context->current_state == AUDIT_STATE_RECORD)\\n\\t\\t\\t\\taudit_log_exit();\\n\\t\\t} else if (context->context == AUDIT_CTX_URING) {\\n\\t\\t\\t/* TODO: verify this case is real and valid */\\n\\t\\t\\taudit_filter_uring(tsk, context);\\n\\t\\t\\taudit_filter_inodes(tsk, context);\\n\\t\\t\\tif (context->current_state == AUDIT_STATE_RECORD)\\n\\t\\t\\t\\taudit_log_uring(context);\\n\\t\\t}\\n\\t}\\n\\n\\taudit_set_context(tsk, NULL);\\n\\taudit_free_context(context);\\n}\\n\\n/**\\n * audit_return_fixup - fixup the return codes in the audit_context\\n * @ctx: the audit_context\\n * @success: true/false value to indicate if the operation succeeded or not\\n * @code: operation return code\\n *\\n * We need to fixup the return code in the audit logs if the actual return\\n * codes are later going to be fixed by the arch specific signal handlers.\\n */\\nstatic void audit_return_fixup(struct audit_context *ctx,\\n\\t\\t\\t       int success, long code)\\n{\\n\\t/*\\n\\t * This is actually a test for:\\n\\t * (rc == ERESTARTSYS ) || (rc == ERESTARTNOINTR) ||\\n\\t * (rc == ERESTARTNOHAND) || (rc == ERESTART_RESTARTBLOCK)\\n\\t *\\n\\t * but is faster than a bunch of ||\\n\\t */\\n\\tif (unlikely(code <= -ERESTARTSYS) &&\\n\\t    (code >= -ERESTART_RESTARTBLOCK) &&\\n\\t    (code != -ENOIOCTLCMD))\\n\\t\\tctx->return_code = -EINTR;\\n\\telse\\n\\t\\tctx->return_code  = code;\\n\\tctx->return_valid = (success ? AUDITSC_SUCCESS : AUDITSC_FAILURE);\\n}\\n\\n/**\\n * __audit_uring_entry - prepare the kernel task\\'s audit context for io_uring\\n * @op: the io_uring opcode\\n *\\n * This is similar to audit_syscall_entry() but is intended for use by io_uring\\n * operations.  This function should only ever be called from\\n * audit_uring_entry() as we rely on the audit context checking present in that\\n * function.\\n */\\nvoid __audit_uring_entry(u8 op)\\n{\\n\\tstruct audit_context *ctx = audit_context();\\n\\n\\tif (ctx->state == AUDIT_STATE_DISABLED)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * NOTE: It\\'s possible that we can be called from the process\\' context\\n\\t *       before it returns to userspace, and before audit_syscall_exit()\\n\\t *       is called.  In this case there is not much to do, just record\\n\\t *       the io_uring details and return.\\n\\t */\\n\\tctx->uring_op = op;\\n\\tif (ctx->context == AUDIT_CTX_SYSCALL)\\n\\t\\treturn;\\n\\n\\tctx->dummy = !audit_n_rules;\\n\\tif (!ctx->dummy && ctx->state == AUDIT_STATE_BUILD)\\n\\t\\tctx->prio = 0;\\n\\n\\tctx->context = AUDIT_CTX_URING;\\n\\tctx->current_state = ctx->state;\\n\\tktime_get_coarse_real_ts64(&ctx->ctime);\\n}\\n\\n/**\\n * __audit_uring_exit - wrap up the kernel task\\'s audit context after io_uring\\n * @success: true/false value to indicate if the operation succeeded or not\\n * @code: operation return code\\n *\\n * This is similar to audit_syscall_exit() but is intended for use by io_uring\\n * operations.  This function should only ever be called from\\n * audit_uring_exit() as we rely on the audit context checking present in that\\n * function.\\n */\\nvoid __audit_uring_exit(int success, long code)\\n{\\n\\tstruct audit_context *ctx = audit_context();\\n\\n\\tif (ctx->dummy) {\\n\\t\\tif (ctx->context != AUDIT_CTX_URING)\\n\\t\\t\\treturn;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\taudit_return_fixup(ctx, success, code);\\n\\tif (ctx->context == AUDIT_CTX_SYSCALL) {\\n\\t\\t/*\\n\\t\\t * NOTE: See the note in __audit_uring_entry() about the case\\n\\t\\t *       where we may be called from process context before we\\n\\t\\t *       return to userspace via audit_syscall_exit().  In this\\n\\t\\t *       case we simply emit a URINGOP record and bail, the\\n\\t\\t *       normal syscall exit handling will take care of\\n\\t\\t *       everything else.\\n\\t\\t *       It is also worth mentioning that when we are called,\\n\\t\\t *       the current process creds may differ from the creds\\n\\t\\t *       used during the normal syscall processing; keep that\\n\\t\\t *       in mind if/when we move the record generation code.\\n\\t\\t */\\n\\n\\t\\t/*\\n\\t\\t * We need to filter on the syscall info here to decide if we\\n\\t\\t * should emit a URINGOP record.  I know it seems odd but this\\n\\t\\t * solves the problem where users have a filter to block *all*\\n\\t\\t * syscall records in the \"exit\" filter; we want to preserve\\n\\t\\t * the behavior here.\\n\\t\\t */\\n\\t\\taudit_filter_syscall(current, ctx);\\n\\t\\tif (ctx->current_state != AUDIT_STATE_RECORD)\\n\\t\\t\\taudit_filter_uring(current, ctx);\\n\\t\\taudit_filter_inodes(current, ctx);\\n\\t\\tif (ctx->current_state != AUDIT_STATE_RECORD)\\n\\t\\t\\treturn;\\n\\n\\t\\taudit_log_uring(ctx);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* this may generate CONFIG_CHANGE records */\\n\\tif (!list_empty(&ctx->killed_trees))\\n\\t\\taudit_kill_trees(ctx);\\n\\n\\t/* run through both filters to ensure we set the filterkey properly */\\n\\taudit_filter_uring(current, ctx);\\n\\taudit_filter_inodes(current, ctx);\\n\\tif (ctx->current_state != AUDIT_STATE_RECORD)\\n\\t\\tgoto out;\\n\\taudit_log_exit();\\n\\nout:\\n\\taudit_reset_context(ctx);\\n}\\n\\n/**\\n * __audit_syscall_entry - fill in an audit record at syscall entry\\n * @major: major syscall type (function)\\n * @a1: additional syscall register 1\\n * @a2: additional syscall register 2\\n * @a3: additional syscall register 3\\n * @a4: additional syscall register 4\\n *\\n * Fill in audit context at syscall entry.  This only happens if the\\n * audit context was created when the task was created and the state or\\n * filters demand the audit context be built.  If the state from the\\n * per-task filter or from the per-syscall filter is AUDIT_STATE_RECORD,\\n * then the record will be written at syscall exit time (otherwise, it\\n * will only be written if another part of the kernel requests that it\\n * be written).\\n */\\nvoid __audit_syscall_entry(int major, unsigned long a1, unsigned long a2,\\n\\t\\t\\t   unsigned long a3, unsigned long a4)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\tenum audit_state     state;\\n\\n\\tif (!audit_enabled || !context)\\n\\t\\treturn;\\n\\n\\tWARN_ON(context->context != AUDIT_CTX_UNUSED);\\n\\tWARN_ON(context->name_count);\\n\\tif (context->context != AUDIT_CTX_UNUSED || context->name_count) {\\n\\t\\taudit_panic(\"unrecoverable error in audit_syscall_entry()\");\\n\\t\\treturn;\\n\\t}\\n\\n\\tstate = context->state;\\n\\tif (state == AUDIT_STATE_DISABLED)\\n\\t\\treturn;\\n\\n\\tcontext->dummy = !audit_n_rules;\\n\\tif (!context->dummy && state == AUDIT_STATE_BUILD) {\\n\\t\\tcontext->prio = 0;\\n\\t\\tif (auditd_test_task(current))\\n\\t\\t\\treturn;\\n\\t}\\n\\n\\tcontext->arch\\t    = syscall_get_arch(current);\\n\\tcontext->major      = major;\\n\\tcontext->argv[0]    = a1;\\n\\tcontext->argv[1]    = a2;\\n\\tcontext->argv[2]    = a3;\\n\\tcontext->argv[3]    = a4;\\n\\tcontext->context = AUDIT_CTX_SYSCALL;\\n\\tcontext->current_state  = state;\\n\\tktime_get_coarse_real_ts64(&context->ctime);\\n}\\n\\n/**\\n * __audit_syscall_exit - deallocate audit context after a system call\\n * @success: success value of the syscall\\n * @return_code: return value of the syscall\\n *\\n * Tear down after system call.  If the audit context has been marked as\\n * auditable (either because of the AUDIT_STATE_RECORD state from\\n * filtering, or because some other part of the kernel wrote an audit\\n * message), then write out the syscall information.  In call cases,\\n * free the names stored from getname().\\n */\\nvoid __audit_syscall_exit(int success, long return_code)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tif (!context || context->dummy ||\\n\\t    context->context != AUDIT_CTX_SYSCALL)\\n\\t\\tgoto out;\\n\\n\\t/* this may generate CONFIG_CHANGE records */\\n\\tif (!list_empty(&context->killed_trees))\\n\\t\\taudit_kill_trees(context);\\n\\n\\taudit_return_fixup(context, success, return_code);\\n\\t/* run through both filters to ensure we set the filterkey properly */\\n\\taudit_filter_syscall(current, context);\\n\\taudit_filter_inodes(current, context);\\n\\tif (context->current_state != AUDIT_STATE_RECORD)\\n\\t\\tgoto out;\\n\\n\\taudit_log_exit();\\n\\nout:\\n\\taudit_reset_context(context);\\n}\\n\\nstatic inline void handle_one(const struct inode *inode)\\n{\\n\\tstruct audit_context *context;\\n\\tstruct audit_tree_refs *p;\\n\\tstruct audit_chunk *chunk;\\n\\tint count;\\n\\n\\tif (likely(!inode->i_fsnotify_marks))\\n\\t\\treturn;\\n\\tcontext = audit_context();\\n\\tp = context->trees;\\n\\tcount = context->tree_count;\\n\\trcu_read_lock();\\n\\tchunk = audit_tree_lookup(inode);\\n\\trcu_read_unlock();\\n\\tif (!chunk)\\n\\t\\treturn;\\n\\tif (likely(put_tree_ref(context, chunk)))\\n\\t\\treturn;\\n\\tif (unlikely(!grow_tree_refs(context))) {\\n\\t\\tpr_warn(\"out of memory, audit has lost a tree reference\\\\n\");\\n\\t\\taudit_set_auditable(context);\\n\\t\\taudit_put_chunk(chunk);\\n\\t\\tunroll_tree_refs(context, p, count);\\n\\t\\treturn;\\n\\t}\\n\\tput_tree_ref(context, chunk);\\n}\\n\\nstatic void handle_path(const struct dentry *dentry)\\n{\\n\\tstruct audit_context *context;\\n\\tstruct audit_tree_refs *p;\\n\\tconst struct dentry *d, *parent;\\n\\tstruct audit_chunk *drop;\\n\\tunsigned long seq;\\n\\tint count;\\n\\n\\tcontext = audit_context();\\n\\tp = context->trees;\\n\\tcount = context->tree_count;\\nretry:\\n\\tdrop = NULL;\\n\\td = dentry;\\n\\trcu_read_lock();\\n\\tseq = read_seqbegin(&rename_lock);\\n\\tfor (;;) {\\n\\t\\tstruct inode *inode = d_backing_inode(d);\\n\\n\\t\\tif (inode && unlikely(inode->i_fsnotify_marks)) {\\n\\t\\t\\tstruct audit_chunk *chunk;\\n\\n\\t\\t\\tchunk = audit_tree_lookup(inode);\\n\\t\\t\\tif (chunk) {\\n\\t\\t\\t\\tif (unlikely(!put_tree_ref(context, chunk))) {\\n\\t\\t\\t\\t\\tdrop = chunk;\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tparent = d->d_parent;\\n\\t\\tif (parent == d)\\n\\t\\t\\tbreak;\\n\\t\\td = parent;\\n\\t}\\n\\tif (unlikely(read_seqretry(&rename_lock, seq) || drop)) {  /* in this order */\\n\\t\\trcu_read_unlock();\\n\\t\\tif (!drop) {\\n\\t\\t\\t/* just a race with rename */\\n\\t\\t\\tunroll_tree_refs(context, p, count);\\n\\t\\t\\tgoto retry;\\n\\t\\t}\\n\\t\\taudit_put_chunk(drop);\\n\\t\\tif (grow_tree_refs(context)) {\\n\\t\\t\\t/* OK, got more space */\\n\\t\\t\\tunroll_tree_refs(context, p, count);\\n\\t\\t\\tgoto retry;\\n\\t\\t}\\n\\t\\t/* too bad */\\n\\t\\tpr_warn(\"out of memory, audit has lost a tree reference\\\\n\");\\n\\t\\tunroll_tree_refs(context, p, count);\\n\\t\\taudit_set_auditable(context);\\n\\t\\treturn;\\n\\t}\\n\\trcu_read_unlock();\\n}\\n\\nstatic struct audit_names *audit_alloc_name(struct audit_context *context,\\n\\t\\t\\t\\t\\t\\tunsigned char type)\\n{\\n\\tstruct audit_names *aname;\\n\\n\\tif (context->name_count < AUDIT_NAMES) {\\n\\t\\taname = &context->preallocated_names[context->name_count];\\n\\t\\tmemset(aname, 0, sizeof(*aname));\\n\\t} else {\\n\\t\\taname = kzalloc(sizeof(*aname), GFP_NOFS);\\n\\t\\tif (!aname)\\n\\t\\t\\treturn NULL;\\n\\t\\taname->should_free = true;\\n\\t}\\n\\n\\taname->ino = AUDIT_INO_UNSET;\\n\\taname->type = type;\\n\\tlist_add_tail(&aname->list, &context->names_list);\\n\\n\\tcontext->name_count++;\\n\\tif (!context->pwd.dentry)\\n\\t\\tget_fs_pwd(current->fs, &context->pwd);\\n\\treturn aname;\\n}\\n\\n/**\\n * __audit_reusename - fill out filename with info from existing entry\\n * @uptr: userland ptr to pathname\\n *\\n * Search the audit_names list for the current audit context. If there is an\\n * existing entry with a matching \"uptr\" then return the filename\\n * associated with that audit_name. If not, return NULL.\\n */\\nstruct filename *\\n__audit_reusename(const __user char *uptr)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\tstruct audit_names *n;\\n\\n\\tlist_for_each_entry(n, &context->names_list, list) {\\n\\t\\tif (!n->name)\\n\\t\\t\\tcontinue;\\n\\t\\tif (n->name->uptr == uptr) {\\n\\t\\t\\tatomic_inc(&n->name->refcnt);\\n\\t\\t\\treturn n->name;\\n\\t\\t}\\n\\t}\\n\\treturn NULL;\\n}\\n\\n/**\\n * __audit_getname - add a name to the list\\n * @name: name to add\\n *\\n * Add a name to the list of audit names for this context.\\n * Called from fs/namei.c:getname().\\n */\\nvoid __audit_getname(struct filename *name)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\tstruct audit_names *n;\\n\\n\\tif (context->context == AUDIT_CTX_UNUSED)\\n\\t\\treturn;\\n\\n\\tn = audit_alloc_name(context, AUDIT_TYPE_UNKNOWN);\\n\\tif (!n)\\n\\t\\treturn;\\n\\n\\tn->name = name;\\n\\tn->name_len = AUDIT_NAME_FULL;\\n\\tname->aname = n;\\n\\tatomic_inc(&name->refcnt);\\n}\\n\\nstatic inline int audit_copy_fcaps(struct audit_names *name,\\n\\t\\t\\t\\t   const struct dentry *dentry)\\n{\\n\\tstruct cpu_vfs_cap_data caps;\\n\\tint rc;\\n\\n\\tif (!dentry)\\n\\t\\treturn 0;\\n\\n\\trc = get_vfs_caps_from_disk(&nop_mnt_idmap, dentry, &caps);\\n\\tif (rc)\\n\\t\\treturn rc;\\n\\n\\tname->fcap.permitted = caps.permitted;\\n\\tname->fcap.inheritable = caps.inheritable;\\n\\tname->fcap.fE = !!(caps.magic_etc & VFS_CAP_FLAGS_EFFECTIVE);\\n\\tname->fcap.rootid = caps.rootid;\\n\\tname->fcap_ver = (caps.magic_etc & VFS_CAP_REVISION_MASK) >>\\n\\t\\t\\t\\tVFS_CAP_REVISION_SHIFT;\\n\\n\\treturn 0;\\n}\\n\\n/* Copy inode data into an audit_names. */\\nstatic void audit_copy_inode(struct audit_names *name,\\n\\t\\t\\t     const struct dentry *dentry,\\n\\t\\t\\t     struct inode *inode, unsigned int flags)\\n{\\n\\tname->ino   = inode->i_ino;\\n\\tname->dev   = inode->i_sb->s_dev;\\n\\tname->mode  = inode->i_mode;\\n\\tname->uid   = inode->i_uid;\\n\\tname->gid   = inode->i_gid;\\n\\tname->rdev  = inode->i_rdev;\\n\\tsecurity_inode_getlsmprop(inode, &name->oprop);\\n\\tif (flags & AUDIT_INODE_NOEVAL) {\\n\\t\\tname->fcap_ver = -1;\\n\\t\\treturn;\\n\\t}\\n\\taudit_copy_fcaps(name, dentry);\\n}\\n\\n/**\\n * __audit_inode - store the inode and device from a lookup\\n * @name: name being audited\\n * @dentry: dentry being audited\\n * @flags: attributes for this particular entry\\n */\\nvoid __audit_inode(struct filename *name, const struct dentry *dentry,\\n\\t\\t   unsigned int flags)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\tstruct inode *inode = d_backing_inode(dentry);\\n\\tstruct audit_names *n;\\n\\tbool parent = flags & AUDIT_INODE_PARENT;\\n\\tstruct audit_entry *e;\\n\\tstruct list_head *list = &audit_filter_list[AUDIT_FILTER_FS];\\n\\tint i;\\n\\n\\tif (context->context == AUDIT_CTX_UNUSED)\\n\\t\\treturn;\\n\\n\\trcu_read_lock();\\n\\tlist_for_each_entry_rcu(e, list, list) {\\n\\t\\tfor (i = 0; i < e->rule.field_count; i++) {\\n\\t\\t\\tstruct audit_field *f = &e->rule.fields[i];\\n\\n\\t\\t\\tif (f->type == AUDIT_FSTYPE\\n\\t\\t\\t    && audit_comparator(inode->i_sb->s_magic,\\n\\t\\t\\t\\t\\t\\tf->op, f->val)\\n\\t\\t\\t    && e->rule.action == AUDIT_NEVER) {\\n\\t\\t\\t\\trcu_read_unlock();\\n\\t\\t\\t\\treturn;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\trcu_read_unlock();\\n\\n\\tif (!name)\\n\\t\\tgoto out_alloc;\\n\\n\\t/*\\n\\t * If we have a pointer to an audit_names entry already, then we can\\n\\t * just use it directly if the type is correct.\\n\\t */\\n\\tn = name->aname;\\n\\tif (n) {\\n\\t\\tif (parent) {\\n\\t\\t\\tif (n->type == AUDIT_TYPE_PARENT ||\\n\\t\\t\\t    n->type == AUDIT_TYPE_UNKNOWN)\\n\\t\\t\\t\\tgoto out;\\n\\t\\t} else {\\n\\t\\t\\tif (n->type != AUDIT_TYPE_PARENT)\\n\\t\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\n\\tlist_for_each_entry_reverse(n, &context->names_list, list) {\\n\\t\\tif (n->ino) {\\n\\t\\t\\t/* valid inode number, use that for the comparison */\\n\\t\\t\\tif (n->ino != inode->i_ino ||\\n\\t\\t\\t    n->dev != inode->i_sb->s_dev)\\n\\t\\t\\t\\tcontinue;\\n\\t\\t} else if (n->name) {\\n\\t\\t\\t/* inode number has not been set, check the name */\\n\\t\\t\\tif (strcmp(n->name->name, name->name))\\n\\t\\t\\t\\tcontinue;\\n\\t\\t} else\\n\\t\\t\\t/* no inode and no name (?!) ... this is odd ... */\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* match the correct record type */\\n\\t\\tif (parent) {\\n\\t\\t\\tif (n->type == AUDIT_TYPE_PARENT ||\\n\\t\\t\\t    n->type == AUDIT_TYPE_UNKNOWN)\\n\\t\\t\\t\\tgoto out;\\n\\t\\t} else {\\n\\t\\t\\tif (n->type != AUDIT_TYPE_PARENT)\\n\\t\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\nout_alloc:\\n\\t/* unable to find an entry with both a matching name and type */\\n\\tn = audit_alloc_name(context, AUDIT_TYPE_UNKNOWN);\\n\\tif (!n)\\n\\t\\treturn;\\n\\tif (name) {\\n\\t\\tn->name = name;\\n\\t\\tatomic_inc(&name->refcnt);\\n\\t}\\n\\nout:\\n\\tif (parent) {\\n\\t\\tn->name_len = n->name ? parent_len(n->name->name) : AUDIT_NAME_FULL;\\n\\t\\tn->type = AUDIT_TYPE_PARENT;\\n\\t\\tif (flags & AUDIT_INODE_HIDDEN)\\n\\t\\t\\tn->hidden = true;\\n\\t} else {\\n\\t\\tn->name_len = AUDIT_NAME_FULL;\\n\\t\\tn->type = AUDIT_TYPE_NORMAL;\\n\\t}\\n\\thandle_path(dentry);\\n\\taudit_copy_inode(n, dentry, inode, flags & AUDIT_INODE_NOEVAL);\\n}\\n\\nvoid __audit_file(const struct file *file)\\n{\\n\\t__audit_inode(NULL, file->f_path.dentry, 0);\\n}\\n\\n/**\\n * __audit_inode_child - collect inode info for created/removed objects\\n * @parent: inode of dentry parent\\n * @dentry: dentry being audited\\n * @type:   AUDIT_TYPE_* value that we\\'re looking for\\n *\\n * For syscalls that create or remove filesystem objects, audit_inode\\n * can only collect information for the filesystem object\\'s parent.\\n * This call updates the audit context with the child\\'s information.\\n * Syscalls that create a new filesystem object must be hooked after\\n * the object is created.  Syscalls that remove a filesystem object\\n * must be hooked prior, in order to capture the target inode during\\n * unsuccessful attempts.\\n */\\nvoid __audit_inode_child(struct inode *parent,\\n\\t\\t\\t const struct dentry *dentry,\\n\\t\\t\\t const unsigned char type)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\tstruct inode *inode = d_backing_inode(dentry);\\n\\tconst struct qstr *dname = &dentry->d_name;\\n\\tstruct audit_names *n, *found_parent = NULL, *found_child = NULL;\\n\\tstruct audit_entry *e;\\n\\tstruct list_head *list = &audit_filter_list[AUDIT_FILTER_FS];\\n\\tint i;\\n\\n\\tif (context->context == AUDIT_CTX_UNUSED)\\n\\t\\treturn;\\n\\n\\trcu_read_lock();\\n\\tlist_for_each_entry_rcu(e, list, list) {\\n\\t\\tfor (i = 0; i < e->rule.field_count; i++) {\\n\\t\\t\\tstruct audit_field *f = &e->rule.fields[i];\\n\\n\\t\\t\\tif (f->type == AUDIT_FSTYPE\\n\\t\\t\\t    && audit_comparator(parent->i_sb->s_magic,\\n\\t\\t\\t\\t\\t\\tf->op, f->val)\\n\\t\\t\\t    && e->rule.action == AUDIT_NEVER) {\\n\\t\\t\\t\\trcu_read_unlock();\\n\\t\\t\\t\\treturn;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\trcu_read_unlock();\\n\\n\\tif (inode)\\n\\t\\thandle_one(inode);\\n\\n\\t/* look for a parent entry first */\\n\\tlist_for_each_entry(n, &context->names_list, list) {\\n\\t\\tif (!n->name ||\\n\\t\\t    (n->type != AUDIT_TYPE_PARENT &&\\n\\t\\t     n->type != AUDIT_TYPE_UNKNOWN))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (n->ino == parent->i_ino && n->dev == parent->i_sb->s_dev &&\\n\\t\\t    !audit_compare_dname_path(dname,\\n\\t\\t\\t\\t\\t      n->name->name, n->name_len)) {\\n\\t\\t\\tif (n->type == AUDIT_TYPE_UNKNOWN)\\n\\t\\t\\t\\tn->type = AUDIT_TYPE_PARENT;\\n\\t\\t\\tfound_parent = n;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\tcond_resched();\\n\\n\\t/* is there a matching child entry? */\\n\\tlist_for_each_entry(n, &context->names_list, list) {\\n\\t\\t/* can only match entries that have a name */\\n\\t\\tif (!n->name ||\\n\\t\\t    (n->type != type && n->type != AUDIT_TYPE_UNKNOWN))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (!strcmp(dname->name, n->name->name) ||\\n\\t\\t    !audit_compare_dname_path(dname, n->name->name,\\n\\t\\t\\t\\t\\t\\tfound_parent ?\\n\\t\\t\\t\\t\\t\\tfound_parent->name_len :\\n\\t\\t\\t\\t\\t\\tAUDIT_NAME_FULL)) {\\n\\t\\t\\tif (n->type == AUDIT_TYPE_UNKNOWN)\\n\\t\\t\\t\\tn->type = type;\\n\\t\\t\\tfound_child = n;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\tif (!found_parent) {\\n\\t\\t/* create a new, \"anonymous\" parent record */\\n\\t\\tn = audit_alloc_name(context, AUDIT_TYPE_PARENT);\\n\\t\\tif (!n)\\n\\t\\t\\treturn;\\n\\t\\taudit_copy_inode(n, NULL, parent, 0);\\n\\t}\\n\\n\\tif (!found_child) {\\n\\t\\tfound_child = audit_alloc_name(context, type);\\n\\t\\tif (!found_child)\\n\\t\\t\\treturn;\\n\\n\\t\\t/* Re-use the name belonging to the slot for a matching parent\\n\\t\\t * directory. All names for this context are relinquished in\\n\\t\\t * audit_free_names() */\\n\\t\\tif (found_parent) {\\n\\t\\t\\tfound_child->name = found_parent->name;\\n\\t\\t\\tfound_child->name_len = AUDIT_NAME_FULL;\\n\\t\\t\\tatomic_inc(&found_child->name->refcnt);\\n\\t\\t}\\n\\t}\\n\\n\\tif (inode)\\n\\t\\taudit_copy_inode(found_child, dentry, inode, 0);\\n\\telse\\n\\t\\tfound_child->ino = AUDIT_INO_UNSET;\\n}\\nEXPORT_SYMBOL_GPL(__audit_inode_child);\\n\\n/**\\n * auditsc_get_stamp - get local copies of audit_context values\\n * @ctx: audit_context for the task\\n * @t: timespec64 to store time recorded in the audit_context\\n * @serial: serial value that is recorded in the audit_context\\n *\\n * Also sets the context as auditable.\\n */\\nint auditsc_get_stamp(struct audit_context *ctx,\\n\\t\\t       struct timespec64 *t, unsigned int *serial)\\n{\\n\\tif (ctx->context == AUDIT_CTX_UNUSED)\\n\\t\\treturn 0;\\n\\tif (!ctx->serial)\\n\\t\\tctx->serial = audit_serial();\\n\\tt->tv_sec  = ctx->ctime.tv_sec;\\n\\tt->tv_nsec = ctx->ctime.tv_nsec;\\n\\t*serial    = ctx->serial;\\n\\tif (!ctx->prio) {\\n\\t\\tctx->prio = 1;\\n\\t\\tctx->current_state = AUDIT_STATE_RECORD;\\n\\t}\\n\\treturn 1;\\n}\\n\\n/**\\n * __audit_mq_open - record audit data for a POSIX MQ open\\n * @oflag: open flag\\n * @mode: mode bits\\n * @attr: queue attributes\\n *\\n */\\nvoid __audit_mq_open(int oflag, umode_t mode, struct mq_attr *attr)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tif (attr)\\n\\t\\tmemcpy(&context->mq_open.attr, attr, sizeof(struct mq_attr));\\n\\telse\\n\\t\\tmemset(&context->mq_open.attr, 0, sizeof(struct mq_attr));\\n\\n\\tcontext->mq_open.oflag = oflag;\\n\\tcontext->mq_open.mode = mode;\\n\\n\\tcontext->type = AUDIT_MQ_OPEN;\\n}\\n\\n/**\\n * __audit_mq_sendrecv - record audit data for a POSIX MQ timed send/receive\\n * @mqdes: MQ descriptor\\n * @msg_len: Message length\\n * @msg_prio: Message priority\\n * @abs_timeout: Message timeout in absolute time\\n *\\n */\\nvoid __audit_mq_sendrecv(mqd_t mqdes, size_t msg_len, unsigned int msg_prio,\\n\\t\\t\\tconst struct timespec64 *abs_timeout)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\tstruct timespec64 *p = &context->mq_sendrecv.abs_timeout;\\n\\n\\tif (abs_timeout)\\n\\t\\tmemcpy(p, abs_timeout, sizeof(*p));\\n\\telse\\n\\t\\tmemset(p, 0, sizeof(*p));\\n\\n\\tcontext->mq_sendrecv.mqdes = mqdes;\\n\\tcontext->mq_sendrecv.msg_len = msg_len;\\n\\tcontext->mq_sendrecv.msg_prio = msg_prio;\\n\\n\\tcontext->type = AUDIT_MQ_SENDRECV;\\n}\\n\\n/**\\n * __audit_mq_notify - record audit data for a POSIX MQ notify\\n * @mqdes: MQ descriptor\\n * @notification: Notification event\\n *\\n */\\n\\nvoid __audit_mq_notify(mqd_t mqdes, const struct sigevent *notification)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tif (notification)\\n\\t\\tcontext->mq_notify.sigev_signo = notification->sigev_signo;\\n\\telse\\n\\t\\tcontext->mq_notify.sigev_signo = 0;\\n\\n\\tcontext->mq_notify.mqdes = mqdes;\\n\\tcontext->type = AUDIT_MQ_NOTIFY;\\n}\\n\\n/**\\n * __audit_mq_getsetattr - record audit data for a POSIX MQ get/set attribute\\n * @mqdes: MQ descriptor\\n * @mqstat: MQ flags\\n *\\n */\\nvoid __audit_mq_getsetattr(mqd_t mqdes, struct mq_attr *mqstat)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->mq_getsetattr.mqdes = mqdes;\\n\\tcontext->mq_getsetattr.mqstat = *mqstat;\\n\\tcontext->type = AUDIT_MQ_GETSETATTR;\\n}\\n\\n/**\\n * __audit_ipc_obj - record audit data for ipc object\\n * @ipcp: ipc permissions\\n *\\n */\\nvoid __audit_ipc_obj(struct kern_ipc_perm *ipcp)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->ipc.uid = ipcp->uid;\\n\\tcontext->ipc.gid = ipcp->gid;\\n\\tcontext->ipc.mode = ipcp->mode;\\n\\tcontext->ipc.has_perm = 0;\\n\\tsecurity_ipc_getlsmprop(ipcp, &context->ipc.oprop);\\n\\tcontext->type = AUDIT_IPC;\\n}\\n\\n/**\\n * __audit_ipc_set_perm - record audit data for new ipc permissions\\n * @qbytes: msgq bytes\\n * @uid: msgq user id\\n * @gid: msgq group id\\n * @mode: msgq mode (permissions)\\n *\\n * Called only after audit_ipc_obj().\\n */\\nvoid __audit_ipc_set_perm(unsigned long qbytes, uid_t uid, gid_t gid, umode_t mode)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->ipc.qbytes = qbytes;\\n\\tcontext->ipc.perm_uid = uid;\\n\\tcontext->ipc.perm_gid = gid;\\n\\tcontext->ipc.perm_mode = mode;\\n\\tcontext->ipc.has_perm = 1;\\n}\\n\\nvoid __audit_bprm(struct linux_binprm *bprm)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->type = AUDIT_EXECVE;\\n\\tcontext->execve.argc = bprm->argc;\\n}\\n\\n\\n/**\\n * __audit_socketcall - record audit data for sys_socketcall\\n * @nargs: number of args, which should not be more than AUDITSC_ARGS.\\n * @args: args array\\n *\\n */\\nint __audit_socketcall(int nargs, unsigned long *args)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tif (nargs <= 0 || nargs > AUDITSC_ARGS || !args)\\n\\t\\treturn -EINVAL;\\n\\tcontext->type = AUDIT_SOCKETCALL;\\n\\tcontext->socketcall.nargs = nargs;\\n\\tmemcpy(context->socketcall.args, args, nargs * sizeof(unsigned long));\\n\\treturn 0;\\n}\\n\\n/**\\n * __audit_fd_pair - record audit data for pipe and socketpair\\n * @fd1: the first file descriptor\\n * @fd2: the second file descriptor\\n *\\n */\\nvoid __audit_fd_pair(int fd1, int fd2)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->fds[0] = fd1;\\n\\tcontext->fds[1] = fd2;\\n}\\n\\n/**\\n * __audit_sockaddr - record audit data for sys_bind, sys_connect, sys_sendto\\n * @len: data length in user space\\n * @a: data address in kernel space\\n *\\n * Returns 0 for success or NULL context or < 0 on error.\\n */\\nint __audit_sockaddr(int len, void *a)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tif (!context->sockaddr) {\\n\\t\\tvoid *p = kmalloc(sizeof(struct sockaddr_storage), GFP_KERNEL);\\n\\n\\t\\tif (!p)\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\tcontext->sockaddr = p;\\n\\t}\\n\\n\\tcontext->sockaddr_len = len;\\n\\tmemcpy(context->sockaddr, a, len);\\n\\treturn 0;\\n}\\n\\nvoid __audit_ptrace(struct task_struct *t)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->target_pid = task_tgid_nr(t);\\n\\tcontext->target_auid = audit_get_loginuid(t);\\n\\tcontext->target_uid = task_uid(t);\\n\\tcontext->target_sessionid = audit_get_sessionid(t);\\n\\tstrscpy(context->target_comm, t->comm);\\n\\tsecurity_task_getlsmprop_obj(t, &context->target_ref);\\n}\\n\\n/**\\n * audit_signal_info_syscall - record signal info for syscalls\\n * @t: task being signaled\\n *\\n * If the audit subsystem is being terminated, record the task (pid)\\n * and uid that is doing that.\\n */\\nint audit_signal_info_syscall(struct task_struct *t)\\n{\\n\\tstruct audit_aux_data_pids *axp;\\n\\tstruct audit_context *ctx = audit_context();\\n\\tkuid_t t_uid = task_uid(t);\\n\\n\\tif (!audit_signals || audit_dummy_context())\\n\\t\\treturn 0;\\n\\n\\t/* optimize the common case by putting first signal recipient directly\\n\\t * in audit_context */\\n\\tif (!ctx->target_pid) {\\n\\t\\tctx->target_pid = task_tgid_nr(t);\\n\\t\\tctx->target_auid = audit_get_loginuid(t);\\n\\t\\tctx->target_uid = t_uid;\\n\\t\\tctx->target_sessionid = audit_get_sessionid(t);\\n\\t\\tstrscpy(ctx->target_comm, t->comm);\\n\\t\\tsecurity_task_getlsmprop_obj(t, &ctx->target_ref);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\taxp = (void *)ctx->aux_pids;\\n\\tif (!axp || axp->pid_count == AUDIT_AUX_PIDS) {\\n\\t\\taxp = kzalloc(sizeof(*axp), GFP_ATOMIC);\\n\\t\\tif (!axp)\\n\\t\\t\\treturn -ENOMEM;\\n\\n\\t\\taxp->d.type = AUDIT_OBJ_PID;\\n\\t\\taxp->d.next = ctx->aux_pids;\\n\\t\\tctx->aux_pids = (void *)axp;\\n\\t}\\n\\tBUG_ON(axp->pid_count >= AUDIT_AUX_PIDS);\\n\\n\\taxp->target_pid[axp->pid_count] = task_tgid_nr(t);\\n\\taxp->target_auid[axp->pid_count] = audit_get_loginuid(t);\\n\\taxp->target_uid[axp->pid_count] = t_uid;\\n\\taxp->target_sessionid[axp->pid_count] = audit_get_sessionid(t);\\n\\tsecurity_task_getlsmprop_obj(t, &axp->target_ref[axp->pid_count]);\\n\\tstrscpy(axp->target_comm[axp->pid_count], t->comm);\\n\\taxp->pid_count++;\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * __audit_log_bprm_fcaps - store information about a loading bprm and relevant fcaps\\n * @bprm: pointer to the bprm being processed\\n * @new: the proposed new credentials\\n * @old: the old credentials\\n *\\n * Simply check if the proc already has the caps given by the file and if not\\n * store the priv escalation info for later auditing at the end of the syscall\\n *\\n * -Eric\\n */\\nint __audit_log_bprm_fcaps(struct linux_binprm *bprm,\\n\\t\\t\\t   const struct cred *new, const struct cred *old)\\n{\\n\\tstruct audit_aux_data_bprm_fcaps *ax;\\n\\tstruct audit_context *context = audit_context();\\n\\tstruct cpu_vfs_cap_data vcaps;\\n\\n\\tax = kmalloc(sizeof(*ax), GFP_KERNEL);\\n\\tif (!ax)\\n\\t\\treturn -ENOMEM;\\n\\n\\tax->d.type = AUDIT_BPRM_FCAPS;\\n\\tax->d.next = context->aux;\\n\\tcontext->aux = (void *)ax;\\n\\n\\tget_vfs_caps_from_disk(&nop_mnt_idmap,\\n\\t\\t\\t       bprm->file->f_path.dentry, &vcaps);\\n\\n\\tax->fcap.permitted = vcaps.permitted;\\n\\tax->fcap.inheritable = vcaps.inheritable;\\n\\tax->fcap.fE = !!(vcaps.magic_etc & VFS_CAP_FLAGS_EFFECTIVE);\\n\\tax->fcap.rootid = vcaps.rootid;\\n\\tax->fcap_ver = (vcaps.magic_etc & VFS_CAP_REVISION_MASK) >> VFS_CAP_REVISION_SHIFT;\\n\\n\\tax->old_pcap.permitted   = old->cap_permitted;\\n\\tax->old_pcap.inheritable = old->cap_inheritable;\\n\\tax->old_pcap.effective   = old->cap_effective;\\n\\tax->old_pcap.ambient     = old->cap_ambient;\\n\\n\\tax->new_pcap.permitted   = new->cap_permitted;\\n\\tax->new_pcap.inheritable = new->cap_inheritable;\\n\\tax->new_pcap.effective   = new->cap_effective;\\n\\tax->new_pcap.ambient     = new->cap_ambient;\\n\\treturn 0;\\n}\\n\\n/**\\n * __audit_log_capset - store information about the arguments to the capset syscall\\n * @new: the new credentials\\n * @old: the old (current) credentials\\n *\\n * Record the arguments userspace sent to sys_capset for later printing by the\\n * audit system if applicable\\n */\\nvoid __audit_log_capset(const struct cred *new, const struct cred *old)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->capset.pid = task_tgid_nr(current);\\n\\tcontext->capset.cap.effective   = new->cap_effective;\\n\\tcontext->capset.cap.inheritable = new->cap_effective;\\n\\tcontext->capset.cap.permitted   = new->cap_permitted;\\n\\tcontext->capset.cap.ambient     = new->cap_ambient;\\n\\tcontext->type = AUDIT_CAPSET;\\n}\\n\\nvoid __audit_mmap_fd(int fd, int flags)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->mmap.fd = fd;\\n\\tcontext->mmap.flags = flags;\\n\\tcontext->type = AUDIT_MMAP;\\n}\\n\\nvoid __audit_openat2_how(struct open_how *how)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->openat2.flags = how->flags;\\n\\tcontext->openat2.mode = how->mode;\\n\\tcontext->openat2.resolve = how->resolve;\\n\\tcontext->type = AUDIT_OPENAT2;\\n}\\n\\nvoid __audit_log_kern_module(char *name)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\tcontext->module.name = kstrdup(name, GFP_KERNEL);\\n\\tif (!context->module.name)\\n\\t\\taudit_log_lost(\"out of memory in __audit_log_kern_module\");\\n\\tcontext->type = AUDIT_KERN_MODULE;\\n}\\n\\nvoid __audit_fanotify(u32 response, struct fanotify_response_info_audit_rule *friar)\\n{\\n\\t/* {subj,obj}_trust values are {0,1,2}: no,yes,unknown */\\n\\tswitch (friar->hdr.type) {\\n\\tcase FAN_RESPONSE_INFO_NONE:\\n\\t\\taudit_log(audit_context(), GFP_KERNEL, AUDIT_FANOTIFY,\\n\\t\\t\\t  \"resp=%u fan_type=%u fan_info=0 subj_trust=2 obj_trust=2\",\\n\\t\\t\\t  response, FAN_RESPONSE_INFO_NONE);\\n\\t\\tbreak;\\n\\tcase FAN_RESPONSE_INFO_AUDIT_RULE:\\n\\t\\taudit_log(audit_context(), GFP_KERNEL, AUDIT_FANOTIFY,\\n\\t\\t\\t  \"resp=%u fan_type=%u fan_info=%X subj_trust=%u obj_trust=%u\",\\n\\t\\t\\t  response, friar->hdr.type, friar->rule_number,\\n\\t\\t\\t  friar->subj_trust, friar->obj_trust);\\n\\t}\\n}\\n\\nvoid __audit_tk_injoffset(struct timespec64 offset)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\n\\t/* only set type if not already set by NTP */\\n\\tif (!context->type)\\n\\t\\tcontext->type = AUDIT_TIME_INJOFFSET;\\n\\tmemcpy(&context->time.tk_injoffset, &offset, sizeof(offset));\\n}\\n\\nvoid __audit_ntp_log(const struct audit_ntp_data *ad)\\n{\\n\\tstruct audit_context *context = audit_context();\\n\\tint type;\\n\\n\\tfor (type = 0; type < AUDIT_NTP_NVALS; type++)\\n\\t\\tif (ad->vals[type].newval != ad->vals[type].oldval) {\\n\\t\\t\\t/* unconditionally set type, overwriting TK */\\n\\t\\t\\tcontext->type = AUDIT_TIME_ADJNTPVAL;\\n\\t\\t\\tmemcpy(&context->time.ntp_data, ad, sizeof(*ad));\\n\\t\\t\\tbreak;\\n\\t\\t}\\n}\\n\\nvoid __audit_log_nfcfg(const char *name, u8 af, unsigned int nentries,\\n\\t\\t       enum audit_nfcfgop op, gfp_t gfp)\\n{\\n\\tstruct audit_buffer *ab;\\n\\tchar comm[sizeof(current->comm)];\\n\\n\\tab = audit_log_start(audit_context(), gfp, AUDIT_NETFILTER_CFG);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\taudit_log_format(ab, \"table=%s family=%u entries=%u op=%s\",\\n\\t\\t\\t name, af, nentries, audit_nfcfgs[op].s);\\n\\n\\taudit_log_format(ab, \" pid=%u\", task_tgid_nr(current));\\n\\taudit_log_task_context(ab); /* subj= */\\n\\taudit_log_format(ab, \" comm=\");\\n\\taudit_log_untrustedstring(ab, get_task_comm(comm, current));\\n\\taudit_log_end(ab);\\n}\\nEXPORT_SYMBOL_GPL(__audit_log_nfcfg);\\n\\nstatic void audit_log_task(struct audit_buffer *ab)\\n{\\n\\tkuid_t auid, uid;\\n\\tkgid_t gid;\\n\\tunsigned int sessionid;\\n\\tchar comm[sizeof(current->comm)];\\n\\n\\tauid = audit_get_loginuid(current);\\n\\tsessionid = audit_get_sessionid(current);\\n\\tcurrent_uid_gid(&uid, &gid);\\n\\n\\taudit_log_format(ab, \"auid=%u uid=%u gid=%u ses=%u\",\\n\\t\\t\\t from_kuid(&init_user_ns, auid),\\n\\t\\t\\t from_kuid(&init_user_ns, uid),\\n\\t\\t\\t from_kgid(&init_user_ns, gid),\\n\\t\\t\\t sessionid);\\n\\taudit_log_task_context(ab);\\n\\taudit_log_format(ab, \" pid=%d comm=\", task_tgid_nr(current));\\n\\taudit_log_untrustedstring(ab, get_task_comm(comm, current));\\n\\taudit_log_d_path_exe(ab, current->mm);\\n}\\n\\n/**\\n * audit_core_dumps - record information about processes that end abnormally\\n * @signr: signal value\\n *\\n * If a process ends with a core dump, something fishy is going on and we\\n * should record the event for investigation.\\n */\\nvoid audit_core_dumps(long signr)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\n\\tif (signr == SIGQUIT)\\t/* don\\'t care for those */\\n\\t\\treturn;\\n\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_ANOM_ABEND);\\n\\tif (unlikely(!ab))\\n\\t\\treturn;\\n\\taudit_log_task(ab);\\n\\taudit_log_format(ab, \" sig=%ld res=1\", signr);\\n\\taudit_log_end(ab);\\n}\\n\\n/**\\n * audit_seccomp - record information about a seccomp action\\n * @syscall: syscall number\\n * @signr: signal value\\n * @code: the seccomp action\\n *\\n * Record the information associated with a seccomp action. Event filtering for\\n * seccomp actions that are not to be logged is done in seccomp_log().\\n * Therefore, this function forces auditing independent of the audit_enabled\\n * and dummy context state because seccomp actions should be logged even when\\n * audit is not in use.\\n */\\nvoid audit_seccomp(unsigned long syscall, long signr, int code)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_SECCOMP);\\n\\tif (unlikely(!ab))\\n\\t\\treturn;\\n\\taudit_log_task(ab);\\n\\taudit_log_format(ab, \" sig=%ld arch=%x syscall=%ld compat=%d ip=0x%lx code=0x%x\",\\n\\t\\t\\t signr, syscall_get_arch(current), syscall,\\n\\t\\t\\t in_compat_syscall(), KSTK_EIP(current), code);\\n\\taudit_log_end(ab);\\n}\\n\\nvoid audit_seccomp_actions_logged(const char *names, const char *old_names,\\n\\t\\t\\t\\t  int res)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\n\\tab = audit_log_start(audit_context(), GFP_KERNEL,\\n\\t\\t\\t     AUDIT_CONFIG_CHANGE);\\n\\tif (unlikely(!ab))\\n\\t\\treturn;\\n\\n\\taudit_log_format(ab,\\n\\t\\t\\t \"op=seccomp-logging actions=%s old-actions=%s res=%d\",\\n\\t\\t\\t names, old_names, res);\\n\\taudit_log_end(ab);\\n}\\n\\nstruct list_head *audit_killed_trees(void)\\n{\\n\\tstruct audit_context *ctx = audit_context();\\n\\tif (likely(!ctx || ctx->context == AUDIT_CTX_UNUSED))\\n\\t\\treturn NULL;\\n\\treturn &ctx->killed_trees;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * linux/kernel/dma.c: A DMA channel allocator. Inspired by linux/kernel/irq.c.\\n *\\n * Written by Hennus Bergman, 1992.\\n *\\n * 1994/12/26: Changes by Alex Nash to fix a minor bug in /proc/dma.\\n *   In the previous version the reported device could end up being wrong,\\n *   if a device requested a DMA channel that was already in use.\\n *   [It also happened to remove the sizeof(char *) == sizeof(int)\\n *   assumption introduced because of those /proc/dma patches. -- Hennus]\\n */\\n#include <linux/export.h>\\n#include <linux/kernel.h>\\n#include <linux/errno.h>\\n#include <linux/spinlock.h>\\n#include <linux/string.h>\\n#include <linux/seq_file.h>\\n#include <linux/proc_fs.h>\\n#include <linux/init.h>\\n#include <asm/dma.h>\\n\\n\\n\\n/* A note on resource allocation:\\n *\\n * All drivers needing DMA channels, should allocate and release them\\n * through the public routines `request_dma()\\' and `free_dma()\\'.\\n *\\n * In order to avoid problems, all processes should allocate resources in\\n * the same sequence and release them in the reverse order.\\n *\\n * So, when allocating DMAs and IRQs, first allocate the IRQ, then the DMA.\\n * When releasing them, first release the DMA, then release the IRQ.\\n * If you don\\'t, you may cause allocation requests to fail unnecessarily.\\n * This doesn\\'t really matter now, but it will once we get real semaphores\\n * in the kernel.\\n */\\n\\n\\nDEFINE_SPINLOCK(dma_spin_lock);\\n\\n/*\\n *\\tIf our port doesn\\'t define this it has no PC like DMA\\n */\\n\\n#ifdef MAX_DMA_CHANNELS\\n\\n\\n/* Channel n is busy iff dma_chan_busy[n].lock != 0.\\n * DMA0 used to be reserved for DRAM refresh, but apparently not any more...\\n * DMA4 is reserved for cascading.\\n */\\n\\nstruct dma_chan {\\n\\tint  lock;\\n\\tconst char *device_id;\\n};\\n\\nstatic struct dma_chan dma_chan_busy[MAX_DMA_CHANNELS] = {\\n\\t[4] = { 1, \"cascade\" },\\n};\\n\\n\\n/**\\n * request_dma - request and reserve a system DMA channel\\n * @dmanr: DMA channel number\\n * @device_id: reserving device ID string, used in /proc/dma\\n */\\nint request_dma(unsigned int dmanr, const char * device_id)\\n{\\n\\tif (dmanr >= MAX_DMA_CHANNELS)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (xchg(&dma_chan_busy[dmanr].lock, 1) != 0)\\n\\t\\treturn -EBUSY;\\n\\n\\tdma_chan_busy[dmanr].device_id = device_id;\\n\\n\\t/* old flag was 0, now contains 1 to indicate busy */\\n\\treturn 0;\\n} /* request_dma */\\n\\n/**\\n * free_dma - free a reserved system DMA channel\\n * @dmanr: DMA channel number\\n */\\nvoid free_dma(unsigned int dmanr)\\n{\\n\\tif (dmanr >= MAX_DMA_CHANNELS) {\\n\\t\\tprintk(KERN_WARNING \"Trying to free DMA%d\\\\n\", dmanr);\\n\\t\\treturn;\\n\\t}\\n\\n\\tif (xchg(&dma_chan_busy[dmanr].lock, 0) == 0) {\\n\\t\\tprintk(KERN_WARNING \"Trying to free free DMA%d\\\\n\", dmanr);\\n\\t\\treturn;\\n\\t}\\n\\n} /* free_dma */\\n\\n#else\\n\\nint request_dma(unsigned int dmanr, const char *device_id)\\n{\\n\\treturn -EINVAL;\\n}\\n\\nvoid free_dma(unsigned int dmanr)\\n{\\n}\\n\\n#endif\\n\\n#ifdef CONFIG_PROC_FS\\n\\n#ifdef MAX_DMA_CHANNELS\\nstatic int proc_dma_show(struct seq_file *m, void *v)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0 ; i < MAX_DMA_CHANNELS ; i++) {\\n\\t\\tif (dma_chan_busy[i].lock) {\\n\\t\\t\\tseq_printf(m, \"%2d: %s\\\\n\", i,\\n\\t\\t\\t\\t   dma_chan_busy[i].device_id);\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n#else\\nstatic int proc_dma_show(struct seq_file *m, void *v)\\n{\\n\\tseq_puts(m, \"No DMA\\\\n\");\\n\\treturn 0;\\n}\\n#endif /* MAX_DMA_CHANNELS */\\n\\nstatic int __init proc_dma_init(void)\\n{\\n\\tproc_create_single(\"dma\", 0, NULL, proc_dma_show);\\n\\treturn 0;\\n}\\n\\n__initcall(proc_dma_init);\\n#endif\\n\\nEXPORT_SYMBOL(request_dma);\\nEXPORT_SYMBOL(free_dma);\\nEXPORT_SYMBOL(dma_spin_lock);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * fail_function.c: Function-based error injection\\n */\\n#include <linux/error-injection.h>\\n#include <linux/debugfs.h>\\n#include <linux/fault-inject.h>\\n#include <linux/kallsyms.h>\\n#include <linux/kprobes.h>\\n#include <linux/module.h>\\n#include <linux/mutex.h>\\n#include <linux/slab.h>\\n#include <linux/uaccess.h>\\n\\nstatic int fei_kprobe_handler(struct kprobe *kp, struct pt_regs *regs);\\n\\nstatic void fei_post_handler(struct kprobe *kp, struct pt_regs *regs,\\n\\t\\t\\t     unsigned long flags)\\n{\\n\\t/*\\n\\t * A dummy post handler is required to prohibit optimizing, because\\n\\t * jump optimization does not support execution path overriding.\\n\\t */\\n}\\n\\nstruct fei_attr {\\n\\tstruct list_head list;\\n\\tstruct kprobe kp;\\n\\tunsigned long retval;\\n};\\nstatic DEFINE_MUTEX(fei_lock);\\nstatic LIST_HEAD(fei_attr_list);\\nstatic DECLARE_FAULT_ATTR(fei_fault_attr);\\nstatic struct dentry *fei_debugfs_dir;\\n\\nstatic unsigned long adjust_error_retval(unsigned long addr, unsigned long retv)\\n{\\n\\tswitch (get_injectable_error_type(addr)) {\\n\\tcase EI_ETYPE_NULL:\\n\\t\\treturn 0;\\n\\tcase EI_ETYPE_ERRNO:\\n\\t\\tif (retv < (unsigned long)-MAX_ERRNO)\\n\\t\\t\\treturn (unsigned long)-EINVAL;\\n\\t\\tbreak;\\n\\tcase EI_ETYPE_ERRNO_NULL:\\n\\t\\tif (retv != 0 && retv < (unsigned long)-MAX_ERRNO)\\n\\t\\t\\treturn (unsigned long)-EINVAL;\\n\\t\\tbreak;\\n\\tcase EI_ETYPE_TRUE:\\n\\t\\treturn 1;\\n\\t}\\n\\n\\treturn retv;\\n}\\n\\nstatic struct fei_attr *fei_attr_new(const char *sym, unsigned long addr)\\n{\\n\\tstruct fei_attr *attr;\\n\\n\\tattr = kzalloc(sizeof(*attr), GFP_KERNEL);\\n\\tif (attr) {\\n\\t\\tattr->kp.symbol_name = kstrdup(sym, GFP_KERNEL);\\n\\t\\tif (!attr->kp.symbol_name) {\\n\\t\\t\\tkfree(attr);\\n\\t\\t\\treturn NULL;\\n\\t\\t}\\n\\t\\tattr->kp.pre_handler = fei_kprobe_handler;\\n\\t\\tattr->kp.post_handler = fei_post_handler;\\n\\t\\tattr->retval = adjust_error_retval(addr, 0);\\n\\t\\tINIT_LIST_HEAD(&attr->list);\\n\\t}\\n\\treturn attr;\\n}\\n\\nstatic void fei_attr_free(struct fei_attr *attr)\\n{\\n\\tif (attr) {\\n\\t\\tkfree(attr->kp.symbol_name);\\n\\t\\tkfree(attr);\\n\\t}\\n}\\n\\nstatic struct fei_attr *fei_attr_lookup(const char *sym)\\n{\\n\\tstruct fei_attr *attr;\\n\\n\\tlist_for_each_entry(attr, &fei_attr_list, list) {\\n\\t\\tif (!strcmp(attr->kp.symbol_name, sym))\\n\\t\\t\\treturn attr;\\n\\t}\\n\\n\\treturn NULL;\\n}\\n\\nstatic bool fei_attr_is_valid(struct fei_attr *_attr)\\n{\\n\\tstruct fei_attr *attr;\\n\\n\\tlist_for_each_entry(attr, &fei_attr_list, list) {\\n\\t\\tif (attr == _attr)\\n\\t\\t\\treturn true;\\n\\t}\\n\\n\\treturn false;\\n}\\n\\nstatic int fei_retval_set(void *data, u64 val)\\n{\\n\\tstruct fei_attr *attr = data;\\n\\tunsigned long retv = (unsigned long)val;\\n\\tint err = 0;\\n\\n\\tmutex_lock(&fei_lock);\\n\\t/*\\n\\t * Since this operation can be done after retval file is removed,\\n\\t * It is safer to check the attr is still valid before accessing\\n\\t * its member.\\n\\t */\\n\\tif (!fei_attr_is_valid(attr)) {\\n\\t\\terr = -ENOENT;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (attr->kp.addr) {\\n\\t\\tif (adjust_error_retval((unsigned long)attr->kp.addr,\\n\\t\\t\\t\\t\\tval) != retv)\\n\\t\\t\\terr = -EINVAL;\\n\\t}\\n\\tif (!err)\\n\\t\\tattr->retval = val;\\nout:\\n\\tmutex_unlock(&fei_lock);\\n\\n\\treturn err;\\n}\\n\\nstatic int fei_retval_get(void *data, u64 *val)\\n{\\n\\tstruct fei_attr *attr = data;\\n\\tint err = 0;\\n\\n\\tmutex_lock(&fei_lock);\\n\\t/* Here we also validate @attr to ensure it still exists. */\\n\\tif (!fei_attr_is_valid(attr))\\n\\t\\terr = -ENOENT;\\n\\telse\\n\\t\\t*val = attr->retval;\\n\\tmutex_unlock(&fei_lock);\\n\\n\\treturn err;\\n}\\nDEFINE_DEBUGFS_ATTRIBUTE(fei_retval_ops, fei_retval_get, fei_retval_set,\\n\\t\\t\\t \"%llx\\\\n\");\\n\\nstatic void fei_debugfs_add_attr(struct fei_attr *attr)\\n{\\n\\tstruct dentry *dir;\\n\\n\\tdir = debugfs_create_dir(attr->kp.symbol_name, fei_debugfs_dir);\\n\\n\\tdebugfs_create_file(\"retval\", 0600, dir, attr, &fei_retval_ops);\\n}\\n\\nstatic void fei_debugfs_remove_attr(struct fei_attr *attr)\\n{\\n\\tdebugfs_lookup_and_remove(attr->kp.symbol_name, fei_debugfs_dir);\\n}\\n\\nstatic int fei_kprobe_handler(struct kprobe *kp, struct pt_regs *regs)\\n{\\n\\tstruct fei_attr *attr = container_of(kp, struct fei_attr, kp);\\n\\n\\tif (should_fail(&fei_fault_attr, 1)) {\\n\\t\\tregs_set_return_value(regs, attr->retval);\\n\\t\\toverride_function_with_return(regs);\\n\\t\\treturn 1;\\n\\t}\\n\\n\\treturn 0;\\n}\\nNOKPROBE_SYMBOL(fei_kprobe_handler)\\n\\nstatic void *fei_seq_start(struct seq_file *m, loff_t *pos)\\n{\\n\\tmutex_lock(&fei_lock);\\n\\treturn seq_list_start(&fei_attr_list, *pos);\\n}\\n\\nstatic void fei_seq_stop(struct seq_file *m, void *v)\\n{\\n\\tmutex_unlock(&fei_lock);\\n}\\n\\nstatic void *fei_seq_next(struct seq_file *m, void *v, loff_t *pos)\\n{\\n\\treturn seq_list_next(v, &fei_attr_list, pos);\\n}\\n\\nstatic int fei_seq_show(struct seq_file *m, void *v)\\n{\\n\\tstruct fei_attr *attr = list_entry(v, struct fei_attr, list);\\n\\n\\tseq_printf(m, \"%ps\\\\n\", attr->kp.addr);\\n\\treturn 0;\\n}\\n\\nstatic const struct seq_operations fei_seq_ops = {\\n\\t.start\\t= fei_seq_start,\\n\\t.next\\t= fei_seq_next,\\n\\t.stop\\t= fei_seq_stop,\\n\\t.show\\t= fei_seq_show,\\n};\\n\\nstatic int fei_open(struct inode *inode, struct file *file)\\n{\\n\\treturn seq_open(file, &fei_seq_ops);\\n}\\n\\nstatic void fei_attr_remove(struct fei_attr *attr)\\n{\\n\\tfei_debugfs_remove_attr(attr);\\n\\tunregister_kprobe(&attr->kp);\\n\\tlist_del(&attr->list);\\n\\tfei_attr_free(attr);\\n}\\n\\nstatic void fei_attr_remove_all(void)\\n{\\n\\tstruct fei_attr *attr, *n;\\n\\n\\tlist_for_each_entry_safe(attr, n, &fei_attr_list, list) {\\n\\t\\tfei_attr_remove(attr);\\n\\t}\\n}\\n\\nstatic ssize_t fei_write(struct file *file, const char __user *buffer,\\n\\t\\t\\t size_t count, loff_t *ppos)\\n{\\n\\tstruct fei_attr *attr;\\n\\tunsigned long addr;\\n\\tchar *buf, *sym;\\n\\tint ret;\\n\\n\\t/* cut off if it is too long */\\n\\tif (count > KSYM_NAME_LEN)\\n\\t\\tcount = KSYM_NAME_LEN;\\n\\n\\tbuf = memdup_user_nul(buffer, count);\\n\\tif (IS_ERR(buf))\\n\\t\\treturn PTR_ERR(buf);\\n\\n\\tsym = strstrip(buf);\\n\\n\\tmutex_lock(&fei_lock);\\n\\n\\t/* Writing just spaces will remove all injection points */\\n\\tif (sym[0] == \\'\\\\0\\') {\\n\\t\\tfei_attr_remove_all();\\n\\t\\tret = count;\\n\\t\\tgoto out;\\n\\t}\\n\\t/* Writing !function will remove one injection point */\\n\\tif (sym[0] == \\'!\\') {\\n\\t\\tattr = fei_attr_lookup(sym + 1);\\n\\t\\tif (!attr) {\\n\\t\\t\\tret = -ENOENT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tfei_attr_remove(attr);\\n\\t\\tret = count;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\taddr = kallsyms_lookup_name(sym);\\n\\tif (!addr) {\\n\\t\\tret = -EINVAL;\\n\\t\\tgoto out;\\n\\t}\\n\\tif (!within_error_injection_list(addr)) {\\n\\t\\tret = -ERANGE;\\n\\t\\tgoto out;\\n\\t}\\n\\tif (fei_attr_lookup(sym)) {\\n\\t\\tret = -EBUSY;\\n\\t\\tgoto out;\\n\\t}\\n\\tattr = fei_attr_new(sym, addr);\\n\\tif (!attr) {\\n\\t\\tret = -ENOMEM;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tret = register_kprobe(&attr->kp);\\n\\tif (ret) {\\n\\t\\tfei_attr_free(attr);\\n\\t\\tgoto out;\\n\\t}\\n\\tfei_debugfs_add_attr(attr);\\n\\tlist_add_tail(&attr->list, &fei_attr_list);\\n\\tret = count;\\nout:\\n\\tmutex_unlock(&fei_lock);\\n\\tkfree(buf);\\n\\treturn ret;\\n}\\n\\nstatic const struct file_operations fei_ops = {\\n\\t.open =\\t\\tfei_open,\\n\\t.read =\\t\\tseq_read,\\n\\t.write =\\tfei_write,\\n\\t.llseek =\\tseq_lseek,\\n\\t.release =\\tseq_release,\\n};\\n\\nstatic int __init fei_debugfs_init(void)\\n{\\n\\tstruct dentry *dir;\\n\\n\\tdir = fault_create_debugfs_attr(\"fail_function\", NULL,\\n\\t\\t\\t\\t\\t&fei_fault_attr);\\n\\tif (IS_ERR(dir))\\n\\t\\treturn PTR_ERR(dir);\\n\\n\\t/* injectable attribute is just a symlink of error_inject/list */\\n\\tdebugfs_create_symlink(\"injectable\", dir, \"../error_injection/list\");\\n\\n\\tdebugfs_create_file(\"inject\", 0600, dir, NULL, &fei_ops);\\n\\n\\tfei_debugfs_dir = dir;\\n\\n\\treturn 0;\\n}\\n\\nlate_initcall(fei_debugfs_init);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#include <linux/kernel.h>\\n#include <linux/syscalls.h>\\n#include <linux/fdtable.h>\\n#include <linux/string.h>\\n#include <linux/random.h>\\n#include <linux/module.h>\\n#include <linux/ptrace.h>\\n#include <linux/init.h>\\n#include <linux/errno.h>\\n#include <linux/cache.h>\\n#include <linux/bug.h>\\n#include <linux/err.h>\\n#include <linux/kcmp.h>\\n#include <linux/capability.h>\\n#include <linux/list.h>\\n#include <linux/eventpoll.h>\\n#include <linux/file.h>\\n\\n#include <asm/unistd.h>\\n\\n/*\\n * We don\\'t expose the real in-memory order of objects for security reasons.\\n * But still the comparison results should be suitable for sorting. So we\\n * obfuscate kernel pointers values and compare the production instead.\\n *\\n * The obfuscation is done in two steps. First we xor the kernel pointer with\\n * a random value, which puts pointer into a new position in a reordered space.\\n * Secondly we multiply the xor production with a large odd random number to\\n * permute its bits even more (the odd multiplier guarantees that the product\\n * is unique ever after the high bits are truncated, since any odd number is\\n * relative prime to 2^n).\\n *\\n * Note also that the obfuscation itself is invisible to userspace and if needed\\n * it can be changed to an alternate scheme.\\n */\\nstatic unsigned long cookies[KCMP_TYPES][2] __read_mostly;\\n\\nstatic long kptr_obfuscate(long v, int type)\\n{\\n\\treturn (v ^ cookies[type][0]) * cookies[type][1];\\n}\\n\\n/*\\n * 0 - equal, i.e. v1 = v2\\n * 1 - less than, i.e. v1 < v2\\n * 2 - greater than, i.e. v1 > v2\\n * 3 - not equal but ordering unavailable (reserved for future)\\n */\\nstatic int kcmp_ptr(void *v1, void *v2, enum kcmp_type type)\\n{\\n\\tlong t1, t2;\\n\\n\\tt1 = kptr_obfuscate((long)v1, type);\\n\\tt2 = kptr_obfuscate((long)v2, type);\\n\\n\\treturn (t1 < t2) | ((t1 > t2) << 1);\\n}\\n\\n/* The caller must have pinned the task */\\nstatic struct file *\\nget_file_raw_ptr(struct task_struct *task, unsigned int idx)\\n{\\n\\tstruct file *file;\\n\\n\\tfile = fget_task(task, idx);\\n\\tif (file)\\n\\t\\tfput(file);\\n\\n\\treturn file;\\n}\\n\\nstatic void kcmp_unlock(struct rw_semaphore *l1, struct rw_semaphore *l2)\\n{\\n\\tif (likely(l2 != l1))\\n\\t\\tup_read(l2);\\n\\tup_read(l1);\\n}\\n\\nstatic int kcmp_lock(struct rw_semaphore *l1, struct rw_semaphore *l2)\\n{\\n\\tint err;\\n\\n\\tif (l2 > l1)\\n\\t\\tswap(l1, l2);\\n\\n\\terr = down_read_killable(l1);\\n\\tif (!err && likely(l1 != l2)) {\\n\\t\\terr = down_read_killable_nested(l2, SINGLE_DEPTH_NESTING);\\n\\t\\tif (err)\\n\\t\\t\\tup_read(l1);\\n\\t}\\n\\n\\treturn err;\\n}\\n\\n#ifdef CONFIG_EPOLL\\nstatic int kcmp_epoll_target(struct task_struct *task1,\\n\\t\\t\\t     struct task_struct *task2,\\n\\t\\t\\t     unsigned long idx1,\\n\\t\\t\\t     struct kcmp_epoll_slot __user *uslot)\\n{\\n\\tstruct file *filp, *filp_epoll, *filp_tgt;\\n\\tstruct kcmp_epoll_slot slot;\\n\\n\\tif (copy_from_user(&slot, uslot, sizeof(slot)))\\n\\t\\treturn -EFAULT;\\n\\n\\tfilp = get_file_raw_ptr(task1, idx1);\\n\\tif (!filp)\\n\\t\\treturn -EBADF;\\n\\n\\tfilp_epoll = fget_task(task2, slot.efd);\\n\\tif (!filp_epoll)\\n\\t\\treturn -EBADF;\\n\\n\\tfilp_tgt = get_epoll_tfile_raw_ptr(filp_epoll, slot.tfd, slot.toff);\\n\\tfput(filp_epoll);\\n\\n\\tif (IS_ERR(filp_tgt))\\n\\t\\treturn PTR_ERR(filp_tgt);\\n\\n\\treturn kcmp_ptr(filp, filp_tgt, KCMP_FILE);\\n}\\n#else\\nstatic int kcmp_epoll_target(struct task_struct *task1,\\n\\t\\t\\t     struct task_struct *task2,\\n\\t\\t\\t     unsigned long idx1,\\n\\t\\t\\t     struct kcmp_epoll_slot __user *uslot)\\n{\\n\\treturn -EOPNOTSUPP;\\n}\\n#endif\\n\\nSYSCALL_DEFINE5(kcmp, pid_t, pid1, pid_t, pid2, int, type,\\n\\t\\tunsigned long, idx1, unsigned long, idx2)\\n{\\n\\tstruct task_struct *task1, *task2;\\n\\tint ret;\\n\\n\\trcu_read_lock();\\n\\n\\t/*\\n\\t * Tasks are looked up in caller\\'s PID namespace only.\\n\\t */\\n\\ttask1 = find_task_by_vpid(pid1);\\n\\ttask2 = find_task_by_vpid(pid2);\\n\\tif (!task1 || !task2)\\n\\t\\tgoto err_no_task;\\n\\n\\tget_task_struct(task1);\\n\\tget_task_struct(task2);\\n\\n\\trcu_read_unlock();\\n\\n\\t/*\\n\\t * One should have enough rights to inspect task details.\\n\\t */\\n\\tret = kcmp_lock(&task1->signal->exec_update_lock,\\n\\t\\t\\t&task2->signal->exec_update_lock);\\n\\tif (ret)\\n\\t\\tgoto err;\\n\\tif (!ptrace_may_access(task1, PTRACE_MODE_READ_REALCREDS) ||\\n\\t    !ptrace_may_access(task2, PTRACE_MODE_READ_REALCREDS)) {\\n\\t\\tret = -EPERM;\\n\\t\\tgoto err_unlock;\\n\\t}\\n\\n\\tswitch (type) {\\n\\tcase KCMP_FILE: {\\n\\t\\tstruct file *filp1, *filp2;\\n\\n\\t\\tfilp1 = get_file_raw_ptr(task1, idx1);\\n\\t\\tfilp2 = get_file_raw_ptr(task2, idx2);\\n\\n\\t\\tif (filp1 && filp2)\\n\\t\\t\\tret = kcmp_ptr(filp1, filp2, KCMP_FILE);\\n\\t\\telse\\n\\t\\t\\tret = -EBADF;\\n\\t\\tbreak;\\n\\t}\\n\\tcase KCMP_VM:\\n\\t\\tret = kcmp_ptr(task1->mm, task2->mm, KCMP_VM);\\n\\t\\tbreak;\\n\\tcase KCMP_FILES:\\n\\t\\tret = kcmp_ptr(task1->files, task2->files, KCMP_FILES);\\n\\t\\tbreak;\\n\\tcase KCMP_FS:\\n\\t\\tret = kcmp_ptr(task1->fs, task2->fs, KCMP_FS);\\n\\t\\tbreak;\\n\\tcase KCMP_SIGHAND:\\n\\t\\tret = kcmp_ptr(task1->sighand, task2->sighand, KCMP_SIGHAND);\\n\\t\\tbreak;\\n\\tcase KCMP_IO:\\n\\t\\tret = kcmp_ptr(task1->io_context, task2->io_context, KCMP_IO);\\n\\t\\tbreak;\\n\\tcase KCMP_SYSVSEM:\\n#ifdef CONFIG_SYSVIPC\\n\\t\\tret = kcmp_ptr(task1->sysvsem.undo_list,\\n\\t\\t\\t       task2->sysvsem.undo_list,\\n\\t\\t\\t       KCMP_SYSVSEM);\\n#else\\n\\t\\tret = -EOPNOTSUPP;\\n#endif\\n\\t\\tbreak;\\n\\tcase KCMP_EPOLL_TFD:\\n\\t\\tret = kcmp_epoll_target(task1, task2, idx1, (void *)idx2);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tret = -EINVAL;\\n\\t\\tbreak;\\n\\t}\\n\\nerr_unlock:\\n\\tkcmp_unlock(&task1->signal->exec_update_lock,\\n\\t\\t    &task2->signal->exec_update_lock);\\nerr:\\n\\tput_task_struct(task1);\\n\\tput_task_struct(task2);\\n\\n\\treturn ret;\\n\\nerr_no_task:\\n\\trcu_read_unlock();\\n\\treturn -ESRCH;\\n}\\n\\nstatic __init int kcmp_cookies_init(void)\\n{\\n\\tint i;\\n\\n\\tget_random_bytes(cookies, sizeof(cookies));\\n\\n\\tfor (i = 0; i < KCMP_TYPES; i++)\\n\\t\\tcookies[i][1] |= (~(~0UL >>  1) | 1);\\n\\n\\treturn 0;\\n}\\narch_initcall(kcmp_cookies_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * crash.c - kernel crash support code.\\n * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>\\n */\\n\\n#include <linux/buildid.h>\\n#include <linux/init.h>\\n#include <linux/utsname.h>\\n#include <linux/vmalloc.h>\\n#include <linux/sizes.h>\\n#include <linux/kexec.h>\\n#include <linux/memory.h>\\n#include <linux/cpuhotplug.h>\\n#include <linux/memblock.h>\\n#include <linux/kmemleak.h>\\n\\n#include <asm/page.h>\\n#include <asm/sections.h>\\n\\n#include <crypto/sha1.h>\\n\\n#include \"kallsyms_internal.h\"\\n#include \"kexec_internal.h\"\\n\\n/* Location of the reserved area for the crash kernel */\\nstruct resource crashk_res = {\\n\\t.name  = \"Crash kernel\",\\n\\t.start = 0,\\n\\t.end   = 0,\\n\\t.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,\\n\\t.desc  = IORES_DESC_CRASH_KERNEL\\n};\\nstruct resource crashk_low_res = {\\n\\t.name  = \"Crash kernel\",\\n\\t.start = 0,\\n\\t.end   = 0,\\n\\t.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,\\n\\t.desc  = IORES_DESC_CRASH_KERNEL\\n};\\n\\n/*\\n * parsing the \"crashkernel\" commandline\\n *\\n * this code is intended to be called from architecture specific code\\n */\\n\\n\\n/*\\n * This function parses command lines in the format\\n *\\n *   crashkernel=ramsize-range:size[,...][@offset]\\n *\\n * The function returns 0 on success and -EINVAL on failure.\\n */\\nstatic int __init parse_crashkernel_mem(char *cmdline,\\n\\t\\t\\t\\t\\tunsigned long long system_ram,\\n\\t\\t\\t\\t\\tunsigned long long *crash_size,\\n\\t\\t\\t\\t\\tunsigned long long *crash_base)\\n{\\n\\tchar *cur = cmdline, *tmp;\\n\\tunsigned long long total_mem = system_ram;\\n\\n\\t/*\\n\\t * Firmware sometimes reserves some memory regions for its own use,\\n\\t * so the system memory size is less than the actual physical memory\\n\\t * size. Work around this by rounding up the total size to 128M,\\n\\t * which is enough for most test cases.\\n\\t */\\n\\ttotal_mem = roundup(total_mem, SZ_128M);\\n\\n\\t/* for each entry of the comma-separated list */\\n\\tdo {\\n\\t\\tunsigned long long start, end = ULLONG_MAX, size;\\n\\n\\t\\t/* get the start of the range */\\n\\t\\tstart = memparse(cur, &tmp);\\n\\t\\tif (cur == tmp) {\\n\\t\\t\\tpr_warn(\"crashkernel: Memory value expected\\\\n\");\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\tcur = tmp;\\n\\t\\tif (*cur != \\'-\\') {\\n\\t\\t\\tpr_warn(\"crashkernel: \\'-\\' expected\\\\n\");\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\tcur++;\\n\\n\\t\\t/* if no \\':\\' is here, than we read the end */\\n\\t\\tif (*cur != \\':\\') {\\n\\t\\t\\tend = memparse(cur, &tmp);\\n\\t\\t\\tif (cur == tmp) {\\n\\t\\t\\t\\tpr_warn(\"crashkernel: Memory value expected\\\\n\");\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\t}\\n\\t\\t\\tcur = tmp;\\n\\t\\t\\tif (end <= start) {\\n\\t\\t\\t\\tpr_warn(\"crashkernel: end <= start\\\\n\");\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\tif (*cur != \\':\\') {\\n\\t\\t\\tpr_warn(\"crashkernel: \\':\\' expected\\\\n\");\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\tcur++;\\n\\n\\t\\tsize = memparse(cur, &tmp);\\n\\t\\tif (cur == tmp) {\\n\\t\\t\\tpr_warn(\"crashkernel: Memory value expected\\\\n\");\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\tcur = tmp;\\n\\t\\tif (size >= total_mem) {\\n\\t\\t\\tpr_warn(\"crashkernel: invalid size\\\\n\");\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\n\\t\\t/* match ? */\\n\\t\\tif (total_mem >= start && total_mem < end) {\\n\\t\\t\\t*crash_size = size;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t} while (*cur++ == \\',\\');\\n\\n\\tif (*crash_size > 0) {\\n\\t\\twhile (*cur && *cur != \\' \\' && *cur != \\'@\\')\\n\\t\\t\\tcur++;\\n\\t\\tif (*cur == \\'@\\') {\\n\\t\\t\\tcur++;\\n\\t\\t\\t*crash_base = memparse(cur, &tmp);\\n\\t\\t\\tif (cur == tmp) {\\n\\t\\t\\t\\tpr_warn(\"crahskernel: Memory value expected after \\'@\\'\\\\n\");\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t} else\\n\\t\\tpr_info(\"crashkernel size resulted in zero bytes\\\\n\");\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * That function parses \"simple\" (old) crashkernel command lines like\\n *\\n *\\tcrashkernel=size[@offset]\\n *\\n * It returns 0 on success and -EINVAL on failure.\\n */\\nstatic int __init parse_crashkernel_simple(char *cmdline,\\n\\t\\t\\t\\t\\t   unsigned long long *crash_size,\\n\\t\\t\\t\\t\\t   unsigned long long *crash_base)\\n{\\n\\tchar *cur = cmdline;\\n\\n\\t*crash_size = memparse(cmdline, &cur);\\n\\tif (cmdline == cur) {\\n\\t\\tpr_warn(\"crashkernel: memory value expected\\\\n\");\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tif (*cur == \\'@\\')\\n\\t\\t*crash_base = memparse(cur+1, &cur);\\n\\telse if (*cur != \\' \\' && *cur != \\'\\\\0\\') {\\n\\t\\tpr_warn(\"crashkernel: unrecognized char: %c\\\\n\", *cur);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n#define SUFFIX_HIGH 0\\n#define SUFFIX_LOW  1\\n#define SUFFIX_NULL 2\\nstatic __initdata char *suffix_tbl[] = {\\n\\t[SUFFIX_HIGH] = \",high\",\\n\\t[SUFFIX_LOW]  = \",low\",\\n\\t[SUFFIX_NULL] = NULL,\\n};\\n\\n/*\\n * That function parses \"suffix\"  crashkernel command lines like\\n *\\n *\\tcrashkernel=size,[high|low]\\n *\\n * It returns 0 on success and -EINVAL on failure.\\n */\\nstatic int __init parse_crashkernel_suffix(char *cmdline,\\n\\t\\t\\t\\t\\t   unsigned long long *crash_size,\\n\\t\\t\\t\\t\\t   const char *suffix)\\n{\\n\\tchar *cur = cmdline;\\n\\n\\t*crash_size = memparse(cmdline, &cur);\\n\\tif (cmdline == cur) {\\n\\t\\tpr_warn(\"crashkernel: memory value expected\\\\n\");\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\t/* check with suffix */\\n\\tif (strncmp(cur, suffix, strlen(suffix))) {\\n\\t\\tpr_warn(\"crashkernel: unrecognized char: %c\\\\n\", *cur);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\tcur += strlen(suffix);\\n\\tif (*cur != \\' \\' && *cur != \\'\\\\0\\') {\\n\\t\\tpr_warn(\"crashkernel: unrecognized char: %c\\\\n\", *cur);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic __init char *get_last_crashkernel(char *cmdline,\\n\\t\\t\\t     const char *name,\\n\\t\\t\\t     const char *suffix)\\n{\\n\\tchar *p = cmdline, *ck_cmdline = NULL;\\n\\n\\t/* find crashkernel and use the last one if there are more */\\n\\tp = strstr(p, name);\\n\\twhile (p) {\\n\\t\\tchar *end_p = strchr(p, \\' \\');\\n\\t\\tchar *q;\\n\\n\\t\\tif (!end_p)\\n\\t\\t\\tend_p = p + strlen(p);\\n\\n\\t\\tif (!suffix) {\\n\\t\\t\\tint i;\\n\\n\\t\\t\\t/* skip the one with any known suffix */\\n\\t\\t\\tfor (i = 0; suffix_tbl[i]; i++) {\\n\\t\\t\\t\\tq = end_p - strlen(suffix_tbl[i]);\\n\\t\\t\\t\\tif (!strncmp(q, suffix_tbl[i],\\n\\t\\t\\t\\t\\t     strlen(suffix_tbl[i])))\\n\\t\\t\\t\\t\\tgoto next;\\n\\t\\t\\t}\\n\\t\\t\\tck_cmdline = p;\\n\\t\\t} else {\\n\\t\\t\\tq = end_p - strlen(suffix);\\n\\t\\t\\tif (!strncmp(q, suffix, strlen(suffix)))\\n\\t\\t\\t\\tck_cmdline = p;\\n\\t\\t}\\nnext:\\n\\t\\tp = strstr(p+1, name);\\n\\t}\\n\\n\\treturn ck_cmdline;\\n}\\n\\nstatic int __init __parse_crashkernel(char *cmdline,\\n\\t\\t\\t     unsigned long long system_ram,\\n\\t\\t\\t     unsigned long long *crash_size,\\n\\t\\t\\t     unsigned long long *crash_base,\\n\\t\\t\\t     const char *suffix)\\n{\\n\\tchar *first_colon, *first_space;\\n\\tchar *ck_cmdline;\\n\\tchar *name = \"crashkernel=\";\\n\\n\\tBUG_ON(!crash_size || !crash_base);\\n\\t*crash_size = 0;\\n\\t*crash_base = 0;\\n\\n\\tck_cmdline = get_last_crashkernel(cmdline, name, suffix);\\n\\tif (!ck_cmdline)\\n\\t\\treturn -ENOENT;\\n\\n\\tck_cmdline += strlen(name);\\n\\n\\tif (suffix)\\n\\t\\treturn parse_crashkernel_suffix(ck_cmdline, crash_size,\\n\\t\\t\\t\\tsuffix);\\n\\t/*\\n\\t * if the commandline contains a \\':\\', then that\\'s the extended\\n\\t * syntax -- if not, it must be the classic syntax\\n\\t */\\n\\tfirst_colon = strchr(ck_cmdline, \\':\\');\\n\\tfirst_space = strchr(ck_cmdline, \\' \\');\\n\\tif (first_colon && (!first_space || first_colon < first_space))\\n\\t\\treturn parse_crashkernel_mem(ck_cmdline, system_ram,\\n\\t\\t\\t\\tcrash_size, crash_base);\\n\\n\\treturn parse_crashkernel_simple(ck_cmdline, crash_size, crash_base);\\n}\\n\\n/*\\n * That function is the entry point for command line parsing and should be\\n * called from the arch-specific code.\\n *\\n * If crashkernel=,high|low is supported on architecture, non-NULL values\\n * should be passed to parameters \\'low_size\\' and \\'high\\'.\\n */\\nint __init parse_crashkernel(char *cmdline,\\n\\t\\t\\t     unsigned long long system_ram,\\n\\t\\t\\t     unsigned long long *crash_size,\\n\\t\\t\\t     unsigned long long *crash_base,\\n\\t\\t\\t     unsigned long long *low_size,\\n\\t\\t\\t     bool *high)\\n{\\n\\tint ret;\\n\\n\\t/* crashkernel=X[@offset] */\\n\\tret = __parse_crashkernel(cmdline, system_ram, crash_size,\\n\\t\\t\\t\\tcrash_base, NULL);\\n#ifdef CONFIG_ARCH_HAS_GENERIC_CRASHKERNEL_RESERVATION\\n\\t/*\\n\\t * If non-NULL \\'high\\' passed in and no normal crashkernel\\n\\t * setting detected, try parsing crashkernel=,high|low.\\n\\t */\\n\\tif (high && ret == -ENOENT) {\\n\\t\\tret = __parse_crashkernel(cmdline, 0, crash_size,\\n\\t\\t\\t\\tcrash_base, suffix_tbl[SUFFIX_HIGH]);\\n\\t\\tif (ret || !*crash_size)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\t/*\\n\\t\\t * crashkernel=Y,low can be specified or not, but invalid value\\n\\t\\t * is not allowed.\\n\\t\\t */\\n\\t\\tret = __parse_crashkernel(cmdline, 0, low_size,\\n\\t\\t\\t\\tcrash_base, suffix_tbl[SUFFIX_LOW]);\\n\\t\\tif (ret == -ENOENT) {\\n\\t\\t\\t*low_size = DEFAULT_CRASH_KERNEL_LOW_SIZE;\\n\\t\\t\\tret = 0;\\n\\t\\t} else if (ret) {\\n\\t\\t\\treturn ret;\\n\\t\\t}\\n\\n\\t\\t*high = true;\\n\\t}\\n#endif\\n\\tif (!*crash_size)\\n\\t\\tret = -EINVAL;\\n\\n\\tif (*crash_size >= system_ram)\\n\\t\\tret = -EINVAL;\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * Add a dummy early_param handler to mark crashkernel= as a known command line\\n * parameter and suppress incorrect warnings in init/main.c.\\n */\\nstatic int __init parse_crashkernel_dummy(char *arg)\\n{\\n\\treturn 0;\\n}\\nearly_param(\"crashkernel\", parse_crashkernel_dummy);\\n\\n#ifdef CONFIG_ARCH_HAS_GENERIC_CRASHKERNEL_RESERVATION\\nstatic int __init reserve_crashkernel_low(unsigned long long low_size)\\n{\\n#ifdef CONFIG_64BIT\\n\\tunsigned long long low_base;\\n\\n\\tlow_base = memblock_phys_alloc_range(low_size, CRASH_ALIGN, 0, CRASH_ADDR_LOW_MAX);\\n\\tif (!low_base) {\\n\\t\\tpr_err(\"cannot allocate crashkernel low memory (size:0x%llx).\\\\n\", low_size);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tpr_info(\"crashkernel low memory reserved: 0x%08llx - 0x%08llx (%lld MB)\\\\n\",\\n\\t\\tlow_base, low_base + low_size, low_size >> 20);\\n\\n\\tcrashk_low_res.start = low_base;\\n\\tcrashk_low_res.end   = low_base + low_size - 1;\\n#ifdef HAVE_ARCH_ADD_CRASH_RES_TO_IOMEM_EARLY\\n\\tinsert_resource(&iomem_resource, &crashk_low_res);\\n#endif\\n#endif\\n\\treturn 0;\\n}\\n\\nvoid __init reserve_crashkernel_generic(char *cmdline,\\n\\t\\t\\t     unsigned long long crash_size,\\n\\t\\t\\t     unsigned long long crash_base,\\n\\t\\t\\t     unsigned long long crash_low_size,\\n\\t\\t\\t     bool high)\\n{\\n\\tunsigned long long search_end = CRASH_ADDR_LOW_MAX, search_base = 0;\\n\\tbool fixed_base = false;\\n\\n\\t/* User specifies base address explicitly. */\\n\\tif (crash_base) {\\n\\t\\tfixed_base = true;\\n\\t\\tsearch_base = crash_base;\\n\\t\\tsearch_end = crash_base + crash_size;\\n\\t} else if (high) {\\n\\t\\tsearch_base = CRASH_ADDR_LOW_MAX;\\n\\t\\tsearch_end = CRASH_ADDR_HIGH_MAX;\\n\\t}\\n\\nretry:\\n\\tcrash_base = memblock_phys_alloc_range(crash_size, CRASH_ALIGN,\\n\\t\\t\\t\\t\\t       search_base, search_end);\\n\\tif (!crash_base) {\\n\\t\\t/*\\n\\t\\t * For crashkernel=size[KMG]@offset[KMG], print out failure\\n\\t\\t * message if can\\'t reserve the specified region.\\n\\t\\t */\\n\\t\\tif (fixed_base) {\\n\\t\\t\\tpr_warn(\"crashkernel reservation failed - memory is in use.\\\\n\");\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * For crashkernel=size[KMG], if the first attempt was for\\n\\t\\t * low memory, fall back to high memory, the minimum required\\n\\t\\t * low memory will be reserved later.\\n\\t\\t */\\n\\t\\tif (!high && search_end == CRASH_ADDR_LOW_MAX) {\\n\\t\\t\\tsearch_end = CRASH_ADDR_HIGH_MAX;\\n\\t\\t\\tsearch_base = CRASH_ADDR_LOW_MAX;\\n\\t\\t\\tcrash_low_size = DEFAULT_CRASH_KERNEL_LOW_SIZE;\\n\\t\\t\\tgoto retry;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * For crashkernel=size[KMG],high, if the first attempt was\\n\\t\\t * for high memory, fall back to low memory.\\n\\t\\t */\\n\\t\\tif (high && search_end == CRASH_ADDR_HIGH_MAX) {\\n\\t\\t\\tsearch_end = CRASH_ADDR_LOW_MAX;\\n\\t\\t\\tsearch_base = 0;\\n\\t\\t\\tif (search_end != CRASH_ADDR_HIGH_MAX)\\n\\t\\t\\t\\tgoto retry;\\n\\t\\t}\\n\\t\\tpr_warn(\"cannot allocate crashkernel (size:0x%llx)\\\\n\",\\n\\t\\t\\tcrash_size);\\n\\t\\treturn;\\n\\t}\\n\\n\\tif ((crash_base >= CRASH_ADDR_LOW_MAX) &&\\n\\t     crash_low_size && reserve_crashkernel_low(crash_low_size)) {\\n\\t\\tmemblock_phys_free(crash_base, crash_size);\\n\\t\\treturn;\\n\\t}\\n\\n\\tpr_info(\"crashkernel reserved: 0x%016llx - 0x%016llx (%lld MB)\\\\n\",\\n\\t\\tcrash_base, crash_base + crash_size, crash_size >> 20);\\n\\n\\t/*\\n\\t * The crashkernel memory will be removed from the kernel linear\\n\\t * map. Inform kmemleak so that it won\\'t try to access it.\\n\\t */\\n\\tkmemleak_ignore_phys(crash_base);\\n\\tif (crashk_low_res.end)\\n\\t\\tkmemleak_ignore_phys(crashk_low_res.start);\\n\\n\\tcrashk_res.start = crash_base;\\n\\tcrashk_res.end = crash_base + crash_size - 1;\\n#ifdef HAVE_ARCH_ADD_CRASH_RES_TO_IOMEM_EARLY\\n\\tinsert_resource(&iomem_resource, &crashk_res);\\n#endif\\n}\\n\\n#ifndef HAVE_ARCH_ADD_CRASH_RES_TO_IOMEM_EARLY\\nstatic __init int insert_crashkernel_resources(void)\\n{\\n\\tif (crashk_res.start < crashk_res.end)\\n\\t\\tinsert_resource(&iomem_resource, &crashk_res);\\n\\n\\tif (crashk_low_res.start < crashk_low_res.end)\\n\\t\\tinsert_resource(&iomem_resource, &crashk_low_res);\\n\\n\\treturn 0;\\n}\\nearly_initcall(insert_crashkernel_resources);\\n#endif\\n#endif\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * kexec.c - kexec system call core code.\\n * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/btf.h>\\n#include <linux/capability.h>\\n#include <linux/mm.h>\\n#include <linux/file.h>\\n#include <linux/slab.h>\\n#include <linux/fs.h>\\n#include <linux/kexec.h>\\n#include <linux/mutex.h>\\n#include <linux/list.h>\\n#include <linux/highmem.h>\\n#include <linux/syscalls.h>\\n#include <linux/reboot.h>\\n#include <linux/ioport.h>\\n#include <linux/hardirq.h>\\n#include <linux/elf.h>\\n#include <linux/elfcore.h>\\n#include <linux/utsname.h>\\n#include <linux/numa.h>\\n#include <linux/suspend.h>\\n#include <linux/device.h>\\n#include <linux/freezer.h>\\n#include <linux/panic_notifier.h>\\n#include <linux/pm.h>\\n#include <linux/cpu.h>\\n#include <linux/uaccess.h>\\n#include <linux/io.h>\\n#include <linux/console.h>\\n#include <linux/vmalloc.h>\\n#include <linux/swap.h>\\n#include <linux/syscore_ops.h>\\n#include <linux/compiler.h>\\n#include <linux/hugetlb.h>\\n#include <linux/objtool.h>\\n#include <linux/kmsg_dump.h>\\n\\n#include <asm/page.h>\\n#include <asm/sections.h>\\n\\n#include <crypto/hash.h>\\n#include \"kexec_internal.h\"\\n\\natomic_t __kexec_lock = ATOMIC_INIT(0);\\n\\n/* Flag to indicate we are going to kexec a new kernel */\\nbool kexec_in_progress = false;\\n\\nbool kexec_file_dbg_print;\\n\\n/*\\n * When kexec transitions to the new kernel there is a one-to-one\\n * mapping between physical and virtual addresses.  On processors\\n * where you can disable the MMU this is trivial, and easy.  For\\n * others it is still a simple predictable page table to setup.\\n *\\n * In that environment kexec copies the new kernel to its final\\n * resting place.  This means I can only support memory whose\\n * physical address can fit in an unsigned long.  In particular\\n * addresses where (pfn << PAGE_SHIFT) > ULONG_MAX cannot be handled.\\n * If the assembly stub has more restrictive requirements\\n * KEXEC_SOURCE_MEMORY_LIMIT and KEXEC_DEST_MEMORY_LIMIT can be\\n * defined more restrictively in <asm/kexec.h>.\\n *\\n * The code for the transition from the current kernel to the\\n * new kernel is placed in the control_code_buffer, whose size\\n * is given by KEXEC_CONTROL_PAGE_SIZE.  In the best case only a single\\n * page of memory is necessary, but some architectures require more.\\n * Because this memory must be identity mapped in the transition from\\n * virtual to physical addresses it must live in the range\\n * 0 - TASK_SIZE, as only the user space mappings are arbitrarily\\n * modifiable.\\n *\\n * The assembly stub in the control code buffer is passed a linked list\\n * of descriptor pages detailing the source pages of the new kernel,\\n * and the destination addresses of those source pages.  As this data\\n * structure is not used in the context of the current OS, it must\\n * be self-contained.\\n *\\n * The code has been made to work with highmem pages and will use a\\n * destination page in its final resting place (if it happens\\n * to allocate it).  The end product of this is that most of the\\n * physical address space, and most of RAM can be used.\\n *\\n * Future directions include:\\n *  - allocating a page table with the control code buffer identity\\n *    mapped, to simplify machine_kexec and make kexec_on_panic more\\n *    reliable.\\n */\\n\\n/*\\n * KIMAGE_NO_DEST is an impossible destination address..., for\\n * allocating pages whose destination address we do not care about.\\n */\\n#define KIMAGE_NO_DEST (-1UL)\\n#define PAGE_COUNT(x) (((x) + PAGE_SIZE - 1) >> PAGE_SHIFT)\\n\\nstatic struct page *kimage_alloc_page(struct kimage *image,\\n\\t\\t\\t\\t       gfp_t gfp_mask,\\n\\t\\t\\t\\t       unsigned long dest);\\n\\nint sanity_check_segment_list(struct kimage *image)\\n{\\n\\tint i;\\n\\tunsigned long nr_segments = image->nr_segments;\\n\\tunsigned long total_pages = 0;\\n\\tunsigned long nr_pages = totalram_pages();\\n\\n\\t/*\\n\\t * Verify we have good destination addresses.  The caller is\\n\\t * responsible for making certain we don\\'t attempt to load\\n\\t * the new image into invalid or reserved areas of RAM.  This\\n\\t * just verifies it is an address we can use.\\n\\t *\\n\\t * Since the kernel does everything in page size chunks ensure\\n\\t * the destination addresses are page aligned.  Too many\\n\\t * special cases crop of when we don\\'t do this.  The most\\n\\t * insidious is getting overlapping destination addresses\\n\\t * simply because addresses are changed to page size\\n\\t * granularity.\\n\\t */\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tunsigned long mstart, mend;\\n\\n\\t\\tmstart = image->segment[i].mem;\\n\\t\\tmend   = mstart + image->segment[i].memsz;\\n\\t\\tif (mstart > mend)\\n\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t\\tif ((mstart & ~PAGE_MASK) || (mend & ~PAGE_MASK))\\n\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t\\tif (mend >= KEXEC_DESTINATION_MEMORY_LIMIT)\\n\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t}\\n\\n\\t/* Verify our destination addresses do not overlap.\\n\\t * If we alloed overlapping destination addresses\\n\\t * through very weird things can happen with no\\n\\t * easy explanation as one segment stops on another.\\n\\t */\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tunsigned long mstart, mend;\\n\\t\\tunsigned long j;\\n\\n\\t\\tmstart = image->segment[i].mem;\\n\\t\\tmend   = mstart + image->segment[i].memsz;\\n\\t\\tfor (j = 0; j < i; j++) {\\n\\t\\t\\tunsigned long pstart, pend;\\n\\n\\t\\t\\tpstart = image->segment[j].mem;\\n\\t\\t\\tpend   = pstart + image->segment[j].memsz;\\n\\t\\t\\t/* Do the segments overlap ? */\\n\\t\\t\\tif ((mend > pstart) && (mstart < pend))\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t}\\n\\n\\t/* Ensure our buffer sizes are strictly less than\\n\\t * our memory sizes.  This should always be the case,\\n\\t * and it is easier to check up front than to be surprised\\n\\t * later on.\\n\\t */\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tif (image->segment[i].bufsz > image->segment[i].memsz)\\n\\t\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\t/*\\n\\t * Verify that no more than half of memory will be consumed. If the\\n\\t * request from userspace is too large, a large amount of time will be\\n\\t * wasted allocating pages, which can cause a soft lockup.\\n\\t */\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tif (PAGE_COUNT(image->segment[i].memsz) > nr_pages / 2)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\ttotal_pages += PAGE_COUNT(image->segment[i].memsz);\\n\\t}\\n\\n\\tif (total_pages > nr_pages / 2)\\n\\t\\treturn -EINVAL;\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\t/*\\n\\t * Verify we have good destination addresses.  Normally\\n\\t * the caller is responsible for making certain we don\\'t\\n\\t * attempt to load the new image into invalid or reserved\\n\\t * areas of RAM.  But crash kernels are preloaded into a\\n\\t * reserved area of ram.  We must ensure the addresses\\n\\t * are in the reserved area otherwise preloading the\\n\\t * kernel could corrupt things.\\n\\t */\\n\\n\\tif (image->type == KEXEC_TYPE_CRASH) {\\n\\t\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\t\\tunsigned long mstart, mend;\\n\\n\\t\\t\\tmstart = image->segment[i].mem;\\n\\t\\t\\tmend = mstart + image->segment[i].memsz - 1;\\n\\t\\t\\t/* Ensure we are within the crash kernel limits */\\n\\t\\t\\tif ((mstart < phys_to_boot_phys(crashk_res.start)) ||\\n\\t\\t\\t    (mend > phys_to_boot_phys(crashk_res.end)))\\n\\t\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t\\t}\\n\\t}\\n#endif\\n\\n\\treturn 0;\\n}\\n\\nstruct kimage *do_kimage_alloc_init(void)\\n{\\n\\tstruct kimage *image;\\n\\n\\t/* Allocate a controlling structure */\\n\\timage = kzalloc(sizeof(*image), GFP_KERNEL);\\n\\tif (!image)\\n\\t\\treturn NULL;\\n\\n\\timage->head = 0;\\n\\timage->entry = &image->head;\\n\\timage->last_entry = &image->head;\\n\\timage->control_page = ~0; /* By default this does not apply */\\n\\timage->type = KEXEC_TYPE_DEFAULT;\\n\\n\\t/* Initialize the list of control pages */\\n\\tINIT_LIST_HEAD(&image->control_pages);\\n\\n\\t/* Initialize the list of destination pages */\\n\\tINIT_LIST_HEAD(&image->dest_pages);\\n\\n\\t/* Initialize the list of unusable pages */\\n\\tINIT_LIST_HEAD(&image->unusable_pages);\\n\\n#ifdef CONFIG_CRASH_HOTPLUG\\n\\timage->hp_action = KEXEC_CRASH_HP_NONE;\\n\\timage->elfcorehdr_index = -1;\\n\\timage->elfcorehdr_updated = false;\\n#endif\\n\\n\\treturn image;\\n}\\n\\nint kimage_is_destination_range(struct kimage *image,\\n\\t\\t\\t\\t\\tunsigned long start,\\n\\t\\t\\t\\t\\tunsigned long end)\\n{\\n\\tunsigned long i;\\n\\n\\tfor (i = 0; i < image->nr_segments; i++) {\\n\\t\\tunsigned long mstart, mend;\\n\\n\\t\\tmstart = image->segment[i].mem;\\n\\t\\tmend = mstart + image->segment[i].memsz - 1;\\n\\t\\tif ((end >= mstart) && (start <= mend))\\n\\t\\t\\treturn 1;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic struct page *kimage_alloc_pages(gfp_t gfp_mask, unsigned int order)\\n{\\n\\tstruct page *pages;\\n\\n\\tif (fatal_signal_pending(current))\\n\\t\\treturn NULL;\\n\\tpages = alloc_pages(gfp_mask & ~__GFP_ZERO, order);\\n\\tif (pages) {\\n\\t\\tunsigned int count, i;\\n\\n\\t\\tpages->mapping = NULL;\\n\\t\\tset_page_private(pages, order);\\n\\t\\tcount = 1 << order;\\n\\t\\tfor (i = 0; i < count; i++)\\n\\t\\t\\tSetPageReserved(pages + i);\\n\\n\\t\\tarch_kexec_post_alloc_pages(page_address(pages), count,\\n\\t\\t\\t\\t\\t    gfp_mask);\\n\\n\\t\\tif (gfp_mask & __GFP_ZERO)\\n\\t\\t\\tfor (i = 0; i < count; i++)\\n\\t\\t\\t\\tclear_highpage(pages + i);\\n\\t}\\n\\n\\treturn pages;\\n}\\n\\nstatic void kimage_free_pages(struct page *page)\\n{\\n\\tunsigned int order, count, i;\\n\\n\\torder = page_private(page);\\n\\tcount = 1 << order;\\n\\n\\tarch_kexec_pre_free_pages(page_address(page), count);\\n\\n\\tfor (i = 0; i < count; i++)\\n\\t\\tClearPageReserved(page + i);\\n\\t__free_pages(page, order);\\n}\\n\\nvoid kimage_free_page_list(struct list_head *list)\\n{\\n\\tstruct page *page, *next;\\n\\n\\tlist_for_each_entry_safe(page, next, list, lru) {\\n\\t\\tlist_del(&page->lru);\\n\\t\\tkimage_free_pages(page);\\n\\t}\\n}\\n\\nstatic struct page *kimage_alloc_normal_control_pages(struct kimage *image,\\n\\t\\t\\t\\t\\t\\t\\tunsigned int order)\\n{\\n\\t/* Control pages are special, they are the intermediaries\\n\\t * that are needed while we copy the rest of the pages\\n\\t * to their final resting place.  As such they must\\n\\t * not conflict with either the destination addresses\\n\\t * or memory the kernel is already using.\\n\\t *\\n\\t * The only case where we really need more than one of\\n\\t * these are for architectures where we cannot disable\\n\\t * the MMU and must instead generate an identity mapped\\n\\t * page table for all of the memory.\\n\\t *\\n\\t * At worst this runs in O(N) of the image size.\\n\\t */\\n\\tstruct list_head extra_pages;\\n\\tstruct page *pages;\\n\\tunsigned int count;\\n\\n\\tcount = 1 << order;\\n\\tINIT_LIST_HEAD(&extra_pages);\\n\\n\\t/* Loop while I can allocate a page and the page allocated\\n\\t * is a destination page.\\n\\t */\\n\\tdo {\\n\\t\\tunsigned long pfn, epfn, addr, eaddr;\\n\\n\\t\\tpages = kimage_alloc_pages(KEXEC_CONTROL_MEMORY_GFP, order);\\n\\t\\tif (!pages)\\n\\t\\t\\tbreak;\\n\\t\\tpfn   = page_to_boot_pfn(pages);\\n\\t\\tepfn  = pfn + count;\\n\\t\\taddr  = pfn << PAGE_SHIFT;\\n\\t\\teaddr = (epfn << PAGE_SHIFT) - 1;\\n\\t\\tif ((epfn >= (KEXEC_CONTROL_MEMORY_LIMIT >> PAGE_SHIFT)) ||\\n\\t\\t\\t      kimage_is_destination_range(image, addr, eaddr)) {\\n\\t\\t\\tlist_add(&pages->lru, &extra_pages);\\n\\t\\t\\tpages = NULL;\\n\\t\\t}\\n\\t} while (!pages);\\n\\n\\tif (pages) {\\n\\t\\t/* Remember the allocated page... */\\n\\t\\tlist_add(&pages->lru, &image->control_pages);\\n\\n\\t\\t/* Because the page is already in it\\'s destination\\n\\t\\t * location we will never allocate another page at\\n\\t\\t * that address.  Therefore kimage_alloc_pages\\n\\t\\t * will not return it (again) and we don\\'t need\\n\\t\\t * to give it an entry in image->segment[].\\n\\t\\t */\\n\\t}\\n\\t/* Deal with the destination pages I have inadvertently allocated.\\n\\t *\\n\\t * Ideally I would convert multi-page allocations into single\\n\\t * page allocations, and add everything to image->dest_pages.\\n\\t *\\n\\t * For now it is simpler to just free the pages.\\n\\t */\\n\\tkimage_free_page_list(&extra_pages);\\n\\n\\treturn pages;\\n}\\n\\n#ifdef CONFIG_CRASH_DUMP\\nstatic struct page *kimage_alloc_crash_control_pages(struct kimage *image,\\n\\t\\t\\t\\t\\t\\t      unsigned int order)\\n{\\n\\t/* Control pages are special, they are the intermediaries\\n\\t * that are needed while we copy the rest of the pages\\n\\t * to their final resting place.  As such they must\\n\\t * not conflict with either the destination addresses\\n\\t * or memory the kernel is already using.\\n\\t *\\n\\t * Control pages are also the only pags we must allocate\\n\\t * when loading a crash kernel.  All of the other pages\\n\\t * are specified by the segments and we just memcpy\\n\\t * into them directly.\\n\\t *\\n\\t * The only case where we really need more than one of\\n\\t * these are for architectures where we cannot disable\\n\\t * the MMU and must instead generate an identity mapped\\n\\t * page table for all of the memory.\\n\\t *\\n\\t * Given the low demand this implements a very simple\\n\\t * allocator that finds the first hole of the appropriate\\n\\t * size in the reserved memory region, and allocates all\\n\\t * of the memory up to and including the hole.\\n\\t */\\n\\tunsigned long hole_start, hole_end, size;\\n\\tstruct page *pages;\\n\\n\\tpages = NULL;\\n\\tsize = (1 << order) << PAGE_SHIFT;\\n\\thole_start = ALIGN(image->control_page, size);\\n\\thole_end   = hole_start + size - 1;\\n\\twhile (hole_end <= crashk_res.end) {\\n\\t\\tunsigned long i;\\n\\n\\t\\tcond_resched();\\n\\n\\t\\tif (hole_end > KEXEC_CRASH_CONTROL_MEMORY_LIMIT)\\n\\t\\t\\tbreak;\\n\\t\\t/* See if I overlap any of the segments */\\n\\t\\tfor (i = 0; i < image->nr_segments; i++) {\\n\\t\\t\\tunsigned long mstart, mend;\\n\\n\\t\\t\\tmstart = image->segment[i].mem;\\n\\t\\t\\tmend   = mstart + image->segment[i].memsz - 1;\\n\\t\\t\\tif ((hole_end >= mstart) && (hole_start <= mend)) {\\n\\t\\t\\t\\t/* Advance the hole to the end of the segment */\\n\\t\\t\\t\\thole_start = ALIGN(mend, size);\\n\\t\\t\\t\\thole_end   = hole_start + size - 1;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\t/* If I don\\'t overlap any segments I have found my hole! */\\n\\t\\tif (i == image->nr_segments) {\\n\\t\\t\\tpages = pfn_to_page(hole_start >> PAGE_SHIFT);\\n\\t\\t\\timage->control_page = hole_end + 1;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\t/* Ensure that these pages are decrypted if SME is enabled. */\\n\\tif (pages)\\n\\t\\tarch_kexec_post_alloc_pages(page_address(pages), 1 << order, 0);\\n\\n\\treturn pages;\\n}\\n#endif\\n\\n\\nstruct page *kimage_alloc_control_pages(struct kimage *image,\\n\\t\\t\\t\\t\\t unsigned int order)\\n{\\n\\tstruct page *pages = NULL;\\n\\n\\tswitch (image->type) {\\n\\tcase KEXEC_TYPE_DEFAULT:\\n\\t\\tpages = kimage_alloc_normal_control_pages(image, order);\\n\\t\\tbreak;\\n#ifdef CONFIG_CRASH_DUMP\\n\\tcase KEXEC_TYPE_CRASH:\\n\\t\\tpages = kimage_alloc_crash_control_pages(image, order);\\n\\t\\tbreak;\\n#endif\\n\\t}\\n\\n\\treturn pages;\\n}\\n\\nstatic int kimage_add_entry(struct kimage *image, kimage_entry_t entry)\\n{\\n\\tif (*image->entry != 0)\\n\\t\\timage->entry++;\\n\\n\\tif (image->entry == image->last_entry) {\\n\\t\\tkimage_entry_t *ind_page;\\n\\t\\tstruct page *page;\\n\\n\\t\\tpage = kimage_alloc_page(image, GFP_KERNEL, KIMAGE_NO_DEST);\\n\\t\\tif (!page)\\n\\t\\t\\treturn -ENOMEM;\\n\\n\\t\\tind_page = page_address(page);\\n\\t\\t*image->entry = virt_to_boot_phys(ind_page) | IND_INDIRECTION;\\n\\t\\timage->entry = ind_page;\\n\\t\\timage->last_entry = ind_page +\\n\\t\\t\\t\\t      ((PAGE_SIZE/sizeof(kimage_entry_t)) - 1);\\n\\t}\\n\\t*image->entry = entry;\\n\\timage->entry++;\\n\\t*image->entry = 0;\\n\\n\\treturn 0;\\n}\\n\\nstatic int kimage_set_destination(struct kimage *image,\\n\\t\\t\\t\\t   unsigned long destination)\\n{\\n\\tdestination &= PAGE_MASK;\\n\\n\\treturn kimage_add_entry(image, destination | IND_DESTINATION);\\n}\\n\\n\\nstatic int kimage_add_page(struct kimage *image, unsigned long page)\\n{\\n\\tpage &= PAGE_MASK;\\n\\n\\treturn kimage_add_entry(image, page | IND_SOURCE);\\n}\\n\\n\\nstatic void kimage_free_extra_pages(struct kimage *image)\\n{\\n\\t/* Walk through and free any extra destination pages I may have */\\n\\tkimage_free_page_list(&image->dest_pages);\\n\\n\\t/* Walk through and free any unusable pages I have cached */\\n\\tkimage_free_page_list(&image->unusable_pages);\\n\\n}\\n\\nvoid kimage_terminate(struct kimage *image)\\n{\\n\\tif (*image->entry != 0)\\n\\t\\timage->entry++;\\n\\n\\t*image->entry = IND_DONE;\\n}\\n\\n#define for_each_kimage_entry(image, ptr, entry) \\\\\\n\\tfor (ptr = &image->head; (entry = *ptr) && !(entry & IND_DONE); \\\\\\n\\t\\tptr = (entry & IND_INDIRECTION) ? \\\\\\n\\t\\t\\tboot_phys_to_virt((entry & PAGE_MASK)) : ptr + 1)\\n\\nstatic void kimage_free_entry(kimage_entry_t entry)\\n{\\n\\tstruct page *page;\\n\\n\\tpage = boot_pfn_to_page(entry >> PAGE_SHIFT);\\n\\tkimage_free_pages(page);\\n}\\n\\nvoid kimage_free(struct kimage *image)\\n{\\n\\tkimage_entry_t *ptr, entry;\\n\\tkimage_entry_t ind = 0;\\n\\n\\tif (!image)\\n\\t\\treturn;\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\tif (image->vmcoreinfo_data_copy) {\\n\\t\\tcrash_update_vmcoreinfo_safecopy(NULL);\\n\\t\\tvunmap(image->vmcoreinfo_data_copy);\\n\\t}\\n#endif\\n\\n\\tkimage_free_extra_pages(image);\\n\\tfor_each_kimage_entry(image, ptr, entry) {\\n\\t\\tif (entry & IND_INDIRECTION) {\\n\\t\\t\\t/* Free the previous indirection page */\\n\\t\\t\\tif (ind & IND_INDIRECTION)\\n\\t\\t\\t\\tkimage_free_entry(ind);\\n\\t\\t\\t/* Save this indirection page until we are\\n\\t\\t\\t * done with it.\\n\\t\\t\\t */\\n\\t\\t\\tind = entry;\\n\\t\\t} else if (entry & IND_SOURCE)\\n\\t\\t\\tkimage_free_entry(entry);\\n\\t}\\n\\t/* Free the final indirection page */\\n\\tif (ind & IND_INDIRECTION)\\n\\t\\tkimage_free_entry(ind);\\n\\n\\t/* Handle any machine specific cleanup */\\n\\tmachine_kexec_cleanup(image);\\n\\n\\t/* Free the kexec control pages... */\\n\\tkimage_free_page_list(&image->control_pages);\\n\\n\\t/*\\n\\t * Free up any temporary buffers allocated. This might hit if\\n\\t * error occurred much later after buffer allocation.\\n\\t */\\n\\tif (image->file_mode)\\n\\t\\tkimage_file_post_load_cleanup(image);\\n\\n\\tkfree(image);\\n}\\n\\nstatic kimage_entry_t *kimage_dst_used(struct kimage *image,\\n\\t\\t\\t\\t\\tunsigned long page)\\n{\\n\\tkimage_entry_t *ptr, entry;\\n\\tunsigned long destination = 0;\\n\\n\\tfor_each_kimage_entry(image, ptr, entry) {\\n\\t\\tif (entry & IND_DESTINATION)\\n\\t\\t\\tdestination = entry & PAGE_MASK;\\n\\t\\telse if (entry & IND_SOURCE) {\\n\\t\\t\\tif (page == destination)\\n\\t\\t\\t\\treturn ptr;\\n\\t\\t\\tdestination += PAGE_SIZE;\\n\\t\\t}\\n\\t}\\n\\n\\treturn NULL;\\n}\\n\\nstatic struct page *kimage_alloc_page(struct kimage *image,\\n\\t\\t\\t\\t\\tgfp_t gfp_mask,\\n\\t\\t\\t\\t\\tunsigned long destination)\\n{\\n\\t/*\\n\\t * Here we implement safeguards to ensure that a source page\\n\\t * is not copied to its destination page before the data on\\n\\t * the destination page is no longer useful.\\n\\t *\\n\\t * To do this we maintain the invariant that a source page is\\n\\t * either its own destination page, or it is not a\\n\\t * destination page at all.\\n\\t *\\n\\t * That is slightly stronger than required, but the proof\\n\\t * that no problems will not occur is trivial, and the\\n\\t * implementation is simply to verify.\\n\\t *\\n\\t * When allocating all pages normally this algorithm will run\\n\\t * in O(N) time, but in the worst case it will run in O(N^2)\\n\\t * time.   If the runtime is a problem the data structures can\\n\\t * be fixed.\\n\\t */\\n\\tstruct page *page;\\n\\tunsigned long addr;\\n\\n\\t/*\\n\\t * Walk through the list of destination pages, and see if I\\n\\t * have a match.\\n\\t */\\n\\tlist_for_each_entry(page, &image->dest_pages, lru) {\\n\\t\\taddr = page_to_boot_pfn(page) << PAGE_SHIFT;\\n\\t\\tif (addr == destination) {\\n\\t\\t\\tlist_del(&page->lru);\\n\\t\\t\\treturn page;\\n\\t\\t}\\n\\t}\\n\\tpage = NULL;\\n\\twhile (1) {\\n\\t\\tkimage_entry_t *old;\\n\\n\\t\\t/* Allocate a page, if we run out of memory give up */\\n\\t\\tpage = kimage_alloc_pages(gfp_mask, 0);\\n\\t\\tif (!page)\\n\\t\\t\\treturn NULL;\\n\\t\\t/* If the page cannot be used file it away */\\n\\t\\tif (page_to_boot_pfn(page) >\\n\\t\\t\\t\\t(KEXEC_SOURCE_MEMORY_LIMIT >> PAGE_SHIFT)) {\\n\\t\\t\\tlist_add(&page->lru, &image->unusable_pages);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\taddr = page_to_boot_pfn(page) << PAGE_SHIFT;\\n\\n\\t\\t/* If it is the destination page we want use it */\\n\\t\\tif (addr == destination)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* If the page is not a destination page use it */\\n\\t\\tif (!kimage_is_destination_range(image, addr,\\n\\t\\t\\t\\t\\t\\t  addr + PAGE_SIZE - 1))\\n\\t\\t\\tbreak;\\n\\n\\t\\t/*\\n\\t\\t * I know that the page is someones destination page.\\n\\t\\t * See if there is already a source page for this\\n\\t\\t * destination page.  And if so swap the source pages.\\n\\t\\t */\\n\\t\\told = kimage_dst_used(image, addr);\\n\\t\\tif (old) {\\n\\t\\t\\t/* If so move it */\\n\\t\\t\\tunsigned long old_addr;\\n\\t\\t\\tstruct page *old_page;\\n\\n\\t\\t\\told_addr = *old & PAGE_MASK;\\n\\t\\t\\told_page = boot_pfn_to_page(old_addr >> PAGE_SHIFT);\\n\\t\\t\\tcopy_highpage(page, old_page);\\n\\t\\t\\t*old = addr | (*old & ~PAGE_MASK);\\n\\n\\t\\t\\t/* The old page I have found cannot be a\\n\\t\\t\\t * destination page, so return it if it\\'s\\n\\t\\t\\t * gfp_flags honor the ones passed in.\\n\\t\\t\\t */\\n\\t\\t\\tif (!(gfp_mask & __GFP_HIGHMEM) &&\\n\\t\\t\\t    PageHighMem(old_page)) {\\n\\t\\t\\t\\tkimage_free_pages(old_page);\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\t\\t\\tpage = old_page;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\t/* Place the page on the destination list, to be used later */\\n\\t\\tlist_add(&page->lru, &image->dest_pages);\\n\\t}\\n\\n\\treturn page;\\n}\\n\\nstatic int kimage_load_normal_segment(struct kimage *image,\\n\\t\\t\\t\\t\\t struct kexec_segment *segment)\\n{\\n\\tunsigned long maddr;\\n\\tsize_t ubytes, mbytes;\\n\\tint result;\\n\\tunsigned char __user *buf = NULL;\\n\\tunsigned char *kbuf = NULL;\\n\\n\\tif (image->file_mode)\\n\\t\\tkbuf = segment->kbuf;\\n\\telse\\n\\t\\tbuf = segment->buf;\\n\\tubytes = segment->bufsz;\\n\\tmbytes = segment->memsz;\\n\\tmaddr = segment->mem;\\n\\n\\tresult = kimage_set_destination(image, maddr);\\n\\tif (result < 0)\\n\\t\\tgoto out;\\n\\n\\twhile (mbytes) {\\n\\t\\tstruct page *page;\\n\\t\\tchar *ptr;\\n\\t\\tsize_t uchunk, mchunk;\\n\\n\\t\\tpage = kimage_alloc_page(image, GFP_HIGHUSER, maddr);\\n\\t\\tif (!page) {\\n\\t\\t\\tresult  = -ENOMEM;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tresult = kimage_add_page(image, page_to_boot_pfn(page)\\n\\t\\t\\t\\t\\t\\t\\t\\t<< PAGE_SHIFT);\\n\\t\\tif (result < 0)\\n\\t\\t\\tgoto out;\\n\\n\\t\\tptr = kmap_local_page(page);\\n\\t\\t/* Start with a clear page */\\n\\t\\tclear_page(ptr);\\n\\t\\tptr += maddr & ~PAGE_MASK;\\n\\t\\tmchunk = min_t(size_t, mbytes,\\n\\t\\t\\t\\tPAGE_SIZE - (maddr & ~PAGE_MASK));\\n\\t\\tuchunk = min(ubytes, mchunk);\\n\\n\\t\\tif (uchunk) {\\n\\t\\t\\t/* For file based kexec, source pages are in kernel memory */\\n\\t\\t\\tif (image->file_mode)\\n\\t\\t\\t\\tmemcpy(ptr, kbuf, uchunk);\\n\\t\\t\\telse\\n\\t\\t\\t\\tresult = copy_from_user(ptr, buf, uchunk);\\n\\t\\t\\tubytes -= uchunk;\\n\\t\\t\\tif (image->file_mode)\\n\\t\\t\\t\\tkbuf += uchunk;\\n\\t\\t\\telse\\n\\t\\t\\t\\tbuf += uchunk;\\n\\t\\t}\\n\\t\\tkunmap_local(ptr);\\n\\t\\tif (result) {\\n\\t\\t\\tresult = -EFAULT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tmaddr  += mchunk;\\n\\t\\tmbytes -= mchunk;\\n\\n\\t\\tcond_resched();\\n\\t}\\nout:\\n\\treturn result;\\n}\\n\\n#ifdef CONFIG_CRASH_DUMP\\nstatic int kimage_load_crash_segment(struct kimage *image,\\n\\t\\t\\t\\t\\tstruct kexec_segment *segment)\\n{\\n\\t/* For crash dumps kernels we simply copy the data from\\n\\t * user space to it\\'s destination.\\n\\t * We do things a page at a time for the sake of kmap.\\n\\t */\\n\\tunsigned long maddr;\\n\\tsize_t ubytes, mbytes;\\n\\tint result;\\n\\tunsigned char __user *buf = NULL;\\n\\tunsigned char *kbuf = NULL;\\n\\n\\tresult = 0;\\n\\tif (image->file_mode)\\n\\t\\tkbuf = segment->kbuf;\\n\\telse\\n\\t\\tbuf = segment->buf;\\n\\tubytes = segment->bufsz;\\n\\tmbytes = segment->memsz;\\n\\tmaddr = segment->mem;\\n\\twhile (mbytes) {\\n\\t\\tstruct page *page;\\n\\t\\tchar *ptr;\\n\\t\\tsize_t uchunk, mchunk;\\n\\n\\t\\tpage = boot_pfn_to_page(maddr >> PAGE_SHIFT);\\n\\t\\tif (!page) {\\n\\t\\t\\tresult  = -ENOMEM;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tarch_kexec_post_alloc_pages(page_address(page), 1, 0);\\n\\t\\tptr = kmap_local_page(page);\\n\\t\\tptr += maddr & ~PAGE_MASK;\\n\\t\\tmchunk = min_t(size_t, mbytes,\\n\\t\\t\\t\\tPAGE_SIZE - (maddr & ~PAGE_MASK));\\n\\t\\tuchunk = min(ubytes, mchunk);\\n\\t\\tif (mchunk > uchunk) {\\n\\t\\t\\t/* Zero the trailing part of the page */\\n\\t\\t\\tmemset(ptr + uchunk, 0, mchunk - uchunk);\\n\\t\\t}\\n\\n\\t\\tif (uchunk) {\\n\\t\\t\\t/* For file based kexec, source pages are in kernel memory */\\n\\t\\t\\tif (image->file_mode)\\n\\t\\t\\t\\tmemcpy(ptr, kbuf, uchunk);\\n\\t\\t\\telse\\n\\t\\t\\t\\tresult = copy_from_user(ptr, buf, uchunk);\\n\\t\\t\\tubytes -= uchunk;\\n\\t\\t\\tif (image->file_mode)\\n\\t\\t\\t\\tkbuf += uchunk;\\n\\t\\t\\telse\\n\\t\\t\\t\\tbuf += uchunk;\\n\\t\\t}\\n\\t\\tkexec_flush_icache_page(page);\\n\\t\\tkunmap_local(ptr);\\n\\t\\tarch_kexec_pre_free_pages(page_address(page), 1);\\n\\t\\tif (result) {\\n\\t\\t\\tresult = -EFAULT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tmaddr  += mchunk;\\n\\t\\tmbytes -= mchunk;\\n\\n\\t\\tcond_resched();\\n\\t}\\nout:\\n\\treturn result;\\n}\\n#endif\\n\\nint kimage_load_segment(struct kimage *image,\\n\\t\\t\\t\\tstruct kexec_segment *segment)\\n{\\n\\tint result = -ENOMEM;\\n\\n\\tswitch (image->type) {\\n\\tcase KEXEC_TYPE_DEFAULT:\\n\\t\\tresult = kimage_load_normal_segment(image, segment);\\n\\t\\tbreak;\\n#ifdef CONFIG_CRASH_DUMP\\n\\tcase KEXEC_TYPE_CRASH:\\n\\t\\tresult = kimage_load_crash_segment(image, segment);\\n\\t\\tbreak;\\n#endif\\n\\t}\\n\\n\\treturn result;\\n}\\n\\nstruct kexec_load_limit {\\n\\t/* Mutex protects the limit count. */\\n\\tstruct mutex mutex;\\n\\tint limit;\\n};\\n\\nstatic struct kexec_load_limit load_limit_reboot = {\\n\\t.mutex = __MUTEX_INITIALIZER(load_limit_reboot.mutex),\\n\\t.limit = -1,\\n};\\n\\nstatic struct kexec_load_limit load_limit_panic = {\\n\\t.mutex = __MUTEX_INITIALIZER(load_limit_panic.mutex),\\n\\t.limit = -1,\\n};\\n\\nstruct kimage *kexec_image;\\nstruct kimage *kexec_crash_image;\\nstatic int kexec_load_disabled;\\n\\n#ifdef CONFIG_SYSCTL\\nstatic int kexec_limit_handler(const struct ctl_table *table, int write,\\n\\t\\t\\t       void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct kexec_load_limit *limit = table->data;\\n\\tint val;\\n\\tstruct ctl_table tmp = {\\n\\t\\t.data = &val,\\n\\t\\t.maxlen = sizeof(val),\\n\\t\\t.mode = table->mode,\\n\\t};\\n\\tint ret;\\n\\n\\tif (write) {\\n\\t\\tret = proc_dointvec(&tmp, write, buffer, lenp, ppos);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\n\\t\\tif (val < 0)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tmutex_lock(&limit->mutex);\\n\\t\\tif (limit->limit != -1 && val >= limit->limit)\\n\\t\\t\\tret = -EINVAL;\\n\\t\\telse\\n\\t\\t\\tlimit->limit = val;\\n\\t\\tmutex_unlock(&limit->mutex);\\n\\n\\t\\treturn ret;\\n\\t}\\n\\n\\tmutex_lock(&limit->mutex);\\n\\tval = limit->limit;\\n\\tmutex_unlock(&limit->mutex);\\n\\n\\treturn proc_dointvec(&tmp, write, buffer, lenp, ppos);\\n}\\n\\nstatic struct ctl_table kexec_core_sysctls[] = {\\n\\t{\\n\\t\\t.procname\\t= \"kexec_load_disabled\",\\n\\t\\t.data\\t\\t= &kexec_load_disabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t/* only handle a transition from default \"0\" to \"1\" */\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ONE,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"kexec_load_limit_panic\",\\n\\t\\t.data\\t\\t= &load_limit_panic,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= kexec_limit_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"kexec_load_limit_reboot\",\\n\\t\\t.data\\t\\t= &load_limit_reboot,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= kexec_limit_handler,\\n\\t},\\n};\\n\\nstatic int __init kexec_core_sysctl_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", kexec_core_sysctls);\\n\\treturn 0;\\n}\\nlate_initcall(kexec_core_sysctl_init);\\n#endif\\n\\nbool kexec_load_permitted(int kexec_image_type)\\n{\\n\\tstruct kexec_load_limit *limit;\\n\\n\\t/*\\n\\t * Only the superuser can use the kexec syscall and if it has not\\n\\t * been disabled.\\n\\t */\\n\\tif (!capable(CAP_SYS_BOOT) || kexec_load_disabled)\\n\\t\\treturn false;\\n\\n\\t/* Check limit counter and decrease it.*/\\n\\tlimit = (kexec_image_type == KEXEC_TYPE_CRASH) ?\\n\\t\\t&load_limit_panic : &load_limit_reboot;\\n\\tmutex_lock(&limit->mutex);\\n\\tif (!limit->limit) {\\n\\t\\tmutex_unlock(&limit->mutex);\\n\\t\\treturn false;\\n\\t}\\n\\tif (limit->limit != -1)\\n\\t\\tlimit->limit--;\\n\\tmutex_unlock(&limit->mutex);\\n\\n\\treturn true;\\n}\\n\\n/*\\n * Move into place and start executing a preloaded standalone\\n * executable.  If nothing was preloaded return an error.\\n */\\nint kernel_kexec(void)\\n{\\n\\tint error = 0;\\n\\n\\tif (!kexec_trylock())\\n\\t\\treturn -EBUSY;\\n\\tif (!kexec_image) {\\n\\t\\terror = -EINVAL;\\n\\t\\tgoto Unlock;\\n\\t}\\n\\n#ifdef CONFIG_KEXEC_JUMP\\n\\tif (kexec_image->preserve_context) {\\n\\t\\tpm_prepare_console();\\n\\t\\terror = freeze_processes();\\n\\t\\tif (error) {\\n\\t\\t\\terror = -EBUSY;\\n\\t\\t\\tgoto Restore_console;\\n\\t\\t}\\n\\t\\tsuspend_console();\\n\\t\\terror = dpm_suspend_start(PMSG_FREEZE);\\n\\t\\tif (error)\\n\\t\\t\\tgoto Resume_console;\\n\\t\\t/* At this point, dpm_suspend_start() has been called,\\n\\t\\t * but *not* dpm_suspend_end(). We *must* call\\n\\t\\t * dpm_suspend_end() now.  Otherwise, drivers for\\n\\t\\t * some devices (e.g. interrupt controllers) become\\n\\t\\t * desynchronized with the actual state of the\\n\\t\\t * hardware at resume time, and evil weirdness ensues.\\n\\t\\t */\\n\\t\\terror = dpm_suspend_end(PMSG_FREEZE);\\n\\t\\tif (error)\\n\\t\\t\\tgoto Resume_devices;\\n\\t\\terror = suspend_disable_secondary_cpus();\\n\\t\\tif (error)\\n\\t\\t\\tgoto Enable_cpus;\\n\\t\\tlocal_irq_disable();\\n\\t\\terror = syscore_suspend();\\n\\t\\tif (error)\\n\\t\\t\\tgoto Enable_irqs;\\n\\t} else\\n#endif\\n\\t{\\n\\t\\tkexec_in_progress = true;\\n\\t\\tkernel_restart_prepare(\"kexec reboot\");\\n\\t\\tmigrate_to_reboot_cpu();\\n\\t\\tsyscore_shutdown();\\n\\n\\t\\t/*\\n\\t\\t * migrate_to_reboot_cpu() disables CPU hotplug assuming that\\n\\t\\t * no further code needs to use CPU hotplug (which is true in\\n\\t\\t * the reboot case). However, the kexec path depends on using\\n\\t\\t * CPU hotplug again; so re-enable it here.\\n\\t\\t */\\n\\t\\tcpu_hotplug_enable();\\n\\t\\tpr_notice(\"Starting new kernel\\\\n\");\\n\\t\\tmachine_shutdown();\\n\\t}\\n\\n\\tkmsg_dump(KMSG_DUMP_SHUTDOWN);\\n\\tmachine_kexec(kexec_image);\\n\\n#ifdef CONFIG_KEXEC_JUMP\\n\\tif (kexec_image->preserve_context) {\\n\\t\\tsyscore_resume();\\n Enable_irqs:\\n\\t\\tlocal_irq_enable();\\n Enable_cpus:\\n\\t\\tsuspend_enable_secondary_cpus();\\n\\t\\tdpm_resume_start(PMSG_RESTORE);\\n Resume_devices:\\n\\t\\tdpm_resume_end(PMSG_RESTORE);\\n Resume_console:\\n\\t\\tresume_console();\\n\\t\\tthaw_processes();\\n Restore_console:\\n\\t\\tpm_restore_console();\\n\\t}\\n#endif\\n\\n Unlock:\\n\\tkexec_unlock();\\n\\treturn error;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Generate definitions needed by the preprocessor.\\n * This code generates raw asm output which is post-processed\\n * to extract and format the required data.\\n */\\n\\n#define __GENERATING_BOUNDS_H\\n/* Include headers that define the enum constants of interest */\\n#include <linux/page-flags.h>\\n#include <linux/mmzone.h>\\n#include <linux/kbuild.h>\\n#include <linux/log2.h>\\n#include <linux/spinlock_types.h>\\n\\nint main(void)\\n{\\n\\t/* The enum constants to put into include/generated/bounds.h */\\n\\tDEFINE(NR_PAGEFLAGS, __NR_PAGEFLAGS);\\n\\tDEFINE(MAX_NR_ZONES, __MAX_NR_ZONES);\\n#ifdef CONFIG_SMP\\n\\tDEFINE(NR_CPUS_BITS, order_base_2(CONFIG_NR_CPUS));\\n#endif\\n\\tDEFINE(SPINLOCK_SIZE, sizeof(spinlock_t));\\n#ifdef CONFIG_LRU_GEN\\n\\tDEFINE(LRU_GEN_WIDTH, order_base_2(MAX_NR_GENS + 1));\\n\\tDEFINE(__LRU_REFS_WIDTH, MAX_NR_TIERS - 2);\\n#else\\n\\tDEFINE(LRU_GEN_WIDTH, 0);\\n\\tDEFINE(__LRU_REFS_WIDTH, 0);\\n#endif\\n\\t/* End of constants */\\n\\n\\treturn 0;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  linux/kernel/exit.c\\n *\\n *  Copyright (C) 1991, 1992  Linus Torvalds\\n */\\n\\n#include <linux/mm.h>\\n#include <linux/slab.h>\\n#include <linux/sched/autogroup.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/stat.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/task_stack.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/interrupt.h>\\n#include <linux/module.h>\\n#include <linux/capability.h>\\n#include <linux/completion.h>\\n#include <linux/personality.h>\\n#include <linux/tty.h>\\n#include <linux/iocontext.h>\\n#include <linux/key.h>\\n#include <linux/cpu.h>\\n#include <linux/acct.h>\\n#include <linux/tsacct_kern.h>\\n#include <linux/file.h>\\n#include <linux/freezer.h>\\n#include <linux/binfmts.h>\\n#include <linux/nsproxy.h>\\n#include <linux/pid_namespace.h>\\n#include <linux/ptrace.h>\\n#include <linux/profile.h>\\n#include <linux/mount.h>\\n#include <linux/proc_fs.h>\\n#include <linux/kthread.h>\\n#include <linux/mempolicy.h>\\n#include <linux/taskstats_kern.h>\\n#include <linux/delayacct.h>\\n#include <linux/cgroup.h>\\n#include <linux/syscalls.h>\\n#include <linux/signal.h>\\n#include <linux/posix-timers.h>\\n#include <linux/cn_proc.h>\\n#include <linux/mutex.h>\\n#include <linux/futex.h>\\n#include <linux/pipe_fs_i.h>\\n#include <linux/audit.h> /* for audit_free() */\\n#include <linux/resource.h>\\n#include <linux/task_io_accounting_ops.h>\\n#include <linux/blkdev.h>\\n#include <linux/task_work.h>\\n#include <linux/fs_struct.h>\\n#include <linux/init_task.h>\\n#include <linux/perf_event.h>\\n#include <trace/events/sched.h>\\n#include <linux/hw_breakpoint.h>\\n#include <linux/oom.h>\\n#include <linux/writeback.h>\\n#include <linux/shm.h>\\n#include <linux/kcov.h>\\n#include <linux/kmsan.h>\\n#include <linux/random.h>\\n#include <linux/rcuwait.h>\\n#include <linux/compat.h>\\n#include <linux/io_uring.h>\\n#include <linux/kprobes.h>\\n#include <linux/rethook.h>\\n#include <linux/sysfs.h>\\n#include <linux/user_events.h>\\n#include <linux/uaccess.h>\\n\\n#include <uapi/linux/wait.h>\\n\\n#include <asm/unistd.h>\\n#include <asm/mmu_context.h>\\n\\n#include \"exit.h\"\\n\\n/*\\n * The default value should be high enough to not crash a system that randomly\\n * crashes its kernel from time to time, but low enough to at least not permit\\n * overflowing 32-bit refcounts or the ldsem writer count.\\n */\\nstatic unsigned int oops_limit = 10000;\\n\\n#ifdef CONFIG_SYSCTL\\nstatic struct ctl_table kern_exit_table[] = {\\n\\t{\\n\\t\\t.procname       = \"oops_limit\",\\n\\t\\t.data           = &oops_limit,\\n\\t\\t.maxlen         = sizeof(oops_limit),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = proc_douintvec,\\n\\t},\\n};\\n\\nstatic __init int kernel_exit_sysctls_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", kern_exit_table);\\n\\treturn 0;\\n}\\nlate_initcall(kernel_exit_sysctls_init);\\n#endif\\n\\nstatic atomic_t oops_count = ATOMIC_INIT(0);\\n\\n#ifdef CONFIG_SYSFS\\nstatic ssize_t oops_count_show(struct kobject *kobj, struct kobj_attribute *attr,\\n\\t\\t\\t       char *page)\\n{\\n\\treturn sysfs_emit(page, \"%d\\\\n\", atomic_read(&oops_count));\\n}\\n\\nstatic struct kobj_attribute oops_count_attr = __ATTR_RO(oops_count);\\n\\nstatic __init int kernel_exit_sysfs_init(void)\\n{\\n\\tsysfs_add_file_to_group(kernel_kobj, &oops_count_attr.attr, NULL);\\n\\treturn 0;\\n}\\nlate_initcall(kernel_exit_sysfs_init);\\n#endif\\n\\nstatic void __unhash_process(struct task_struct *p, bool group_dead)\\n{\\n\\tnr_threads--;\\n\\tdetach_pid(p, PIDTYPE_PID);\\n\\tif (group_dead) {\\n\\t\\tdetach_pid(p, PIDTYPE_TGID);\\n\\t\\tdetach_pid(p, PIDTYPE_PGID);\\n\\t\\tdetach_pid(p, PIDTYPE_SID);\\n\\n\\t\\tlist_del_rcu(&p->tasks);\\n\\t\\tlist_del_init(&p->sibling);\\n\\t\\t__this_cpu_dec(process_counts);\\n\\t}\\n\\tlist_del_rcu(&p->thread_node);\\n}\\n\\n/*\\n * This function expects the tasklist_lock write-locked.\\n */\\nstatic void __exit_signal(struct task_struct *tsk)\\n{\\n\\tstruct signal_struct *sig = tsk->signal;\\n\\tbool group_dead = thread_group_leader(tsk);\\n\\tstruct sighand_struct *sighand;\\n\\tstruct tty_struct *tty;\\n\\tu64 utime, stime;\\n\\n\\tsighand = rcu_dereference_check(tsk->sighand,\\n\\t\\t\\t\\t\\tlockdep_tasklist_lock_is_held());\\n\\tspin_lock(&sighand->siglock);\\n\\n#ifdef CONFIG_POSIX_TIMERS\\n\\tposix_cpu_timers_exit(tsk);\\n\\tif (group_dead)\\n\\t\\tposix_cpu_timers_exit_group(tsk);\\n#endif\\n\\n\\tif (group_dead) {\\n\\t\\ttty = sig->tty;\\n\\t\\tsig->tty = NULL;\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * If there is any task waiting for the group exit\\n\\t\\t * then notify it:\\n\\t\\t */\\n\\t\\tif (sig->notify_count > 0 && !--sig->notify_count)\\n\\t\\t\\twake_up_process(sig->group_exec_task);\\n\\n\\t\\tif (tsk == sig->curr_target)\\n\\t\\t\\tsig->curr_target = next_thread(tsk);\\n\\t}\\n\\n\\tadd_device_randomness((const void*) &tsk->se.sum_exec_runtime,\\n\\t\\t\\t      sizeof(unsigned long long));\\n\\n\\t/*\\n\\t * Accumulate here the counters for all threads as they die. We could\\n\\t * skip the group leader because it is the last user of signal_struct,\\n\\t * but we want to avoid the race with thread_group_cputime() which can\\n\\t * see the empty ->thread_head list.\\n\\t */\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\twrite_seqlock(&sig->stats_lock);\\n\\tsig->utime += utime;\\n\\tsig->stime += stime;\\n\\tsig->gtime += task_gtime(tsk);\\n\\tsig->min_flt += tsk->min_flt;\\n\\tsig->maj_flt += tsk->maj_flt;\\n\\tsig->nvcsw += tsk->nvcsw;\\n\\tsig->nivcsw += tsk->nivcsw;\\n\\tsig->inblock += task_io_get_inblock(tsk);\\n\\tsig->oublock += task_io_get_oublock(tsk);\\n\\ttask_io_accounting_add(&sig->ioac, &tsk->ioac);\\n\\tsig->sum_sched_runtime += tsk->se.sum_exec_runtime;\\n\\tsig->nr_threads--;\\n\\t__unhash_process(tsk, group_dead);\\n\\twrite_sequnlock(&sig->stats_lock);\\n\\n\\t/*\\n\\t * Do this under ->siglock, we can race with another thread\\n\\t * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.\\n\\t */\\n\\tflush_sigqueue(&tsk->pending);\\n\\ttsk->sighand = NULL;\\n\\tspin_unlock(&sighand->siglock);\\n\\n\\t__cleanup_sighand(sighand);\\n\\tclear_tsk_thread_flag(tsk, TIF_SIGPENDING);\\n\\tif (group_dead) {\\n\\t\\tflush_sigqueue(&sig->shared_pending);\\n\\t\\ttty_kref_put(tty);\\n\\t}\\n}\\n\\nstatic void delayed_put_task_struct(struct rcu_head *rhp)\\n{\\n\\tstruct task_struct *tsk = container_of(rhp, struct task_struct, rcu);\\n\\n\\tkprobe_flush_task(tsk);\\n\\trethook_flush_task(tsk);\\n\\tperf_event_delayed_put(tsk);\\n\\ttrace_sched_process_free(tsk);\\n\\tput_task_struct(tsk);\\n}\\n\\nvoid put_task_struct_rcu_user(struct task_struct *task)\\n{\\n\\tif (refcount_dec_and_test(&task->rcu_users))\\n\\t\\tcall_rcu(&task->rcu, delayed_put_task_struct);\\n}\\n\\nvoid __weak release_thread(struct task_struct *dead_task)\\n{\\n}\\n\\nvoid release_task(struct task_struct *p)\\n{\\n\\tstruct task_struct *leader;\\n\\tstruct pid *thread_pid;\\n\\tint zap_leader;\\nrepeat:\\n\\t/* don\\'t need to get the RCU readlock here - the process is dead and\\n\\t * can\\'t be modifying its own credentials. But shut RCU-lockdep up */\\n\\trcu_read_lock();\\n\\tdec_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);\\n\\trcu_read_unlock();\\n\\n\\tcgroup_release(p);\\n\\n\\twrite_lock_irq(&tasklist_lock);\\n\\tptrace_release_task(p);\\n\\tthread_pid = get_pid(p->thread_pid);\\n\\t__exit_signal(p);\\n\\n\\t/*\\n\\t * If we are the last non-leader member of the thread\\n\\t * group, and the leader is zombie, then notify the\\n\\t * group leader\\'s parent process. (if it wants notification.)\\n\\t */\\n\\tzap_leader = 0;\\n\\tleader = p->group_leader;\\n\\tif (leader != p && thread_group_empty(leader)\\n\\t\\t\\t&& leader->exit_state == EXIT_ZOMBIE) {\\n\\t\\t/*\\n\\t\\t * If we were the last child thread and the leader has\\n\\t\\t * exited already, and the leader\\'s parent ignores SIGCHLD,\\n\\t\\t * then we are the one who should release the leader.\\n\\t\\t */\\n\\t\\tzap_leader = do_notify_parent(leader, leader->exit_signal);\\n\\t\\tif (zap_leader)\\n\\t\\t\\tleader->exit_state = EXIT_DEAD;\\n\\t}\\n\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\tproc_flush_pid(thread_pid);\\n\\tput_pid(thread_pid);\\n\\trelease_thread(p);\\n\\tput_task_struct_rcu_user(p);\\n\\n\\tp = leader;\\n\\tif (unlikely(zap_leader))\\n\\t\\tgoto repeat;\\n}\\n\\nint rcuwait_wake_up(struct rcuwait *w)\\n{\\n\\tint ret = 0;\\n\\tstruct task_struct *task;\\n\\n\\trcu_read_lock();\\n\\n\\t/*\\n\\t * Order condition vs @task, such that everything prior to the load\\n\\t * of @task is visible. This is the condition as to why the user called\\n\\t * rcuwait_wake() in the first place. Pairs with set_current_state()\\n\\t * barrier (A) in rcuwait_wait_event().\\n\\t *\\n\\t *    WAIT                WAKE\\n\\t *    [S] tsk = current\\t  [S] cond = true\\n\\t *        MB (A)\\t      MB (B)\\n\\t *    [L] cond\\t\\t  [L] tsk\\n\\t */\\n\\tsmp_mb(); /* (B) */\\n\\n\\ttask = rcu_dereference(w->task);\\n\\tif (task)\\n\\t\\tret = wake_up_process(task);\\n\\trcu_read_unlock();\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(rcuwait_wake_up);\\n\\n/*\\n * Determine if a process group is \"orphaned\", according to the POSIX\\n * definition in 2.2.2.52.  Orphaned process groups are not to be affected\\n * by terminal-generated stop signals.  Newly orphaned process groups are\\n * to receive a SIGHUP and a SIGCONT.\\n *\\n * \"I ask you, have you ever known what it is to be an orphan?\"\\n */\\nstatic int will_become_orphaned_pgrp(struct pid *pgrp,\\n\\t\\t\\t\\t\\tstruct task_struct *ignored_task)\\n{\\n\\tstruct task_struct *p;\\n\\n\\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\\n\\t\\tif ((p == ignored_task) ||\\n\\t\\t    (p->exit_state && thread_group_empty(p)) ||\\n\\t\\t    is_global_init(p->real_parent))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (task_pgrp(p->real_parent) != pgrp &&\\n\\t\\t    task_session(p->real_parent) == task_session(p))\\n\\t\\t\\treturn 0;\\n\\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\\n\\n\\treturn 1;\\n}\\n\\nint is_current_pgrp_orphaned(void)\\n{\\n\\tint retval;\\n\\n\\tread_lock(&tasklist_lock);\\n\\tretval = will_become_orphaned_pgrp(task_pgrp(current), NULL);\\n\\tread_unlock(&tasklist_lock);\\n\\n\\treturn retval;\\n}\\n\\nstatic bool has_stopped_jobs(struct pid *pgrp)\\n{\\n\\tstruct task_struct *p;\\n\\n\\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\\n\\t\\tif (p->signal->flags & SIGNAL_STOP_STOPPED)\\n\\t\\t\\treturn true;\\n\\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\\n\\n\\treturn false;\\n}\\n\\n/*\\n * Check to see if any process groups have become orphaned as\\n * a result of our exiting, and if they have any stopped jobs,\\n * send them a SIGHUP and then a SIGCONT. (POSIX 3.2.2.2)\\n */\\nstatic void\\nkill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)\\n{\\n\\tstruct pid *pgrp = task_pgrp(tsk);\\n\\tstruct task_struct *ignored_task = tsk;\\n\\n\\tif (!parent)\\n\\t\\t/* exit: our father is in a different pgrp than\\n\\t\\t * we are and we were the only connection outside.\\n\\t\\t */\\n\\t\\tparent = tsk->real_parent;\\n\\telse\\n\\t\\t/* reparent: our child is in a different pgrp than\\n\\t\\t * we are, and it was the only connection outside.\\n\\t\\t */\\n\\t\\tignored_task = NULL;\\n\\n\\tif (task_pgrp(parent) != pgrp &&\\n\\t    task_session(parent) == task_session(tsk) &&\\n\\t    will_become_orphaned_pgrp(pgrp, ignored_task) &&\\n\\t    has_stopped_jobs(pgrp)) {\\n\\t\\t__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);\\n\\t\\t__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);\\n\\t}\\n}\\n\\nstatic void coredump_task_exit(struct task_struct *tsk)\\n{\\n\\tstruct core_state *core_state;\\n\\n\\t/*\\n\\t * Serialize with any possible pending coredump.\\n\\t * We must hold siglock around checking core_state\\n\\t * and setting PF_POSTCOREDUMP.  The core-inducing thread\\n\\t * will increment ->nr_threads for each thread in the\\n\\t * group without PF_POSTCOREDUMP set.\\n\\t */\\n\\tspin_lock_irq(&tsk->sighand->siglock);\\n\\ttsk->flags |= PF_POSTCOREDUMP;\\n\\tcore_state = tsk->signal->core_state;\\n\\tspin_unlock_irq(&tsk->sighand->siglock);\\n\\tif (core_state) {\\n\\t\\tstruct core_thread self;\\n\\n\\t\\tself.task = current;\\n\\t\\tif (self.task->flags & PF_SIGNALED)\\n\\t\\t\\tself.next = xchg(&core_state->dumper.next, &self);\\n\\t\\telse\\n\\t\\t\\tself.task = NULL;\\n\\t\\t/*\\n\\t\\t * Implies mb(), the result of xchg() must be visible\\n\\t\\t * to core_state->dumper.\\n\\t\\t */\\n\\t\\tif (atomic_dec_and_test(&core_state->nr_threads))\\n\\t\\t\\tcomplete(&core_state->startup);\\n\\n\\t\\tfor (;;) {\\n\\t\\t\\tset_current_state(TASK_IDLE|TASK_FREEZABLE);\\n\\t\\t\\tif (!self.task) /* see coredump_finish() */\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tschedule();\\n\\t\\t}\\n\\t\\t__set_current_state(TASK_RUNNING);\\n\\t}\\n}\\n\\n#ifdef CONFIG_MEMCG\\n/* drops tasklist_lock if succeeds */\\nstatic bool __try_to_set_owner(struct task_struct *tsk, struct mm_struct *mm)\\n{\\n\\tbool ret = false;\\n\\n\\ttask_lock(tsk);\\n\\tif (likely(tsk->mm == mm)) {\\n\\t\\t/* tsk can\\'t pass exit_mm/exec_mmap and exit */\\n\\t\\tread_unlock(&tasklist_lock);\\n\\t\\tWRITE_ONCE(mm->owner, tsk);\\n\\t\\tlru_gen_migrate_mm(mm);\\n\\t\\tret = true;\\n\\t}\\n\\ttask_unlock(tsk);\\n\\treturn ret;\\n}\\n\\nstatic bool try_to_set_owner(struct task_struct *g, struct mm_struct *mm)\\n{\\n\\tstruct task_struct *t;\\n\\n\\tfor_each_thread(g, t) {\\n\\t\\tstruct mm_struct *t_mm = READ_ONCE(t->mm);\\n\\t\\tif (t_mm == mm) {\\n\\t\\t\\tif (__try_to_set_owner(t, mm))\\n\\t\\t\\t\\treturn true;\\n\\t\\t} else if (t_mm)\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\treturn false;\\n}\\n\\n/*\\n * A task is exiting.   If it owned this mm, find a new owner for the mm.\\n */\\nvoid mm_update_next_owner(struct mm_struct *mm)\\n{\\n\\tstruct task_struct *g, *p = current;\\n\\n\\t/*\\n\\t * If the exiting or execing task is not the owner, it\\'s\\n\\t * someone else\\'s problem.\\n\\t */\\n\\tif (mm->owner != p)\\n\\t\\treturn;\\n\\t/*\\n\\t * The current owner is exiting/execing and there are no other\\n\\t * candidates.  Do not leave the mm pointing to a possibly\\n\\t * freed task structure.\\n\\t */\\n\\tif (atomic_read(&mm->mm_users) <= 1) {\\n\\t\\tWRITE_ONCE(mm->owner, NULL);\\n\\t\\treturn;\\n\\t}\\n\\n\\tread_lock(&tasklist_lock);\\n\\t/*\\n\\t * Search in the children\\n\\t */\\n\\tlist_for_each_entry(g, &p->children, sibling) {\\n\\t\\tif (try_to_set_owner(g, mm))\\n\\t\\t\\tgoto ret;\\n\\t}\\n\\t/*\\n\\t * Search in the siblings\\n\\t */\\n\\tlist_for_each_entry(g, &p->real_parent->children, sibling) {\\n\\t\\tif (try_to_set_owner(g, mm))\\n\\t\\t\\tgoto ret;\\n\\t}\\n\\t/*\\n\\t * Search through everything else, we should not get here often.\\n\\t */\\n\\tfor_each_process(g) {\\n\\t\\tif (atomic_read(&mm->mm_users) <= 1)\\n\\t\\t\\tbreak;\\n\\t\\tif (g->flags & PF_KTHREAD)\\n\\t\\t\\tcontinue;\\n\\t\\tif (try_to_set_owner(g, mm))\\n\\t\\t\\tgoto ret;\\n\\t}\\n\\tread_unlock(&tasklist_lock);\\n\\t/*\\n\\t * We found no owner yet mm_users > 1: this implies that we are\\n\\t * most likely racing with swapoff (try_to_unuse()) or /proc or\\n\\t * ptrace or page migration (get_task_mm()).  Mark owner as NULL.\\n\\t */\\n\\tWRITE_ONCE(mm->owner, NULL);\\n ret:\\n\\treturn;\\n\\n}\\n#endif /* CONFIG_MEMCG */\\n\\n/*\\n * Turn us into a lazy TLB process if we\\n * aren\\'t already..\\n */\\nstatic void exit_mm(void)\\n{\\n\\tstruct mm_struct *mm = current->mm;\\n\\n\\texit_mm_release(current, mm);\\n\\tif (!mm)\\n\\t\\treturn;\\n\\tmmap_read_lock(mm);\\n\\tmmgrab_lazy_tlb(mm);\\n\\tBUG_ON(mm != current->active_mm);\\n\\t/* more a memory barrier than a real lock */\\n\\ttask_lock(current);\\n\\t/*\\n\\t * When a thread stops operating on an address space, the loop\\n\\t * in membarrier_private_expedited() may not observe that\\n\\t * tsk->mm, and the loop in membarrier_global_expedited() may\\n\\t * not observe a MEMBARRIER_STATE_GLOBAL_EXPEDITED\\n\\t * rq->membarrier_state, so those would not issue an IPI.\\n\\t * Membarrier requires a memory barrier after accessing\\n\\t * user-space memory, before clearing tsk->mm or the\\n\\t * rq->membarrier_state.\\n\\t */\\n\\tsmp_mb__after_spinlock();\\n\\tlocal_irq_disable();\\n\\tcurrent->mm = NULL;\\n\\tmembarrier_update_current_mm(NULL);\\n\\tenter_lazy_tlb(mm, current);\\n\\tlocal_irq_enable();\\n\\ttask_unlock(current);\\n\\tmmap_read_unlock(mm);\\n\\tmm_update_next_owner(mm);\\n\\tmmput(mm);\\n\\tif (test_thread_flag(TIF_MEMDIE))\\n\\t\\texit_oom_victim();\\n}\\n\\nstatic struct task_struct *find_alive_thread(struct task_struct *p)\\n{\\n\\tstruct task_struct *t;\\n\\n\\tfor_each_thread(p, t) {\\n\\t\\tif (!(t->flags & PF_EXITING))\\n\\t\\t\\treturn t;\\n\\t}\\n\\treturn NULL;\\n}\\n\\nstatic struct task_struct *find_child_reaper(struct task_struct *father,\\n\\t\\t\\t\\t\\t\\tstruct list_head *dead)\\n\\t__releases(&tasklist_lock)\\n\\t__acquires(&tasklist_lock)\\n{\\n\\tstruct pid_namespace *pid_ns = task_active_pid_ns(father);\\n\\tstruct task_struct *reaper = pid_ns->child_reaper;\\n\\tstruct task_struct *p, *n;\\n\\n\\tif (likely(reaper != father))\\n\\t\\treturn reaper;\\n\\n\\treaper = find_alive_thread(father);\\n\\tif (reaper) {\\n\\t\\tpid_ns->child_reaper = reaper;\\n\\t\\treturn reaper;\\n\\t}\\n\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\n\\tlist_for_each_entry_safe(p, n, dead, ptrace_entry) {\\n\\t\\tlist_del_init(&p->ptrace_entry);\\n\\t\\trelease_task(p);\\n\\t}\\n\\n\\tzap_pid_ns_processes(pid_ns);\\n\\twrite_lock_irq(&tasklist_lock);\\n\\n\\treturn father;\\n}\\n\\n/*\\n * When we die, we re-parent all our children, and try to:\\n * 1. give them to another thread in our thread group, if such a member exists\\n * 2. give it to the first ancestor process which prctl\\'d itself as a\\n *    child_subreaper for its children (like a service manager)\\n * 3. give it to the init process (PID 1) in our pid namespace\\n */\\nstatic struct task_struct *find_new_reaper(struct task_struct *father,\\n\\t\\t\\t\\t\\t   struct task_struct *child_reaper)\\n{\\n\\tstruct task_struct *thread, *reaper;\\n\\n\\tthread = find_alive_thread(father);\\n\\tif (thread)\\n\\t\\treturn thread;\\n\\n\\tif (father->signal->has_child_subreaper) {\\n\\t\\tunsigned int ns_level = task_pid(father)->level;\\n\\t\\t/*\\n\\t\\t * Find the first ->is_child_subreaper ancestor in our pid_ns.\\n\\t\\t * We can\\'t check reaper != child_reaper to ensure we do not\\n\\t\\t * cross the namespaces, the exiting parent could be injected\\n\\t\\t * by setns() + fork().\\n\\t\\t * We check pid->level, this is slightly more efficient than\\n\\t\\t * task_active_pid_ns(reaper) != task_active_pid_ns(father).\\n\\t\\t */\\n\\t\\tfor (reaper = father->real_parent;\\n\\t\\t     task_pid(reaper)->level == ns_level;\\n\\t\\t     reaper = reaper->real_parent) {\\n\\t\\t\\tif (reaper == &init_task)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tif (!reaper->signal->is_child_subreaper)\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\tthread = find_alive_thread(reaper);\\n\\t\\t\\tif (thread)\\n\\t\\t\\t\\treturn thread;\\n\\t\\t}\\n\\t}\\n\\n\\treturn child_reaper;\\n}\\n\\n/*\\n* Any that need to be release_task\\'d are put on the @dead list.\\n */\\nstatic void reparent_leader(struct task_struct *father, struct task_struct *p,\\n\\t\\t\\t\\tstruct list_head *dead)\\n{\\n\\tif (unlikely(p->exit_state == EXIT_DEAD))\\n\\t\\treturn;\\n\\n\\t/* We don\\'t want people slaying init. */\\n\\tp->exit_signal = SIGCHLD;\\n\\n\\t/* If it has exited notify the new parent about this child\\'s death. */\\n\\tif (!p->ptrace &&\\n\\t    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {\\n\\t\\tif (do_notify_parent(p, p->exit_signal)) {\\n\\t\\t\\tp->exit_state = EXIT_DEAD;\\n\\t\\t\\tlist_add(&p->ptrace_entry, dead);\\n\\t\\t}\\n\\t}\\n\\n\\tkill_orphaned_pgrp(p, father);\\n}\\n\\n/*\\n * This does two things:\\n *\\n * A.  Make init inherit all the child processes\\n * B.  Check to see if any process groups have become orphaned\\n *\\tas a result of our exiting, and if they have any stopped\\n *\\tjobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)\\n */\\nstatic void forget_original_parent(struct task_struct *father,\\n\\t\\t\\t\\t\\tstruct list_head *dead)\\n{\\n\\tstruct task_struct *p, *t, *reaper;\\n\\n\\tif (unlikely(!list_empty(&father->ptraced)))\\n\\t\\texit_ptrace(father, dead);\\n\\n\\t/* Can drop and reacquire tasklist_lock */\\n\\treaper = find_child_reaper(father, dead);\\n\\tif (list_empty(&father->children))\\n\\t\\treturn;\\n\\n\\treaper = find_new_reaper(father, reaper);\\n\\tlist_for_each_entry(p, &father->children, sibling) {\\n\\t\\tfor_each_thread(p, t) {\\n\\t\\t\\tRCU_INIT_POINTER(t->real_parent, reaper);\\n\\t\\t\\tBUG_ON((!t->ptrace) != (rcu_access_pointer(t->parent) == father));\\n\\t\\t\\tif (likely(!t->ptrace))\\n\\t\\t\\t\\tt->parent = t->real_parent;\\n\\t\\t\\tif (t->pdeath_signal)\\n\\t\\t\\t\\tgroup_send_sig_info(t->pdeath_signal,\\n\\t\\t\\t\\t\\t\\t    SEND_SIG_NOINFO, t,\\n\\t\\t\\t\\t\\t\\t    PIDTYPE_TGID);\\n\\t\\t}\\n\\t\\t/*\\n\\t\\t * If this is a threaded reparent there is no need to\\n\\t\\t * notify anyone anything has happened.\\n\\t\\t */\\n\\t\\tif (!same_thread_group(reaper, father))\\n\\t\\t\\treparent_leader(father, p, dead);\\n\\t}\\n\\tlist_splice_tail_init(&father->children, &reaper->children);\\n}\\n\\n/*\\n * Send signals to all our closest relatives so that they know\\n * to properly mourn us..\\n */\\nstatic void exit_notify(struct task_struct *tsk, int group_dead)\\n{\\n\\tbool autoreap;\\n\\tstruct task_struct *p, *n;\\n\\tLIST_HEAD(dead);\\n\\n\\twrite_lock_irq(&tasklist_lock);\\n\\tforget_original_parent(tsk, &dead);\\n\\n\\tif (group_dead)\\n\\t\\tkill_orphaned_pgrp(tsk->group_leader, NULL);\\n\\n\\ttsk->exit_state = EXIT_ZOMBIE;\\n\\t/*\\n\\t * sub-thread or delay_group_leader(), wake up the\\n\\t * PIDFD_THREAD waiters.\\n\\t */\\n\\tif (!thread_group_empty(tsk))\\n\\t\\tdo_notify_pidfd(tsk);\\n\\n\\tif (unlikely(tsk->ptrace)) {\\n\\t\\tint sig = thread_group_leader(tsk) &&\\n\\t\\t\\t\\tthread_group_empty(tsk) &&\\n\\t\\t\\t\\t!ptrace_reparented(tsk) ?\\n\\t\\t\\ttsk->exit_signal : SIGCHLD;\\n\\t\\tautoreap = do_notify_parent(tsk, sig);\\n\\t} else if (thread_group_leader(tsk)) {\\n\\t\\tautoreap = thread_group_empty(tsk) &&\\n\\t\\t\\tdo_notify_parent(tsk, tsk->exit_signal);\\n\\t} else {\\n\\t\\tautoreap = true;\\n\\t}\\n\\n\\tif (autoreap) {\\n\\t\\ttsk->exit_state = EXIT_DEAD;\\n\\t\\tlist_add(&tsk->ptrace_entry, &dead);\\n\\t}\\n\\n\\t/* mt-exec, de_thread() is waiting for group leader */\\n\\tif (unlikely(tsk->signal->notify_count < 0))\\n\\t\\twake_up_process(tsk->signal->group_exec_task);\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\n\\tlist_for_each_entry_safe(p, n, &dead, ptrace_entry) {\\n\\t\\tlist_del_init(&p->ptrace_entry);\\n\\t\\trelease_task(p);\\n\\t}\\n}\\n\\n#ifdef CONFIG_DEBUG_STACK_USAGE\\nunsigned long stack_not_used(struct task_struct *p)\\n{\\n\\tunsigned long *n = end_of_stack(p);\\n\\n\\tdo {\\t/* Skip over canary */\\n# ifdef CONFIG_STACK_GROWSUP\\n\\t\\tn--;\\n# else\\n\\t\\tn++;\\n# endif\\n\\t} while (!*n);\\n\\n# ifdef CONFIG_STACK_GROWSUP\\n\\treturn (unsigned long)end_of_stack(p) - (unsigned long)n;\\n# else\\n\\treturn (unsigned long)n - (unsigned long)end_of_stack(p);\\n# endif\\n}\\n\\n/* Count the maximum pages reached in kernel stacks */\\nstatic inline void kstack_histogram(unsigned long used_stack)\\n{\\n#ifdef CONFIG_VM_EVENT_COUNTERS\\n\\tif (used_stack <= 1024)\\n\\t\\tcount_vm_event(KSTACK_1K);\\n#if THREAD_SIZE > 1024\\n\\telse if (used_stack <= 2048)\\n\\t\\tcount_vm_event(KSTACK_2K);\\n#endif\\n#if THREAD_SIZE > 2048\\n\\telse if (used_stack <= 4096)\\n\\t\\tcount_vm_event(KSTACK_4K);\\n#endif\\n#if THREAD_SIZE > 4096\\n\\telse if (used_stack <= 8192)\\n\\t\\tcount_vm_event(KSTACK_8K);\\n#endif\\n#if THREAD_SIZE > 8192\\n\\telse if (used_stack <= 16384)\\n\\t\\tcount_vm_event(KSTACK_16K);\\n#endif\\n#if THREAD_SIZE > 16384\\n\\telse if (used_stack <= 32768)\\n\\t\\tcount_vm_event(KSTACK_32K);\\n#endif\\n#if THREAD_SIZE > 32768\\n\\telse if (used_stack <= 65536)\\n\\t\\tcount_vm_event(KSTACK_64K);\\n#endif\\n#if THREAD_SIZE > 65536\\n\\telse\\n\\t\\tcount_vm_event(KSTACK_REST);\\n#endif\\n#endif /* CONFIG_VM_EVENT_COUNTERS */\\n}\\n\\nstatic void check_stack_usage(void)\\n{\\n\\tstatic DEFINE_SPINLOCK(low_water_lock);\\n\\tstatic int lowest_to_date = THREAD_SIZE;\\n\\tunsigned long free;\\n\\n\\tfree = stack_not_used(current);\\n\\tkstack_histogram(THREAD_SIZE - free);\\n\\n\\tif (free >= lowest_to_date)\\n\\t\\treturn;\\n\\n\\tspin_lock(&low_water_lock);\\n\\tif (free < lowest_to_date) {\\n\\t\\tpr_info(\"%s (%d) used greatest stack depth: %lu bytes left\\\\n\",\\n\\t\\t\\tcurrent->comm, task_pid_nr(current), free);\\n\\t\\tlowest_to_date = free;\\n\\t}\\n\\tspin_unlock(&low_water_lock);\\n}\\n#else\\nstatic inline void check_stack_usage(void) {}\\n#endif\\n\\nstatic void synchronize_group_exit(struct task_struct *tsk, long code)\\n{\\n\\tstruct sighand_struct *sighand = tsk->sighand;\\n\\tstruct signal_struct *signal = tsk->signal;\\n\\n\\tspin_lock_irq(&sighand->siglock);\\n\\tsignal->quick_threads--;\\n\\tif ((signal->quick_threads == 0) &&\\n\\t    !(signal->flags & SIGNAL_GROUP_EXIT)) {\\n\\t\\tsignal->flags = SIGNAL_GROUP_EXIT;\\n\\t\\tsignal->group_exit_code = code;\\n\\t\\tsignal->group_stop_count = 0;\\n\\t}\\n\\tspin_unlock_irq(&sighand->siglock);\\n}\\n\\nvoid __noreturn do_exit(long code)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\tint group_dead;\\n\\n\\tWARN_ON(irqs_disabled());\\n\\n\\tsynchronize_group_exit(tsk, code);\\n\\n\\tWARN_ON(tsk->plug);\\n\\n\\tkcov_task_exit(tsk);\\n\\tkmsan_task_exit(tsk);\\n\\n\\tcoredump_task_exit(tsk);\\n\\tptrace_event(PTRACE_EVENT_EXIT, code);\\n\\tuser_events_exit(tsk);\\n\\n\\tio_uring_files_cancel();\\n\\texit_signals(tsk);  /* sets PF_EXITING */\\n\\n\\tseccomp_filter_release(tsk);\\n\\n\\tacct_update_integrals(tsk);\\n\\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);\\n\\tif (group_dead) {\\n\\t\\t/*\\n\\t\\t * If the last thread of global init has exited, panic\\n\\t\\t * immediately to get a useable coredump.\\n\\t\\t */\\n\\t\\tif (unlikely(is_global_init(tsk)))\\n\\t\\t\\tpanic(\"Attempted to kill init! exitcode=0x%08x\\\\n\",\\n\\t\\t\\t\\ttsk->signal->group_exit_code ?: (int)code);\\n\\n#ifdef CONFIG_POSIX_TIMERS\\n\\t\\thrtimer_cancel(&tsk->signal->real_timer);\\n\\t\\texit_itimers(tsk);\\n#endif\\n\\t\\tif (tsk->mm)\\n\\t\\t\\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\\n\\t}\\n\\tacct_collect(code, group_dead);\\n\\tif (group_dead)\\n\\t\\ttty_audit_exit();\\n\\taudit_free(tsk);\\n\\n\\ttsk->exit_code = code;\\n\\ttaskstats_exit(tsk, group_dead);\\n\\n\\texit_mm();\\n\\n\\tif (group_dead)\\n\\t\\tacct_process();\\n\\ttrace_sched_process_exit(tsk);\\n\\n\\texit_sem(tsk);\\n\\texit_shm(tsk);\\n\\texit_files(tsk);\\n\\texit_fs(tsk);\\n\\tif (group_dead)\\n\\t\\tdisassociate_ctty(1);\\n\\texit_task_namespaces(tsk);\\n\\texit_task_work(tsk);\\n\\texit_thread(tsk);\\n\\n\\t/*\\n\\t * Flush inherited counters to the parent - before the parent\\n\\t * gets woken up by child-exit notifications.\\n\\t *\\n\\t * because of cgroup mode, must be called before cgroup_exit()\\n\\t */\\n\\tperf_event_exit_task(tsk);\\n\\n\\tsched_autogroup_exit_task(tsk);\\n\\tcgroup_exit(tsk);\\n\\n\\t/*\\n\\t * FIXME: do that only when needed, using sched_exit tracepoint\\n\\t */\\n\\tflush_ptrace_hw_breakpoint(tsk);\\n\\n\\texit_tasks_rcu_start();\\n\\texit_notify(tsk, group_dead);\\n\\tproc_exit_connector(tsk);\\n\\tmpol_put_task_policy(tsk);\\n#ifdef CONFIG_FUTEX\\n\\tif (unlikely(current->pi_state_cache))\\n\\t\\tkfree(current->pi_state_cache);\\n#endif\\n\\t/*\\n\\t * Make sure we are holding no locks:\\n\\t */\\n\\tdebug_check_no_locks_held();\\n\\n\\tif (tsk->io_context)\\n\\t\\texit_io_context(tsk);\\n\\n\\tif (tsk->splice_pipe)\\n\\t\\tfree_pipe_info(tsk->splice_pipe);\\n\\n\\tif (tsk->task_frag.page)\\n\\t\\tput_page(tsk->task_frag.page);\\n\\n\\texit_task_stack_account(tsk);\\n\\n\\tcheck_stack_usage();\\n\\tpreempt_disable();\\n\\tif (tsk->nr_dirtied)\\n\\t\\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);\\n\\texit_rcu();\\n\\texit_tasks_rcu_finish();\\n\\n\\tlockdep_free_task(tsk);\\n\\tdo_task_dead();\\n}\\n\\nvoid __noreturn make_task_dead(int signr)\\n{\\n\\t/*\\n\\t * Take the task off the cpu after something catastrophic has\\n\\t * happened.\\n\\t *\\n\\t * We can get here from a kernel oops, sometimes with preemption off.\\n\\t * Start by checking for critical errors.\\n\\t * Then fix up important state like USER_DS and preemption.\\n\\t * Then do everything else.\\n\\t */\\n\\tstruct task_struct *tsk = current;\\n\\tunsigned int limit;\\n\\n\\tif (unlikely(in_interrupt()))\\n\\t\\tpanic(\"Aiee, killing interrupt handler!\");\\n\\tif (unlikely(!tsk->pid))\\n\\t\\tpanic(\"Attempted to kill the idle task!\");\\n\\n\\tif (unlikely(irqs_disabled())) {\\n\\t\\tpr_info(\"note: %s[%d] exited with irqs disabled\\\\n\",\\n\\t\\t\\tcurrent->comm, task_pid_nr(current));\\n\\t\\tlocal_irq_enable();\\n\\t}\\n\\tif (unlikely(in_atomic())) {\\n\\t\\tpr_info(\"note: %s[%d] exited with preempt_count %d\\\\n\",\\n\\t\\t\\tcurrent->comm, task_pid_nr(current),\\n\\t\\t\\tpreempt_count());\\n\\t\\tpreempt_count_set(PREEMPT_ENABLED);\\n\\t}\\n\\n\\t/*\\n\\t * Every time the system oopses, if the oops happens while a reference\\n\\t * to an object was held, the reference leaks.\\n\\t * If the oops doesn\\'t also leak memory, repeated oopsing can cause\\n\\t * reference counters to wrap around (if they\\'re not using refcount_t).\\n\\t * This means that repeated oopsing can make unexploitable-looking bugs\\n\\t * exploitable through repeated oopsing.\\n\\t * To make sure this can\\'t happen, place an upper bound on how often the\\n\\t * kernel may oops without panic().\\n\\t */\\n\\tlimit = READ_ONCE(oops_limit);\\n\\tif (atomic_inc_return(&oops_count) >= limit && limit)\\n\\t\\tpanic(\"Oopsed too often (kernel.oops_limit is %d)\", limit);\\n\\n\\t/*\\n\\t * We\\'re taking recursive faults here in make_task_dead. Safest is to just\\n\\t * leave this task alone and wait for reboot.\\n\\t */\\n\\tif (unlikely(tsk->flags & PF_EXITING)) {\\n\\t\\tpr_alert(\"Fixing recursive fault but reboot is needed!\\\\n\");\\n\\t\\tfutex_exit_recursive(tsk);\\n\\t\\ttsk->exit_state = EXIT_DEAD;\\n\\t\\trefcount_inc(&tsk->rcu_users);\\n\\t\\tdo_task_dead();\\n\\t}\\n\\n\\tdo_exit(signr);\\n}\\n\\nSYSCALL_DEFINE1(exit, int, error_code)\\n{\\n\\tdo_exit((error_code&0xff)<<8);\\n}\\n\\n/*\\n * Take down every thread in the group.  This is called by fatal signals\\n * as well as by sys_exit_group (below).\\n */\\nvoid __noreturn\\ndo_group_exit(int exit_code)\\n{\\n\\tstruct signal_struct *sig = current->signal;\\n\\n\\tif (sig->flags & SIGNAL_GROUP_EXIT)\\n\\t\\texit_code = sig->group_exit_code;\\n\\telse if (sig->group_exec_task)\\n\\t\\texit_code = 0;\\n\\telse {\\n\\t\\tstruct sighand_struct *const sighand = current->sighand;\\n\\n\\t\\tspin_lock_irq(&sighand->siglock);\\n\\t\\tif (sig->flags & SIGNAL_GROUP_EXIT)\\n\\t\\t\\t/* Another thread got here before we took the lock.  */\\n\\t\\t\\texit_code = sig->group_exit_code;\\n\\t\\telse if (sig->group_exec_task)\\n\\t\\t\\texit_code = 0;\\n\\t\\telse {\\n\\t\\t\\tsig->group_exit_code = exit_code;\\n\\t\\t\\tsig->flags = SIGNAL_GROUP_EXIT;\\n\\t\\t\\tzap_other_threads(current);\\n\\t\\t}\\n\\t\\tspin_unlock_irq(&sighand->siglock);\\n\\t}\\n\\n\\tdo_exit(exit_code);\\n\\t/* NOTREACHED */\\n}\\n\\n/*\\n * this kills every thread in the thread group. Note that any externally\\n * wait4()-ing process will get the correct exit code - even if this\\n * thread is not the thread group leader.\\n */\\nSYSCALL_DEFINE1(exit_group, int, error_code)\\n{\\n\\tdo_group_exit((error_code & 0xff) << 8);\\n\\t/* NOTREACHED */\\n\\treturn 0;\\n}\\n\\nstatic int eligible_pid(struct wait_opts *wo, struct task_struct *p)\\n{\\n\\treturn\\two->wo_type == PIDTYPE_MAX ||\\n\\t\\ttask_pid_type(p, wo->wo_type) == wo->wo_pid;\\n}\\n\\nstatic int\\neligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)\\n{\\n\\tif (!eligible_pid(wo, p))\\n\\t\\treturn 0;\\n\\n\\t/*\\n\\t * Wait for all children (clone and not) if __WALL is set or\\n\\t * if it is traced by us.\\n\\t */\\n\\tif (ptrace || (wo->wo_flags & __WALL))\\n\\t\\treturn 1;\\n\\n\\t/*\\n\\t * Otherwise, wait for clone children *only* if __WCLONE is set;\\n\\t * otherwise, wait for non-clone children *only*.\\n\\t *\\n\\t * Note: a \"clone\" child here is one that reports to its parent\\n\\t * using a signal other than SIGCHLD, or a non-leader thread which\\n\\t * we can only see if it is traced by us.\\n\\t */\\n\\tif ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))\\n\\t\\treturn 0;\\n\\n\\treturn 1;\\n}\\n\\n/*\\n * Handle sys_wait4 work for one task in state EXIT_ZOMBIE.  We hold\\n * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold\\n * the lock and this task is uninteresting.  If we return nonzero, we have\\n * released the lock and the system call should return.\\n */\\nstatic int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)\\n{\\n\\tint state, status;\\n\\tpid_t pid = task_pid_vnr(p);\\n\\tuid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));\\n\\tstruct waitid_info *infop;\\n\\n\\tif (!likely(wo->wo_flags & WEXITED))\\n\\t\\treturn 0;\\n\\n\\tif (unlikely(wo->wo_flags & WNOWAIT)) {\\n\\t\\tstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)\\n\\t\\t\\t? p->signal->group_exit_code : p->exit_code;\\n\\t\\tget_task_struct(p);\\n\\t\\tread_unlock(&tasklist_lock);\\n\\t\\tsched_annotate_sleep();\\n\\t\\tif (wo->wo_rusage)\\n\\t\\t\\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\\n\\t\\tput_task_struct(p);\\n\\t\\tgoto out_info;\\n\\t}\\n\\t/*\\n\\t * Move the task\\'s state to DEAD/TRACE, only one thread can do this.\\n\\t */\\n\\tstate = (ptrace_reparented(p) && thread_group_leader(p)) ?\\n\\t\\tEXIT_TRACE : EXIT_DEAD;\\n\\tif (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)\\n\\t\\treturn 0;\\n\\t/*\\n\\t * We own this thread, nobody else can reap it.\\n\\t */\\n\\tread_unlock(&tasklist_lock);\\n\\tsched_annotate_sleep();\\n\\n\\t/*\\n\\t * Check thread_group_leader() to exclude the traced sub-threads.\\n\\t */\\n\\tif (state == EXIT_DEAD && thread_group_leader(p)) {\\n\\t\\tstruct signal_struct *sig = p->signal;\\n\\t\\tstruct signal_struct *psig = current->signal;\\n\\t\\tunsigned long maxrss;\\n\\t\\tu64 tgutime, tgstime;\\n\\n\\t\\t/*\\n\\t\\t * The resource counters for the group leader are in its\\n\\t\\t * own task_struct.  Those for dead threads in the group\\n\\t\\t * are in its signal_struct, as are those for the child\\n\\t\\t * processes it has previously reaped.  All these\\n\\t\\t * accumulate in the parent\\'s signal_struct c* fields.\\n\\t\\t *\\n\\t\\t * We don\\'t bother to take a lock here to protect these\\n\\t\\t * p->signal fields because the whole thread group is dead\\n\\t\\t * and nobody can change them.\\n\\t\\t *\\n\\t\\t * psig->stats_lock also protects us from our sub-threads\\n\\t\\t * which can reap other children at the same time.\\n\\t\\t *\\n\\t\\t * We use thread_group_cputime_adjusted() to get times for\\n\\t\\t * the thread group, which consolidates times for all threads\\n\\t\\t * in the group including the group leader.\\n\\t\\t */\\n\\t\\tthread_group_cputime_adjusted(p, &tgutime, &tgstime);\\n\\t\\twrite_seqlock_irq(&psig->stats_lock);\\n\\t\\tpsig->cutime += tgutime + sig->cutime;\\n\\t\\tpsig->cstime += tgstime + sig->cstime;\\n\\t\\tpsig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;\\n\\t\\tpsig->cmin_flt +=\\n\\t\\t\\tp->min_flt + sig->min_flt + sig->cmin_flt;\\n\\t\\tpsig->cmaj_flt +=\\n\\t\\t\\tp->maj_flt + sig->maj_flt + sig->cmaj_flt;\\n\\t\\tpsig->cnvcsw +=\\n\\t\\t\\tp->nvcsw + sig->nvcsw + sig->cnvcsw;\\n\\t\\tpsig->cnivcsw +=\\n\\t\\t\\tp->nivcsw + sig->nivcsw + sig->cnivcsw;\\n\\t\\tpsig->cinblock +=\\n\\t\\t\\ttask_io_get_inblock(p) +\\n\\t\\t\\tsig->inblock + sig->cinblock;\\n\\t\\tpsig->coublock +=\\n\\t\\t\\ttask_io_get_oublock(p) +\\n\\t\\t\\tsig->oublock + sig->coublock;\\n\\t\\tmaxrss = max(sig->maxrss, sig->cmaxrss);\\n\\t\\tif (psig->cmaxrss < maxrss)\\n\\t\\t\\tpsig->cmaxrss = maxrss;\\n\\t\\ttask_io_accounting_add(&psig->ioac, &p->ioac);\\n\\t\\ttask_io_accounting_add(&psig->ioac, &sig->ioac);\\n\\t\\twrite_sequnlock_irq(&psig->stats_lock);\\n\\t}\\n\\n\\tif (wo->wo_rusage)\\n\\t\\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\\n\\tstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)\\n\\t\\t? p->signal->group_exit_code : p->exit_code;\\n\\two->wo_stat = status;\\n\\n\\tif (state == EXIT_TRACE) {\\n\\t\\twrite_lock_irq(&tasklist_lock);\\n\\t\\t/* We dropped tasklist, ptracer could die and untrace */\\n\\t\\tptrace_unlink(p);\\n\\n\\t\\t/* If parent wants a zombie, don\\'t release it now */\\n\\t\\tstate = EXIT_ZOMBIE;\\n\\t\\tif (do_notify_parent(p, p->exit_signal))\\n\\t\\t\\tstate = EXIT_DEAD;\\n\\t\\tp->exit_state = state;\\n\\t\\twrite_unlock_irq(&tasklist_lock);\\n\\t}\\n\\tif (state == EXIT_DEAD)\\n\\t\\trelease_task(p);\\n\\nout_info:\\n\\tinfop = wo->wo_info;\\n\\tif (infop) {\\n\\t\\tif ((status & 0x7f) == 0) {\\n\\t\\t\\tinfop->cause = CLD_EXITED;\\n\\t\\t\\tinfop->status = status >> 8;\\n\\t\\t} else {\\n\\t\\t\\tinfop->cause = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;\\n\\t\\t\\tinfop->status = status & 0x7f;\\n\\t\\t}\\n\\t\\tinfop->pid = pid;\\n\\t\\tinfop->uid = uid;\\n\\t}\\n\\n\\treturn pid;\\n}\\n\\nstatic int *task_stopped_code(struct task_struct *p, bool ptrace)\\n{\\n\\tif (ptrace) {\\n\\t\\tif (task_is_traced(p) && !(p->jobctl & JOBCTL_LISTENING))\\n\\t\\t\\treturn &p->exit_code;\\n\\t} else {\\n\\t\\tif (p->signal->flags & SIGNAL_STOP_STOPPED)\\n\\t\\t\\treturn &p->signal->group_exit_code;\\n\\t}\\n\\treturn NULL;\\n}\\n\\n/**\\n * wait_task_stopped - Wait for %TASK_STOPPED or %TASK_TRACED\\n * @wo: wait options\\n * @ptrace: is the wait for ptrace\\n * @p: task to wait for\\n *\\n * Handle sys_wait4() work for %p in state %TASK_STOPPED or %TASK_TRACED.\\n *\\n * CONTEXT:\\n * read_lock(&tasklist_lock), which is released if return value is\\n * non-zero.  Also, grabs and releases @p->sighand->siglock.\\n *\\n * RETURNS:\\n * 0 if wait condition didn\\'t exist and search for other wait conditions\\n * should continue.  Non-zero return, -errno on failure and @p\\'s pid on\\n * success, implies that tasklist_lock is released and wait condition\\n * search should terminate.\\n */\\nstatic int wait_task_stopped(struct wait_opts *wo,\\n\\t\\t\\t\\tint ptrace, struct task_struct *p)\\n{\\n\\tstruct waitid_info *infop;\\n\\tint exit_code, *p_code, why;\\n\\tuid_t uid = 0; /* unneeded, required by compiler */\\n\\tpid_t pid;\\n\\n\\t/*\\n\\t * Traditionally we see ptrace\\'d stopped tasks regardless of options.\\n\\t */\\n\\tif (!ptrace && !(wo->wo_flags & WUNTRACED))\\n\\t\\treturn 0;\\n\\n\\tif (!task_stopped_code(p, ptrace))\\n\\t\\treturn 0;\\n\\n\\texit_code = 0;\\n\\tspin_lock_irq(&p->sighand->siglock);\\n\\n\\tp_code = task_stopped_code(p, ptrace);\\n\\tif (unlikely(!p_code))\\n\\t\\tgoto unlock_sig;\\n\\n\\texit_code = *p_code;\\n\\tif (!exit_code)\\n\\t\\tgoto unlock_sig;\\n\\n\\tif (!unlikely(wo->wo_flags & WNOWAIT))\\n\\t\\t*p_code = 0;\\n\\n\\tuid = from_kuid_munged(current_user_ns(), task_uid(p));\\nunlock_sig:\\n\\tspin_unlock_irq(&p->sighand->siglock);\\n\\tif (!exit_code)\\n\\t\\treturn 0;\\n\\n\\t/*\\n\\t * Now we are pretty sure this task is interesting.\\n\\t * Make sure it doesn\\'t get reaped out from under us while we\\n\\t * give up the lock and then examine it below.  We don\\'t want to\\n\\t * keep holding onto the tasklist_lock while we call getrusage and\\n\\t * possibly take page faults for user memory.\\n\\t */\\n\\tget_task_struct(p);\\n\\tpid = task_pid_vnr(p);\\n\\twhy = ptrace ? CLD_TRAPPED : CLD_STOPPED;\\n\\tread_unlock(&tasklist_lock);\\n\\tsched_annotate_sleep();\\n\\tif (wo->wo_rusage)\\n\\t\\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\\n\\tput_task_struct(p);\\n\\n\\tif (likely(!(wo->wo_flags & WNOWAIT)))\\n\\t\\two->wo_stat = (exit_code << 8) | 0x7f;\\n\\n\\tinfop = wo->wo_info;\\n\\tif (infop) {\\n\\t\\tinfop->cause = why;\\n\\t\\tinfop->status = exit_code;\\n\\t\\tinfop->pid = pid;\\n\\t\\tinfop->uid = uid;\\n\\t}\\n\\treturn pid;\\n}\\n\\n/*\\n * Handle do_wait work for one task in a live, non-stopped state.\\n * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold\\n * the lock and this task is uninteresting.  If we return nonzero, we have\\n * released the lock and the system call should return.\\n */\\nstatic int wait_task_continued(struct wait_opts *wo, struct task_struct *p)\\n{\\n\\tstruct waitid_info *infop;\\n\\tpid_t pid;\\n\\tuid_t uid;\\n\\n\\tif (!unlikely(wo->wo_flags & WCONTINUED))\\n\\t\\treturn 0;\\n\\n\\tif (!(p->signal->flags & SIGNAL_STOP_CONTINUED))\\n\\t\\treturn 0;\\n\\n\\tspin_lock_irq(&p->sighand->siglock);\\n\\t/* Re-check with the lock held.  */\\n\\tif (!(p->signal->flags & SIGNAL_STOP_CONTINUED)) {\\n\\t\\tspin_unlock_irq(&p->sighand->siglock);\\n\\t\\treturn 0;\\n\\t}\\n\\tif (!unlikely(wo->wo_flags & WNOWAIT))\\n\\t\\tp->signal->flags &= ~SIGNAL_STOP_CONTINUED;\\n\\tuid = from_kuid_munged(current_user_ns(), task_uid(p));\\n\\tspin_unlock_irq(&p->sighand->siglock);\\n\\n\\tpid = task_pid_vnr(p);\\n\\tget_task_struct(p);\\n\\tread_unlock(&tasklist_lock);\\n\\tsched_annotate_sleep();\\n\\tif (wo->wo_rusage)\\n\\t\\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\\n\\tput_task_struct(p);\\n\\n\\tinfop = wo->wo_info;\\n\\tif (!infop) {\\n\\t\\two->wo_stat = 0xffff;\\n\\t} else {\\n\\t\\tinfop->cause = CLD_CONTINUED;\\n\\t\\tinfop->pid = pid;\\n\\t\\tinfop->uid = uid;\\n\\t\\tinfop->status = SIGCONT;\\n\\t}\\n\\treturn pid;\\n}\\n\\n/*\\n * Consider @p for a wait by @parent.\\n *\\n * -ECHILD should be in ->notask_error before the first call.\\n * Returns nonzero for a final return, when we have unlocked tasklist_lock.\\n * Returns zero if the search for a child should continue;\\n * then ->notask_error is 0 if @p is an eligible child,\\n * or still -ECHILD.\\n */\\nstatic int wait_consider_task(struct wait_opts *wo, int ptrace,\\n\\t\\t\\t\\tstruct task_struct *p)\\n{\\n\\t/*\\n\\t * We can race with wait_task_zombie() from another thread.\\n\\t * Ensure that EXIT_ZOMBIE -> EXIT_DEAD/EXIT_TRACE transition\\n\\t * can\\'t confuse the checks below.\\n\\t */\\n\\tint exit_state = READ_ONCE(p->exit_state);\\n\\tint ret;\\n\\n\\tif (unlikely(exit_state == EXIT_DEAD))\\n\\t\\treturn 0;\\n\\n\\tret = eligible_child(wo, ptrace, p);\\n\\tif (!ret)\\n\\t\\treturn ret;\\n\\n\\tif (unlikely(exit_state == EXIT_TRACE)) {\\n\\t\\t/*\\n\\t\\t * ptrace == 0 means we are the natural parent. In this case\\n\\t\\t * we should clear notask_error, debugger will notify us.\\n\\t\\t */\\n\\t\\tif (likely(!ptrace))\\n\\t\\t\\two->notask_error = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (likely(!ptrace) && unlikely(p->ptrace)) {\\n\\t\\t/*\\n\\t\\t * If it is traced by its real parent\\'s group, just pretend\\n\\t\\t * the caller is ptrace_do_wait() and reap this child if it\\n\\t\\t * is zombie.\\n\\t\\t *\\n\\t\\t * This also hides group stop state from real parent; otherwise\\n\\t\\t * a single stop can be reported twice as group and ptrace stop.\\n\\t\\t * If a ptracer wants to distinguish these two events for its\\n\\t\\t * own children it should create a separate process which takes\\n\\t\\t * the role of real parent.\\n\\t\\t */\\n\\t\\tif (!ptrace_reparented(p))\\n\\t\\t\\tptrace = 1;\\n\\t}\\n\\n\\t/* slay zombie? */\\n\\tif (exit_state == EXIT_ZOMBIE) {\\n\\t\\t/* we don\\'t reap group leaders with subthreads */\\n\\t\\tif (!delay_group_leader(p)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * A zombie ptracee is only visible to its ptracer.\\n\\t\\t\\t * Notification and reaping will be cascaded to the\\n\\t\\t\\t * real parent when the ptracer detaches.\\n\\t\\t\\t */\\n\\t\\t\\tif (unlikely(ptrace) || likely(!p->ptrace))\\n\\t\\t\\t\\treturn wait_task_zombie(wo, p);\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Allow access to stopped/continued state via zombie by\\n\\t\\t * falling through.  Clearing of notask_error is complex.\\n\\t\\t *\\n\\t\\t * When !@ptrace:\\n\\t\\t *\\n\\t\\t * If WEXITED is set, notask_error should naturally be\\n\\t\\t * cleared.  If not, subset of WSTOPPED|WCONTINUED is set,\\n\\t\\t * so, if there are live subthreads, there are events to\\n\\t\\t * wait for.  If all subthreads are dead, it\\'s still safe\\n\\t\\t * to clear - this function will be called again in finite\\n\\t\\t * amount time once all the subthreads are released and\\n\\t\\t * will then return without clearing.\\n\\t\\t *\\n\\t\\t * When @ptrace:\\n\\t\\t *\\n\\t\\t * Stopped state is per-task and thus can\\'t change once the\\n\\t\\t * target task dies.  Only continued and exited can happen.\\n\\t\\t * Clear notask_error if WCONTINUED | WEXITED.\\n\\t\\t */\\n\\t\\tif (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))\\n\\t\\t\\two->notask_error = 0;\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * @p is alive and it\\'s gonna stop, continue or exit, so\\n\\t\\t * there always is something to wait for.\\n\\t\\t */\\n\\t\\two->notask_error = 0;\\n\\t}\\n\\n\\t/*\\n\\t * Wait for stopped.  Depending on @ptrace, different stopped state\\n\\t * is used and the two don\\'t interact with each other.\\n\\t */\\n\\tret = wait_task_stopped(wo, ptrace, p);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/*\\n\\t * Wait for continued.  There\\'s only one continued state and the\\n\\t * ptracer can consume it which can confuse the real parent.  Don\\'t\\n\\t * use WCONTINUED from ptracer.  You don\\'t need or want it.\\n\\t */\\n\\treturn wait_task_continued(wo, p);\\n}\\n\\n/*\\n * Do the work of do_wait() for one thread in the group, @tsk.\\n *\\n * -ECHILD should be in ->notask_error before the first call.\\n * Returns nonzero for a final return, when we have unlocked tasklist_lock.\\n * Returns zero if the search for a child should continue; then\\n * ->notask_error is 0 if there were any eligible children,\\n * or still -ECHILD.\\n */\\nstatic int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)\\n{\\n\\tstruct task_struct *p;\\n\\n\\tlist_for_each_entry(p, &tsk->children, sibling) {\\n\\t\\tint ret = wait_consider_task(wo, 0, p);\\n\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)\\n{\\n\\tstruct task_struct *p;\\n\\n\\tlist_for_each_entry(p, &tsk->ptraced, ptrace_entry) {\\n\\t\\tint ret = wait_consider_task(wo, 1, p);\\n\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nbool pid_child_should_wake(struct wait_opts *wo, struct task_struct *p)\\n{\\n\\tif (!eligible_pid(wo, p))\\n\\t\\treturn false;\\n\\n\\tif ((wo->wo_flags & __WNOTHREAD) && wo->child_wait.private != p->parent)\\n\\t\\treturn false;\\n\\n\\treturn true;\\n}\\n\\nstatic int child_wait_callback(wait_queue_entry_t *wait, unsigned mode,\\n\\t\\t\\t\\tint sync, void *key)\\n{\\n\\tstruct wait_opts *wo = container_of(wait, struct wait_opts,\\n\\t\\t\\t\\t\\t\\tchild_wait);\\n\\tstruct task_struct *p = key;\\n\\n\\tif (pid_child_should_wake(wo, p))\\n\\t\\treturn default_wake_function(wait, mode, sync, key);\\n\\n\\treturn 0;\\n}\\n\\nvoid __wake_up_parent(struct task_struct *p, struct task_struct *parent)\\n{\\n\\t__wake_up_sync_key(&parent->signal->wait_chldexit,\\n\\t\\t\\t   TASK_INTERRUPTIBLE, p);\\n}\\n\\nstatic bool is_effectively_child(struct wait_opts *wo, bool ptrace,\\n\\t\\t\\t\\t struct task_struct *target)\\n{\\n\\tstruct task_struct *parent =\\n\\t\\t!ptrace ? target->real_parent : target->parent;\\n\\n\\treturn current == parent || (!(wo->wo_flags & __WNOTHREAD) &&\\n\\t\\t\\t\\t     same_thread_group(current, parent));\\n}\\n\\n/*\\n * Optimization for waiting on PIDTYPE_PID. No need to iterate through child\\n * and tracee lists to find the target task.\\n */\\nstatic int do_wait_pid(struct wait_opts *wo)\\n{\\n\\tbool ptrace;\\n\\tstruct task_struct *target;\\n\\tint retval;\\n\\n\\tptrace = false;\\n\\ttarget = pid_task(wo->wo_pid, PIDTYPE_TGID);\\n\\tif (target && is_effectively_child(wo, ptrace, target)) {\\n\\t\\tretval = wait_consider_task(wo, ptrace, target);\\n\\t\\tif (retval)\\n\\t\\t\\treturn retval;\\n\\t}\\n\\n\\tptrace = true;\\n\\ttarget = pid_task(wo->wo_pid, PIDTYPE_PID);\\n\\tif (target && target->ptrace &&\\n\\t    is_effectively_child(wo, ptrace, target)) {\\n\\t\\tretval = wait_consider_task(wo, ptrace, target);\\n\\t\\tif (retval)\\n\\t\\t\\treturn retval;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nlong __do_wait(struct wait_opts *wo)\\n{\\n\\tlong retval;\\n\\n\\t/*\\n\\t * If there is nothing that can match our criteria, just get out.\\n\\t * We will clear ->notask_error to zero if we see any child that\\n\\t * might later match our criteria, even if we are not able to reap\\n\\t * it yet.\\n\\t */\\n\\two->notask_error = -ECHILD;\\n\\tif ((wo->wo_type < PIDTYPE_MAX) &&\\n\\t   (!wo->wo_pid || !pid_has_task(wo->wo_pid, wo->wo_type)))\\n\\t\\tgoto notask;\\n\\n\\tread_lock(&tasklist_lock);\\n\\n\\tif (wo->wo_type == PIDTYPE_PID) {\\n\\t\\tretval = do_wait_pid(wo);\\n\\t\\tif (retval)\\n\\t\\t\\treturn retval;\\n\\t} else {\\n\\t\\tstruct task_struct *tsk = current;\\n\\n\\t\\tdo {\\n\\t\\t\\tretval = do_wait_thread(wo, tsk);\\n\\t\\t\\tif (retval)\\n\\t\\t\\t\\treturn retval;\\n\\n\\t\\t\\tretval = ptrace_do_wait(wo, tsk);\\n\\t\\t\\tif (retval)\\n\\t\\t\\t\\treturn retval;\\n\\n\\t\\t\\tif (wo->wo_flags & __WNOTHREAD)\\n\\t\\t\\t\\tbreak;\\n\\t\\t} while_each_thread(current, tsk);\\n\\t}\\n\\tread_unlock(&tasklist_lock);\\n\\nnotask:\\n\\tretval = wo->notask_error;\\n\\tif (!retval && !(wo->wo_flags & WNOHANG))\\n\\t\\treturn -ERESTARTSYS;\\n\\n\\treturn retval;\\n}\\n\\nstatic long do_wait(struct wait_opts *wo)\\n{\\n\\tint retval;\\n\\n\\ttrace_sched_process_wait(wo->wo_pid);\\n\\n\\tinit_waitqueue_func_entry(&wo->child_wait, child_wait_callback);\\n\\two->child_wait.private = current;\\n\\tadd_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\\n\\n\\tdo {\\n\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\t\\tretval = __do_wait(wo);\\n\\t\\tif (retval != -ERESTARTSYS)\\n\\t\\t\\tbreak;\\n\\t\\tif (signal_pending(current))\\n\\t\\t\\tbreak;\\n\\t\\tschedule();\\n\\t} while (1);\\n\\n\\t__set_current_state(TASK_RUNNING);\\n\\tremove_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\\n\\treturn retval;\\n}\\n\\nint kernel_waitid_prepare(struct wait_opts *wo, int which, pid_t upid,\\n\\t\\t\\t  struct waitid_info *infop, int options,\\n\\t\\t\\t  struct rusage *ru)\\n{\\n\\tunsigned int f_flags = 0;\\n\\tstruct pid *pid = NULL;\\n\\tenum pid_type type;\\n\\n\\tif (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED|\\n\\t\\t\\t__WNOTHREAD|__WCLONE|__WALL))\\n\\t\\treturn -EINVAL;\\n\\tif (!(options & (WEXITED|WSTOPPED|WCONTINUED)))\\n\\t\\treturn -EINVAL;\\n\\n\\tswitch (which) {\\n\\tcase P_ALL:\\n\\t\\ttype = PIDTYPE_MAX;\\n\\t\\tbreak;\\n\\tcase P_PID:\\n\\t\\ttype = PIDTYPE_PID;\\n\\t\\tif (upid <= 0)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tpid = find_get_pid(upid);\\n\\t\\tbreak;\\n\\tcase P_PGID:\\n\\t\\ttype = PIDTYPE_PGID;\\n\\t\\tif (upid < 0)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tif (upid)\\n\\t\\t\\tpid = find_get_pid(upid);\\n\\t\\telse\\n\\t\\t\\tpid = get_task_pid(current, PIDTYPE_PGID);\\n\\t\\tbreak;\\n\\tcase P_PIDFD:\\n\\t\\ttype = PIDTYPE_PID;\\n\\t\\tif (upid < 0)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tpid = pidfd_get_pid(upid, &f_flags);\\n\\t\\tif (IS_ERR(pid))\\n\\t\\t\\treturn PTR_ERR(pid);\\n\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\two->wo_type\\t= type;\\n\\two->wo_pid\\t= pid;\\n\\two->wo_flags\\t= options;\\n\\two->wo_info\\t= infop;\\n\\two->wo_rusage\\t= ru;\\n\\tif (f_flags & O_NONBLOCK)\\n\\t\\two->wo_flags |= WNOHANG;\\n\\n\\treturn 0;\\n}\\n\\nstatic long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,\\n\\t\\t\\t  int options, struct rusage *ru)\\n{\\n\\tstruct wait_opts wo;\\n\\tlong ret;\\n\\n\\tret = kernel_waitid_prepare(&wo, which, upid, infop, options, ru);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tret = do_wait(&wo);\\n\\tif (!ret && !(options & WNOHANG) && (wo.wo_flags & WNOHANG))\\n\\t\\tret = -EAGAIN;\\n\\n\\tput_pid(wo.wo_pid);\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\\n\\t\\tinfop, int, options, struct rusage __user *, ru)\\n{\\n\\tstruct rusage r;\\n\\tstruct waitid_info info = {.status = 0};\\n\\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\\n\\tint signo = 0;\\n\\n\\tif (err > 0) {\\n\\t\\tsigno = SIGCHLD;\\n\\t\\terr = 0;\\n\\t\\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\tif (!infop)\\n\\t\\treturn err;\\n\\n\\tif (!user_write_access_begin(infop, sizeof(*infop)))\\n\\t\\treturn -EFAULT;\\n\\n\\tunsafe_put_user(signo, &infop->si_signo, Efault);\\n\\tunsafe_put_user(0, &infop->si_errno, Efault);\\n\\tunsafe_put_user(info.cause, &infop->si_code, Efault);\\n\\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\\n\\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\\n\\tunsafe_put_user(info.status, &infop->si_status, Efault);\\n\\tuser_write_access_end();\\n\\treturn err;\\nEfault:\\n\\tuser_write_access_end();\\n\\treturn -EFAULT;\\n}\\n\\nlong kernel_wait4(pid_t upid, int __user *stat_addr, int options,\\n\\t\\t  struct rusage *ru)\\n{\\n\\tstruct wait_opts wo;\\n\\tstruct pid *pid = NULL;\\n\\tenum pid_type type;\\n\\tlong ret;\\n\\n\\tif (options & ~(WNOHANG|WUNTRACED|WCONTINUED|\\n\\t\\t\\t__WNOTHREAD|__WCLONE|__WALL))\\n\\t\\treturn -EINVAL;\\n\\n\\t/* -INT_MIN is not defined */\\n\\tif (upid == INT_MIN)\\n\\t\\treturn -ESRCH;\\n\\n\\tif (upid == -1)\\n\\t\\ttype = PIDTYPE_MAX;\\n\\telse if (upid < 0) {\\n\\t\\ttype = PIDTYPE_PGID;\\n\\t\\tpid = find_get_pid(-upid);\\n\\t} else if (upid == 0) {\\n\\t\\ttype = PIDTYPE_PGID;\\n\\t\\tpid = get_task_pid(current, PIDTYPE_PGID);\\n\\t} else /* upid > 0 */ {\\n\\t\\ttype = PIDTYPE_PID;\\n\\t\\tpid = find_get_pid(upid);\\n\\t}\\n\\n\\two.wo_type\\t= type;\\n\\two.wo_pid\\t= pid;\\n\\two.wo_flags\\t= options | WEXITED;\\n\\two.wo_info\\t= NULL;\\n\\two.wo_stat\\t= 0;\\n\\two.wo_rusage\\t= ru;\\n\\tret = do_wait(&wo);\\n\\tput_pid(pid);\\n\\tif (ret > 0 && stat_addr && put_user(wo.wo_stat, stat_addr))\\n\\t\\tret = -EFAULT;\\n\\n\\treturn ret;\\n}\\n\\nint kernel_wait(pid_t pid, int *stat)\\n{\\n\\tstruct wait_opts wo = {\\n\\t\\t.wo_type\\t= PIDTYPE_PID,\\n\\t\\t.wo_pid\\t\\t= find_get_pid(pid),\\n\\t\\t.wo_flags\\t= WEXITED,\\n\\t};\\n\\tint ret;\\n\\n\\tret = do_wait(&wo);\\n\\tif (ret > 0 && wo.wo_stat)\\n\\t\\t*stat = wo.wo_stat;\\n\\tput_pid(wo.wo_pid);\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,\\n\\t\\tint, options, struct rusage __user *, ru)\\n{\\n\\tstruct rusage r;\\n\\tlong err = kernel_wait4(upid, stat_addr, options, ru ? &r : NULL);\\n\\n\\tif (err > 0) {\\n\\t\\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\treturn err;\\n}\\n\\n#ifdef __ARCH_WANT_SYS_WAITPID\\n\\n/*\\n * sys_waitpid() remains for compatibility. waitpid() should be\\n * implemented by calling sys_wait4() from libc.a.\\n */\\nSYSCALL_DEFINE3(waitpid, pid_t, pid, int __user *, stat_addr, int, options)\\n{\\n\\treturn kernel_wait4(pid, stat_addr, options, NULL);\\n}\\n\\n#endif\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE4(wait4,\\n\\tcompat_pid_t, pid,\\n\\tcompat_uint_t __user *, stat_addr,\\n\\tint, options,\\n\\tstruct compat_rusage __user *, ru)\\n{\\n\\tstruct rusage r;\\n\\tlong err = kernel_wait4(pid, stat_addr, options, ru ? &r : NULL);\\n\\tif (err > 0) {\\n\\t\\tif (ru && put_compat_rusage(&r, ru))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\treturn err;\\n}\\n\\nCOMPAT_SYSCALL_DEFINE5(waitid,\\n\\t\\tint, which, compat_pid_t, pid,\\n\\t\\tstruct compat_siginfo __user *, infop, int, options,\\n\\t\\tstruct compat_rusage __user *, uru)\\n{\\n\\tstruct rusage ru;\\n\\tstruct waitid_info info = {.status = 0};\\n\\tlong err = kernel_waitid(which, pid, &info, options, uru ? &ru : NULL);\\n\\tint signo = 0;\\n\\tif (err > 0) {\\n\\t\\tsigno = SIGCHLD;\\n\\t\\terr = 0;\\n\\t\\tif (uru) {\\n\\t\\t\\t/* kernel_waitid() overwrites everything in ru */\\n\\t\\t\\tif (COMPAT_USE_64BIT_TIME)\\n\\t\\t\\t\\terr = copy_to_user(uru, &ru, sizeof(ru));\\n\\t\\t\\telse\\n\\t\\t\\t\\terr = put_compat_rusage(&ru, uru);\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\treturn -EFAULT;\\n\\t\\t}\\n\\t}\\n\\n\\tif (!infop)\\n\\t\\treturn err;\\n\\n\\tif (!user_write_access_begin(infop, sizeof(*infop)))\\n\\t\\treturn -EFAULT;\\n\\n\\tunsafe_put_user(signo, &infop->si_signo, Efault);\\n\\tunsafe_put_user(0, &infop->si_errno, Efault);\\n\\tunsafe_put_user(info.cause, &infop->si_code, Efault);\\n\\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\\n\\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\\n\\tunsafe_put_user(info.status, &infop->si_status, Efault);\\n\\tuser_write_access_end();\\n\\treturn err;\\nEfault:\\n\\tuser_write_access_end();\\n\\treturn -EFAULT;\\n}\\n#endif\\n\\n/*\\n * This needs to be __function_aligned as GCC implicitly makes any\\n * implementation of abort() cold and drops alignment specified by\\n * -falign-functions=N.\\n *\\n * See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88345#c11\\n */\\n__weak __function_aligned void abort(void)\\n{\\n\\tBUG();\\n\\n\\t/* if that doesn\\'t kill us, halt */\\n\\tpanic(\"Oops failed to kill thread\");\\n}\\nEXPORT_SYMBOL(abort);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n#include <linux/kernel.h>\\n#include <linux/crash_dump.h>\\n#include <linux/init.h>\\n#include <linux/errno.h>\\n#include <linux/export.h>\\n\\n/*\\n * stores the physical address of elf header of crash image\\n *\\n * Note: elfcorehdr_addr is not just limited to vmcore. It is also used by\\n * is_kdump_kernel() to determine if we are booting after a panic. Hence put\\n * it under CONFIG_CRASH_DUMP and not CONFIG_PROC_VMCORE.\\n */\\nunsigned long long elfcorehdr_addr = ELFCORE_ADDR_MAX;\\nEXPORT_SYMBOL_GPL(elfcorehdr_addr);\\n\\n/*\\n * stores the size of elf header of crash image\\n */\\nunsigned long long elfcorehdr_size;\\n\\n/*\\n * elfcorehdr= specifies the location of elf core header stored by the crashed\\n * kernel. This option will be passed by kexec loader to the capture kernel.\\n *\\n * Syntax: elfcorehdr=[size[KMG]@]offset[KMG]\\n */\\nstatic int __init setup_elfcorehdr(char *arg)\\n{\\n\\tchar *end;\\n\\tif (!arg)\\n\\t\\treturn -EINVAL;\\n\\telfcorehdr_addr = memparse(arg, &end);\\n\\tif (*end == \\'@\\') {\\n\\t\\telfcorehdr_size = elfcorehdr_addr;\\n\\t\\telfcorehdr_addr = memparse(end + 1, &end);\\n\\t}\\n\\treturn end > arg ? 0 : -EINVAL;\\n}\\nearly_param(\"elfcorehdr\", setup_elfcorehdr);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Provide kernel headers useful to build tracing programs\\n * such as for running eBPF tracing tools.\\n *\\n * (Borrowed code from kernel/configs.c)\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/module.h>\\n#include <linux/kobject.h>\\n#include <linux/init.h>\\n\\n/*\\n * Define kernel_headers_data and kernel_headers_data_end, within which the\\n * compressed kernel headers are stored. The file is first compressed with xz.\\n */\\n\\nasm (\\n\"\\t.pushsection .rodata, \\\\\"a\\\\\"\\t\\t\\\\n\"\\n\"\\t.global kernel_headers_data\\t\\t\\\\n\"\\n\"kernel_headers_data:\\t\\t\\t\\t\\\\n\"\\n\"\\t.incbin \\\\\"kernel/kheaders_data.tar.xz\\\\\"\\t\\\\n\"\\n\"\\t.global kernel_headers_data_end\\t\\t\\\\n\"\\n\"kernel_headers_data_end:\\t\\t\\t\\\\n\"\\n\"\\t.popsection\\t\\t\\t\\t\\\\n\"\\n);\\n\\nextern char kernel_headers_data[];\\nextern char kernel_headers_data_end[];\\n\\nstatic ssize_t\\nikheaders_read(struct file *file,  struct kobject *kobj,\\n\\t       struct bin_attribute *bin_attr,\\n\\t       char *buf, loff_t off, size_t len)\\n{\\n\\tmemcpy(buf, &kernel_headers_data[off], len);\\n\\treturn len;\\n}\\n\\nstatic struct bin_attribute kheaders_attr __ro_after_init = {\\n\\t.attr = {\\n\\t\\t.name = \"kheaders.tar.xz\",\\n\\t\\t.mode = 0444,\\n\\t},\\n\\t.read = &ikheaders_read,\\n};\\n\\nstatic int __init ikheaders_init(void)\\n{\\n\\tkheaders_attr.size = (kernel_headers_data_end -\\n\\t\\t\\t      kernel_headers_data);\\n\\treturn sysfs_create_bin_file(kernel_kobj, &kheaders_attr);\\n}\\n\\nstatic void __exit ikheaders_cleanup(void)\\n{\\n\\tsysfs_remove_bin_file(kernel_kobj, &kheaders_attr);\\n}\\n\\nmodule_init(ikheaders_init);\\nmodule_exit(ikheaders_cleanup);\\n\\nMODULE_LICENSE(\"GPL v2\");\\nMODULE_AUTHOR(\"Joel Fernandes\");\\nMODULE_DESCRIPTION(\"Echo the kernel header artifacts used to build the kernel\");\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Clang Control Flow Integrity (CFI) error handling.\\n *\\n * Copyright (C) 2022 Google LLC\\n */\\n\\n#include <linux/cfi.h>\\n\\nenum bug_trap_type report_cfi_failure(struct pt_regs *regs, unsigned long addr,\\n\\t\\t\\t\\t      unsigned long *target, u32 type)\\n{\\n\\tif (target)\\n\\t\\tpr_err(\"CFI failure at %pS (target: %pS; expected type: 0x%08x)\\\\n\",\\n\\t\\t       (void *)addr, (void *)*target, type);\\n\\telse\\n\\t\\tpr_err(\"CFI failure at %pS (no target information)\\\\n\",\\n\\t\\t       (void *)addr);\\n\\n\\tif (IS_ENABLED(CONFIG_CFI_PERMISSIVE)) {\\n\\t\\t__warn(NULL, 0, (void *)addr, 0, regs, NULL);\\n\\t\\treturn BUG_TRAP_TYPE_WARN;\\n\\t}\\n\\n\\treturn BUG_TRAP_TYPE_BUG;\\n}\\n\\n#ifdef CONFIG_ARCH_USES_CFI_TRAPS\\nstatic inline unsigned long trap_address(s32 *p)\\n{\\n\\treturn (unsigned long)((long)p + (long)*p);\\n}\\n\\nstatic bool is_trap(unsigned long addr, s32 *start, s32 *end)\\n{\\n\\ts32 *p;\\n\\n\\tfor (p = start; p < end; ++p) {\\n\\t\\tif (trap_address(p) == addr)\\n\\t\\t\\treturn true;\\n\\t}\\n\\n\\treturn false;\\n}\\n\\n#ifdef CONFIG_MODULES\\n/* Populates `kcfi_trap(_end)?` fields in `struct module`. */\\nvoid module_cfi_finalize(const Elf_Ehdr *hdr, const Elf_Shdr *sechdrs,\\n\\t\\t\\t struct module *mod)\\n{\\n\\tchar *secstrings;\\n\\tunsigned int i;\\n\\n\\tmod->kcfi_traps = NULL;\\n\\tmod->kcfi_traps_end = NULL;\\n\\n\\tsecstrings = (char *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;\\n\\n\\tfor (i = 1; i < hdr->e_shnum; i++) {\\n\\t\\tif (strcmp(secstrings + sechdrs[i].sh_name, \"__kcfi_traps\"))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tmod->kcfi_traps = (s32 *)sechdrs[i].sh_addr;\\n\\t\\tmod->kcfi_traps_end = (s32 *)(sechdrs[i].sh_addr + sechdrs[i].sh_size);\\n\\t\\tbreak;\\n\\t}\\n}\\n\\nstatic bool is_module_cfi_trap(unsigned long addr)\\n{\\n\\tstruct module *mod;\\n\\tbool found = false;\\n\\n\\trcu_read_lock_sched_notrace();\\n\\n\\tmod = __module_address(addr);\\n\\tif (mod)\\n\\t\\tfound = is_trap(addr, mod->kcfi_traps, mod->kcfi_traps_end);\\n\\n\\trcu_read_unlock_sched_notrace();\\n\\n\\treturn found;\\n}\\n#else /* CONFIG_MODULES */\\nstatic inline bool is_module_cfi_trap(unsigned long addr)\\n{\\n\\treturn false;\\n}\\n#endif /* CONFIG_MODULES */\\n\\nextern s32 __start___kcfi_traps[];\\nextern s32 __stop___kcfi_traps[];\\n\\nbool is_cfi_trap(unsigned long addr)\\n{\\n\\tif (is_trap(addr, __start___kcfi_traps, __stop___kcfi_traps))\\n\\t\\treturn true;\\n\\n\\treturn is_module_cfi_trap(addr);\\n}\\n#endif /* CONFIG_ARCH_USES_CFI_TRAPS */\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Supplementary group IDs\\n */\\n#include <linux/cred.h>\\n#include <linux/export.h>\\n#include <linux/slab.h>\\n#include <linux/security.h>\\n#include <linux/sort.h>\\n#include <linux/syscalls.h>\\n#include <linux/user_namespace.h>\\n#include <linux/vmalloc.h>\\n#include <linux/uaccess.h>\\n\\nstruct group_info *groups_alloc(int gidsetsize)\\n{\\n\\tstruct group_info *gi;\\n\\tgi = kvmalloc(struct_size(gi, gid, gidsetsize), GFP_KERNEL_ACCOUNT);\\n\\tif (!gi)\\n\\t\\treturn NULL;\\n\\n\\trefcount_set(&gi->usage, 1);\\n\\tgi->ngroups = gidsetsize;\\n\\treturn gi;\\n}\\n\\nEXPORT_SYMBOL(groups_alloc);\\n\\nvoid groups_free(struct group_info *group_info)\\n{\\n\\tkvfree(group_info);\\n}\\n\\nEXPORT_SYMBOL(groups_free);\\n\\n/* export the group_info to a user-space array */\\nstatic int groups_to_user(gid_t __user *grouplist,\\n\\t\\t\\t  const struct group_info *group_info)\\n{\\n\\tstruct user_namespace *user_ns = current_user_ns();\\n\\tint i;\\n\\tunsigned int count = group_info->ngroups;\\n\\n\\tfor (i = 0; i < count; i++) {\\n\\t\\tgid_t gid;\\n\\t\\tgid = from_kgid_munged(user_ns, group_info->gid[i]);\\n\\t\\tif (put_user(gid, grouplist+i))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\treturn 0;\\n}\\n\\n/* fill a group_info from a user-space array - it must be allocated already */\\nstatic int groups_from_user(struct group_info *group_info,\\n    gid_t __user *grouplist)\\n{\\n\\tstruct user_namespace *user_ns = current_user_ns();\\n\\tint i;\\n\\tunsigned int count = group_info->ngroups;\\n\\n\\tfor (i = 0; i < count; i++) {\\n\\t\\tgid_t gid;\\n\\t\\tkgid_t kgid;\\n\\t\\tif (get_user(gid, grouplist+i))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\tkgid = make_kgid(user_ns, gid);\\n\\t\\tif (!gid_valid(kgid))\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tgroup_info->gid[i] = kgid;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int gid_cmp(const void *_a, const void *_b)\\n{\\n\\tkgid_t a = *(kgid_t *)_a;\\n\\tkgid_t b = *(kgid_t *)_b;\\n\\n\\treturn gid_gt(a, b) - gid_lt(a, b);\\n}\\n\\nvoid groups_sort(struct group_info *group_info)\\n{\\n\\tsort(group_info->gid, group_info->ngroups, sizeof(*group_info->gid),\\n\\t     gid_cmp, NULL);\\n}\\nEXPORT_SYMBOL(groups_sort);\\n\\n/* a simple bsearch */\\nint groups_search(const struct group_info *group_info, kgid_t grp)\\n{\\n\\tunsigned int left, right;\\n\\n\\tif (!group_info)\\n\\t\\treturn 0;\\n\\n\\tleft = 0;\\n\\tright = group_info->ngroups;\\n\\twhile (left < right) {\\n\\t\\tunsigned int mid = (left+right)/2;\\n\\t\\tif (gid_gt(grp, group_info->gid[mid]))\\n\\t\\t\\tleft = mid + 1;\\n\\t\\telse if (gid_lt(grp, group_info->gid[mid]))\\n\\t\\t\\tright = mid;\\n\\t\\telse\\n\\t\\t\\treturn 1;\\n\\t}\\n\\treturn 0;\\n}\\n\\n/**\\n * set_groups - Change a group subscription in a set of credentials\\n * @new: The newly prepared set of credentials to alter\\n * @group_info: The group list to install\\n */\\nvoid set_groups(struct cred *new, struct group_info *group_info)\\n{\\n\\tput_group_info(new->group_info);\\n\\tget_group_info(group_info);\\n\\tnew->group_info = group_info;\\n}\\n\\nEXPORT_SYMBOL(set_groups);\\n\\n/**\\n * set_current_groups - Change current\\'s group subscription\\n * @group_info: The group list to impose\\n *\\n * Validate a group subscription and, if valid, impose it upon current\\'s task\\n * security record.\\n */\\nint set_current_groups(struct group_info *group_info)\\n{\\n\\tstruct cred *new;\\n\\tconst struct cred *old;\\n\\tint retval;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\n\\told = current_cred();\\n\\n\\tset_groups(new, group_info);\\n\\n\\tretval = security_task_fix_setgroups(new, old);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nEXPORT_SYMBOL(set_current_groups);\\n\\nSYSCALL_DEFINE2(getgroups, int, gidsetsize, gid_t __user *, grouplist)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint i;\\n\\n\\tif (gidsetsize < 0)\\n\\t\\treturn -EINVAL;\\n\\n\\t/* no need to grab task_lock here; it cannot change */\\n\\ti = cred->group_info->ngroups;\\n\\tif (gidsetsize) {\\n\\t\\tif (i > gidsetsize) {\\n\\t\\t\\ti = -EINVAL;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tif (groups_to_user(grouplist, cred->group_info)) {\\n\\t\\t\\ti = -EFAULT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\nout:\\n\\treturn i;\\n}\\n\\nbool may_setgroups(void)\\n{\\n\\tstruct user_namespace *user_ns = current_user_ns();\\n\\n\\treturn ns_capable_setid(user_ns, CAP_SETGID) &&\\n\\t\\tuserns_may_setgroups(user_ns);\\n}\\n\\n/*\\n *\\tSMP: Our groups are copy-on-write. We can set them safely\\n *\\twithout another task interfering.\\n */\\n\\nSYSCALL_DEFINE2(setgroups, int, gidsetsize, gid_t __user *, grouplist)\\n{\\n\\tstruct group_info *group_info;\\n\\tint retval;\\n\\n\\tif (!may_setgroups())\\n\\t\\treturn -EPERM;\\n\\tif ((unsigned)gidsetsize > NGROUPS_MAX)\\n\\t\\treturn -EINVAL;\\n\\n\\tgroup_info = groups_alloc(gidsetsize);\\n\\tif (!group_info)\\n\\t\\treturn -ENOMEM;\\n\\tretval = groups_from_user(group_info, grouplist);\\n\\tif (retval) {\\n\\t\\tput_group_info(group_info);\\n\\t\\treturn retval;\\n\\t}\\n\\n\\tgroups_sort(group_info);\\n\\tretval = set_current_groups(group_info);\\n\\tput_group_info(group_info);\\n\\n\\treturn retval;\\n}\\n\\n/*\\n * Check whether we\\'re fsgid/egid or in the supplemental group..\\n */\\nint in_group_p(kgid_t grp)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint retval = 1;\\n\\n\\tif (!gid_eq(grp, cred->fsgid))\\n\\t\\tretval = groups_search(cred->group_info, grp);\\n\\treturn retval;\\n}\\n\\nEXPORT_SYMBOL(in_group_p);\\n\\nint in_egroup_p(kgid_t grp)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint retval = 1;\\n\\n\\tif (!gid_eq(grp, cred->egid))\\n\\t\\tretval = groups_search(cred->group_info, grp);\\n\\treturn retval;\\n}\\n\\nEXPORT_SYMBOL(in_egroup_p);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * crash.c - kernel crash support code.\\n * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/buildid.h>\\n#include <linux/init.h>\\n#include <linux/utsname.h>\\n#include <linux/vmalloc.h>\\n#include <linux/sizes.h>\\n#include <linux/kexec.h>\\n#include <linux/memory.h>\\n#include <linux/mm.h>\\n#include <linux/cpuhotplug.h>\\n#include <linux/memblock.h>\\n#include <linux/kmemleak.h>\\n#include <linux/crash_core.h>\\n#include <linux/reboot.h>\\n#include <linux/btf.h>\\n#include <linux/objtool.h>\\n\\n#include <asm/page.h>\\n#include <asm/sections.h>\\n\\n#include <crypto/sha1.h>\\n\\n#include \"kallsyms_internal.h\"\\n#include \"kexec_internal.h\"\\n\\n/* Per cpu memory for storing cpu states in case of system crash. */\\nnote_buf_t __percpu *crash_notes;\\n\\n#ifdef CONFIG_CRASH_DUMP\\n\\nint kimage_crash_copy_vmcoreinfo(struct kimage *image)\\n{\\n\\tstruct page *vmcoreinfo_page;\\n\\tvoid *safecopy;\\n\\n\\tif (!IS_ENABLED(CONFIG_CRASH_DUMP))\\n\\t\\treturn 0;\\n\\tif (image->type != KEXEC_TYPE_CRASH)\\n\\t\\treturn 0;\\n\\n\\t/*\\n\\t * For kdump, allocate one vmcoreinfo safe copy from the\\n\\t * crash memory. as we have arch_kexec_protect_crashkres()\\n\\t * after kexec syscall, we naturally protect it from write\\n\\t * (even read) access under kernel direct mapping. But on\\n\\t * the other hand, we still need to operate it when crash\\n\\t * happens to generate vmcoreinfo note, hereby we rely on\\n\\t * vmap for this purpose.\\n\\t */\\n\\tvmcoreinfo_page = kimage_alloc_control_pages(image, 0);\\n\\tif (!vmcoreinfo_page) {\\n\\t\\tpr_warn(\"Could not allocate vmcoreinfo buffer\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\tsafecopy = vmap(&vmcoreinfo_page, 1, VM_MAP, PAGE_KERNEL);\\n\\tif (!safecopy) {\\n\\t\\tpr_warn(\"Could not vmap vmcoreinfo buffer\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\timage->vmcoreinfo_data_copy = safecopy;\\n\\tcrash_update_vmcoreinfo_safecopy(safecopy);\\n\\n\\treturn 0;\\n}\\n\\n\\n\\nint kexec_should_crash(struct task_struct *p)\\n{\\n\\t/*\\n\\t * If crash_kexec_post_notifiers is enabled, don\\'t run\\n\\t * crash_kexec() here yet, which must be run after panic\\n\\t * notifiers in panic().\\n\\t */\\n\\tif (crash_kexec_post_notifiers)\\n\\t\\treturn 0;\\n\\t/*\\n\\t * There are 4 panic() calls in make_task_dead() path, each of which\\n\\t * corresponds to each of these 4 conditions.\\n\\t */\\n\\tif (in_interrupt() || !p->pid || is_global_init(p) || panic_on_oops)\\n\\t\\treturn 1;\\n\\treturn 0;\\n}\\n\\nint kexec_crash_loaded(void)\\n{\\n\\treturn !!kexec_crash_image;\\n}\\nEXPORT_SYMBOL_GPL(kexec_crash_loaded);\\n\\n/*\\n * No panic_cpu check version of crash_kexec().  This function is called\\n * only when panic_cpu holds the current CPU number; this is the only CPU\\n * which processes crash_kexec routines.\\n */\\nvoid __noclone __crash_kexec(struct pt_regs *regs)\\n{\\n\\t/* Take the kexec_lock here to prevent sys_kexec_load\\n\\t * running on one cpu from replacing the crash kernel\\n\\t * we are using after a panic on a different cpu.\\n\\t *\\n\\t * If the crash kernel was not located in a fixed area\\n\\t * of memory the xchg(&kexec_crash_image) would be\\n\\t * sufficient.  But since I reuse the memory...\\n\\t */\\n\\tif (kexec_trylock()) {\\n\\t\\tif (kexec_crash_image) {\\n\\t\\t\\tstruct pt_regs fixed_regs;\\n\\n\\t\\t\\tcrash_setup_regs(&fixed_regs, regs);\\n\\t\\t\\tcrash_save_vmcoreinfo();\\n\\t\\t\\tmachine_crash_shutdown(&fixed_regs);\\n\\t\\t\\tmachine_kexec(kexec_crash_image);\\n\\t\\t}\\n\\t\\tkexec_unlock();\\n\\t}\\n}\\nSTACK_FRAME_NON_STANDARD(__crash_kexec);\\n\\n__bpf_kfunc void crash_kexec(struct pt_regs *regs)\\n{\\n\\tint old_cpu, this_cpu;\\n\\n\\t/*\\n\\t * Only one CPU is allowed to execute the crash_kexec() code as with\\n\\t * panic().  Otherwise parallel calls of panic() and crash_kexec()\\n\\t * may stop each other.  To exclude them, we use panic_cpu here too.\\n\\t */\\n\\told_cpu = PANIC_CPU_INVALID;\\n\\tthis_cpu = raw_smp_processor_id();\\n\\n\\tif (atomic_try_cmpxchg(&panic_cpu, &old_cpu, this_cpu)) {\\n\\t\\t/* This is the 1st CPU which comes here, so go ahead. */\\n\\t\\t__crash_kexec(regs);\\n\\n\\t\\t/*\\n\\t\\t * Reset panic_cpu to allow another panic()/crash_kexec()\\n\\t\\t * call.\\n\\t\\t */\\n\\t\\tatomic_set(&panic_cpu, PANIC_CPU_INVALID);\\n\\t}\\n}\\n\\nstatic inline resource_size_t crash_resource_size(const struct resource *res)\\n{\\n\\treturn !res->end ? 0 : resource_size(res);\\n}\\n\\n\\n\\n\\nint crash_prepare_elf64_headers(struct crash_mem *mem, int need_kernel_map,\\n\\t\\t\\t  void **addr, unsigned long *sz)\\n{\\n\\tElf64_Ehdr *ehdr;\\n\\tElf64_Phdr *phdr;\\n\\tunsigned long nr_cpus = num_possible_cpus(), nr_phdr, elf_sz;\\n\\tunsigned char *buf;\\n\\tunsigned int cpu, i;\\n\\tunsigned long long notes_addr;\\n\\tunsigned long mstart, mend;\\n\\n\\t/* extra phdr for vmcoreinfo ELF note */\\n\\tnr_phdr = nr_cpus + 1;\\n\\tnr_phdr += mem->nr_ranges;\\n\\n\\t/*\\n\\t * kexec-tools creates an extra PT_LOAD phdr for kernel text mapping\\n\\t * area (for example, ffffffff80000000 - ffffffffa0000000 on x86_64).\\n\\t * I think this is required by tools like gdb. So same physical\\n\\t * memory will be mapped in two ELF headers. One will contain kernel\\n\\t * text virtual addresses and other will have __va(physical) addresses.\\n\\t */\\n\\n\\tnr_phdr++;\\n\\telf_sz = sizeof(Elf64_Ehdr) + nr_phdr * sizeof(Elf64_Phdr);\\n\\telf_sz = ALIGN(elf_sz, ELF_CORE_HEADER_ALIGN);\\n\\n\\tbuf = vzalloc(elf_sz);\\n\\tif (!buf)\\n\\t\\treturn -ENOMEM;\\n\\n\\tehdr = (Elf64_Ehdr *)buf;\\n\\tphdr = (Elf64_Phdr *)(ehdr + 1);\\n\\tmemcpy(ehdr->e_ident, ELFMAG, SELFMAG);\\n\\tehdr->e_ident[EI_CLASS] = ELFCLASS64;\\n\\tehdr->e_ident[EI_DATA] = ELFDATA2LSB;\\n\\tehdr->e_ident[EI_VERSION] = EV_CURRENT;\\n\\tehdr->e_ident[EI_OSABI] = ELF_OSABI;\\n\\tmemset(ehdr->e_ident + EI_PAD, 0, EI_NIDENT - EI_PAD);\\n\\tehdr->e_type = ET_CORE;\\n\\tehdr->e_machine = ELF_ARCH;\\n\\tehdr->e_version = EV_CURRENT;\\n\\tehdr->e_phoff = sizeof(Elf64_Ehdr);\\n\\tehdr->e_ehsize = sizeof(Elf64_Ehdr);\\n\\tehdr->e_phentsize = sizeof(Elf64_Phdr);\\n\\n\\t/* Prepare one phdr of type PT_NOTE for each possible CPU */\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tphdr->p_type = PT_NOTE;\\n\\t\\tnotes_addr = per_cpu_ptr_to_phys(per_cpu_ptr(crash_notes, cpu));\\n\\t\\tphdr->p_offset = phdr->p_paddr = notes_addr;\\n\\t\\tphdr->p_filesz = phdr->p_memsz = sizeof(note_buf_t);\\n\\t\\t(ehdr->e_phnum)++;\\n\\t\\tphdr++;\\n\\t}\\n\\n\\t/* Prepare one PT_NOTE header for vmcoreinfo */\\n\\tphdr->p_type = PT_NOTE;\\n\\tphdr->p_offset = phdr->p_paddr = paddr_vmcoreinfo_note();\\n\\tphdr->p_filesz = phdr->p_memsz = VMCOREINFO_NOTE_SIZE;\\n\\t(ehdr->e_phnum)++;\\n\\tphdr++;\\n\\n\\t/* Prepare PT_LOAD type program header for kernel text region */\\n\\tif (need_kernel_map) {\\n\\t\\tphdr->p_type = PT_LOAD;\\n\\t\\tphdr->p_flags = PF_R|PF_W|PF_X;\\n\\t\\tphdr->p_vaddr = (unsigned long) _text;\\n\\t\\tphdr->p_filesz = phdr->p_memsz = _end - _text;\\n\\t\\tphdr->p_offset = phdr->p_paddr = __pa_symbol(_text);\\n\\t\\tehdr->e_phnum++;\\n\\t\\tphdr++;\\n\\t}\\n\\n\\t/* Go through all the ranges in mem->ranges[] and prepare phdr */\\n\\tfor (i = 0; i < mem->nr_ranges; i++) {\\n\\t\\tmstart = mem->ranges[i].start;\\n\\t\\tmend = mem->ranges[i].end;\\n\\n\\t\\tphdr->p_type = PT_LOAD;\\n\\t\\tphdr->p_flags = PF_R|PF_W|PF_X;\\n\\t\\tphdr->p_offset  = mstart;\\n\\n\\t\\tphdr->p_paddr = mstart;\\n\\t\\tphdr->p_vaddr = (unsigned long) __va(mstart);\\n\\t\\tphdr->p_filesz = phdr->p_memsz = mend - mstart + 1;\\n\\t\\tphdr->p_align = 0;\\n\\t\\tehdr->e_phnum++;\\n#ifdef CONFIG_KEXEC_FILE\\n\\t\\tkexec_dprintk(\"Crash PT_LOAD ELF header. phdr=%p vaddr=0x%llx, paddr=0x%llx, sz=0x%llx e_phnum=%d p_offset=0x%llx\\\\n\",\\n\\t\\t\\t      phdr, phdr->p_vaddr, phdr->p_paddr, phdr->p_filesz,\\n\\t\\t\\t      ehdr->e_phnum, phdr->p_offset);\\n#endif\\n\\t\\tphdr++;\\n\\t}\\n\\n\\t*addr = buf;\\n\\t*sz = elf_sz;\\n\\treturn 0;\\n}\\n\\nint crash_exclude_mem_range(struct crash_mem *mem,\\n\\t\\t\\t    unsigned long long mstart, unsigned long long mend)\\n{\\n\\tint i;\\n\\tunsigned long long start, end, p_start, p_end;\\n\\n\\tfor (i = 0; i < mem->nr_ranges; i++) {\\n\\t\\tstart = mem->ranges[i].start;\\n\\t\\tend = mem->ranges[i].end;\\n\\t\\tp_start = mstart;\\n\\t\\tp_end = mend;\\n\\n\\t\\tif (p_start > end)\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * Because the memory ranges in mem->ranges are stored in\\n\\t\\t * ascending order, when we detect `p_end < start`, we can\\n\\t\\t * immediately exit the for loop, as the subsequent memory\\n\\t\\t * ranges will definitely be outside the range we are looking\\n\\t\\t * for.\\n\\t\\t */\\n\\t\\tif (p_end < start)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* Truncate any area outside of range */\\n\\t\\tif (p_start < start)\\n\\t\\t\\tp_start = start;\\n\\t\\tif (p_end > end)\\n\\t\\t\\tp_end = end;\\n\\n\\t\\t/* Found completely overlapping range */\\n\\t\\tif (p_start == start && p_end == end) {\\n\\t\\t\\tmemmove(&mem->ranges[i], &mem->ranges[i + 1],\\n\\t\\t\\t\\t(mem->nr_ranges - (i + 1)) * sizeof(mem->ranges[i]));\\n\\t\\t\\ti--;\\n\\t\\t\\tmem->nr_ranges--;\\n\\t\\t} else if (p_start > start && p_end < end) {\\n\\t\\t\\t/* Split original range */\\n\\t\\t\\tif (mem->nr_ranges >= mem->max_nr_ranges)\\n\\t\\t\\t\\treturn -ENOMEM;\\n\\n\\t\\t\\tmemmove(&mem->ranges[i + 2], &mem->ranges[i + 1],\\n\\t\\t\\t\\t(mem->nr_ranges - (i + 1)) * sizeof(mem->ranges[i]));\\n\\n\\t\\t\\tmem->ranges[i].end = p_start - 1;\\n\\t\\t\\tmem->ranges[i + 1].start = p_end + 1;\\n\\t\\t\\tmem->ranges[i + 1].end = end;\\n\\n\\t\\t\\ti++;\\n\\t\\t\\tmem->nr_ranges++;\\n\\t\\t} else if (p_start != start)\\n\\t\\t\\tmem->ranges[i].end = p_start - 1;\\n\\t\\telse\\n\\t\\t\\tmem->ranges[i].start = p_end + 1;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nssize_t crash_get_memory_size(void)\\n{\\n\\tssize_t size = 0;\\n\\n\\tif (!kexec_trylock())\\n\\t\\treturn -EBUSY;\\n\\n\\tsize += crash_resource_size(&crashk_res);\\n\\tsize += crash_resource_size(&crashk_low_res);\\n\\n\\tkexec_unlock();\\n\\treturn size;\\n}\\n\\nstatic int __crash_shrink_memory(struct resource *old_res,\\n\\t\\t\\t\\t unsigned long new_size)\\n{\\n\\tstruct resource *ram_res;\\n\\n\\tram_res = kzalloc(sizeof(*ram_res), GFP_KERNEL);\\n\\tif (!ram_res)\\n\\t\\treturn -ENOMEM;\\n\\n\\tram_res->start = old_res->start + new_size;\\n\\tram_res->end   = old_res->end;\\n\\tram_res->flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM;\\n\\tram_res->name  = \"System RAM\";\\n\\n\\tif (!new_size) {\\n\\t\\trelease_resource(old_res);\\n\\t\\told_res->start = 0;\\n\\t\\told_res->end   = 0;\\n\\t} else {\\n\\t\\tcrashk_res.end = ram_res->start - 1;\\n\\t}\\n\\n\\tcrash_free_reserved_phys_range(ram_res->start, ram_res->end);\\n\\tinsert_resource(&iomem_resource, ram_res);\\n\\n\\treturn 0;\\n}\\n\\nint crash_shrink_memory(unsigned long new_size)\\n{\\n\\tint ret = 0;\\n\\tunsigned long old_size, low_size;\\n\\n\\tif (!kexec_trylock())\\n\\t\\treturn -EBUSY;\\n\\n\\tif (kexec_crash_image) {\\n\\t\\tret = -ENOENT;\\n\\t\\tgoto unlock;\\n\\t}\\n\\n\\tlow_size = crash_resource_size(&crashk_low_res);\\n\\told_size = crash_resource_size(&crashk_res) + low_size;\\n\\tnew_size = roundup(new_size, KEXEC_CRASH_MEM_ALIGN);\\n\\tif (new_size >= old_size) {\\n\\t\\tret = (new_size == old_size) ? 0 : -EINVAL;\\n\\t\\tgoto unlock;\\n\\t}\\n\\n\\t/*\\n\\t * (low_size > new_size) implies that low_size is greater than zero.\\n\\t * This also means that if low_size is zero, the else branch is taken.\\n\\t *\\n\\t * If low_size is greater than 0, (low_size > new_size) indicates that\\n\\t * crashk_low_res also needs to be shrunken. Otherwise, only crashk_res\\n\\t * needs to be shrunken.\\n\\t */\\n\\tif (low_size > new_size) {\\n\\t\\tret = __crash_shrink_memory(&crashk_res, 0);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto unlock;\\n\\n\\t\\tret = __crash_shrink_memory(&crashk_low_res, new_size);\\n\\t} else {\\n\\t\\tret = __crash_shrink_memory(&crashk_res, new_size - low_size);\\n\\t}\\n\\n\\t/* Swap crashk_res and crashk_low_res if needed */\\n\\tif (!crashk_res.end && crashk_low_res.end) {\\n\\t\\tcrashk_res.start = crashk_low_res.start;\\n\\t\\tcrashk_res.end   = crashk_low_res.end;\\n\\t\\trelease_resource(&crashk_low_res);\\n\\t\\tcrashk_low_res.start = 0;\\n\\t\\tcrashk_low_res.end   = 0;\\n\\t\\tinsert_resource(&iomem_resource, &crashk_res);\\n\\t}\\n\\nunlock:\\n\\tkexec_unlock();\\n\\treturn ret;\\n}\\n\\nvoid crash_save_cpu(struct pt_regs *regs, int cpu)\\n{\\n\\tstruct elf_prstatus prstatus;\\n\\tu32 *buf;\\n\\n\\tif ((cpu < 0) || (cpu >= nr_cpu_ids))\\n\\t\\treturn;\\n\\n\\t/* Using ELF notes here is opportunistic.\\n\\t * I need a well defined structure format\\n\\t * for the data I pass, and I need tags\\n\\t * on the data to indicate what information I have\\n\\t * squirrelled away.  ELF notes happen to provide\\n\\t * all of that, so there is no need to invent something new.\\n\\t */\\n\\tbuf = (u32 *)per_cpu_ptr(crash_notes, cpu);\\n\\tif (!buf)\\n\\t\\treturn;\\n\\tmemset(&prstatus, 0, sizeof(prstatus));\\n\\tprstatus.common.pr_pid = current->pid;\\n\\telf_core_copy_regs(&prstatus.pr_reg, regs);\\n\\tbuf = append_elf_note(buf, KEXEC_CORE_NOTE_NAME, NT_PRSTATUS,\\n\\t\\t\\t      &prstatus, sizeof(prstatus));\\n\\tfinal_note(buf);\\n}\\n\\n\\n\\nstatic int __init crash_notes_memory_init(void)\\n{\\n\\t/* Allocate memory for saving cpu registers. */\\n\\tsize_t size, align;\\n\\n\\t/*\\n\\t * crash_notes could be allocated across 2 vmalloc pages when percpu\\n\\t * is vmalloc based . vmalloc doesn\\'t guarantee 2 continuous vmalloc\\n\\t * pages are also on 2 continuous physical pages. In this case the\\n\\t * 2nd part of crash_notes in 2nd page could be lost since only the\\n\\t * starting address and size of crash_notes are exported through sysfs.\\n\\t * Here round up the size of crash_notes to the nearest power of two\\n\\t * and pass it to __alloc_percpu as align value. This can make sure\\n\\t * crash_notes is allocated inside one physical page.\\n\\t */\\n\\tsize = sizeof(note_buf_t);\\n\\talign = min(roundup_pow_of_two(sizeof(note_buf_t)), PAGE_SIZE);\\n\\n\\t/*\\n\\t * Break compile if size is bigger than PAGE_SIZE since crash_notes\\n\\t * definitely will be in 2 pages with that.\\n\\t */\\n\\tBUILD_BUG_ON(size > PAGE_SIZE);\\n\\n\\tcrash_notes = __alloc_percpu(size, align);\\n\\tif (!crash_notes) {\\n\\t\\tpr_warn(\"Memory allocation for saving cpu register states failed\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\treturn 0;\\n}\\nsubsys_initcall(crash_notes_memory_init);\\n\\n#endif /*CONFIG_CRASH_DUMP*/\\n\\n#ifdef CONFIG_CRASH_HOTPLUG\\n#undef pr_fmt\\n#define pr_fmt(fmt) \"crash hp: \" fmt\\n\\n/*\\n * Different than kexec/kdump loading/unloading/jumping/shrinking which\\n * usually rarely happen, there will be many crash hotplug events notified\\n * during one short period, e.g one memory board is hot added and memory\\n * regions are online. So mutex lock  __crash_hotplug_lock is used to\\n * serialize the crash hotplug handling specifically.\\n */\\nstatic DEFINE_MUTEX(__crash_hotplug_lock);\\n#define crash_hotplug_lock() mutex_lock(&__crash_hotplug_lock)\\n#define crash_hotplug_unlock() mutex_unlock(&__crash_hotplug_lock)\\n\\n/*\\n * This routine utilized when the crash_hotplug sysfs node is read.\\n * It reflects the kernel\\'s ability/permission to update the kdump\\n * image directly.\\n */\\nint crash_check_hotplug_support(void)\\n{\\n\\tint rc = 0;\\n\\n\\tcrash_hotplug_lock();\\n\\t/* Obtain lock while reading crash information */\\n\\tif (!kexec_trylock()) {\\n\\t\\tif (!kexec_in_progress)\\n\\t\\t\\tpr_info(\"kexec_trylock() failed, kdump image may be inaccurate\\\\n\");\\n\\t\\tcrash_hotplug_unlock();\\n\\t\\treturn 0;\\n\\t}\\n\\tif (kexec_crash_image) {\\n\\t\\trc = kexec_crash_image->hotplug_support;\\n\\t}\\n\\t/* Release lock now that update complete */\\n\\tkexec_unlock();\\n\\tcrash_hotplug_unlock();\\n\\n\\treturn rc;\\n}\\n\\n/*\\n * To accurately reflect hot un/plug changes of CPU and Memory resources\\n * (including onling and offlining of those resources), the relevant\\n * kexec segments must be updated with latest CPU and Memory resources.\\n *\\n * Architectures must ensure two things for all segments that need\\n * updating during hotplug events:\\n *\\n * 1. Segments must be large enough to accommodate a growing number of\\n *    resources.\\n * 2. Exclude the segments from SHA verification.\\n *\\n * For example, on most architectures, the elfcorehdr (which is passed\\n * to the crash kernel via the elfcorehdr= parameter) must include the\\n * new list of CPUs and memory. To make changes to the elfcorehdr, it\\n * should be large enough to permit a growing number of CPU and Memory\\n * resources. One can estimate the elfcorehdr memory size based on\\n * NR_CPUS_DEFAULT and CRASH_MAX_MEMORY_RANGES. The elfcorehdr is\\n * excluded from SHA verification by default if the architecture\\n * supports crash hotplug.\\n */\\nstatic void crash_handle_hotplug_event(unsigned int hp_action, unsigned int cpu, void *arg)\\n{\\n\\tstruct kimage *image;\\n\\n\\tcrash_hotplug_lock();\\n\\t/* Obtain lock while changing crash information */\\n\\tif (!kexec_trylock()) {\\n\\t\\tif (!kexec_in_progress)\\n\\t\\t\\tpr_info(\"kexec_trylock() failed, kdump image may be inaccurate\\\\n\");\\n\\t\\tcrash_hotplug_unlock();\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* Check kdump is not loaded */\\n\\tif (!kexec_crash_image)\\n\\t\\tgoto out;\\n\\n\\timage = kexec_crash_image;\\n\\n\\t/* Check that kexec segments update is permitted */\\n\\tif (!image->hotplug_support)\\n\\t\\tgoto out;\\n\\n\\tif (hp_action == KEXEC_CRASH_HP_ADD_CPU ||\\n\\t\\thp_action == KEXEC_CRASH_HP_REMOVE_CPU)\\n\\t\\tpr_debug(\"hp_action %u, cpu %u\\\\n\", hp_action, cpu);\\n\\telse\\n\\t\\tpr_debug(\"hp_action %u\\\\n\", hp_action);\\n\\n\\t/*\\n\\t * The elfcorehdr_index is set to -1 when the struct kimage\\n\\t * is allocated. Find the segment containing the elfcorehdr,\\n\\t * if not already found.\\n\\t */\\n\\tif (image->elfcorehdr_index < 0) {\\n\\t\\tunsigned long mem;\\n\\t\\tunsigned char *ptr;\\n\\t\\tunsigned int n;\\n\\n\\t\\tfor (n = 0; n < image->nr_segments; n++) {\\n\\t\\t\\tmem = image->segment[n].mem;\\n\\t\\t\\tptr = kmap_local_page(pfn_to_page(mem >> PAGE_SHIFT));\\n\\t\\t\\tif (ptr) {\\n\\t\\t\\t\\t/* The segment containing elfcorehdr */\\n\\t\\t\\t\\tif (memcmp(ptr, ELFMAG, SELFMAG) == 0)\\n\\t\\t\\t\\t\\timage->elfcorehdr_index = (int)n;\\n\\t\\t\\t\\tkunmap_local(ptr);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\tif (image->elfcorehdr_index < 0) {\\n\\t\\tpr_err(\"unable to locate elfcorehdr segment\");\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t/* Needed in order for the segments to be updated */\\n\\tarch_kexec_unprotect_crashkres();\\n\\n\\t/* Differentiate between normal load and hotplug update */\\n\\timage->hp_action = hp_action;\\n\\n\\t/* Now invoke arch-specific update handler */\\n\\tarch_crash_handle_hotplug_event(image, arg);\\n\\n\\t/* No longer handling a hotplug event */\\n\\timage->hp_action = KEXEC_CRASH_HP_NONE;\\n\\timage->elfcorehdr_updated = true;\\n\\n\\t/* Change back to read-only */\\n\\tarch_kexec_protect_crashkres();\\n\\n\\t/* Errors in the callback is not a reason to rollback state */\\nout:\\n\\t/* Release lock now that update complete */\\n\\tkexec_unlock();\\n\\tcrash_hotplug_unlock();\\n}\\n\\nstatic int crash_memhp_notifier(struct notifier_block *nb, unsigned long val, void *arg)\\n{\\n\\tswitch (val) {\\n\\tcase MEM_ONLINE:\\n\\t\\tcrash_handle_hotplug_event(KEXEC_CRASH_HP_ADD_MEMORY,\\n\\t\\t\\tKEXEC_CRASH_HP_INVALID_CPU, arg);\\n\\t\\tbreak;\\n\\n\\tcase MEM_OFFLINE:\\n\\t\\tcrash_handle_hotplug_event(KEXEC_CRASH_HP_REMOVE_MEMORY,\\n\\t\\t\\tKEXEC_CRASH_HP_INVALID_CPU, arg);\\n\\t\\tbreak;\\n\\t}\\n\\treturn NOTIFY_OK;\\n}\\n\\nstatic struct notifier_block crash_memhp_nb = {\\n\\t.notifier_call = crash_memhp_notifier,\\n\\t.priority = 0\\n};\\n\\nstatic int crash_cpuhp_online(unsigned int cpu)\\n{\\n\\tcrash_handle_hotplug_event(KEXEC_CRASH_HP_ADD_CPU, cpu, NULL);\\n\\treturn 0;\\n}\\n\\nstatic int crash_cpuhp_offline(unsigned int cpu)\\n{\\n\\tcrash_handle_hotplug_event(KEXEC_CRASH_HP_REMOVE_CPU, cpu, NULL);\\n\\treturn 0;\\n}\\n\\nstatic int __init crash_hotplug_init(void)\\n{\\n\\tint result = 0;\\n\\n\\tif (IS_ENABLED(CONFIG_MEMORY_HOTPLUG))\\n\\t\\tregister_memory_notifier(&crash_memhp_nb);\\n\\n\\tif (IS_ENABLED(CONFIG_HOTPLUG_CPU)) {\\n\\t\\tresult = cpuhp_setup_state_nocalls(CPUHP_BP_PREPARE_DYN,\\n\\t\\t\\t\"crash/cpuhp\", crash_cpuhp_online, crash_cpuhp_offline);\\n\\t}\\n\\n\\treturn result;\\n}\\n\\nsubsys_initcall(crash_hotplug_init);\\n#endif\\n\\n/* CPU control.\\n * (C) 2001, 2002, 2003, 2004 Rusty Russell\\n *\\n * This code is licenced under the GPL.\\n */\\n#include <linux/sched/mm.h>\\n#include <linux/proc_fs.h>\\n#include <linux/smp.h>\\n#include <linux/init.h>\\n#include <linux/notifier.h>\\n#include <linux/sched/signal.h>\\n#include <linux/sched/hotplug.h>\\n#include <linux/sched/isolation.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/smt.h>\\n#include <linux/unistd.h>\\n#include <linux/cpu.h>\\n#include <linux/oom.h>\\n#include <linux/rcupdate.h>\\n#include <linux/delay.h>\\n#include <linux/export.h>\\n#include <linux/bug.h>\\n#include <linux/kthread.h>\\n#include <linux/stop_machine.h>\\n#include <linux/mutex.h>\\n#include <linux/gfp.h>\\n#include <linux/suspend.h>\\n#include <linux/lockdep.h>\\n#include <linux/tick.h>\\n#include <linux/irq.h>\\n#include <linux/nmi.h>\\n#include <linux/smpboot.h>\\n#include <linux/relay.h>\\n#include <linux/slab.h>\\n#include <linux/scs.h>\\n#include <linux/percpu-rwsem.h>\\n#include <linux/cpuset.h>\\n#include <linux/random.h>\\n#include <linux/cc_platform.h>\\n\\n#include <trace/events/power.h>\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/cpuhp.h>\\n\\n#include \"smpboot.h\"\\n\\n/**\\n * struct cpuhp_cpu_state - Per cpu hotplug state storage\\n * @state:\\tThe current cpu state\\n * @target:\\tThe target state\\n * @fail:\\tCurrent CPU hotplug callback state\\n * @thread:\\tPointer to the hotplug thread\\n * @should_run:\\tThread should execute\\n * @rollback:\\tPerform a rollback\\n * @single:\\tSingle callback invocation\\n * @bringup:\\tSingle callback bringup or teardown selector\\n * @node:\\tRemote CPU node; for multi-instance, do a\\n *\\t\\tsingle entry callback for install/remove\\n * @last:\\tFor multi-instance rollback, remember how far we got\\n * @cb_state:\\tThe state for a single callback (install/uninstall)\\n * @result:\\tResult of the operation\\n * @ap_sync_state:\\tState for AP synchronization\\n * @done_up:\\tSignal completion to the issuer of the task for cpu-up\\n * @done_down:\\tSignal completion to the issuer of the task for cpu-down\\n */\\nstruct cpuhp_cpu_state {\\n\\tenum cpuhp_state\\tstate;\\n\\tenum cpuhp_state\\ttarget;\\n\\tenum cpuhp_state\\tfail;\\n#ifdef CONFIG_SMP\\n\\tstruct task_struct\\t*thread;\\n\\tbool\\t\\t\\tshould_run;\\n\\tbool\\t\\t\\trollback;\\n\\tbool\\t\\t\\tsingle;\\n\\tbool\\t\\t\\tbringup;\\n\\tstruct hlist_node\\t*node;\\n\\tstruct hlist_node\\t*last;\\n\\tenum cpuhp_state\\tcb_state;\\n\\tint\\t\\t\\tresult;\\n\\tatomic_t\\t\\tap_sync_state;\\n\\tstruct completion\\tdone_up;\\n\\tstruct completion\\tdone_down;\\n#endif\\n};\\n\\nstatic DEFINE_PER_CPU(struct cpuhp_cpu_state, cpuhp_state) = {\\n\\t.fail = CPUHP_INVALID,\\n};\\n\\n#ifdef CONFIG_SMP\\ncpumask_t cpus_booted_once_mask;\\n#endif\\n\\n#if defined(CONFIG_LOCKDEP) && defined(CONFIG_SMP)\\nstatic struct lockdep_map cpuhp_state_up_map =\\n\\tSTATIC_LOCKDEP_MAP_INIT(\"cpuhp_state-up\", &cpuhp_state_up_map);\\nstatic struct lockdep_map cpuhp_state_down_map =\\n\\tSTATIC_LOCKDEP_MAP_INIT(\"cpuhp_state-down\", &cpuhp_state_down_map);\\n\\n\\nstatic inline void cpuhp_lock_acquire(bool bringup)\\n{\\n\\tlock_map_acquire(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);\\n}\\n\\nstatic inline void cpuhp_lock_release(bool bringup)\\n{\\n\\tlock_map_release(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);\\n}\\n#else\\n\\nstatic inline void cpuhp_lock_acquire(bool bringup) { }\\nstatic inline void cpuhp_lock_release(bool bringup) { }\\n\\n#endif\\n\\n/**\\n * struct cpuhp_step - Hotplug state machine step\\n * @name:\\tName of the step\\n * @startup:\\tStartup function of the step\\n * @teardown:\\tTeardown function of the step\\n * @cant_stop:\\tBringup/teardown can\\'t be stopped at this step\\n * @multi_instance:\\tState has multiple instances which get added afterwards\\n */\\nstruct cpuhp_step {\\n\\tconst char\\t\\t*name;\\n\\tunion {\\n\\t\\tint\\t\\t(*single)(unsigned int cpu);\\n\\t\\tint\\t\\t(*multi)(unsigned int cpu,\\n\\t\\t\\t\\t\\t struct hlist_node *node);\\n\\t} startup;\\n\\tunion {\\n\\t\\tint\\t\\t(*single)(unsigned int cpu);\\n\\t\\tint\\t\\t(*multi)(unsigned int cpu,\\n\\t\\t\\t\\t\\t struct hlist_node *node);\\n\\t} teardown;\\n\\t/* private: */\\n\\tstruct hlist_head\\tlist;\\n\\t/* public: */\\n\\tbool\\t\\t\\tcant_stop;\\n\\tbool\\t\\t\\tmulti_instance;\\n};\\n\\nstatic DEFINE_MUTEX(cpuhp_state_mutex);\\nstatic struct cpuhp_step cpuhp_hp_states[];\\n\\nstatic struct cpuhp_step *cpuhp_get_step(enum cpuhp_state state)\\n{\\n\\treturn cpuhp_hp_states + state;\\n}\\n\\nstatic bool cpuhp_step_empty(bool bringup, struct cpuhp_step *step)\\n{\\n\\treturn bringup ? !step->startup.single : !step->teardown.single;\\n}\\n\\n/**\\n * cpuhp_invoke_callback - Invoke the callbacks for a given state\\n * @cpu:\\tThe cpu for which the callback should be invoked\\n * @state:\\tThe state to do callbacks for\\n * @bringup:\\tTrue if the bringup callback should be invoked\\n * @node:\\tFor multi-instance, do a single entry callback for install/remove\\n * @lastp:\\tFor multi-instance rollback, remember how far we got\\n *\\n * Called from cpu hotplug and from the state register machinery.\\n *\\n * Return: %0 on success or a negative errno code\\n */\\nstatic int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state state,\\n\\t\\t\\t\\t bool bringup, struct hlist_node *node,\\n\\t\\t\\t\\t struct hlist_node **lastp)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tstruct cpuhp_step *step = cpuhp_get_step(state);\\n\\tint (*cbm)(unsigned int cpu, struct hlist_node *node);\\n\\tint (*cb)(unsigned int cpu);\\n\\tint ret, cnt;\\n\\n\\tif (st->fail == state) {\\n\\t\\tst->fail = CPUHP_INVALID;\\n\\t\\treturn -EAGAIN;\\n\\t}\\n\\n\\tif (cpuhp_step_empty(bringup, step)) {\\n\\t\\tWARN_ON_ONCE(1);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (!step->multi_instance) {\\n\\t\\tWARN_ON_ONCE(lastp && *lastp);\\n\\t\\tcb = bringup ? step->startup.single : step->teardown.single;\\n\\n\\t\\ttrace_cpuhp_enter(cpu, st->target, state, cb);\\n\\t\\tret = cb(cpu);\\n\\t\\ttrace_cpuhp_exit(cpu, st->state, state, ret);\\n\\t\\treturn ret;\\n\\t}\\n\\tcbm = bringup ? step->startup.multi : step->teardown.multi;\\n\\n\\t/* Single invocation for instance add/remove */\\n\\tif (node) {\\n\\t\\tWARN_ON_ONCE(lastp && *lastp);\\n\\t\\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\\n\\t\\tret = cbm(cpu, node);\\n\\t\\ttrace_cpuhp_exit(cpu, st->state, state, ret);\\n\\t\\treturn ret;\\n\\t}\\n\\n\\t/* State transition. Invoke on all instances */\\n\\tcnt = 0;\\n\\thlist_for_each(node, &step->list) {\\n\\t\\tif (lastp && node == *lastp)\\n\\t\\t\\tbreak;\\n\\n\\t\\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\\n\\t\\tret = cbm(cpu, node);\\n\\t\\ttrace_cpuhp_exit(cpu, st->state, state, ret);\\n\\t\\tif (ret) {\\n\\t\\t\\tif (!lastp)\\n\\t\\t\\t\\tgoto err;\\n\\n\\t\\t\\t*lastp = node;\\n\\t\\t\\treturn ret;\\n\\t\\t}\\n\\t\\tcnt++;\\n\\t}\\n\\tif (lastp)\\n\\t\\t*lastp = NULL;\\n\\treturn 0;\\nerr:\\n\\t/* Rollback the instances if one failed */\\n\\tcbm = !bringup ? step->startup.multi : step->teardown.multi;\\n\\tif (!cbm)\\n\\t\\treturn ret;\\n\\n\\thlist_for_each(node, &step->list) {\\n\\t\\tif (!cnt--)\\n\\t\\t\\tbreak;\\n\\n\\t\\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\\n\\t\\tret = cbm(cpu, node);\\n\\t\\ttrace_cpuhp_exit(cpu, st->state, state, ret);\\n\\t\\t/*\\n\\t\\t * Rollback must not fail,\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(ret);\\n\\t}\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_SMP\\nstatic bool cpuhp_is_ap_state(enum cpuhp_state state)\\n{\\n\\t/*\\n\\t * The extra check for CPUHP_TEARDOWN_CPU is only for documentation\\n\\t * purposes as that state is handled explicitly in cpu_down.\\n\\t */\\n\\treturn state > CPUHP_BRINGUP_CPU && state != CPUHP_TEARDOWN_CPU;\\n}\\n\\nstatic inline void wait_for_ap_thread(struct cpuhp_cpu_state *st, bool bringup)\\n{\\n\\tstruct completion *done = bringup ? &st->done_up : &st->done_down;\\n\\twait_for_completion(done);\\n}\\n\\nstatic inline void complete_ap_thread(struct cpuhp_cpu_state *st, bool bringup)\\n{\\n\\tstruct completion *done = bringup ? &st->done_up : &st->done_down;\\n\\tcomplete(done);\\n}\\n\\n/*\\n * The former STARTING/DYING states, ran with IRQs disabled and must not fail.\\n */\\nstatic bool cpuhp_is_atomic_state(enum cpuhp_state state)\\n{\\n\\treturn CPUHP_AP_IDLE_DEAD <= state && state < CPUHP_AP_ONLINE;\\n}\\n\\n/* Synchronization state management */\\nenum cpuhp_sync_state {\\n\\tSYNC_STATE_DEAD,\\n\\tSYNC_STATE_KICKED,\\n\\tSYNC_STATE_SHOULD_DIE,\\n\\tSYNC_STATE_ALIVE,\\n\\tSYNC_STATE_SHOULD_ONLINE,\\n\\tSYNC_STATE_ONLINE,\\n};\\n\\n#ifdef CONFIG_HOTPLUG_CORE_SYNC\\n/**\\n * cpuhp_ap_update_sync_state - Update synchronization state during bringup/teardown\\n * @state:\\tThe synchronization state to set\\n *\\n * No synchronization point. Just update of the synchronization state, but implies\\n * a full barrier so that the AP changes are visible before the control CPU proceeds.\\n */\\nstatic inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state)\\n{\\n\\tatomic_t *st = this_cpu_ptr(&cpuhp_state.ap_sync_state);\\n\\n\\t(void)atomic_xchg(st, state);\\n}\\n\\nvoid __weak arch_cpuhp_sync_state_poll(void) { cpu_relax(); }\\n\\nstatic bool cpuhp_wait_for_sync_state(unsigned int cpu, enum cpuhp_sync_state state,\\n\\t\\t\\t\\t      enum cpuhp_sync_state next_state)\\n{\\n\\tatomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);\\n\\tktime_t now, end, start = ktime_get();\\n\\tint sync;\\n\\n\\tend = start + 10ULL * NSEC_PER_SEC;\\n\\n\\tsync = atomic_read(st);\\n\\twhile (1) {\\n\\t\\tif (sync == state) {\\n\\t\\t\\tif (!atomic_try_cmpxchg(st, &sync, next_state))\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\treturn true;\\n\\t\\t}\\n\\n\\t\\tnow = ktime_get();\\n\\t\\tif (now > end) {\\n\\t\\t\\t/* Timeout. Leave the state unchanged */\\n\\t\\t\\treturn false;\\n\\t\\t} else if (now - start < NSEC_PER_MSEC) {\\n\\t\\t\\t/* Poll for one millisecond */\\n\\t\\t\\tarch_cpuhp_sync_state_poll();\\n\\t\\t} else {\\n\\t\\t\\tusleep_range(USEC_PER_MSEC, 2 * USEC_PER_MSEC);\\n\\t\\t}\\n\\t\\tsync = atomic_read(st);\\n\\t}\\n\\treturn true;\\n}\\n#else  /* CONFIG_HOTPLUG_CORE_SYNC */\\nstatic inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state) { }\\n#endif /* !CONFIG_HOTPLUG_CORE_SYNC */\\n\\n#ifdef CONFIG_HOTPLUG_CORE_SYNC_DEAD\\n/**\\n * cpuhp_ap_report_dead - Update synchronization state to DEAD\\n *\\n * No synchronization point. Just update of the synchronization state.\\n */\\nvoid cpuhp_ap_report_dead(void)\\n{\\n\\tcpuhp_ap_update_sync_state(SYNC_STATE_DEAD);\\n}\\n\\nvoid __weak arch_cpuhp_cleanup_dead_cpu(unsigned int cpu) { }\\n\\n/*\\n * Late CPU shutdown synchronization point. Cannot use cpuhp_state::done_down\\n * because the AP cannot issue complete() at this stage.\\n */\\nstatic void cpuhp_bp_sync_dead(unsigned int cpu)\\n{\\n\\tatomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);\\n\\tint sync = atomic_read(st);\\n\\n\\tdo {\\n\\t\\t/* CPU can have reported dead already. Don\\'t overwrite that! */\\n\\t\\tif (sync == SYNC_STATE_DEAD)\\n\\t\\t\\tbreak;\\n\\t} while (!atomic_try_cmpxchg(st, &sync, SYNC_STATE_SHOULD_DIE));\\n\\n\\tif (cpuhp_wait_for_sync_state(cpu, SYNC_STATE_DEAD, SYNC_STATE_DEAD)) {\\n\\t\\t/* CPU reached dead state. Invoke the cleanup function */\\n\\t\\tarch_cpuhp_cleanup_dead_cpu(cpu);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* No further action possible. Emit message and give up. */\\n\\tpr_err(\"CPU%u failed to report dead state\\\\n\", cpu);\\n}\\n#else /* CONFIG_HOTPLUG_CORE_SYNC_DEAD */\\nstatic inline void cpuhp_bp_sync_dead(unsigned int cpu) { }\\n#endif /* !CONFIG_HOTPLUG_CORE_SYNC_DEAD */\\n\\n#ifdef CONFIG_HOTPLUG_CORE_SYNC_FULL\\n/**\\n * cpuhp_ap_sync_alive - Synchronize AP with the control CPU once it is alive\\n *\\n * Updates the AP synchronization state to SYNC_STATE_ALIVE and waits\\n * for the BP to release it.\\n */\\nvoid cpuhp_ap_sync_alive(void)\\n{\\n\\tatomic_t *st = this_cpu_ptr(&cpuhp_state.ap_sync_state);\\n\\n\\tcpuhp_ap_update_sync_state(SYNC_STATE_ALIVE);\\n\\n\\t/* Wait for the control CPU to release it. */\\n\\twhile (atomic_read(st) != SYNC_STATE_SHOULD_ONLINE)\\n\\t\\tcpu_relax();\\n}\\n\\nstatic bool cpuhp_can_boot_ap(unsigned int cpu)\\n{\\n\\tatomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);\\n\\tint sync = atomic_read(st);\\n\\nagain:\\n\\tswitch (sync) {\\n\\tcase SYNC_STATE_DEAD:\\n\\t\\t/* CPU is properly dead */\\n\\t\\tbreak;\\n\\tcase SYNC_STATE_KICKED:\\n\\t\\t/* CPU did not come up in previous attempt */\\n\\t\\tbreak;\\n\\tcase SYNC_STATE_ALIVE:\\n\\t\\t/* CPU is stuck cpuhp_ap_sync_alive(). */\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\t/* CPU failed to report online or dead and is in limbo state. */\\n\\t\\treturn false;\\n\\t}\\n\\n\\t/* Prepare for booting */\\n\\tif (!atomic_try_cmpxchg(st, &sync, SYNC_STATE_KICKED))\\n\\t\\tgoto again;\\n\\n\\treturn true;\\n}\\n\\nvoid __weak arch_cpuhp_cleanup_kick_cpu(unsigned int cpu) { }\\n\\n/*\\n * Early CPU bringup synchronization point. Cannot use cpuhp_state::done_up\\n * because the AP cannot issue complete() so early in the bringup.\\n */\\nstatic int cpuhp_bp_sync_alive(unsigned int cpu)\\n{\\n\\tint ret = 0;\\n\\n\\tif (!IS_ENABLED(CONFIG_HOTPLUG_CORE_SYNC_FULL))\\n\\t\\treturn 0;\\n\\n\\tif (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {\\n\\t\\tpr_err(\"CPU%u failed to report alive state\\\\n\", cpu);\\n\\t\\tret = -EIO;\\n\\t}\\n\\n\\t/* Let the architecture cleanup the kick alive mechanics. */\\n\\tarch_cpuhp_cleanup_kick_cpu(cpu);\\n\\treturn ret;\\n}\\n#else /* CONFIG_HOTPLUG_CORE_SYNC_FULL */\\nstatic inline int cpuhp_bp_sync_alive(unsigned int cpu) { return 0; }\\nstatic inline bool cpuhp_can_boot_ap(unsigned int cpu) { return true; }\\n#endif /* !CONFIG_HOTPLUG_CORE_SYNC_FULL */\\n\\n/* Serializes the updates to cpu_online_mask, cpu_present_mask */\\nstatic DEFINE_MUTEX(cpu_add_remove_lock);\\nbool cpuhp_tasks_frozen;\\nEXPORT_SYMBOL_GPL(cpuhp_tasks_frozen);\\n\\n/*\\n * The following two APIs (cpu_maps_update_begin/done) must be used when\\n * attempting to serialize the updates to cpu_online_mask & cpu_present_mask.\\n */\\nvoid cpu_maps_update_begin(void)\\n{\\n\\tmutex_lock(&cpu_add_remove_lock);\\n}\\n\\nvoid cpu_maps_update_done(void)\\n{\\n\\tmutex_unlock(&cpu_add_remove_lock);\\n}\\n\\n/*\\n * If set, cpu_up and cpu_down will return -EBUSY and do nothing.\\n * Should always be manipulated under cpu_add_remove_lock\\n */\\nstatic int cpu_hotplug_disabled;\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\nDEFINE_STATIC_PERCPU_RWSEM(cpu_hotplug_lock);\\n\\nstatic bool cpu_hotplug_offline_disabled __ro_after_init;\\n\\nvoid cpus_read_lock(void)\\n{\\n\\tpercpu_down_read(&cpu_hotplug_lock);\\n}\\nEXPORT_SYMBOL_GPL(cpus_read_lock);\\n\\nint cpus_read_trylock(void)\\n{\\n\\treturn percpu_down_read_trylock(&cpu_hotplug_lock);\\n}\\nEXPORT_SYMBOL_GPL(cpus_read_trylock);\\n\\nvoid cpus_read_unlock(void)\\n{\\n\\tpercpu_up_read(&cpu_hotplug_lock);\\n}\\nEXPORT_SYMBOL_GPL(cpus_read_unlock);\\n\\nvoid cpus_write_lock(void)\\n{\\n\\tpercpu_down_write(&cpu_hotplug_lock);\\n}\\n\\nvoid cpus_write_unlock(void)\\n{\\n\\tpercpu_up_write(&cpu_hotplug_lock);\\n}\\n\\nvoid lockdep_assert_cpus_held(void)\\n{\\n\\t/*\\n\\t * We can\\'t have hotplug operations before userspace starts running,\\n\\t * and some init codepaths will knowingly not take the hotplug lock.\\n\\t * This is all valid, so mute lockdep until it makes sense to report\\n\\t * unheld locks.\\n\\t */\\n\\tif (system_state < SYSTEM_RUNNING)\\n\\t\\treturn;\\n\\n\\tpercpu_rwsem_assert_held(&cpu_hotplug_lock);\\n}\\n\\n#ifdef CONFIG_LOCKDEP\\nint lockdep_is_cpus_held(void)\\n{\\n\\treturn percpu_rwsem_is_held(&cpu_hotplug_lock);\\n}\\n#endif\\n\\nstatic void lockdep_acquire_cpus_lock(void)\\n{\\n\\trwsem_acquire(&cpu_hotplug_lock.dep_map, 0, 0, _THIS_IP_);\\n}\\n\\nstatic void lockdep_release_cpus_lock(void)\\n{\\n\\trwsem_release(&cpu_hotplug_lock.dep_map, _THIS_IP_);\\n}\\n\\n/* Declare CPU offlining not supported */\\nvoid cpu_hotplug_disable_offlining(void)\\n{\\n\\tcpu_maps_update_begin();\\n\\tcpu_hotplug_offline_disabled = true;\\n\\tcpu_maps_update_done();\\n}\\n\\n/*\\n * Wait for currently running CPU hotplug operations to complete (if any) and\\n * disable future CPU hotplug (from sysfs). The \\'cpu_add_remove_lock\\' protects\\n * the \\'cpu_hotplug_disabled\\' flag. The same lock is also acquired by the\\n * hotplug path before performing hotplug operations. So acquiring that lock\\n * guarantees mutual exclusion from any currently running hotplug operations.\\n */\\nvoid cpu_hotplug_disable(void)\\n{\\n\\tcpu_maps_update_begin();\\n\\tcpu_hotplug_disabled++;\\n\\tcpu_maps_update_done();\\n}\\nEXPORT_SYMBOL_GPL(cpu_hotplug_disable);\\n\\nstatic void __cpu_hotplug_enable(void)\\n{\\n\\tif (WARN_ONCE(!cpu_hotplug_disabled, \"Unbalanced cpu hotplug enable\\\\n\"))\\n\\t\\treturn;\\n\\tcpu_hotplug_disabled--;\\n}\\n\\nvoid cpu_hotplug_enable(void)\\n{\\n\\tcpu_maps_update_begin();\\n\\t__cpu_hotplug_enable();\\n\\tcpu_maps_update_done();\\n}\\nEXPORT_SYMBOL_GPL(cpu_hotplug_enable);\\n\\n#else\\n\\nstatic void lockdep_acquire_cpus_lock(void)\\n{\\n}\\n\\nstatic void lockdep_release_cpus_lock(void)\\n{\\n}\\n\\n#endif\\t/* CONFIG_HOTPLUG_CPU */\\n\\n/*\\n * Architectures that need SMT-specific errata handling during SMT hotplug\\n * should override this.\\n */\\nvoid __weak arch_smt_update(void) { }\\n\\n#ifdef CONFIG_HOTPLUG_SMT\\n\\nenum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;\\nstatic unsigned int cpu_smt_max_threads __ro_after_init;\\nunsigned int cpu_smt_num_threads __read_mostly = UINT_MAX;\\n\\nvoid __init cpu_smt_disable(bool force)\\n{\\n\\tif (!cpu_smt_possible())\\n\\t\\treturn;\\n\\n\\tif (force) {\\n\\t\\tpr_info(\"SMT: Force disabled\\\\n\");\\n\\t\\tcpu_smt_control = CPU_SMT_FORCE_DISABLED;\\n\\t} else {\\n\\t\\tpr_info(\"SMT: disabled\\\\n\");\\n\\t\\tcpu_smt_control = CPU_SMT_DISABLED;\\n\\t}\\n\\tcpu_smt_num_threads = 1;\\n}\\n\\n/*\\n * The decision whether SMT is supported can only be done after the full\\n * CPU identification. Called from architecture code.\\n */\\nvoid __init cpu_smt_set_num_threads(unsigned int num_threads,\\n\\t\\t\\t\\t    unsigned int max_threads)\\n{\\n\\tWARN_ON(!num_threads || (num_threads > max_threads));\\n\\n\\tif (max_threads == 1)\\n\\t\\tcpu_smt_control = CPU_SMT_NOT_SUPPORTED;\\n\\n\\tcpu_smt_max_threads = max_threads;\\n\\n\\t/*\\n\\t * If SMT has been disabled via the kernel command line or SMT is\\n\\t * not supported, set cpu_smt_num_threads to 1 for consistency.\\n\\t * If enabled, take the architecture requested number of threads\\n\\t * to bring up into account.\\n\\t */\\n\\tif (cpu_smt_control != CPU_SMT_ENABLED)\\n\\t\\tcpu_smt_num_threads = 1;\\n\\telse if (num_threads < cpu_smt_num_threads)\\n\\t\\tcpu_smt_num_threads = num_threads;\\n}\\n\\nstatic int __init smt_cmdline_disable(char *str)\\n{\\n\\tcpu_smt_disable(str && !strcmp(str, \"force\"));\\n\\treturn 0;\\n}\\nearly_param(\"nosmt\", smt_cmdline_disable);\\n\\n/*\\n * For Archicture supporting partial SMT states check if the thread is allowed.\\n * Otherwise this has already been checked through cpu_smt_max_threads when\\n * setting the SMT level.\\n */\\nstatic inline bool cpu_smt_thread_allowed(unsigned int cpu)\\n{\\n#ifdef CONFIG_SMT_NUM_THREADS_DYNAMIC\\n\\treturn topology_smt_thread_allowed(cpu);\\n#else\\n\\treturn true;\\n#endif\\n}\\n\\nstatic inline bool cpu_bootable(unsigned int cpu)\\n{\\n\\tif (cpu_smt_control == CPU_SMT_ENABLED && cpu_smt_thread_allowed(cpu))\\n\\t\\treturn true;\\n\\n\\t/* All CPUs are bootable if controls are not configured */\\n\\tif (cpu_smt_control == CPU_SMT_NOT_IMPLEMENTED)\\n\\t\\treturn true;\\n\\n\\t/* All CPUs are bootable if CPU is not SMT capable */\\n\\tif (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)\\n\\t\\treturn true;\\n\\n\\tif (topology_is_primary_thread(cpu))\\n\\t\\treturn true;\\n\\n\\t/*\\n\\t * On x86 it\\'s required to boot all logical CPUs at least once so\\n\\t * that the init code can get a chance to set CR4.MCE on each\\n\\t * CPU. Otherwise, a broadcasted MCE observing CR4.MCE=0b on any\\n\\t * core will shutdown the machine.\\n\\t */\\n\\treturn !cpumask_test_cpu(cpu, &cpus_booted_once_mask);\\n}\\n\\n/* Returns true if SMT is supported and not forcefully (irreversibly) disabled */\\nbool cpu_smt_possible(void)\\n{\\n\\treturn cpu_smt_control != CPU_SMT_FORCE_DISABLED &&\\n\\t\\tcpu_smt_control != CPU_SMT_NOT_SUPPORTED;\\n}\\nEXPORT_SYMBOL_GPL(cpu_smt_possible);\\n\\n#else\\nstatic inline bool cpu_bootable(unsigned int cpu) { return true; }\\n#endif\\n\\nstatic inline enum cpuhp_state\\ncpuhp_set_state(int cpu, struct cpuhp_cpu_state *st, enum cpuhp_state target)\\n{\\n\\tenum cpuhp_state prev_state = st->state;\\n\\tbool bringup = st->state < target;\\n\\n\\tst->rollback = false;\\n\\tst->last = NULL;\\n\\n\\tst->target = target;\\n\\tst->single = false;\\n\\tst->bringup = bringup;\\n\\tif (cpu_dying(cpu) != !bringup)\\n\\t\\tset_cpu_dying(cpu, !bringup);\\n\\n\\treturn prev_state;\\n}\\n\\nstatic inline void\\ncpuhp_reset_state(int cpu, struct cpuhp_cpu_state *st,\\n\\t\\t  enum cpuhp_state prev_state)\\n{\\n\\tbool bringup = !st->bringup;\\n\\n\\tst->target = prev_state;\\n\\n\\t/*\\n\\t * Already rolling back. No need invert the bringup value or to change\\n\\t * the current state.\\n\\t */\\n\\tif (st->rollback)\\n\\t\\treturn;\\n\\n\\tst->rollback = true;\\n\\n\\t/*\\n\\t * If we have st->last we need to undo partial multi_instance of this\\n\\t * state first. Otherwise start undo at the previous state.\\n\\t */\\n\\tif (!st->last) {\\n\\t\\tif (st->bringup)\\n\\t\\t\\tst->state--;\\n\\t\\telse\\n\\t\\t\\tst->state++;\\n\\t}\\n\\n\\tst->bringup = bringup;\\n\\tif (cpu_dying(cpu) != !bringup)\\n\\t\\tset_cpu_dying(cpu, !bringup);\\n}\\n\\n/* Regular hotplug invocation of the AP hotplug thread */\\nstatic void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)\\n{\\n\\tif (!st->single && st->state == st->target)\\n\\t\\treturn;\\n\\n\\tst->result = 0;\\n\\t/*\\n\\t * Make sure the above stores are visible before should_run becomes\\n\\t * true. Paired with the mb() above in cpuhp_thread_fun()\\n\\t */\\n\\tsmp_mb();\\n\\tst->should_run = true;\\n\\twake_up_process(st->thread);\\n\\twait_for_ap_thread(st, st->bringup);\\n}\\n\\nstatic int cpuhp_kick_ap(int cpu, struct cpuhp_cpu_state *st,\\n\\t\\t\\t enum cpuhp_state target)\\n{\\n\\tenum cpuhp_state prev_state;\\n\\tint ret;\\n\\n\\tprev_state = cpuhp_set_state(cpu, st, target);\\n\\t__cpuhp_kick_ap(st);\\n\\tif ((ret = st->result)) {\\n\\t\\tcpuhp_reset_state(cpu, st, prev_state);\\n\\t\\t__cpuhp_kick_ap(st);\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic int bringup_wait_for_ap_online(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\n\\t/* Wait for the CPU to reach CPUHP_AP_ONLINE_IDLE */\\n\\twait_for_ap_thread(st, true);\\n\\tif (WARN_ON_ONCE((!cpu_online(cpu))))\\n\\t\\treturn -ECANCELED;\\n\\n\\t/* Unpark the hotplug thread of the target cpu */\\n\\tkthread_unpark(st->thread);\\n\\n\\t/*\\n\\t * SMT soft disabling on X86 requires to bring the CPU out of the\\n\\t * BIOS \\'wait for SIPI\\' state in order to set the CR4.MCE bit.  The\\n\\t * CPU marked itself as booted_once in notify_cpu_starting() so the\\n\\t * cpu_bootable() check will now return false if this is not the\\n\\t * primary sibling.\\n\\t */\\n\\tif (!cpu_bootable(cpu))\\n\\t\\treturn -ECANCELED;\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_HOTPLUG_SPLIT_STARTUP\\nstatic int cpuhp_kick_ap_alive(unsigned int cpu)\\n{\\n\\tif (!cpuhp_can_boot_ap(cpu))\\n\\t\\treturn -EAGAIN;\\n\\n\\treturn arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));\\n}\\n\\nstatic int cpuhp_bringup_ap(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tint ret;\\n\\n\\t/*\\n\\t * Some architectures have to walk the irq descriptors to\\n\\t * setup the vector space for the cpu which comes online.\\n\\t * Prevent irq alloc/free across the bringup.\\n\\t */\\n\\tirq_lock_sparse();\\n\\n\\tret = cpuhp_bp_sync_alive(cpu);\\n\\tif (ret)\\n\\t\\tgoto out_unlock;\\n\\n\\tret = bringup_wait_for_ap_online(cpu);\\n\\tif (ret)\\n\\t\\tgoto out_unlock;\\n\\n\\tirq_unlock_sparse();\\n\\n\\tif (st->target <= CPUHP_AP_ONLINE_IDLE)\\n\\t\\treturn 0;\\n\\n\\treturn cpuhp_kick_ap(cpu, st, st->target);\\n\\nout_unlock:\\n\\tirq_unlock_sparse();\\n\\treturn ret;\\n}\\n#else\\nstatic int bringup_cpu(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tstruct task_struct *idle = idle_thread_get(cpu);\\n\\tint ret;\\n\\n\\tif (!cpuhp_can_boot_ap(cpu))\\n\\t\\treturn -EAGAIN;\\n\\n\\t/*\\n\\t * Some architectures have to walk the irq descriptors to\\n\\t * setup the vector space for the cpu which comes online.\\n\\t *\\n\\t * Prevent irq alloc/free across the bringup by acquiring the\\n\\t * sparse irq lock. Hold it until the upcoming CPU completes the\\n\\t * startup in cpuhp_online_idle() which allows to avoid\\n\\t * intermediate synchronization points in the architecture code.\\n\\t */\\n\\tirq_lock_sparse();\\n\\n\\tret = __cpu_up(cpu, idle);\\n\\tif (ret)\\n\\t\\tgoto out_unlock;\\n\\n\\tret = cpuhp_bp_sync_alive(cpu);\\n\\tif (ret)\\n\\t\\tgoto out_unlock;\\n\\n\\tret = bringup_wait_for_ap_online(cpu);\\n\\tif (ret)\\n\\t\\tgoto out_unlock;\\n\\n\\tirq_unlock_sparse();\\n\\n\\tif (st->target <= CPUHP_AP_ONLINE_IDLE)\\n\\t\\treturn 0;\\n\\n\\treturn cpuhp_kick_ap(cpu, st, st->target);\\n\\nout_unlock:\\n\\tirq_unlock_sparse();\\n\\treturn ret;\\n}\\n#endif\\n\\nstatic int finish_cpu(unsigned int cpu)\\n{\\n\\tstruct task_struct *idle = idle_thread_get(cpu);\\n\\tstruct mm_struct *mm = idle->active_mm;\\n\\n\\t/*\\n\\t * idle_task_exit() will have switched to &init_mm, now\\n\\t * clean up any remaining active_mm state.\\n\\t */\\n\\tif (mm != &init_mm)\\n\\t\\tidle->active_mm = &init_mm;\\n\\tmmdrop_lazy_tlb(mm);\\n\\treturn 0;\\n}\\n\\n/*\\n * Hotplug state machine related functions\\n */\\n\\n/*\\n * Get the next state to run. Empty ones will be skipped. Returns true if a\\n * state must be run.\\n *\\n * st->state will be modified ahead of time, to match state_to_run, as if it\\n * has already ran.\\n */\\nstatic bool cpuhp_next_state(bool bringup,\\n\\t\\t\\t     enum cpuhp_state *state_to_run,\\n\\t\\t\\t     struct cpuhp_cpu_state *st,\\n\\t\\t\\t     enum cpuhp_state target)\\n{\\n\\tdo {\\n\\t\\tif (bringup) {\\n\\t\\t\\tif (st->state >= target)\\n\\t\\t\\t\\treturn false;\\n\\n\\t\\t\\t*state_to_run = ++st->state;\\n\\t\\t} else {\\n\\t\\t\\tif (st->state <= target)\\n\\t\\t\\t\\treturn false;\\n\\n\\t\\t\\t*state_to_run = st->state--;\\n\\t\\t}\\n\\n\\t\\tif (!cpuhp_step_empty(bringup, cpuhp_get_step(*state_to_run)))\\n\\t\\t\\tbreak;\\n\\t} while (true);\\n\\n\\treturn true;\\n}\\n\\nstatic int __cpuhp_invoke_callback_range(bool bringup,\\n\\t\\t\\t\\t\\t unsigned int cpu,\\n\\t\\t\\t\\t\\t struct cpuhp_cpu_state *st,\\n\\t\\t\\t\\t\\t enum cpuhp_state target,\\n\\t\\t\\t\\t\\t bool nofail)\\n{\\n\\tenum cpuhp_state state;\\n\\tint ret = 0;\\n\\n\\twhile (cpuhp_next_state(bringup, &state, st, target)) {\\n\\t\\tint err;\\n\\n\\t\\terr = cpuhp_invoke_callback(cpu, state, bringup, NULL, NULL);\\n\\t\\tif (!err)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (nofail) {\\n\\t\\t\\tpr_warn(\"CPU %u %s state %s (%d) failed (%d)\\\\n\",\\n\\t\\t\\t\\tcpu, bringup ? \"UP\" : \"DOWN\",\\n\\t\\t\\t\\tcpuhp_get_step(st->state)->name,\\n\\t\\t\\t\\tst->state, err);\\n\\t\\t\\tret = -1;\\n\\t\\t} else {\\n\\t\\t\\tret = err;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic inline int cpuhp_invoke_callback_range(bool bringup,\\n\\t\\t\\t\\t\\t      unsigned int cpu,\\n\\t\\t\\t\\t\\t      struct cpuhp_cpu_state *st,\\n\\t\\t\\t\\t\\t      enum cpuhp_state target)\\n{\\n\\treturn __cpuhp_invoke_callback_range(bringup, cpu, st, target, false);\\n}\\n\\nstatic inline void cpuhp_invoke_callback_range_nofail(bool bringup,\\n\\t\\t\\t\\t\\t\\t      unsigned int cpu,\\n\\t\\t\\t\\t\\t\\t      struct cpuhp_cpu_state *st,\\n\\t\\t\\t\\t\\t\\t      enum cpuhp_state target)\\n{\\n\\t__cpuhp_invoke_callback_range(bringup, cpu, st, target, true);\\n}\\n\\nstatic inline bool can_rollback_cpu(struct cpuhp_cpu_state *st)\\n{\\n\\tif (IS_ENABLED(CONFIG_HOTPLUG_CPU))\\n\\t\\treturn true;\\n\\t/*\\n\\t * When CPU hotplug is disabled, then taking the CPU down is not\\n\\t * possible because takedown_cpu() and the architecture and\\n\\t * subsystem specific mechanisms are not available. So the CPU\\n\\t * which would be completely unplugged again needs to stay around\\n\\t * in the current state.\\n\\t */\\n\\treturn st->state <= CPUHP_BRINGUP_CPU;\\n}\\n\\nstatic int cpuhp_up_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,\\n\\t\\t\\t      enum cpuhp_state target)\\n{\\n\\tenum cpuhp_state prev_state = st->state;\\n\\tint ret = 0;\\n\\n\\tret = cpuhp_invoke_callback_range(true, cpu, st, target);\\n\\tif (ret) {\\n\\t\\tpr_debug(\"CPU UP failed (%d) CPU %u state %s (%d)\\\\n\",\\n\\t\\t\\t ret, cpu, cpuhp_get_step(st->state)->name,\\n\\t\\t\\t st->state);\\n\\n\\t\\tcpuhp_reset_state(cpu, st, prev_state);\\n\\t\\tif (can_rollback_cpu(st))\\n\\t\\t\\tWARN_ON(cpuhp_invoke_callback_range(false, cpu, st,\\n\\t\\t\\t\\t\\t\\t\\t    prev_state));\\n\\t}\\n\\treturn ret;\\n}\\n\\n/*\\n * The cpu hotplug threads manage the bringup and teardown of the cpus\\n */\\nstatic int cpuhp_should_run(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\n\\treturn st->should_run;\\n}\\n\\n/*\\n * Execute teardown/startup callbacks on the plugged cpu. Also used to invoke\\n * callbacks when a state gets [un]installed at runtime.\\n *\\n * Each invocation of this function by the smpboot thread does a single AP\\n * state callback.\\n *\\n * It has 3 modes of operation:\\n *  - single: runs st->cb_state\\n *  - up:     runs ++st->state, while st->state < st->target\\n *  - down:   runs st->state--, while st->state > st->target\\n *\\n * When complete or on error, should_run is cleared and the completion is fired.\\n */\\nstatic void cpuhp_thread_fun(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\tbool bringup = st->bringup;\\n\\tenum cpuhp_state state;\\n\\n\\tif (WARN_ON_ONCE(!st->should_run))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * ACQUIRE for the cpuhp_should_run() load of ->should_run. Ensures\\n\\t * that if we see ->should_run we also see the rest of the state.\\n\\t */\\n\\tsmp_mb();\\n\\n\\t/*\\n\\t * The BP holds the hotplug lock, but we\\'re now running on the AP,\\n\\t * ensure that anybody asserting the lock is held, will actually find\\n\\t * it so.\\n\\t */\\n\\tlockdep_acquire_cpus_lock();\\n\\tcpuhp_lock_acquire(bringup);\\n\\n\\tif (st->single) {\\n\\t\\tstate = st->cb_state;\\n\\t\\tst->should_run = false;\\n\\t} else {\\n\\t\\tst->should_run = cpuhp_next_state(bringup, &state, st, st->target);\\n\\t\\tif (!st->should_run)\\n\\t\\t\\tgoto end;\\n\\t}\\n\\n\\tWARN_ON_ONCE(!cpuhp_is_ap_state(state));\\n\\n\\tif (cpuhp_is_atomic_state(state)) {\\n\\t\\tlocal_irq_disable();\\n\\t\\tst->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);\\n\\t\\tlocal_irq_enable();\\n\\n\\t\\t/*\\n\\t\\t * STARTING/DYING must not fail!\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(st->result);\\n\\t} else {\\n\\t\\tst->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);\\n\\t}\\n\\n\\tif (st->result) {\\n\\t\\t/*\\n\\t\\t * If we fail on a rollback, we\\'re up a creek without no\\n\\t\\t * paddle, no way forward, no way back. We loose, thanks for\\n\\t\\t * playing.\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(st->rollback);\\n\\t\\tst->should_run = false;\\n\\t}\\n\\nend:\\n\\tcpuhp_lock_release(bringup);\\n\\tlockdep_release_cpus_lock();\\n\\n\\tif (!st->should_run)\\n\\t\\tcomplete_ap_thread(st, bringup);\\n}\\n\\n/* Invoke a single callback on a remote cpu */\\nstatic int\\ncpuhp_invoke_ap_callback(int cpu, enum cpuhp_state state, bool bringup,\\n\\t\\t\\t struct hlist_node *node)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tint ret;\\n\\n\\tif (!cpu_online(cpu))\\n\\t\\treturn 0;\\n\\n\\tcpuhp_lock_acquire(false);\\n\\tcpuhp_lock_release(false);\\n\\n\\tcpuhp_lock_acquire(true);\\n\\tcpuhp_lock_release(true);\\n\\n\\t/*\\n\\t * If we are up and running, use the hotplug thread. For early calls\\n\\t * we invoke the thread function directly.\\n\\t */\\n\\tif (!st->thread)\\n\\t\\treturn cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\\n\\n\\tst->rollback = false;\\n\\tst->last = NULL;\\n\\n\\tst->node = node;\\n\\tst->bringup = bringup;\\n\\tst->cb_state = state;\\n\\tst->single = true;\\n\\n\\t__cpuhp_kick_ap(st);\\n\\n\\t/*\\n\\t * If we failed and did a partial, do a rollback.\\n\\t */\\n\\tif ((ret = st->result) && st->last) {\\n\\t\\tst->rollback = true;\\n\\t\\tst->bringup = !bringup;\\n\\n\\t\\t__cpuhp_kick_ap(st);\\n\\t}\\n\\n\\t/*\\n\\t * Clean up the leftovers so the next hotplug operation wont use stale\\n\\t * data.\\n\\t */\\n\\tst->node = st->last = NULL;\\n\\treturn ret;\\n}\\n\\nstatic int cpuhp_kick_ap_work(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tenum cpuhp_state prev_state = st->state;\\n\\tint ret;\\n\\n\\tcpuhp_lock_acquire(false);\\n\\tcpuhp_lock_release(false);\\n\\n\\tcpuhp_lock_acquire(true);\\n\\tcpuhp_lock_release(true);\\n\\n\\ttrace_cpuhp_enter(cpu, st->target, prev_state, cpuhp_kick_ap_work);\\n\\tret = cpuhp_kick_ap(cpu, st, st->target);\\n\\ttrace_cpuhp_exit(cpu, st->state, prev_state, ret);\\n\\n\\treturn ret;\\n}\\n\\nstatic struct smp_hotplug_thread cpuhp_threads = {\\n\\t.store\\t\\t\\t= &cpuhp_state.thread,\\n\\t.thread_should_run\\t= cpuhp_should_run,\\n\\t.thread_fn\\t\\t= cpuhp_thread_fun,\\n\\t.thread_comm\\t\\t= \"cpuhp/%u\",\\n\\t.selfparking\\t\\t= true,\\n};\\n\\nstatic __init void cpuhp_init_state(void)\\n{\\n\\tstruct cpuhp_cpu_state *st;\\n\\tint cpu;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tst = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tinit_completion(&st->done_up);\\n\\t\\tinit_completion(&st->done_down);\\n\\t}\\n}\\n\\nvoid __init cpuhp_threads_init(void)\\n{\\n\\tcpuhp_init_state();\\n\\tBUG_ON(smpboot_register_percpu_thread(&cpuhp_threads));\\n\\tkthread_unpark(this_cpu_read(cpuhp_state.thread));\\n}\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\n#ifndef arch_clear_mm_cpumask_cpu\\n#define arch_clear_mm_cpumask_cpu(cpu, mm) cpumask_clear_cpu(cpu, mm_cpumask(mm))\\n#endif\\n\\n/**\\n * clear_tasks_mm_cpumask - Safely clear tasks\\' mm_cpumask for a CPU\\n * @cpu: a CPU id\\n *\\n * This function walks all processes, finds a valid mm struct for each one and\\n * then clears a corresponding bit in mm\\'s cpumask.  While this all sounds\\n * trivial, there are various non-obvious corner cases, which this function\\n * tries to solve in a safe manner.\\n *\\n * Also note that the function uses a somewhat relaxed locking scheme, so it may\\n * be called only for an already offlined CPU.\\n */\\nvoid clear_tasks_mm_cpumask(int cpu)\\n{\\n\\tstruct task_struct *p;\\n\\n\\t/*\\n\\t * This function is called after the cpu is taken down and marked\\n\\t * offline, so its not like new tasks will ever get this cpu set in\\n\\t * their mm mask. -- Peter Zijlstra\\n\\t * Thus, we may use rcu_read_lock() here, instead of grabbing\\n\\t * full-fledged tasklist_lock.\\n\\t */\\n\\tWARN_ON(cpu_online(cpu));\\n\\trcu_read_lock();\\n\\tfor_each_process(p) {\\n\\t\\tstruct task_struct *t;\\n\\n\\t\\t/*\\n\\t\\t * Main thread might exit, but other threads may still have\\n\\t\\t * a valid mm. Find one.\\n\\t\\t */\\n\\t\\tt = find_lock_task_mm(p);\\n\\t\\tif (!t)\\n\\t\\t\\tcontinue;\\n\\t\\tarch_clear_mm_cpumask_cpu(cpu, t->mm);\\n\\t\\ttask_unlock(t);\\n\\t}\\n\\trcu_read_unlock();\\n}\\n\\n/* Take this CPU down. */\\nstatic int take_cpu_down(void *_param)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\tenum cpuhp_state target = max((int)st->target, CPUHP_AP_OFFLINE);\\n\\tint err, cpu = smp_processor_id();\\n\\n\\t/* Ensure this CPU doesn\\'t handle any more interrupts. */\\n\\terr = __cpu_disable();\\n\\tif (err < 0)\\n\\t\\treturn err;\\n\\n\\t/*\\n\\t * Must be called from CPUHP_TEARDOWN_CPU, which means, as we are going\\n\\t * down, that the current state is CPUHP_TEARDOWN_CPU - 1.\\n\\t */\\n\\tWARN_ON(st->state != (CPUHP_TEARDOWN_CPU - 1));\\n\\n\\t/*\\n\\t * Invoke the former CPU_DYING callbacks. DYING must not fail!\\n\\t */\\n\\tcpuhp_invoke_callback_range_nofail(false, cpu, st, target);\\n\\n\\t/* Park the stopper thread */\\n\\tstop_machine_park(cpu);\\n\\treturn 0;\\n}\\n\\nstatic int takedown_cpu(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tint err;\\n\\n\\t/* Park the smpboot threads */\\n\\tkthread_park(st->thread);\\n\\n\\t/*\\n\\t * Prevent irq alloc/free while the dying cpu reorganizes the\\n\\t * interrupt affinities.\\n\\t */\\n\\tirq_lock_sparse();\\n\\n\\t/*\\n\\t * So now all preempt/rcu users must observe !cpu_active().\\n\\t */\\n\\terr = stop_machine_cpuslocked(take_cpu_down, NULL, cpumask_of(cpu));\\n\\tif (err) {\\n\\t\\t/* CPU refused to die */\\n\\t\\tirq_unlock_sparse();\\n\\t\\t/* Unpark the hotplug thread so we can rollback there */\\n\\t\\tkthread_unpark(st->thread);\\n\\t\\treturn err;\\n\\t}\\n\\tBUG_ON(cpu_online(cpu));\\n\\n\\t/*\\n\\t * The teardown callback for CPUHP_AP_SCHED_STARTING will have removed\\n\\t * all runnable tasks from the CPU, there\\'s only the idle task left now\\n\\t * that the migration thread is done doing the stop_machine thing.\\n\\t *\\n\\t * Wait for the stop thread to go away.\\n\\t */\\n\\twait_for_ap_thread(st, false);\\n\\tBUG_ON(st->state != CPUHP_AP_IDLE_DEAD);\\n\\n\\t/* Interrupts are moved away from the dying cpu, reenable alloc/free */\\n\\tirq_unlock_sparse();\\n\\n\\thotplug_cpu__broadcast_tick_pull(cpu);\\n\\t/* This actually kills the CPU. */\\n\\t__cpu_die(cpu);\\n\\n\\tcpuhp_bp_sync_dead(cpu);\\n\\n\\tlockdep_cleanup_dead_cpu(cpu, idle_thread_get(cpu));\\n\\n\\t/*\\n\\t * Callbacks must be re-integrated right away to the RCU state machine.\\n\\t * Otherwise an RCU callback could block a further teardown function\\n\\t * waiting for its completion.\\n\\t */\\n\\trcutree_migrate_callbacks(cpu);\\n\\n\\treturn 0;\\n}\\n\\nstatic void cpuhp_complete_idle_dead(void *arg)\\n{\\n\\tstruct cpuhp_cpu_state *st = arg;\\n\\n\\tcomplete_ap_thread(st, false);\\n}\\n\\nvoid cpuhp_report_idle_dead(void)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\n\\tBUG_ON(st->state != CPUHP_AP_OFFLINE);\\n\\ttick_assert_timekeeping_handover();\\n\\trcutree_report_cpu_dead();\\n\\tst->state = CPUHP_AP_IDLE_DEAD;\\n\\t/*\\n\\t * We cannot call complete after rcutree_report_cpu_dead() so we delegate it\\n\\t * to an online cpu.\\n\\t */\\n\\tsmp_call_function_single(cpumask_first(cpu_online_mask),\\n\\t\\t\\t\\t cpuhp_complete_idle_dead, st, 0);\\n}\\n\\nstatic int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,\\n\\t\\t\\t\\tenum cpuhp_state target)\\n{\\n\\tenum cpuhp_state prev_state = st->state;\\n\\tint ret = 0;\\n\\n\\tret = cpuhp_invoke_callback_range(false, cpu, st, target);\\n\\tif (ret) {\\n\\t\\tpr_debug(\"CPU DOWN failed (%d) CPU %u state %s (%d)\\\\n\",\\n\\t\\t\\t ret, cpu, cpuhp_get_step(st->state)->name,\\n\\t\\t\\t st->state);\\n\\n\\t\\tcpuhp_reset_state(cpu, st, prev_state);\\n\\n\\t\\tif (st->state < prev_state)\\n\\t\\t\\tWARN_ON(cpuhp_invoke_callback_range(true, cpu, st,\\n\\t\\t\\t\\t\\t\\t\\t    prev_state));\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\n/* Requires cpu_add_remove_lock to be held */\\nstatic int __ref _cpu_down(unsigned int cpu, int tasks_frozen,\\n\\t\\t\\t   enum cpuhp_state target)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tint prev_state, ret = 0;\\n\\n\\tif (num_online_cpus() == 1)\\n\\t\\treturn -EBUSY;\\n\\n\\tif (!cpu_present(cpu))\\n\\t\\treturn -EINVAL;\\n\\n\\tcpus_write_lock();\\n\\n\\tcpuhp_tasks_frozen = tasks_frozen;\\n\\n\\tprev_state = cpuhp_set_state(cpu, st, target);\\n\\t/*\\n\\t * If the current CPU state is in the range of the AP hotplug thread,\\n\\t * then we need to kick the thread.\\n\\t */\\n\\tif (st->state > CPUHP_TEARDOWN_CPU) {\\n\\t\\tst->target = max((int)target, CPUHP_TEARDOWN_CPU);\\n\\t\\tret = cpuhp_kick_ap_work(cpu);\\n\\t\\t/*\\n\\t\\t * The AP side has done the error rollback already. Just\\n\\t\\t * return the error code..\\n\\t\\t */\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/*\\n\\t\\t * We might have stopped still in the range of the AP hotplug\\n\\t\\t * thread. Nothing to do anymore.\\n\\t\\t */\\n\\t\\tif (st->state > CPUHP_TEARDOWN_CPU)\\n\\t\\t\\tgoto out;\\n\\n\\t\\tst->target = target;\\n\\t}\\n\\t/*\\n\\t * The AP brought itself down to CPUHP_TEARDOWN_CPU. So we need\\n\\t * to do the further cleanups.\\n\\t */\\n\\tret = cpuhp_down_callbacks(cpu, st, target);\\n\\tif (ret && st->state < prev_state) {\\n\\t\\tif (st->state == CPUHP_TEARDOWN_CPU) {\\n\\t\\t\\tcpuhp_reset_state(cpu, st, prev_state);\\n\\t\\t\\t__cpuhp_kick_ap(st);\\n\\t\\t} else {\\n\\t\\t\\tWARN(1, \"DEAD callback error for CPU%d\", cpu);\\n\\t\\t}\\n\\t}\\n\\nout:\\n\\tcpus_write_unlock();\\n\\t/*\\n\\t * Do post unplug cleanup. This is still protected against\\n\\t * concurrent CPU hotplug via cpu_add_remove_lock.\\n\\t */\\n\\tlockup_detector_cleanup();\\n\\tarch_smt_update();\\n\\treturn ret;\\n}\\n\\nstruct cpu_down_work {\\n\\tunsigned int\\t\\tcpu;\\n\\tenum cpuhp_state\\ttarget;\\n};\\n\\nstatic long __cpu_down_maps_locked(void *arg)\\n{\\n\\tstruct cpu_down_work *work = arg;\\n\\n\\treturn _cpu_down(work->cpu, 0, work->target);\\n}\\n\\nstatic int cpu_down_maps_locked(unsigned int cpu, enum cpuhp_state target)\\n{\\n\\tstruct cpu_down_work work = { .cpu = cpu, .target = target, };\\n\\n\\t/*\\n\\t * If the platform does not support hotplug, report it explicitly to\\n\\t * differentiate it from a transient offlining failure.\\n\\t */\\n\\tif (cpu_hotplug_offline_disabled)\\n\\t\\treturn -EOPNOTSUPP;\\n\\tif (cpu_hotplug_disabled)\\n\\t\\treturn -EBUSY;\\n\\n\\t/*\\n\\t * Ensure that the control task does not run on the to be offlined\\n\\t * CPU to prevent a deadlock against cfs_b->period_timer.\\n\\t * Also keep at least one housekeeping cpu onlined to avoid generating\\n\\t * an empty sched_domain span.\\n\\t */\\n\\tfor_each_cpu_and(cpu, cpu_online_mask, housekeeping_cpumask(HK_TYPE_DOMAIN)) {\\n\\t\\tif (cpu != work.cpu)\\n\\t\\t\\treturn work_on_cpu(cpu, __cpu_down_maps_locked, &work);\\n\\t}\\n\\treturn -EBUSY;\\n}\\n\\nstatic int cpu_down(unsigned int cpu, enum cpuhp_state target)\\n{\\n\\tint err;\\n\\n\\tcpu_maps_update_begin();\\n\\terr = cpu_down_maps_locked(cpu, target);\\n\\tcpu_maps_update_done();\\n\\treturn err;\\n}\\n\\n/**\\n * cpu_device_down - Bring down a cpu device\\n * @dev: Pointer to the cpu device to offline\\n *\\n * This function is meant to be used by device core cpu subsystem only.\\n *\\n * Other subsystems should use remove_cpu() instead.\\n *\\n * Return: %0 on success or a negative errno code\\n */\\nint cpu_device_down(struct device *dev)\\n{\\n\\treturn cpu_down(dev->id, CPUHP_OFFLINE);\\n}\\n\\nint remove_cpu(unsigned int cpu)\\n{\\n\\tint ret;\\n\\n\\tlock_device_hotplug();\\n\\tret = device_offline(get_cpu_device(cpu));\\n\\tunlock_device_hotplug();\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(remove_cpu);\\n\\nvoid smp_shutdown_nonboot_cpus(unsigned int primary_cpu)\\n{\\n\\tunsigned int cpu;\\n\\tint error;\\n\\n\\tcpu_maps_update_begin();\\n\\n\\t/*\\n\\t * Make certain the cpu I\\'m about to reboot on is online.\\n\\t *\\n\\t * This is inline to what migrate_to_reboot_cpu() already do.\\n\\t */\\n\\tif (!cpu_online(primary_cpu))\\n\\t\\tprimary_cpu = cpumask_first(cpu_online_mask);\\n\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tif (cpu == primary_cpu)\\n\\t\\t\\tcontinue;\\n\\n\\t\\terror = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);\\n\\t\\tif (error) {\\n\\t\\t\\tpr_err(\"Failed to offline CPU%d - error=%d\",\\n\\t\\t\\t\\tcpu, error);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * Ensure all but the reboot CPU are offline.\\n\\t */\\n\\tBUG_ON(num_online_cpus() > 1);\\n\\n\\t/*\\n\\t * Make sure the CPUs won\\'t be enabled by someone else after this\\n\\t * point. Kexec will reboot to a new kernel shortly resetting\\n\\t * everything along the way.\\n\\t */\\n\\tcpu_hotplug_disabled++;\\n\\n\\tcpu_maps_update_done();\\n}\\n\\n#else\\n#define takedown_cpu\\t\\tNULL\\n#endif /*CONFIG_HOTPLUG_CPU*/\\n\\n/**\\n * notify_cpu_starting(cpu) - Invoke the callbacks on the starting CPU\\n * @cpu: cpu that just started\\n *\\n * It must be called by the arch code on the new cpu, before the new cpu\\n * enables interrupts and before the \"boot\" cpu returns from __cpu_up().\\n */\\nvoid notify_cpu_starting(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tenum cpuhp_state target = min((int)st->target, CPUHP_AP_ONLINE);\\n\\n\\trcutree_report_cpu_starting(cpu);\\t/* Enables RCU usage on this CPU. */\\n\\tcpumask_set_cpu(cpu, &cpus_booted_once_mask);\\n\\n\\t/*\\n\\t * STARTING must not fail!\\n\\t */\\n\\tcpuhp_invoke_callback_range_nofail(true, cpu, st, target);\\n}\\n\\n/*\\n * Called from the idle task. Wake up the controlling task which brings the\\n * hotplug thread of the upcoming CPU up and then delegates the rest of the\\n * online bringup to the hotplug thread.\\n */\\nvoid cpuhp_online_idle(enum cpuhp_state state)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\n\\t/* Happens for the boot cpu */\\n\\tif (state != CPUHP_AP_ONLINE_IDLE)\\n\\t\\treturn;\\n\\n\\tcpuhp_ap_update_sync_state(SYNC_STATE_ONLINE);\\n\\n\\t/*\\n\\t * Unpark the stopper thread before we start the idle loop (and start\\n\\t * scheduling); this ensures the stopper task is always available.\\n\\t */\\n\\tstop_machine_unpark(smp_processor_id());\\n\\n\\tst->state = CPUHP_AP_ONLINE_IDLE;\\n\\tcomplete_ap_thread(st, true);\\n}\\n\\n/* Requires cpu_add_remove_lock to be held */\\nstatic int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tstruct task_struct *idle;\\n\\tint ret = 0;\\n\\n\\tcpus_write_lock();\\n\\n\\tif (!cpu_present(cpu)) {\\n\\t\\tret = -EINVAL;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * The caller of cpu_up() might have raced with another\\n\\t * caller. Nothing to do.\\n\\t */\\n\\tif (st->state >= target)\\n\\t\\tgoto out;\\n\\n\\tif (st->state == CPUHP_OFFLINE) {\\n\\t\\t/* Let it fail before we try to bring the cpu up */\\n\\t\\tidle = idle_thread_get(cpu);\\n\\t\\tif (IS_ERR(idle)) {\\n\\t\\t\\tret = PTR_ERR(idle);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Reset stale stack state from the last time this CPU was online.\\n\\t\\t */\\n\\t\\tscs_task_reset(idle);\\n\\t\\tkasan_unpoison_task_stack(idle);\\n\\t}\\n\\n\\tcpuhp_tasks_frozen = tasks_frozen;\\n\\n\\tcpuhp_set_state(cpu, st, target);\\n\\t/*\\n\\t * If the current CPU state is in the range of the AP hotplug thread,\\n\\t * then we need to kick the thread once more.\\n\\t */\\n\\tif (st->state > CPUHP_BRINGUP_CPU) {\\n\\t\\tret = cpuhp_kick_ap_work(cpu);\\n\\t\\t/*\\n\\t\\t * The AP side has done the error rollback already. Just\\n\\t\\t * return the error code..\\n\\t\\t */\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * Try to reach the target state. We max out on the BP at\\n\\t * CPUHP_BRINGUP_CPU. After that the AP hotplug thread is\\n\\t * responsible for bringing it up to the target state.\\n\\t */\\n\\ttarget = min((int)target, CPUHP_BRINGUP_CPU);\\n\\tret = cpuhp_up_callbacks(cpu, st, target);\\nout:\\n\\tcpus_write_unlock();\\n\\tarch_smt_update();\\n\\treturn ret;\\n}\\n\\nstatic int cpu_up(unsigned int cpu, enum cpuhp_state target)\\n{\\n\\tint err = 0;\\n\\n\\tif (!cpu_possible(cpu)) {\\n\\t\\tpr_err(\"can\\'t online cpu %d because it is not configured as may-hotadd at boot time\\\\n\",\\n\\t\\t       cpu);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\terr = try_online_node(cpu_to_node(cpu));\\n\\tif (err)\\n\\t\\treturn err;\\n\\n\\tcpu_maps_update_begin();\\n\\n\\tif (cpu_hotplug_disabled) {\\n\\t\\terr = -EBUSY;\\n\\t\\tgoto out;\\n\\t}\\n\\tif (!cpu_bootable(cpu)) {\\n\\t\\terr = -EPERM;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\terr = _cpu_up(cpu, 0, target);\\nout:\\n\\tcpu_maps_update_done();\\n\\treturn err;\\n}\\n\\n/**\\n * cpu_device_up - Bring up a cpu device\\n * @dev: Pointer to the cpu device to online\\n *\\n * This function is meant to be used by device core cpu subsystem only.\\n *\\n * Other subsystems should use add_cpu() instead.\\n *\\n * Return: %0 on success or a negative errno code\\n */\\nint cpu_device_up(struct device *dev)\\n{\\n\\treturn cpu_up(dev->id, CPUHP_ONLINE);\\n}\\n\\nint add_cpu(unsigned int cpu)\\n{\\n\\tint ret;\\n\\n\\tlock_device_hotplug();\\n\\tret = device_online(get_cpu_device(cpu));\\n\\tunlock_device_hotplug();\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(add_cpu);\\n\\n/**\\n * bringup_hibernate_cpu - Bring up the CPU that we hibernated on\\n * @sleep_cpu: The cpu we hibernated on and should be brought up.\\n *\\n * On some architectures like arm64, we can hibernate on any CPU, but on\\n * wake up the CPU we hibernated on might be offline as a side effect of\\n * using maxcpus= for example.\\n *\\n * Return: %0 on success or a negative errno code\\n */\\nint bringup_hibernate_cpu(unsigned int sleep_cpu)\\n{\\n\\tint ret;\\n\\n\\tif (!cpu_online(sleep_cpu)) {\\n\\t\\tpr_info(\"Hibernated on a CPU that is offline! Bringing CPU up.\\\\n\");\\n\\t\\tret = cpu_up(sleep_cpu, CPUHP_ONLINE);\\n\\t\\tif (ret) {\\n\\t\\t\\tpr_err(\"Failed to bring hibernate-CPU up!\\\\n\");\\n\\t\\t\\treturn ret;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic void __init cpuhp_bringup_mask(const struct cpumask *mask, unsigned int ncpus,\\n\\t\\t\\t\\t      enum cpuhp_state target)\\n{\\n\\tunsigned int cpu;\\n\\n\\tfor_each_cpu(cpu, mask) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\n\\t\\tif (cpu_up(cpu, target) && can_rollback_cpu(st)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * If this failed then cpu_up() might have only\\n\\t\\t\\t * rolled back to CPUHP_BP_KICK_AP for the final\\n\\t\\t\\t * online. Clean it up. NOOP if already rolled back.\\n\\t\\t\\t */\\n\\t\\t\\tWARN_ON(cpuhp_invoke_callback_range(false, cpu, st, CPUHP_OFFLINE));\\n\\t\\t}\\n\\n\\t\\tif (!--ncpus)\\n\\t\\t\\tbreak;\\n\\t}\\n}\\n\\n#ifdef CONFIG_HOTPLUG_PARALLEL\\nstatic bool __cpuhp_parallel_bringup __ro_after_init = true;\\n\\nstatic int __init parallel_bringup_parse_param(char *arg)\\n{\\n\\treturn kstrtobool(arg, &__cpuhp_parallel_bringup);\\n}\\nearly_param(\"cpuhp.parallel\", parallel_bringup_parse_param);\\n\\n#ifdef CONFIG_HOTPLUG_SMT\\nstatic inline bool cpuhp_smt_aware(void)\\n{\\n\\treturn cpu_smt_max_threads > 1;\\n}\\n\\nstatic inline const struct cpumask *cpuhp_get_primary_thread_mask(void)\\n{\\n\\treturn cpu_primary_thread_mask;\\n}\\n#else\\nstatic inline bool cpuhp_smt_aware(void)\\n{\\n\\treturn false;\\n}\\nstatic inline const struct cpumask *cpuhp_get_primary_thread_mask(void)\\n{\\n\\treturn cpu_none_mask;\\n}\\n#endif\\n\\nbool __weak arch_cpuhp_init_parallel_bringup(void)\\n{\\n\\treturn true;\\n}\\n\\n/*\\n * On architectures which have enabled parallel bringup this invokes all BP\\n * prepare states for each of the to be onlined APs first. The last state\\n * sends the startup IPI to the APs. The APs proceed through the low level\\n * bringup code in parallel and then wait for the control CPU to release\\n * them one by one for the final onlining procedure.\\n *\\n * This avoids waiting for each AP to respond to the startup IPI in\\n * CPUHP_BRINGUP_CPU.\\n */\\nstatic bool __init cpuhp_bringup_cpus_parallel(unsigned int ncpus)\\n{\\n\\tconst struct cpumask *mask = cpu_present_mask;\\n\\n\\tif (__cpuhp_parallel_bringup)\\n\\t\\t__cpuhp_parallel_bringup = arch_cpuhp_init_parallel_bringup();\\n\\tif (!__cpuhp_parallel_bringup)\\n\\t\\treturn false;\\n\\n\\tif (cpuhp_smt_aware()) {\\n\\t\\tconst struct cpumask *pmask = cpuhp_get_primary_thread_mask();\\n\\t\\tstatic struct cpumask tmp_mask __initdata;\\n\\n\\t\\t/*\\n\\t\\t * X86 requires to prevent that SMT siblings stopped while\\n\\t\\t * the primary thread does a microcode update for various\\n\\t\\t * reasons. Bring the primary threads up first.\\n\\t\\t */\\n\\t\\tcpumask_and(&tmp_mask, mask, pmask);\\n\\t\\tcpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_BP_KICK_AP);\\n\\t\\tcpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_ONLINE);\\n\\t\\t/* Account for the online CPUs */\\n\\t\\tncpus -= num_online_cpus();\\n\\t\\tif (!ncpus)\\n\\t\\t\\treturn true;\\n\\t\\t/* Create the mask for secondary CPUs */\\n\\t\\tcpumask_andnot(&tmp_mask, mask, pmask);\\n\\t\\tmask = &tmp_mask;\\n\\t}\\n\\n\\t/* Bring the not-yet started CPUs up */\\n\\tcpuhp_bringup_mask(mask, ncpus, CPUHP_BP_KICK_AP);\\n\\tcpuhp_bringup_mask(mask, ncpus, CPUHP_ONLINE);\\n\\treturn true;\\n}\\n#else\\nstatic inline bool cpuhp_bringup_cpus_parallel(unsigned int ncpus) { return false; }\\n#endif /* CONFIG_HOTPLUG_PARALLEL */\\n\\nvoid __init bringup_nonboot_cpus(unsigned int max_cpus)\\n{\\n\\tif (!max_cpus)\\n\\t\\treturn;\\n\\n\\t/* Try parallel bringup optimization if enabled */\\n\\tif (cpuhp_bringup_cpus_parallel(max_cpus))\\n\\t\\treturn;\\n\\n\\t/* Full per CPU serialized bringup */\\n\\tcpuhp_bringup_mask(cpu_present_mask, max_cpus, CPUHP_ONLINE);\\n}\\n\\n#ifdef CONFIG_PM_SLEEP_SMP\\nstatic cpumask_var_t frozen_cpus;\\n\\nint freeze_secondary_cpus(int primary)\\n{\\n\\tint cpu, error = 0;\\n\\n\\tcpu_maps_update_begin();\\n\\tif (primary == -1) {\\n\\t\\tprimary = cpumask_first(cpu_online_mask);\\n\\t\\tif (!housekeeping_cpu(primary, HK_TYPE_TIMER))\\n\\t\\t\\tprimary = housekeeping_any_cpu(HK_TYPE_TIMER);\\n\\t} else {\\n\\t\\tif (!cpu_online(primary))\\n\\t\\t\\tprimary = cpumask_first(cpu_online_mask);\\n\\t}\\n\\n\\t/*\\n\\t * We take down all of the non-boot CPUs in one shot to avoid races\\n\\t * with the userspace trying to use the CPU hotplug at the same time\\n\\t */\\n\\tcpumask_clear(frozen_cpus);\\n\\n\\tpr_info(\"Disabling non-boot CPUs ...\\\\n\");\\n\\tfor (cpu = nr_cpu_ids - 1; cpu >= 0; cpu--) {\\n\\t\\tif (!cpu_online(cpu) || cpu == primary)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (pm_wakeup_pending()) {\\n\\t\\t\\tpr_info(\"Wakeup pending. Abort CPU freeze\\\\n\");\\n\\t\\t\\terror = -EBUSY;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\ttrace_suspend_resume(TPS(\"CPU_OFF\"), cpu, true);\\n\\t\\terror = _cpu_down(cpu, 1, CPUHP_OFFLINE);\\n\\t\\ttrace_suspend_resume(TPS(\"CPU_OFF\"), cpu, false);\\n\\t\\tif (!error)\\n\\t\\t\\tcpumask_set_cpu(cpu, frozen_cpus);\\n\\t\\telse {\\n\\t\\t\\tpr_err(\"Error taking CPU%d down: %d\\\\n\", cpu, error);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\tif (!error)\\n\\t\\tBUG_ON(num_online_cpus() > 1);\\n\\telse\\n\\t\\tpr_err(\"Non-boot CPUs are not disabled\\\\n\");\\n\\n\\t/*\\n\\t * Make sure the CPUs won\\'t be enabled by someone else. We need to do\\n\\t * this even in case of failure as all freeze_secondary_cpus() users are\\n\\t * supposed to do thaw_secondary_cpus() on the failure path.\\n\\t */\\n\\tcpu_hotplug_disabled++;\\n\\n\\tcpu_maps_update_done();\\n\\treturn error;\\n}\\n\\nvoid __weak arch_thaw_secondary_cpus_begin(void)\\n{\\n}\\n\\nvoid __weak arch_thaw_secondary_cpus_end(void)\\n{\\n}\\n\\nvoid thaw_secondary_cpus(void)\\n{\\n\\tint cpu, error;\\n\\n\\t/* Allow everyone to use the CPU hotplug again */\\n\\tcpu_maps_update_begin();\\n\\t__cpu_hotplug_enable();\\n\\tif (cpumask_empty(frozen_cpus))\\n\\t\\tgoto out;\\n\\n\\tpr_info(\"Enabling non-boot CPUs ...\\\\n\");\\n\\n\\tarch_thaw_secondary_cpus_begin();\\n\\n\\tfor_each_cpu(cpu, frozen_cpus) {\\n\\t\\ttrace_suspend_resume(TPS(\"CPU_ON\"), cpu, true);\\n\\t\\terror = _cpu_up(cpu, 1, CPUHP_ONLINE);\\n\\t\\ttrace_suspend_resume(TPS(\"CPU_ON\"), cpu, false);\\n\\t\\tif (!error) {\\n\\t\\t\\tpr_info(\"CPU%d is up\\\\n\", cpu);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tpr_warn(\"Error taking CPU%d up: %d\\\\n\", cpu, error);\\n\\t}\\n\\n\\tarch_thaw_secondary_cpus_end();\\n\\n\\tcpumask_clear(frozen_cpus);\\nout:\\n\\tcpu_maps_update_done();\\n}\\n\\nstatic int __init alloc_frozen_cpus(void)\\n{\\n\\tif (!alloc_cpumask_var(&frozen_cpus, GFP_KERNEL|__GFP_ZERO))\\n\\t\\treturn -ENOMEM;\\n\\treturn 0;\\n}\\ncore_initcall(alloc_frozen_cpus);\\n\\n/*\\n * When callbacks for CPU hotplug notifications are being executed, we must\\n * ensure that the state of the system with respect to the tasks being frozen\\n * or not, as reported by the notification, remains unchanged *throughout the\\n * duration* of the execution of the callbacks.\\n * Hence we need to prevent the freezer from racing with regular CPU hotplug.\\n *\\n * This synchronization is implemented by mutually excluding regular CPU\\n * hotplug and Suspend/Hibernate call paths by hooking onto the Suspend/\\n * Hibernate notifications.\\n */\\nstatic int\\ncpu_hotplug_pm_callback(struct notifier_block *nb,\\n\\t\\t\\tunsigned long action, void *ptr)\\n{\\n\\tswitch (action) {\\n\\n\\tcase PM_SUSPEND_PREPARE:\\n\\tcase PM_HIBERNATION_PREPARE:\\n\\t\\tcpu_hotplug_disable();\\n\\t\\tbreak;\\n\\n\\tcase PM_POST_SUSPEND:\\n\\tcase PM_POST_HIBERNATION:\\n\\t\\tcpu_hotplug_enable();\\n\\t\\tbreak;\\n\\n\\tdefault:\\n\\t\\treturn NOTIFY_DONE;\\n\\t}\\n\\n\\treturn NOTIFY_OK;\\n}\\n\\n\\nstatic int __init cpu_hotplug_pm_sync_init(void)\\n{\\n\\t/*\\n\\t * cpu_hotplug_pm_callback has higher priority than x86\\n\\t * bsp_pm_callback which depends on cpu_hotplug_pm_callback\\n\\t * to disable cpu hotplug to avoid cpu hotplug race.\\n\\t */\\n\\tpm_notifier(cpu_hotplug_pm_callback, 0);\\n\\treturn 0;\\n}\\ncore_initcall(cpu_hotplug_pm_sync_init);\\n\\n#endif /* CONFIG_PM_SLEEP_SMP */\\n\\nint __boot_cpu_id;\\n\\n#endif /* CONFIG_SMP */\\n\\n/* Boot processor state steps */\\nstatic struct cpuhp_step cpuhp_hp_states[] = {\\n\\t[CPUHP_OFFLINE] = {\\n\\t\\t.name\\t\\t\\t= \"offline\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n#ifdef CONFIG_SMP\\n\\t[CPUHP_CREATE_THREADS]= {\\n\\t\\t.name\\t\\t\\t= \"threads:prepare\",\\n\\t\\t.startup.single\\t\\t= smpboot_create_threads,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n\\t[CPUHP_PERF_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"perf:prepare\",\\n\\t\\t.startup.single\\t\\t= perf_event_init_cpu,\\n\\t\\t.teardown.single\\t= perf_event_exit_cpu,\\n\\t},\\n\\t[CPUHP_RANDOM_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"random:prepare\",\\n\\t\\t.startup.single\\t\\t= random_prepare_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_WORKQUEUE_PREP] = {\\n\\t\\t.name\\t\\t\\t= \"workqueue:prepare\",\\n\\t\\t.startup.single\\t\\t= workqueue_prepare_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_HRTIMERS_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"hrtimers:prepare\",\\n\\t\\t.startup.single\\t\\t= hrtimers_prepare_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_SMPCFD_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"smpcfd:prepare\",\\n\\t\\t.startup.single\\t\\t= smpcfd_prepare_cpu,\\n\\t\\t.teardown.single\\t= smpcfd_dead_cpu,\\n\\t},\\n\\t[CPUHP_RELAY_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"relay:prepare\",\\n\\t\\t.startup.single\\t\\t= relay_prepare_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_RCUTREE_PREP] = {\\n\\t\\t.name\\t\\t\\t= \"RCU/tree:prepare\",\\n\\t\\t.startup.single\\t\\t= rcutree_prepare_cpu,\\n\\t\\t.teardown.single\\t= rcutree_dead_cpu,\\n\\t},\\n\\t/*\\n\\t * On the tear-down path, timers_dead_cpu() must be invoked\\n\\t * before blk_mq_queue_reinit_notify() from notify_dead(),\\n\\t * otherwise a RCU stall occurs.\\n\\t */\\n\\t[CPUHP_TIMERS_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"timers:prepare\",\\n\\t\\t.startup.single\\t\\t= timers_prepare_cpu,\\n\\t\\t.teardown.single\\t= timers_dead_cpu,\\n\\t},\\n\\n#ifdef CONFIG_HOTPLUG_SPLIT_STARTUP\\n\\t/*\\n\\t * Kicks the AP alive. AP will wait in cpuhp_ap_sync_alive() until\\n\\t * the next step will release it.\\n\\t */\\n\\t[CPUHP_BP_KICK_AP] = {\\n\\t\\t.name\\t\\t\\t= \"cpu:kick_ap\",\\n\\t\\t.startup.single\\t\\t= cpuhp_kick_ap_alive,\\n\\t},\\n\\n\\t/*\\n\\t * Waits for the AP to reach cpuhp_ap_sync_alive() and then\\n\\t * releases it for the complete bringup.\\n\\t */\\n\\t[CPUHP_BRINGUP_CPU] = {\\n\\t\\t.name\\t\\t\\t= \"cpu:bringup\",\\n\\t\\t.startup.single\\t\\t= cpuhp_bringup_ap,\\n\\t\\t.teardown.single\\t= finish_cpu,\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n#else\\n\\t/*\\n\\t * All-in-one CPU bringup state which includes the kick alive.\\n\\t */\\n\\t[CPUHP_BRINGUP_CPU] = {\\n\\t\\t.name\\t\\t\\t= \"cpu:bringup\",\\n\\t\\t.startup.single\\t\\t= bringup_cpu,\\n\\t\\t.teardown.single\\t= finish_cpu,\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n#endif\\n\\t/* Final state before CPU kills itself */\\n\\t[CPUHP_AP_IDLE_DEAD] = {\\n\\t\\t.name\\t\\t\\t= \"idle:dead\",\\n\\t},\\n\\t/*\\n\\t * Last state before CPU enters the idle loop to die. Transient state\\n\\t * for synchronization.\\n\\t */\\n\\t[CPUHP_AP_OFFLINE] = {\\n\\t\\t.name\\t\\t\\t= \"ap:offline\",\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n\\t/* First state is scheduler control. Interrupts are disabled */\\n\\t[CPUHP_AP_SCHED_STARTING] = {\\n\\t\\t.name\\t\\t\\t= \"sched:starting\",\\n\\t\\t.startup.single\\t\\t= sched_cpu_starting,\\n\\t\\t.teardown.single\\t= sched_cpu_dying,\\n\\t},\\n\\t[CPUHP_AP_RCUTREE_DYING] = {\\n\\t\\t.name\\t\\t\\t= \"RCU/tree:dying\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= rcutree_dying_cpu,\\n\\t},\\n\\t[CPUHP_AP_SMPCFD_DYING] = {\\n\\t\\t.name\\t\\t\\t= \"smpcfd:dying\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= smpcfd_dying_cpu,\\n\\t},\\n\\t[CPUHP_AP_HRTIMERS_DYING] = {\\n\\t\\t.name\\t\\t\\t= \"hrtimers:dying\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= hrtimers_cpu_dying,\\n\\t},\\n\\t[CPUHP_AP_TICK_DYING] = {\\n\\t\\t.name\\t\\t\\t= \"tick:dying\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= tick_cpu_dying,\\n\\t},\\n\\t/* Entry state on starting. Interrupts enabled from here on. Transient\\n\\t * state for synchronsization */\\n\\t[CPUHP_AP_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"ap:online\",\\n\\t},\\n\\t/*\\n\\t * Handled on control processor until the plugged processor manages\\n\\t * this itself.\\n\\t */\\n\\t[CPUHP_TEARDOWN_CPU] = {\\n\\t\\t.name\\t\\t\\t= \"cpu:teardown\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= takedown_cpu,\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n\\n\\t[CPUHP_AP_SCHED_WAIT_EMPTY] = {\\n\\t\\t.name\\t\\t\\t= \"sched:waitempty\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= sched_cpu_wait_empty,\\n\\t},\\n\\n\\t/* Handle smpboot threads park/unpark */\\n\\t[CPUHP_AP_SMPBOOT_THREADS] = {\\n\\t\\t.name\\t\\t\\t= \"smpboot/threads:online\",\\n\\t\\t.startup.single\\t\\t= smpboot_unpark_threads,\\n\\t\\t.teardown.single\\t= smpboot_park_threads,\\n\\t},\\n\\t[CPUHP_AP_IRQ_AFFINITY_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"irq/affinity:online\",\\n\\t\\t.startup.single\\t\\t= irq_affinity_online_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_AP_PERF_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"perf:online\",\\n\\t\\t.startup.single\\t\\t= perf_event_init_cpu,\\n\\t\\t.teardown.single\\t= perf_event_exit_cpu,\\n\\t},\\n\\t[CPUHP_AP_WATCHDOG_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"lockup_detector:online\",\\n\\t\\t.startup.single\\t\\t= lockup_detector_online_cpu,\\n\\t\\t.teardown.single\\t= lockup_detector_offline_cpu,\\n\\t},\\n\\t[CPUHP_AP_WORKQUEUE_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"workqueue:online\",\\n\\t\\t.startup.single\\t\\t= workqueue_online_cpu,\\n\\t\\t.teardown.single\\t= workqueue_offline_cpu,\\n\\t},\\n\\t[CPUHP_AP_RANDOM_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"random:online\",\\n\\t\\t.startup.single\\t\\t= random_online_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_AP_RCUTREE_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"RCU/tree:online\",\\n\\t\\t.startup.single\\t\\t= rcutree_online_cpu,\\n\\t\\t.teardown.single\\t= rcutree_offline_cpu,\\n\\t},\\n#endif\\n\\t/*\\n\\t * The dynamically registered state space is here\\n\\t */\\n\\n#ifdef CONFIG_SMP\\n\\t/* Last state is scheduler control setting the cpu active */\\n\\t[CPUHP_AP_ACTIVE] = {\\n\\t\\t.name\\t\\t\\t= \"sched:active\",\\n\\t\\t.startup.single\\t\\t= sched_cpu_activate,\\n\\t\\t.teardown.single\\t= sched_cpu_deactivate,\\n\\t},\\n#endif\\n\\n\\t/* CPU is fully up and running. */\\n\\t[CPUHP_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"online\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n};\\n\\n/* Sanity check for callbacks */\\nstatic int cpuhp_cb_check(enum cpuhp_state state)\\n{\\n\\tif (state <= CPUHP_OFFLINE || state >= CPUHP_ONLINE)\\n\\t\\treturn -EINVAL;\\n\\treturn 0;\\n}\\n\\n/*\\n * Returns a free for dynamic slot assignment of the Online state. The states\\n * are protected by the cpuhp_slot_states mutex and an empty slot is identified\\n * by having no name assigned.\\n */\\nstatic int cpuhp_reserve_state(enum cpuhp_state state)\\n{\\n\\tenum cpuhp_state i, end;\\n\\tstruct cpuhp_step *step;\\n\\n\\tswitch (state) {\\n\\tcase CPUHP_AP_ONLINE_DYN:\\n\\t\\tstep = cpuhp_hp_states + CPUHP_AP_ONLINE_DYN;\\n\\t\\tend = CPUHP_AP_ONLINE_DYN_END;\\n\\t\\tbreak;\\n\\tcase CPUHP_BP_PREPARE_DYN:\\n\\t\\tstep = cpuhp_hp_states + CPUHP_BP_PREPARE_DYN;\\n\\t\\tend = CPUHP_BP_PREPARE_DYN_END;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tfor (i = state; i <= end; i++, step++) {\\n\\t\\tif (!step->name)\\n\\t\\t\\treturn i;\\n\\t}\\n\\tWARN(1, \"No more dynamic states available for CPU hotplug\\\\n\");\\n\\treturn -ENOSPC;\\n}\\n\\nstatic int cpuhp_store_callbacks(enum cpuhp_state state, const char *name,\\n\\t\\t\\t\\t int (*startup)(unsigned int cpu),\\n\\t\\t\\t\\t int (*teardown)(unsigned int cpu),\\n\\t\\t\\t\\t bool multi_instance)\\n{\\n\\t/* (Un)Install the callbacks for further cpu hotplug operations */\\n\\tstruct cpuhp_step *sp;\\n\\tint ret = 0;\\n\\n\\t/*\\n\\t * If name is NULL, then the state gets removed.\\n\\t *\\n\\t * CPUHP_AP_ONLINE_DYN and CPUHP_BP_PREPARE_DYN are handed out on\\n\\t * the first allocation from these dynamic ranges, so the removal\\n\\t * would trigger a new allocation and clear the wrong (already\\n\\t * empty) state, leaving the callbacks of the to be cleared state\\n\\t * dangling, which causes wreckage on the next hotplug operation.\\n\\t */\\n\\tif (name && (state == CPUHP_AP_ONLINE_DYN ||\\n\\t\\t     state == CPUHP_BP_PREPARE_DYN)) {\\n\\t\\tret = cpuhp_reserve_state(state);\\n\\t\\tif (ret < 0)\\n\\t\\t\\treturn ret;\\n\\t\\tstate = ret;\\n\\t}\\n\\tsp = cpuhp_get_step(state);\\n\\tif (name && sp->name)\\n\\t\\treturn -EBUSY;\\n\\n\\tsp->startup.single = startup;\\n\\tsp->teardown.single = teardown;\\n\\tsp->name = name;\\n\\tsp->multi_instance = multi_instance;\\n\\tINIT_HLIST_HEAD(&sp->list);\\n\\treturn ret;\\n}\\n\\nstatic void *cpuhp_get_teardown_cb(enum cpuhp_state state)\\n{\\n\\treturn cpuhp_get_step(state)->teardown.single;\\n}\\n\\n/*\\n * Call the startup/teardown function for a step either on the AP or\\n * on the current CPU.\\n */\\nstatic int cpuhp_issue_call(int cpu, enum cpuhp_state state, bool bringup,\\n\\t\\t\\t    struct hlist_node *node)\\n{\\n\\tstruct cpuhp_step *sp = cpuhp_get_step(state);\\n\\tint ret;\\n\\n\\t/*\\n\\t * If there\\'s nothing to do, we done.\\n\\t * Relies on the union for multi_instance.\\n\\t */\\n\\tif (cpuhp_step_empty(bringup, sp))\\n\\t\\treturn 0;\\n\\t/*\\n\\t * The non AP bound callbacks can fail on bringup. On teardown\\n\\t * e.g. module removal we crash for now.\\n\\t */\\n#ifdef CONFIG_SMP\\n\\tif (cpuhp_is_ap_state(state))\\n\\t\\tret = cpuhp_invoke_ap_callback(cpu, state, bringup, node);\\n\\telse\\n\\t\\tret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\\n#else\\n\\tret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\\n#endif\\n\\tBUG_ON(ret && !bringup);\\n\\treturn ret;\\n}\\n\\n/*\\n * Called from __cpuhp_setup_state on a recoverable failure.\\n *\\n * Note: The teardown callbacks for rollback are not allowed to fail!\\n */\\nstatic void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,\\n\\t\\t\\t\\t   struct hlist_node *node)\\n{\\n\\tint cpu;\\n\\n\\t/* Roll back the already executed steps on the other cpus */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpu >= failedcpu)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* Did we invoke the startup call on that cpu ? */\\n\\t\\tif (cpustate >= state)\\n\\t\\t\\tcpuhp_issue_call(cpu, state, false, node);\\n\\t}\\n}\\n\\nint __cpuhp_state_add_instance_cpuslocked(enum cpuhp_state state,\\n\\t\\t\\t\\t\\t  struct hlist_node *node,\\n\\t\\t\\t\\t\\t  bool invoke)\\n{\\n\\tstruct cpuhp_step *sp;\\n\\tint cpu;\\n\\tint ret;\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tsp = cpuhp_get_step(state);\\n\\tif (sp->multi_instance == false)\\n\\t\\treturn -EINVAL;\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\n\\tif (!invoke || !sp->startup.multi)\\n\\t\\tgoto add_node;\\n\\n\\t/*\\n\\t * Try to call the startup callback for each present cpu\\n\\t * depending on the hotplug state of the cpu.\\n\\t */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpustate < state)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tret = cpuhp_issue_call(cpu, state, true, node);\\n\\t\\tif (ret) {\\n\\t\\t\\tif (sp->teardown.multi)\\n\\t\\t\\t\\tcpuhp_rollback_install(cpu, state, node);\\n\\t\\t\\tgoto unlock;\\n\\t\\t}\\n\\t}\\nadd_node:\\n\\tret = 0;\\n\\thlist_add_head(node, &sp->list);\\nunlock:\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\treturn ret;\\n}\\n\\nint __cpuhp_state_add_instance(enum cpuhp_state state, struct hlist_node *node,\\n\\t\\t\\t       bool invoke)\\n{\\n\\tint ret;\\n\\n\\tcpus_read_lock();\\n\\tret = __cpuhp_state_add_instance_cpuslocked(state, node, invoke);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(__cpuhp_state_add_instance);\\n\\n/**\\n * __cpuhp_setup_state_cpuslocked - Setup the callbacks for an hotplug machine state\\n * @state:\\t\\tThe state to setup\\n * @name:\\t\\tName of the step\\n * @invoke:\\t\\tIf true, the startup function is invoked for cpus where\\n *\\t\\t\\tcpu state >= @state\\n * @startup:\\t\\tstartup callback function\\n * @teardown:\\t\\tteardown callback function\\n * @multi_instance:\\tState is set up for multiple instances which get\\n *\\t\\t\\tadded afterwards.\\n *\\n * The caller needs to hold cpus read locked while calling this function.\\n * Return:\\n *   On success:\\n *      Positive state number if @state is CPUHP_AP_ONLINE_DYN or CPUHP_BP_PREPARE_DYN;\\n *      0 for all other states\\n *   On failure: proper (negative) error code\\n */\\nint __cpuhp_setup_state_cpuslocked(enum cpuhp_state state,\\n\\t\\t\\t\\t   const char *name, bool invoke,\\n\\t\\t\\t\\t   int (*startup)(unsigned int cpu),\\n\\t\\t\\t\\t   int (*teardown)(unsigned int cpu),\\n\\t\\t\\t\\t   bool multi_instance)\\n{\\n\\tint cpu, ret = 0;\\n\\tbool dynstate;\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tif (cpuhp_cb_check(state) || !name)\\n\\t\\treturn -EINVAL;\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\n\\tret = cpuhp_store_callbacks(state, name, startup, teardown,\\n\\t\\t\\t\\t    multi_instance);\\n\\n\\tdynstate = state == CPUHP_AP_ONLINE_DYN || state == CPUHP_BP_PREPARE_DYN;\\n\\tif (ret > 0 && dynstate) {\\n\\t\\tstate = ret;\\n\\t\\tret = 0;\\n\\t}\\n\\n\\tif (ret || !invoke || !startup)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Try to call the startup callback for each present cpu\\n\\t * depending on the hotplug state of the cpu.\\n\\t */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpustate < state)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tret = cpuhp_issue_call(cpu, state, true, NULL);\\n\\t\\tif (ret) {\\n\\t\\t\\tif (teardown)\\n\\t\\t\\t\\tcpuhp_rollback_install(cpu, state, NULL);\\n\\t\\t\\tcpuhp_store_callbacks(state, NULL, NULL, NULL, false);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\nout:\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\t/*\\n\\t * If the requested state is CPUHP_AP_ONLINE_DYN or CPUHP_BP_PREPARE_DYN,\\n\\t * return the dynamically allocated state in case of success.\\n\\t */\\n\\tif (!ret && dynstate)\\n\\t\\treturn state;\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(__cpuhp_setup_state_cpuslocked);\\n\\nint __cpuhp_setup_state(enum cpuhp_state state,\\n\\t\\t\\tconst char *name, bool invoke,\\n\\t\\t\\tint (*startup)(unsigned int cpu),\\n\\t\\t\\tint (*teardown)(unsigned int cpu),\\n\\t\\t\\tbool multi_instance)\\n{\\n\\tint ret;\\n\\n\\tcpus_read_lock();\\n\\tret = __cpuhp_setup_state_cpuslocked(state, name, invoke, startup,\\n\\t\\t\\t\\t\\t     teardown, multi_instance);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(__cpuhp_setup_state);\\n\\nint __cpuhp_state_remove_instance(enum cpuhp_state state,\\n\\t\\t\\t\\t  struct hlist_node *node, bool invoke)\\n{\\n\\tstruct cpuhp_step *sp = cpuhp_get_step(state);\\n\\tint cpu;\\n\\n\\tBUG_ON(cpuhp_cb_check(state));\\n\\n\\tif (!sp->multi_instance)\\n\\t\\treturn -EINVAL;\\n\\n\\tcpus_read_lock();\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\n\\tif (!invoke || !cpuhp_get_teardown_cb(state))\\n\\t\\tgoto remove;\\n\\t/*\\n\\t * Call the teardown callback for each present cpu depending\\n\\t * on the hotplug state of the cpu. This function is not\\n\\t * allowed to fail currently!\\n\\t */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpustate >= state)\\n\\t\\t\\tcpuhp_issue_call(cpu, state, false, node);\\n\\t}\\n\\nremove:\\n\\thlist_del(node);\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\tcpus_read_unlock();\\n\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(__cpuhp_state_remove_instance);\\n\\n/**\\n * __cpuhp_remove_state_cpuslocked - Remove the callbacks for an hotplug machine state\\n * @state:\\tThe state to remove\\n * @invoke:\\tIf true, the teardown function is invoked for cpus where\\n *\\t\\tcpu state >= @state\\n *\\n * The caller needs to hold cpus read locked while calling this function.\\n * The teardown callback is currently not allowed to fail. Think\\n * about module removal!\\n */\\nvoid __cpuhp_remove_state_cpuslocked(enum cpuhp_state state, bool invoke)\\n{\\n\\tstruct cpuhp_step *sp = cpuhp_get_step(state);\\n\\tint cpu;\\n\\n\\tBUG_ON(cpuhp_cb_check(state));\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\tif (sp->multi_instance) {\\n\\t\\tWARN(!hlist_empty(&sp->list),\\n\\t\\t     \"Error: Removing state %d which has instances left.\\\\n\",\\n\\t\\t     state);\\n\\t\\tgoto remove;\\n\\t}\\n\\n\\tif (!invoke || !cpuhp_get_teardown_cb(state))\\n\\t\\tgoto remove;\\n\\n\\t/*\\n\\t * Call the teardown callback for each present cpu depending\\n\\t * on the hotplug state of the cpu. This function is not\\n\\t * allowed to fail currently!\\n\\t */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpustate >= state)\\n\\t\\t\\tcpuhp_issue_call(cpu, state, false, NULL);\\n\\t}\\nremove:\\n\\tcpuhp_store_callbacks(state, NULL, NULL, NULL, false);\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n}\\nEXPORT_SYMBOL(__cpuhp_remove_state_cpuslocked);\\n\\nvoid __cpuhp_remove_state(enum cpuhp_state state, bool invoke)\\n{\\n\\tcpus_read_lock();\\n\\t__cpuhp_remove_state_cpuslocked(state, invoke);\\n\\tcpus_read_unlock();\\n}\\nEXPORT_SYMBOL(__cpuhp_remove_state);\\n\\n#ifdef CONFIG_HOTPLUG_SMT\\nstatic void cpuhp_offline_cpu_device(unsigned int cpu)\\n{\\n\\tstruct device *dev = get_cpu_device(cpu);\\n\\n\\tdev->offline = true;\\n\\t/* Tell user space about the state change */\\n\\tkobject_uevent(&dev->kobj, KOBJ_OFFLINE);\\n}\\n\\nstatic void cpuhp_online_cpu_device(unsigned int cpu)\\n{\\n\\tstruct device *dev = get_cpu_device(cpu);\\n\\n\\tdev->offline = false;\\n\\t/* Tell user space about the state change */\\n\\tkobject_uevent(&dev->kobj, KOBJ_ONLINE);\\n}\\n\\nint cpuhp_smt_disable(enum cpuhp_smt_control ctrlval)\\n{\\n\\tint cpu, ret = 0;\\n\\n\\tcpu_maps_update_begin();\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tif (topology_is_primary_thread(cpu))\\n\\t\\t\\tcontinue;\\n\\t\\t/*\\n\\t\\t * Disable can be called with CPU_SMT_ENABLED when changing\\n\\t\\t * from a higher to lower number of SMT threads per core.\\n\\t\\t */\\n\\t\\tif (ctrlval == CPU_SMT_ENABLED && cpu_smt_thread_allowed(cpu))\\n\\t\\t\\tcontinue;\\n\\t\\tret = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t\\t/*\\n\\t\\t * As this needs to hold the cpu maps lock it\\'s impossible\\n\\t\\t * to call device_offline() because that ends up calling\\n\\t\\t * cpu_down() which takes cpu maps lock. cpu maps lock\\n\\t\\t * needs to be held as this might race against in kernel\\n\\t\\t * abusers of the hotplug machinery (thermal management).\\n\\t\\t *\\n\\t\\t * So nothing would update device:offline state. That would\\n\\t\\t * leave the sysfs entry stale and prevent onlining after\\n\\t\\t * smt control has been changed to \\'off\\' again. This is\\n\\t\\t * called under the sysfs hotplug lock, so it is properly\\n\\t\\t * serialized against the regular offline usage.\\n\\t\\t */\\n\\t\\tcpuhp_offline_cpu_device(cpu);\\n\\t}\\n\\tif (!ret)\\n\\t\\tcpu_smt_control = ctrlval;\\n\\tcpu_maps_update_done();\\n\\treturn ret;\\n}\\n\\n/* Check if the core a CPU belongs to is online */\\n#if !defined(topology_is_core_online)\\nstatic inline bool topology_is_core_online(unsigned int cpu)\\n{\\n\\treturn true;\\n}\\n#endif\\n\\nint cpuhp_smt_enable(void)\\n{\\n\\tint cpu, ret = 0;\\n\\n\\tcpu_maps_update_begin();\\n\\tcpu_smt_control = CPU_SMT_ENABLED;\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\t/* Skip online CPUs and CPUs on offline nodes */\\n\\t\\tif (cpu_online(cpu) || !node_online(cpu_to_node(cpu)))\\n\\t\\t\\tcontinue;\\n\\t\\tif (!cpu_smt_thread_allowed(cpu) || !topology_is_core_online(cpu))\\n\\t\\t\\tcontinue;\\n\\t\\tret = _cpu_up(cpu, 0, CPUHP_ONLINE);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t\\t/* See comment in cpuhp_smt_disable() */\\n\\t\\tcpuhp_online_cpu_device(cpu);\\n\\t}\\n\\tcpu_maps_update_done();\\n\\treturn ret;\\n}\\n#endif\\n\\n#if defined(CONFIG_SYSFS) && defined(CONFIG_HOTPLUG_CPU)\\nstatic ssize_t state_show(struct device *dev,\\n\\t\\t\\t  struct device_attribute *attr, char *buf)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\n\\treturn sprintf(buf, \"%d\\\\n\", st->state);\\n}\\nstatic DEVICE_ATTR_RO(state);\\n\\nstatic ssize_t target_store(struct device *dev, struct device_attribute *attr,\\n\\t\\t\\t    const char *buf, size_t count)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\tstruct cpuhp_step *sp;\\n\\tint target, ret;\\n\\n\\tret = kstrtoint(buf, 10, &target);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n#ifdef CONFIG_CPU_HOTPLUG_STATE_CONTROL\\n\\tif (target < CPUHP_OFFLINE || target > CPUHP_ONLINE)\\n\\t\\treturn -EINVAL;\\n#else\\n\\tif (target != CPUHP_OFFLINE && target != CPUHP_ONLINE)\\n\\t\\treturn -EINVAL;\\n#endif\\n\\n\\tret = lock_device_hotplug_sysfs();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\tsp = cpuhp_get_step(target);\\n\\tret = !sp->name || sp->cant_stop ? -EINVAL : 0;\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\tif (st->state < target)\\n\\t\\tret = cpu_up(dev->id, target);\\n\\telse if (st->state > target)\\n\\t\\tret = cpu_down(dev->id, target);\\n\\telse if (WARN_ON(st->target != target))\\n\\t\\tst->target = target;\\nout:\\n\\tunlock_device_hotplug();\\n\\treturn ret ? ret : count;\\n}\\n\\nstatic ssize_t target_show(struct device *dev,\\n\\t\\t\\t   struct device_attribute *attr, char *buf)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\n\\treturn sprintf(buf, \"%d\\\\n\", st->target);\\n}\\nstatic DEVICE_ATTR_RW(target);\\n\\nstatic ssize_t fail_store(struct device *dev, struct device_attribute *attr,\\n\\t\\t\\t  const char *buf, size_t count)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\tstruct cpuhp_step *sp;\\n\\tint fail, ret;\\n\\n\\tret = kstrtoint(buf, 10, &fail);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (fail == CPUHP_INVALID) {\\n\\t\\tst->fail = fail;\\n\\t\\treturn count;\\n\\t}\\n\\n\\tif (fail < CPUHP_OFFLINE || fail > CPUHP_ONLINE)\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Cannot fail STARTING/DYING callbacks.\\n\\t */\\n\\tif (cpuhp_is_atomic_state(fail))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * DEAD callbacks cannot fail...\\n\\t * ... neither can CPUHP_BRINGUP_CPU during hotunplug. The latter\\n\\t * triggering STARTING callbacks, a failure in this state would\\n\\t * hinder rollback.\\n\\t */\\n\\tif (fail <= CPUHP_BRINGUP_CPU && st->state > CPUHP_BRINGUP_CPU)\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Cannot fail anything that doesn\\'t have callbacks.\\n\\t */\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\tsp = cpuhp_get_step(fail);\\n\\tif (!sp->startup.single && !sp->teardown.single)\\n\\t\\tret = -EINVAL;\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tst->fail = fail;\\n\\n\\treturn count;\\n}\\n\\nstatic ssize_t fail_show(struct device *dev,\\n\\t\\t\\t struct device_attribute *attr, char *buf)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\n\\treturn sprintf(buf, \"%d\\\\n\", st->fail);\\n}\\n\\nstatic DEVICE_ATTR_RW(fail);\\n\\nstatic struct attribute *cpuhp_cpu_attrs[] = {\\n\\t&dev_attr_state.attr,\\n\\t&dev_attr_target.attr,\\n\\t&dev_attr_fail.attr,\\n\\tNULL\\n};\\n\\nstatic const struct attribute_group cpuhp_cpu_attr_group = {\\n\\t.attrs = cpuhp_cpu_attrs,\\n\\t.name = \"hotplug\",\\n};\\n\\nstatic ssize_t states_show(struct device *dev,\\n\\t\\t\\t\\t struct device_attribute *attr, char *buf)\\n{\\n\\tssize_t cur, res = 0;\\n\\tint i;\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\tfor (i = CPUHP_OFFLINE; i <= CPUHP_ONLINE; i++) {\\n\\t\\tstruct cpuhp_step *sp = cpuhp_get_step(i);\\n\\n\\t\\tif (sp->name) {\\n\\t\\t\\tcur = sprintf(buf, \"%3d: %s\\\\n\", i, sp->name);\\n\\t\\t\\tbuf += cur;\\n\\t\\t\\tres += cur;\\n\\t\\t}\\n\\t}\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\treturn res;\\n}\\nstatic DEVICE_ATTR_RO(states);\\n\\nstatic struct attribute *cpuhp_cpu_root_attrs[] = {\\n\\t&dev_attr_states.attr,\\n\\tNULL\\n};\\n\\nstatic const struct attribute_group cpuhp_cpu_root_attr_group = {\\n\\t.attrs = cpuhp_cpu_root_attrs,\\n\\t.name = \"hotplug\",\\n};\\n\\n#ifdef CONFIG_HOTPLUG_SMT\\n\\nstatic bool cpu_smt_num_threads_valid(unsigned int threads)\\n{\\n\\tif (IS_ENABLED(CONFIG_SMT_NUM_THREADS_DYNAMIC))\\n\\t\\treturn threads >= 1 && threads <= cpu_smt_max_threads;\\n\\treturn threads == 1 || threads == cpu_smt_max_threads;\\n}\\n\\nstatic ssize_t\\n__store_smt_control(struct device *dev, struct device_attribute *attr,\\n\\t\\t    const char *buf, size_t count)\\n{\\n\\tint ctrlval, ret, num_threads, orig_threads;\\n\\tbool force_off;\\n\\n\\tif (cpu_smt_control == CPU_SMT_FORCE_DISABLED)\\n\\t\\treturn -EPERM;\\n\\n\\tif (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)\\n\\t\\treturn -ENODEV;\\n\\n\\tif (sysfs_streq(buf, \"on\")) {\\n\\t\\tctrlval = CPU_SMT_ENABLED;\\n\\t\\tnum_threads = cpu_smt_max_threads;\\n\\t} else if (sysfs_streq(buf, \"off\")) {\\n\\t\\tctrlval = CPU_SMT_DISABLED;\\n\\t\\tnum_threads = 1;\\n\\t} else if (sysfs_streq(buf, \"forceoff\")) {\\n\\t\\tctrlval = CPU_SMT_FORCE_DISABLED;\\n\\t\\tnum_threads = 1;\\n\\t} else if (kstrtoint(buf, 10, &num_threads) == 0) {\\n\\t\\tif (num_threads == 1)\\n\\t\\t\\tctrlval = CPU_SMT_DISABLED;\\n\\t\\telse if (cpu_smt_num_threads_valid(num_threads))\\n\\t\\t\\tctrlval = CPU_SMT_ENABLED;\\n\\t\\telse\\n\\t\\t\\treturn -EINVAL;\\n\\t} else {\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tret = lock_device_hotplug_sysfs();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\torig_threads = cpu_smt_num_threads;\\n\\tcpu_smt_num_threads = num_threads;\\n\\n\\tforce_off = ctrlval != cpu_smt_control && ctrlval == CPU_SMT_FORCE_DISABLED;\\n\\n\\tif (num_threads > orig_threads)\\n\\t\\tret = cpuhp_smt_enable();\\n\\telse if (num_threads < orig_threads || force_off)\\n\\t\\tret = cpuhp_smt_disable(ctrlval);\\n\\n\\tunlock_device_hotplug();\\n\\treturn ret ? ret : count;\\n}\\n\\n#else /* !CONFIG_HOTPLUG_SMT */\\nstatic ssize_t\\n__store_smt_control(struct device *dev, struct device_attribute *attr,\\n\\t\\t    const char *buf, size_t count)\\n{\\n\\treturn -ENODEV;\\n}\\n#endif /* CONFIG_HOTPLUG_SMT */\\n\\nstatic const char *smt_states[] = {\\n\\t[CPU_SMT_ENABLED]\\t\\t= \"on\",\\n\\t[CPU_SMT_DISABLED]\\t\\t= \"off\",\\n\\t[CPU_SMT_FORCE_DISABLED]\\t= \"forceoff\",\\n\\t[CPU_SMT_NOT_SUPPORTED]\\t\\t= \"notsupported\",\\n\\t[CPU_SMT_NOT_IMPLEMENTED]\\t= \"notimplemented\",\\n};\\n\\nstatic ssize_t control_show(struct device *dev,\\n\\t\\t\\t    struct device_attribute *attr, char *buf)\\n{\\n\\tconst char *state = smt_states[cpu_smt_control];\\n\\n#ifdef CONFIG_HOTPLUG_SMT\\n\\t/*\\n\\t * If SMT is enabled but not all threads are enabled then show the\\n\\t * number of threads. If all threads are enabled show \"on\". Otherwise\\n\\t * show the state name.\\n\\t */\\n\\tif (cpu_smt_control == CPU_SMT_ENABLED &&\\n\\t    cpu_smt_num_threads != cpu_smt_max_threads)\\n\\t\\treturn sysfs_emit(buf, \"%d\\\\n\", cpu_smt_num_threads);\\n#endif\\n\\n\\treturn sysfs_emit(buf, \"%s\\\\n\", state);\\n}\\n\\nstatic ssize_t control_store(struct device *dev, struct device_attribute *attr,\\n\\t\\t\\t     const char *buf, size_t count)\\n{\\n\\treturn __store_smt_control(dev, attr, buf, count);\\n}\\nstatic DEVICE_ATTR_RW(control);\\n\\nstatic ssize_t active_show(struct device *dev,\\n\\t\\t\\t   struct device_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", sched_smt_active());\\n}\\nstatic DEVICE_ATTR_RO(active);\\n\\nstatic struct attribute *cpuhp_smt_attrs[] = {\\n\\t&dev_attr_control.attr,\\n\\t&dev_attr_active.attr,\\n\\tNULL\\n};\\n\\nstatic const struct attribute_group cpuhp_smt_attr_group = {\\n\\t.attrs = cpuhp_smt_attrs,\\n\\t.name = \"smt\",\\n};\\n\\nstatic int __init cpu_smt_sysfs_init(void)\\n{\\n\\tstruct device *dev_root;\\n\\tint ret = -ENODEV;\\n\\n\\tdev_root = bus_get_dev_root(&cpu_subsys);\\n\\tif (dev_root) {\\n\\t\\tret = sysfs_create_group(&dev_root->kobj, &cpuhp_smt_attr_group);\\n\\t\\tput_device(dev_root);\\n\\t}\\n\\treturn ret;\\n}\\n\\nstatic int __init cpuhp_sysfs_init(void)\\n{\\n\\tstruct device *dev_root;\\n\\tint cpu, ret;\\n\\n\\tret = cpu_smt_sysfs_init();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tdev_root = bus_get_dev_root(&cpu_subsys);\\n\\tif (dev_root) {\\n\\t\\tret = sysfs_create_group(&dev_root->kobj, &cpuhp_cpu_root_attr_group);\\n\\t\\tput_device(dev_root);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tstruct device *dev = get_cpu_device(cpu);\\n\\n\\t\\tif (!dev)\\n\\t\\t\\tcontinue;\\n\\t\\tret = sysfs_create_group(&dev->kobj, &cpuhp_cpu_attr_group);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\treturn 0;\\n}\\ndevice_initcall(cpuhp_sysfs_init);\\n#endif /* CONFIG_SYSFS && CONFIG_HOTPLUG_CPU */\\n\\n/*\\n * cpu_bit_bitmap[] is a special, \"compressed\" data structure that\\n * represents all NR_CPUS bits binary values of 1<<nr.\\n *\\n * It is used by cpumask_of() to get a constant address to a CPU\\n * mask value that has a single bit set only.\\n */\\n\\n/* cpu_bit_bitmap[0] is empty - so we can back into it */\\n#define MASK_DECLARE_1(x)\\t[x+1][0] = (1UL << (x))\\n#define MASK_DECLARE_2(x)\\tMASK_DECLARE_1(x), MASK_DECLARE_1(x+1)\\n#define MASK_DECLARE_4(x)\\tMASK_DECLARE_2(x), MASK_DECLARE_2(x+2)\\n#define MASK_DECLARE_8(x)\\tMASK_DECLARE_4(x), MASK_DECLARE_4(x+4)\\n\\nconst unsigned long cpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)] = {\\n\\n\\tMASK_DECLARE_8(0),\\tMASK_DECLARE_8(8),\\n\\tMASK_DECLARE_8(16),\\tMASK_DECLARE_8(24),\\n#if BITS_PER_LONG > 32\\n\\tMASK_DECLARE_8(32),\\tMASK_DECLARE_8(40),\\n\\tMASK_DECLARE_8(48),\\tMASK_DECLARE_8(56),\\n#endif\\n};\\nEXPORT_SYMBOL_GPL(cpu_bit_bitmap);\\n\\nconst DECLARE_BITMAP(cpu_all_bits, NR_CPUS) = CPU_BITS_ALL;\\nEXPORT_SYMBOL(cpu_all_bits);\\n\\n#ifdef CONFIG_INIT_ALL_POSSIBLE\\nstruct cpumask __cpu_possible_mask __ro_after_init\\n\\t= {CPU_BITS_ALL};\\n#else\\nstruct cpumask __cpu_possible_mask __ro_after_init;\\n#endif\\nEXPORT_SYMBOL(__cpu_possible_mask);\\n\\nstruct cpumask __cpu_online_mask __read_mostly;\\nEXPORT_SYMBOL(__cpu_online_mask);\\n\\nstruct cpumask __cpu_enabled_mask __read_mostly;\\nEXPORT_SYMBOL(__cpu_enabled_mask);\\n\\nstruct cpumask __cpu_present_mask __read_mostly;\\nEXPORT_SYMBOL(__cpu_present_mask);\\n\\nstruct cpumask __cpu_active_mask __read_mostly;\\nEXPORT_SYMBOL(__cpu_active_mask);\\n\\nstruct cpumask __cpu_dying_mask __read_mostly;\\nEXPORT_SYMBOL(__cpu_dying_mask);\\n\\natomic_t __num_online_cpus __read_mostly;\\nEXPORT_SYMBOL(__num_online_cpus);\\n\\nvoid init_cpu_present(const struct cpumask *src)\\n{\\n\\tcpumask_copy(&__cpu_present_mask, src);\\n}\\n\\nvoid init_cpu_possible(const struct cpumask *src)\\n{\\n\\tcpumask_copy(&__cpu_possible_mask, src);\\n}\\n\\nvoid init_cpu_online(const struct cpumask *src)\\n{\\n\\tcpumask_copy(&__cpu_online_mask, src);\\n}\\n\\nvoid set_cpu_online(unsigned int cpu, bool online)\\n{\\n\\t/*\\n\\t * atomic_inc/dec() is required to handle the horrid abuse of this\\n\\t * function by the reboot and kexec code which invoke it from\\n\\t * IPI/NMI broadcasts when shutting down CPUs. Invocation from\\n\\t * regular CPU hotplug is properly serialized.\\n\\t *\\n\\t * Note, that the fact that __num_online_cpus is of type atomic_t\\n\\t * does not protect readers which are not serialized against\\n\\t * concurrent hotplug operations.\\n\\t */\\n\\tif (online) {\\n\\t\\tif (!cpumask_test_and_set_cpu(cpu, &__cpu_online_mask))\\n\\t\\t\\tatomic_inc(&__num_online_cpus);\\n\\t} else {\\n\\t\\tif (cpumask_test_and_clear_cpu(cpu, &__cpu_online_mask))\\n\\t\\t\\tatomic_dec(&__num_online_cpus);\\n\\t}\\n}\\n\\n/*\\n * Activate the first processor.\\n */\\nvoid __init boot_cpu_init(void)\\n{\\n\\tint cpu = smp_processor_id();\\n\\n\\t/* Mark the boot cpu \"present\", \"online\" etc for SMP and UP case */\\n\\tset_cpu_online(cpu, true);\\n\\tset_cpu_active(cpu, true);\\n\\tset_cpu_present(cpu, true);\\n\\tset_cpu_possible(cpu, true);\\n\\n#ifdef CONFIG_SMP\\n\\t__boot_cpu_id = cpu;\\n#endif\\n}\\n\\n/*\\n * Must be called _AFTER_ setting up the per_cpu areas\\n */\\nvoid __init boot_cpu_hotplug_init(void)\\n{\\n#ifdef CONFIG_SMP\\n\\tcpumask_set_cpu(smp_processor_id(), &cpus_booted_once_mask);\\n\\tatomic_set(this_cpu_ptr(&cpuhp_state.ap_sync_state), SYNC_STATE_ONLINE);\\n#endif\\n\\tthis_cpu_write(cpuhp_state.state, CPUHP_ONLINE);\\n\\tthis_cpu_write(cpuhp_state.target, CPUHP_ONLINE);\\n}\\n\\n#ifdef CONFIG_CPU_MITIGATIONS\\n/*\\n * These are used for a global \"mitigations=\" cmdline option for toggling\\n * optional CPU mitigations.\\n */\\nenum cpu_mitigations {\\n\\tCPU_MITIGATIONS_OFF,\\n\\tCPU_MITIGATIONS_AUTO,\\n\\tCPU_MITIGATIONS_AUTO_NOSMT,\\n};\\n\\nstatic enum cpu_mitigations cpu_mitigations __ro_after_init = CPU_MITIGATIONS_AUTO;\\n\\nstatic int __init mitigations_parse_cmdline(char *arg)\\n{\\n\\tif (!strcmp(arg, \"off\"))\\n\\t\\tcpu_mitigations = CPU_MITIGATIONS_OFF;\\n\\telse if (!strcmp(arg, \"auto\"))\\n\\t\\tcpu_mitigations = CPU_MITIGATIONS_AUTO;\\n\\telse if (!strcmp(arg, \"auto,nosmt\"))\\n\\t\\tcpu_mitigations = CPU_MITIGATIONS_AUTO_NOSMT;\\n\\telse\\n\\t\\tpr_crit(\"Unsupported mitigations=%s, system may still be vulnerable\\\\n\",\\n\\t\\t\\targ);\\n\\n\\treturn 0;\\n}\\n\\n/* mitigations=off */\\nbool cpu_mitigations_off(void)\\n{\\n\\treturn cpu_mitigations == CPU_MITIGATIONS_OFF;\\n}\\nEXPORT_SYMBOL_GPL(cpu_mitigations_off);\\n\\n/* mitigations=auto,nosmt */\\nbool cpu_mitigations_auto_nosmt(void)\\n{\\n\\treturn cpu_mitigations == CPU_MITIGATIONS_AUTO_NOSMT;\\n}\\nEXPORT_SYMBOL_GPL(cpu_mitigations_auto_nosmt);\\n#else\\nstatic int __init mitigations_parse_cmdline(char *arg)\\n{\\n\\tpr_crit(\"Kernel compiled without mitigations, ignoring \\'mitigations\\'; system may still be vulnerable\\\\n\");\\n\\treturn 0;\\n}\\n#endif\\nearly_param(\"mitigations\", mitigations_parse_cmdline);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Detect Hung Task\\n *\\n * kernel/hung_task.c - kernel thread for detecting tasks stuck in D state\\n *\\n */\\n\\n#include <linux/mm.h>\\n#include <linux/cpu.h>\\n#include <linux/nmi.h>\\n#include <linux/init.h>\\n#include <linux/delay.h>\\n#include <linux/freezer.h>\\n#include <linux/kthread.h>\\n#include <linux/lockdep.h>\\n#include <linux/export.h>\\n#include <linux/panic_notifier.h>\\n#include <linux/sysctl.h>\\n#include <linux/suspend.h>\\n#include <linux/utsname.h>\\n#include <linux/sched/signal.h>\\n#include <linux/sched/debug.h>\\n#include <linux/sched/sysctl.h>\\n\\n#include <trace/events/sched.h>\\n\\n/*\\n * The number of tasks checked:\\n */\\nstatic int __read_mostly sysctl_hung_task_check_count = PID_MAX_LIMIT;\\n\\n/*\\n * Total number of tasks detected as hung since boot:\\n */\\nstatic unsigned long __read_mostly sysctl_hung_task_detect_count;\\n\\n/*\\n * Limit number of tasks checked in a batch.\\n *\\n * This value controls the preemptibility of khungtaskd since preemption\\n * is disabled during the critical section. It also controls the size of\\n * the RCU grace period. So it needs to be upper-bound.\\n */\\n#define HUNG_TASK_LOCK_BREAK (HZ / 10)\\n\\n/*\\n * Zero means infinite timeout - no checking done:\\n */\\nunsigned long __read_mostly sysctl_hung_task_timeout_secs = CONFIG_DEFAULT_HUNG_TASK_TIMEOUT;\\nEXPORT_SYMBOL_GPL(sysctl_hung_task_timeout_secs);\\n\\n/*\\n * Zero (default value) means use sysctl_hung_task_timeout_secs:\\n */\\nstatic unsigned long __read_mostly sysctl_hung_task_check_interval_secs;\\n\\nstatic int __read_mostly sysctl_hung_task_warnings = 10;\\n\\nstatic int __read_mostly did_panic;\\nstatic bool hung_task_show_lock;\\nstatic bool hung_task_call_panic;\\nstatic bool hung_task_show_all_bt;\\n\\nstatic struct task_struct *watchdog_task;\\n\\n#ifdef CONFIG_SMP\\n/*\\n * Should we dump all CPUs backtraces in a hung task event?\\n * Defaults to 0, can be changed via sysctl.\\n */\\nstatic unsigned int __read_mostly sysctl_hung_task_all_cpu_backtrace;\\n#else\\n#define sysctl_hung_task_all_cpu_backtrace 0\\n#endif /* CONFIG_SMP */\\n\\n/*\\n * Should we panic (and reboot, if panic_timeout= is set) when a\\n * hung task is detected:\\n */\\nstatic unsigned int __read_mostly sysctl_hung_task_panic =\\n\\tIS_ENABLED(CONFIG_BOOTPARAM_HUNG_TASK_PANIC);\\n\\nstatic int\\nhung_task_panic(struct notifier_block *this, unsigned long event, void *ptr)\\n{\\n\\tdid_panic = 1;\\n\\n\\treturn NOTIFY_DONE;\\n}\\n\\nstatic struct notifier_block panic_block = {\\n\\t.notifier_call = hung_task_panic,\\n};\\n\\nstatic void check_hung_task(struct task_struct *t, unsigned long timeout)\\n{\\n\\tunsigned long switch_count = t->nvcsw + t->nivcsw;\\n\\n\\t/*\\n\\t * Ensure the task is not frozen.\\n\\t * Also, skip vfork and any other user process that freezer should skip.\\n\\t */\\n\\tif (unlikely(READ_ONCE(t->__state) & TASK_FROZEN))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * When a freshly created task is scheduled once, changes its state to\\n\\t * TASK_UNINTERRUPTIBLE without having ever been switched out once, it\\n\\t * musn\\'t be checked.\\n\\t */\\n\\tif (unlikely(!switch_count))\\n\\t\\treturn;\\n\\n\\tif (switch_count != t->last_switch_count) {\\n\\t\\tt->last_switch_count = switch_count;\\n\\t\\tt->last_switch_time = jiffies;\\n\\t\\treturn;\\n\\t}\\n\\tif (time_is_after_jiffies(t->last_switch_time + timeout * HZ))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * This counter tracks the total number of tasks detected as hung\\n\\t * since boot.\\n\\t */\\n\\tsysctl_hung_task_detect_count++;\\n\\n\\ttrace_sched_process_hang(t);\\n\\n\\tif (sysctl_hung_task_panic) {\\n\\t\\tconsole_verbose();\\n\\t\\thung_task_show_lock = true;\\n\\t\\thung_task_call_panic = true;\\n\\t}\\n\\n\\t/*\\n\\t * Ok, the task did not get scheduled for more than 2 minutes,\\n\\t * complain:\\n\\t */\\n\\tif (sysctl_hung_task_warnings || hung_task_call_panic) {\\n\\t\\tif (sysctl_hung_task_warnings > 0)\\n\\t\\t\\tsysctl_hung_task_warnings--;\\n\\t\\tpr_err(\"INFO: task %s:%d blocked for more than %ld seconds.\\\\n\",\\n\\t\\t       t->comm, t->pid, (jiffies - t->last_switch_time) / HZ);\\n\\t\\tpr_err(\"      %s %s %.*s\\\\n\",\\n\\t\\t\\tprint_tainted(), init_utsname()->release,\\n\\t\\t\\t(int)strcspn(init_utsname()->version, \" \"),\\n\\t\\t\\tinit_utsname()->version);\\n\\t\\tpr_err(\"\\\\\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\\\\\"\"\\n\\t\\t\\t\" disables this message.\\\\n\");\\n\\t\\tsched_show_task(t);\\n\\t\\thung_task_show_lock = true;\\n\\n\\t\\tif (sysctl_hung_task_all_cpu_backtrace)\\n\\t\\t\\thung_task_show_all_bt = true;\\n\\t\\tif (!sysctl_hung_task_warnings)\\n\\t\\t\\tpr_info(\"Future hung task reports are suppressed, see sysctl kernel.hung_task_warnings\\\\n\");\\n\\t}\\n\\n\\ttouch_nmi_watchdog();\\n}\\n\\n/*\\n * To avoid extending the RCU grace period for an unbounded amount of time,\\n * periodically exit the critical section and enter a new one.\\n *\\n * For preemptible RCU it is sufficient to call rcu_read_unlock in order\\n * to exit the grace period. For classic RCU, a reschedule is required.\\n */\\nstatic bool rcu_lock_break(struct task_struct *g, struct task_struct *t)\\n{\\n\\tbool can_cont;\\n\\n\\tget_task_struct(g);\\n\\tget_task_struct(t);\\n\\trcu_read_unlock();\\n\\tcond_resched();\\n\\trcu_read_lock();\\n\\tcan_cont = pid_alive(g) && pid_alive(t);\\n\\tput_task_struct(t);\\n\\tput_task_struct(g);\\n\\n\\treturn can_cont;\\n}\\n\\n/*\\n * Check whether a TASK_UNINTERRUPTIBLE does not get woken up for\\n * a really long time (120 seconds). If that happens, print out\\n * a warning.\\n */\\nstatic void check_hung_uninterruptible_tasks(unsigned long timeout)\\n{\\n\\tint max_count = sysctl_hung_task_check_count;\\n\\tunsigned long last_break = jiffies;\\n\\tstruct task_struct *g, *t;\\n\\n\\t/*\\n\\t * If the system crashed already then all bets are off,\\n\\t * do not report extra hung tasks:\\n\\t */\\n\\tif (test_taint(TAINT_DIE) || did_panic)\\n\\t\\treturn;\\n\\n\\thung_task_show_lock = false;\\n\\trcu_read_lock();\\n\\tfor_each_process_thread(g, t) {\\n\\t\\tunsigned int state;\\n\\n\\t\\tif (!max_count--)\\n\\t\\t\\tgoto unlock;\\n\\t\\tif (time_after(jiffies, last_break + HUNG_TASK_LOCK_BREAK)) {\\n\\t\\t\\tif (!rcu_lock_break(g, t))\\n\\t\\t\\t\\tgoto unlock;\\n\\t\\t\\tlast_break = jiffies;\\n\\t\\t}\\n\\t\\t/*\\n\\t\\t * skip the TASK_KILLABLE tasks -- these can be killed\\n\\t\\t * skip the TASK_IDLE tasks -- those are genuinely idle\\n\\t\\t */\\n\\t\\tstate = READ_ONCE(t->__state);\\n\\t\\tif ((state & TASK_UNINTERRUPTIBLE) &&\\n\\t\\t    !(state & TASK_WAKEKILL) &&\\n\\t\\t    !(state & TASK_NOLOAD))\\n\\t\\t\\tcheck_hung_task(t, timeout);\\n\\t}\\n unlock:\\n\\trcu_read_unlock();\\n\\tif (hung_task_show_lock)\\n\\t\\tdebug_show_all_locks();\\n\\n\\tif (hung_task_show_all_bt) {\\n\\t\\thung_task_show_all_bt = false;\\n\\t\\ttrigger_all_cpu_backtrace();\\n\\t}\\n\\n\\tif (hung_task_call_panic)\\n\\t\\tpanic(\"hung_task: blocked tasks\");\\n}\\n\\nstatic long hung_timeout_jiffies(unsigned long last_checked,\\n\\t\\t\\t\\t unsigned long timeout)\\n{\\n\\t/* timeout of 0 will disable the watchdog */\\n\\treturn timeout ? last_checked - jiffies + timeout * HZ :\\n\\t\\tMAX_SCHEDULE_TIMEOUT;\\n}\\n\\n#ifdef CONFIG_SYSCTL\\n/*\\n * Process updating of timeout sysctl\\n */\\nstatic int proc_dohung_task_timeout_secs(const struct ctl_table *table, int write,\\n\\t\\t\\t\\t  void *buffer,\\n\\t\\t\\t\\t  size_t *lenp, loff_t *ppos)\\n{\\n\\tint ret;\\n\\n\\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);\\n\\n\\tif (ret || !write)\\n\\t\\tgoto out;\\n\\n\\twake_up_process(watchdog_task);\\n\\n out:\\n\\treturn ret;\\n}\\n\\n/*\\n * This is needed for proc_doulongvec_minmax of sysctl_hung_task_timeout_secs\\n * and hung_task_check_interval_secs\\n */\\nstatic const unsigned long hung_task_timeout_max = (LONG_MAX / HZ);\\nstatic struct ctl_table hung_task_sysctls[] = {\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_all_cpu_backtrace\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_all_cpu_backtrace,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#endif /* CONFIG_SMP */\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_panic\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_panic,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_check_count\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_check_count,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_timeout_secs\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_timeout_secs,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dohung_task_timeout_secs,\\n\\t\\t.extra2\\t\\t= (void *)&hung_task_timeout_max,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_check_interval_secs\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_check_interval_secs,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dohung_task_timeout_secs,\\n\\t\\t.extra2\\t\\t= (void *)&hung_task_timeout_max,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_warnings\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_warnings,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_NEG_ONE,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_detect_count\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_detect_count,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n};\\n\\nstatic void __init hung_task_sysctl_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", hung_task_sysctls);\\n}\\n#else\\n#define hung_task_sysctl_init() do { } while (0)\\n#endif /* CONFIG_SYSCTL */\\n\\n\\nstatic atomic_t reset_hung_task = ATOMIC_INIT(0);\\n\\nvoid reset_hung_task_detector(void)\\n{\\n\\tatomic_set(&reset_hung_task, 1);\\n}\\nEXPORT_SYMBOL_GPL(reset_hung_task_detector);\\n\\nstatic bool hung_detector_suspended;\\n\\nstatic int hungtask_pm_notify(struct notifier_block *self,\\n\\t\\t\\t      unsigned long action, void *hcpu)\\n{\\n\\tswitch (action) {\\n\\tcase PM_SUSPEND_PREPARE:\\n\\tcase PM_HIBERNATION_PREPARE:\\n\\tcase PM_RESTORE_PREPARE:\\n\\t\\thung_detector_suspended = true;\\n\\t\\tbreak;\\n\\tcase PM_POST_SUSPEND:\\n\\tcase PM_POST_HIBERNATION:\\n\\tcase PM_POST_RESTORE:\\n\\t\\thung_detector_suspended = false;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tbreak;\\n\\t}\\n\\treturn NOTIFY_OK;\\n}\\n\\n/*\\n * kthread which checks for tasks stuck in D state\\n */\\nstatic int watchdog(void *dummy)\\n{\\n\\tunsigned long hung_last_checked = jiffies;\\n\\n\\tset_user_nice(current, 0);\\n\\n\\tfor ( ; ; ) {\\n\\t\\tunsigned long timeout = sysctl_hung_task_timeout_secs;\\n\\t\\tunsigned long interval = sysctl_hung_task_check_interval_secs;\\n\\t\\tlong t;\\n\\n\\t\\tif (interval == 0)\\n\\t\\t\\tinterval = timeout;\\n\\t\\tinterval = min_t(unsigned long, interval, timeout);\\n\\t\\tt = hung_timeout_jiffies(hung_last_checked, interval);\\n\\t\\tif (t <= 0) {\\n\\t\\t\\tif (!atomic_xchg(&reset_hung_task, 0) &&\\n\\t\\t\\t    !hung_detector_suspended)\\n\\t\\t\\t\\tcheck_hung_uninterruptible_tasks(timeout);\\n\\t\\t\\thung_last_checked = jiffies;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tschedule_timeout_interruptible(t);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic int __init hung_task_init(void)\\n{\\n\\tatomic_notifier_chain_register(&panic_notifier_list, &panic_block);\\n\\n\\t/* Disable hung task detector on suspend */\\n\\tpm_notifier(hungtask_pm_notify, 0);\\n\\n\\twatchdog_task = kthread_run(watchdog, NULL, \"khungtaskd\");\\n\\thung_task_sysctl_init();\\n\\n\\treturn 0;\\n}\\nsubsys_initcall(hung_task_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Copyright (C) 2010 Red Hat, Inc., Peter Zijlstra\\n *\\n * Provides a framework for enqueueing and running callbacks from hardirq\\n * context. The enqueueing is NMI-safe.\\n */\\n\\n#include <linux/bug.h>\\n#include <linux/kernel.h>\\n#include <linux/export.h>\\n#include <linux/irq_work.h>\\n#include <linux/percpu.h>\\n#include <linux/hardirq.h>\\n#include <linux/irqflags.h>\\n#include <linux/sched.h>\\n#include <linux/tick.h>\\n#include <linux/cpu.h>\\n#include <linux/notifier.h>\\n#include <linux/smp.h>\\n#include <linux/smpboot.h>\\n#include <asm/processor.h>\\n#include <linux/kasan.h>\\n\\n#include <trace/events/ipi.h>\\n\\nstatic DEFINE_PER_CPU(struct llist_head, raised_list);\\nstatic DEFINE_PER_CPU(struct llist_head, lazy_list);\\nstatic DEFINE_PER_CPU(struct task_struct *, irq_workd);\\n\\nstatic void wake_irq_workd(void)\\n{\\n\\tstruct task_struct *tsk = __this_cpu_read(irq_workd);\\n\\n\\tif (!llist_empty(this_cpu_ptr(&lazy_list)) && tsk)\\n\\t\\twake_up_process(tsk);\\n}\\n\\n#ifdef CONFIG_SMP\\nstatic void irq_work_wake(struct irq_work *entry)\\n{\\n\\twake_irq_workd();\\n}\\n\\nstatic DEFINE_PER_CPU(struct irq_work, irq_work_wakeup) =\\n\\tIRQ_WORK_INIT_HARD(irq_work_wake);\\n#endif\\n\\nstatic int irq_workd_should_run(unsigned int cpu)\\n{\\n\\treturn !llist_empty(this_cpu_ptr(&lazy_list));\\n}\\n\\n/*\\n * Claim the entry so that no one else will poke at it.\\n */\\nstatic bool irq_work_claim(struct irq_work *work)\\n{\\n\\tint oflags;\\n\\n\\toflags = atomic_fetch_or(IRQ_WORK_CLAIMED | CSD_TYPE_IRQ_WORK, &work->node.a_flags);\\n\\t/*\\n\\t * If the work is already pending, no need to raise the IPI.\\n\\t * The pairing smp_mb() in irq_work_single() makes sure\\n\\t * everything we did before is visible.\\n\\t */\\n\\tif (oflags & IRQ_WORK_PENDING)\\n\\t\\treturn false;\\n\\treturn true;\\n}\\n\\nvoid __weak arch_irq_work_raise(void)\\n{\\n\\t/*\\n\\t * Lame architectures will get the timer tick callback\\n\\t */\\n}\\n\\nstatic __always_inline void irq_work_raise(struct irq_work *work)\\n{\\n\\tif (trace_ipi_send_cpu_enabled() && arch_irq_work_has_interrupt())\\n\\t\\ttrace_ipi_send_cpu(smp_processor_id(), _RET_IP_, work->func);\\n\\n\\tarch_irq_work_raise();\\n}\\n\\n/* Enqueue on current CPU, work must already be claimed and preempt disabled */\\nstatic void __irq_work_queue_local(struct irq_work *work)\\n{\\n\\tstruct llist_head *list;\\n\\tbool rt_lazy_work = false;\\n\\tbool lazy_work = false;\\n\\tint work_flags;\\n\\n\\twork_flags = atomic_read(&work->node.a_flags);\\n\\tif (work_flags & IRQ_WORK_LAZY)\\n\\t\\tlazy_work = true;\\n\\telse if (IS_ENABLED(CONFIG_PREEMPT_RT) &&\\n\\t\\t !(work_flags & IRQ_WORK_HARD_IRQ))\\n\\t\\trt_lazy_work = true;\\n\\n\\tif (lazy_work || rt_lazy_work)\\n\\t\\tlist = this_cpu_ptr(&lazy_list);\\n\\telse\\n\\t\\tlist = this_cpu_ptr(&raised_list);\\n\\n\\tif (!llist_add(&work->node.llist, list))\\n\\t\\treturn;\\n\\n\\t/* If the work is \"lazy\", handle it from next tick if any */\\n\\tif (!lazy_work || tick_nohz_tick_stopped())\\n\\t\\tirq_work_raise(work);\\n}\\n\\n/* Enqueue the irq work @work on the current CPU */\\nbool irq_work_queue(struct irq_work *work)\\n{\\n\\t/* Only queue if not already pending */\\n\\tif (!irq_work_claim(work))\\n\\t\\treturn false;\\n\\n\\t/* Queue the entry and raise the IPI if needed. */\\n\\tpreempt_disable();\\n\\t__irq_work_queue_local(work);\\n\\tpreempt_enable();\\n\\n\\treturn true;\\n}\\nEXPORT_SYMBOL_GPL(irq_work_queue);\\n\\n/*\\n * Enqueue the irq_work @work on @cpu unless it\\'s already pending\\n * somewhere.\\n *\\n * Can be re-enqueued while the callback is still in progress.\\n */\\nbool irq_work_queue_on(struct irq_work *work, int cpu)\\n{\\n#ifndef CONFIG_SMP\\n\\treturn irq_work_queue(work);\\n\\n#else /* CONFIG_SMP: */\\n\\t/* All work should have been flushed before going offline */\\n\\tWARN_ON_ONCE(cpu_is_offline(cpu));\\n\\n\\t/* Only queue if not already pending */\\n\\tif (!irq_work_claim(work))\\n\\t\\treturn false;\\n\\n\\tkasan_record_aux_stack_noalloc(work);\\n\\n\\tpreempt_disable();\\n\\tif (cpu != smp_processor_id()) {\\n\\t\\t/* Arch remote IPI send/receive backend aren\\'t NMI safe */\\n\\t\\tWARN_ON_ONCE(in_nmi());\\n\\n\\t\\t/*\\n\\t\\t * On PREEMPT_RT the items which are not marked as\\n\\t\\t * IRQ_WORK_HARD_IRQ are added to the lazy list and a HARD work\\n\\t\\t * item is used on the remote CPU to wake the thread.\\n\\t\\t */\\n\\t\\tif (IS_ENABLED(CONFIG_PREEMPT_RT) &&\\n\\t\\t    !(atomic_read(&work->node.a_flags) & IRQ_WORK_HARD_IRQ)) {\\n\\n\\t\\t\\tif (!llist_add(&work->node.llist, &per_cpu(lazy_list, cpu)))\\n\\t\\t\\t\\tgoto out;\\n\\n\\t\\t\\twork = &per_cpu(irq_work_wakeup, cpu);\\n\\t\\t\\tif (!irq_work_claim(work))\\n\\t\\t\\t\\tgoto out;\\n\\t\\t}\\n\\n\\t\\t__smp_call_single_queue(cpu, &work->node.llist);\\n\\t} else {\\n\\t\\t__irq_work_queue_local(work);\\n\\t}\\nout:\\n\\tpreempt_enable();\\n\\n\\treturn true;\\n#endif /* CONFIG_SMP */\\n}\\n\\nbool irq_work_needs_cpu(void)\\n{\\n\\tstruct llist_head *raised, *lazy;\\n\\n\\traised = this_cpu_ptr(&raised_list);\\n\\tlazy = this_cpu_ptr(&lazy_list);\\n\\n\\tif (llist_empty(raised) || arch_irq_work_has_interrupt())\\n\\t\\tif (llist_empty(lazy))\\n\\t\\t\\treturn false;\\n\\n\\t/* All work should have been flushed before going offline */\\n\\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));\\n\\n\\treturn true;\\n}\\n\\nvoid irq_work_single(void *arg)\\n{\\n\\tstruct irq_work *work = arg;\\n\\tint flags;\\n\\n\\t/*\\n\\t * Clear the PENDING bit, after this point the @work can be re-used.\\n\\t * The PENDING bit acts as a lock, and we own it, so we can clear it\\n\\t * without atomic ops.\\n\\t */\\n\\tflags = atomic_read(&work->node.a_flags);\\n\\tflags &= ~IRQ_WORK_PENDING;\\n\\tatomic_set(&work->node.a_flags, flags);\\n\\n\\t/*\\n\\t * See irq_work_claim().\\n\\t */\\n\\tsmp_mb();\\n\\n\\tlockdep_irq_work_enter(flags);\\n\\twork->func(work);\\n\\tlockdep_irq_work_exit(flags);\\n\\n\\t/*\\n\\t * Clear the BUSY bit, if set, and return to the free state if no-one\\n\\t * else claimed it meanwhile.\\n\\t */\\n\\t(void)atomic_cmpxchg(&work->node.a_flags, flags, flags & ~IRQ_WORK_BUSY);\\n\\n\\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||\\n\\t    !arch_irq_work_has_interrupt())\\n\\t\\trcuwait_wake_up(&work->irqwait);\\n}\\n\\nstatic void irq_work_run_list(struct llist_head *list)\\n{\\n\\tstruct irq_work *work, *tmp;\\n\\tstruct llist_node *llnode;\\n\\n\\t/*\\n\\t * On PREEMPT_RT IRQ-work which is not marked as HARD will be processed\\n\\t * in a per-CPU thread in preemptible context. Only the items which are\\n\\t * marked as IRQ_WORK_HARD_IRQ will be processed in hardirq context.\\n\\t */\\n\\tBUG_ON(!irqs_disabled() && !IS_ENABLED(CONFIG_PREEMPT_RT));\\n\\n\\tif (llist_empty(list))\\n\\t\\treturn;\\n\\n\\tllnode = llist_del_all(list);\\n\\tllist_for_each_entry_safe(work, tmp, llnode, node.llist)\\n\\t\\tirq_work_single(work);\\n}\\n\\n/*\\n * hotplug calls this through:\\n *  hotplug_cfd() -> flush_smp_call_function_queue()\\n */\\nvoid irq_work_run(void)\\n{\\n\\tirq_work_run_list(this_cpu_ptr(&raised_list));\\n\\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\\n\\t\\tirq_work_run_list(this_cpu_ptr(&lazy_list));\\n\\telse\\n\\t\\twake_irq_workd();\\n}\\nEXPORT_SYMBOL_GPL(irq_work_run);\\n\\nvoid irq_work_tick(void)\\n{\\n\\tstruct llist_head *raised = this_cpu_ptr(&raised_list);\\n\\n\\tif (!llist_empty(raised) && !arch_irq_work_has_interrupt())\\n\\t\\tirq_work_run_list(raised);\\n\\n\\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\\n\\t\\tirq_work_run_list(this_cpu_ptr(&lazy_list));\\n\\telse\\n\\t\\twake_irq_workd();\\n}\\n\\n/*\\n * Synchronize against the irq_work @entry, ensures the entry is not\\n * currently in use.\\n */\\nvoid irq_work_sync(struct irq_work *work)\\n{\\n\\tlockdep_assert_irqs_enabled();\\n\\tmight_sleep();\\n\\n\\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||\\n\\t    !arch_irq_work_has_interrupt()) {\\n\\t\\trcuwait_wait_event(&work->irqwait, !irq_work_is_busy(work),\\n\\t\\t\\t\\t   TASK_UNINTERRUPTIBLE);\\n\\t\\treturn;\\n\\t}\\n\\n\\twhile (irq_work_is_busy(work))\\n\\t\\tcpu_relax();\\n}\\nEXPORT_SYMBOL_GPL(irq_work_sync);\\n\\nstatic void run_irq_workd(unsigned int cpu)\\n{\\n\\tirq_work_run_list(this_cpu_ptr(&lazy_list));\\n}\\n\\nstatic void irq_workd_setup(unsigned int cpu)\\n{\\n\\tsched_set_fifo_low(current);\\n}\\n\\nstatic struct smp_hotplug_thread irqwork_threads = {\\n\\t.store                  = &irq_workd,\\n\\t.setup\\t\\t\\t= irq_workd_setup,\\n\\t.thread_should_run      = irq_workd_should_run,\\n\\t.thread_fn              = run_irq_workd,\\n\\t.thread_comm            = \"irq_work/%u\",\\n};\\n\\nstatic __init int irq_work_init_threads(void)\\n{\\n\\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\\n\\t\\tBUG_ON(smpboot_register_percpu_thread(&irqwork_threads));\\n\\treturn 0;\\n}\\nearly_initcall(irq_work_init_threads);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * jump label support\\n *\\n * Copyright (C) 2009 Jason Baron <jbaron@redhat.com>\\n * Copyright (C) 2011 Peter Zijlstra\\n *\\n */\\n#include <linux/memory.h>\\n#include <linux/uaccess.h>\\n#include <linux/module.h>\\n#include <linux/list.h>\\n#include <linux/slab.h>\\n#include <linux/sort.h>\\n#include <linux/err.h>\\n#include <linux/static_key.h>\\n#include <linux/jump_label_ratelimit.h>\\n#include <linux/bug.h>\\n#include <linux/cpu.h>\\n#include <asm/sections.h>\\n\\n/* mutex to protect coming/going of the jump_label table */\\nstatic DEFINE_MUTEX(jump_label_mutex);\\n\\nvoid jump_label_lock(void)\\n{\\n\\tmutex_lock(&jump_label_mutex);\\n}\\n\\nvoid jump_label_unlock(void)\\n{\\n\\tmutex_unlock(&jump_label_mutex);\\n}\\n\\nstatic int jump_label_cmp(const void *a, const void *b)\\n{\\n\\tconst struct jump_entry *jea = a;\\n\\tconst struct jump_entry *jeb = b;\\n\\n\\t/*\\n\\t * Entrires are sorted by key.\\n\\t */\\n\\tif (jump_entry_key(jea) < jump_entry_key(jeb))\\n\\t\\treturn -1;\\n\\n\\tif (jump_entry_key(jea) > jump_entry_key(jeb))\\n\\t\\treturn 1;\\n\\n\\t/*\\n\\t * In the batching mode, entries should also be sorted by the code\\n\\t * inside the already sorted list of entries, enabling a bsearch in\\n\\t * the vector.\\n\\t */\\n\\tif (jump_entry_code(jea) < jump_entry_code(jeb))\\n\\t\\treturn -1;\\n\\n\\tif (jump_entry_code(jea) > jump_entry_code(jeb))\\n\\t\\treturn 1;\\n\\n\\treturn 0;\\n}\\n\\nstatic void jump_label_swap(void *a, void *b, int size)\\n{\\n\\tlong delta = (unsigned long)a - (unsigned long)b;\\n\\tstruct jump_entry *jea = a;\\n\\tstruct jump_entry *jeb = b;\\n\\tstruct jump_entry tmp = *jea;\\n\\n\\tjea->code\\t= jeb->code - delta;\\n\\tjea->target\\t= jeb->target - delta;\\n\\tjea->key\\t= jeb->key - delta;\\n\\n\\tjeb->code\\t= tmp.code + delta;\\n\\tjeb->target\\t= tmp.target + delta;\\n\\tjeb->key\\t= tmp.key + delta;\\n}\\n\\nstatic void\\njump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)\\n{\\n\\tunsigned long size;\\n\\tvoid *swapfn = NULL;\\n\\n\\tif (IS_ENABLED(CONFIG_HAVE_ARCH_JUMP_LABEL_RELATIVE))\\n\\t\\tswapfn = jump_label_swap;\\n\\n\\tsize = (((unsigned long)stop - (unsigned long)start)\\n\\t\\t\\t\\t\\t/ sizeof(struct jump_entry));\\n\\tsort(start, size, sizeof(struct jump_entry), jump_label_cmp, swapfn);\\n}\\n\\nstatic void jump_label_update(struct static_key *key);\\n\\n/*\\n * There are similar definitions for the !CONFIG_JUMP_LABEL case in jump_label.h.\\n * The use of \\'atomic_read()\\' requires atomic.h and its problematic for some\\n * kernel headers such as kernel.h and others. Since static_key_count() is not\\n * used in the branch statements as it is for the !CONFIG_JUMP_LABEL case its ok\\n * to have it be a function here. Similarly, for \\'static_key_enable()\\' and\\n * \\'static_key_disable()\\', which require bug.h. This should allow jump_label.h\\n * to be included from most/all places for CONFIG_JUMP_LABEL.\\n */\\nint static_key_count(struct static_key *key)\\n{\\n\\t/*\\n\\t * -1 means the first static_key_slow_inc() is in progress.\\n\\t *  static_key_enabled() must return true, so return 1 here.\\n\\t */\\n\\tint n = atomic_read(&key->enabled);\\n\\n\\treturn n >= 0 ? n : 1;\\n}\\nEXPORT_SYMBOL_GPL(static_key_count);\\n\\n/*\\n * static_key_fast_inc_not_disabled - adds a user for a static key\\n * @key: static key that must be already enabled\\n *\\n * The caller must make sure that the static key can\\'t get disabled while\\n * in this function. It doesn\\'t patch jump labels, only adds a user to\\n * an already enabled static key.\\n *\\n * Returns true if the increment was done. Unlike refcount_t the ref counter\\n * is not saturated, but will fail to increment on overflow.\\n */\\nbool static_key_fast_inc_not_disabled(struct static_key *key)\\n{\\n\\tint v;\\n\\n\\tSTATIC_KEY_CHECK_USE(key);\\n\\t/*\\n\\t * Negative key->enabled has a special meaning: it sends\\n\\t * static_key_slow_inc/dec() down the slow path, and it is non-zero\\n\\t * so it counts as \"enabled\" in jump_label_update().\\n\\t *\\n\\t * The INT_MAX overflow condition is either used by the networking\\n\\t * code to reset or detected in the slow path of\\n\\t * static_key_slow_inc_cpuslocked().\\n\\t */\\n\\tv = atomic_read(&key->enabled);\\n\\tdo {\\n\\t\\tif (v <= 0 || v == INT_MAX)\\n\\t\\t\\treturn false;\\n\\t} while (!likely(atomic_try_cmpxchg(&key->enabled, &v, v + 1)));\\n\\n\\treturn true;\\n}\\nEXPORT_SYMBOL_GPL(static_key_fast_inc_not_disabled);\\n\\nbool static_key_slow_inc_cpuslocked(struct static_key *key)\\n{\\n\\tlockdep_assert_cpus_held();\\n\\n\\t/*\\n\\t * Careful if we get concurrent static_key_slow_inc/dec() calls;\\n\\t * later calls must wait for the first one to _finish_ the\\n\\t * jump_label_update() process.  At the same time, however,\\n\\t * the jump_label_update() call below wants to see\\n\\t * static_key_enabled(&key) for jumps to be updated properly.\\n\\t */\\n\\tif (static_key_fast_inc_not_disabled(key))\\n\\t\\treturn true;\\n\\n\\tguard(mutex)(&jump_label_mutex);\\n\\t/* Try to mark it as \\'enabling in progress. */\\n\\tif (!atomic_cmpxchg(&key->enabled, 0, -1)) {\\n\\t\\tjump_label_update(key);\\n\\t\\t/*\\n\\t\\t * Ensure that when static_key_fast_inc_not_disabled() or\\n\\t\\t * static_key_dec_not_one() observe the positive value,\\n\\t\\t * they must also observe all the text changes.\\n\\t\\t */\\n\\t\\tatomic_set_release(&key->enabled, 1);\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * While holding the mutex this should never observe\\n\\t\\t * anything else than a value >= 1 and succeed\\n\\t\\t */\\n\\t\\tif (WARN_ON_ONCE(!static_key_fast_inc_not_disabled(key)))\\n\\t\\t\\treturn false;\\n\\t}\\n\\treturn true;\\n}\\n\\nbool static_key_slow_inc(struct static_key *key)\\n{\\n\\tbool ret;\\n\\n\\tcpus_read_lock();\\n\\tret = static_key_slow_inc_cpuslocked(key);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(static_key_slow_inc);\\n\\nvoid static_key_enable_cpuslocked(struct static_key *key)\\n{\\n\\tSTATIC_KEY_CHECK_USE(key);\\n\\tlockdep_assert_cpus_held();\\n\\n\\tif (atomic_read(&key->enabled) > 0) {\\n\\t\\tWARN_ON_ONCE(atomic_read(&key->enabled) != 1);\\n\\t\\treturn;\\n\\t}\\n\\n\\tjump_label_lock();\\n\\tif (atomic_read(&key->enabled) == 0) {\\n\\t\\tatomic_set(&key->enabled, -1);\\n\\t\\tjump_label_update(key);\\n\\t\\t/*\\n\\t\\t * See static_key_slow_inc().\\n\\t\\t */\\n\\t\\tatomic_set_release(&key->enabled, 1);\\n\\t}\\n\\tjump_label_unlock();\\n}\\nEXPORT_SYMBOL_GPL(static_key_enable_cpuslocked);\\n\\nvoid static_key_enable(struct static_key *key)\\n{\\n\\tcpus_read_lock();\\n\\tstatic_key_enable_cpuslocked(key);\\n\\tcpus_read_unlock();\\n}\\nEXPORT_SYMBOL_GPL(static_key_enable);\\n\\nvoid static_key_disable_cpuslocked(struct static_key *key)\\n{\\n\\tSTATIC_KEY_CHECK_USE(key);\\n\\tlockdep_assert_cpus_held();\\n\\n\\tif (atomic_read(&key->enabled) != 1) {\\n\\t\\tWARN_ON_ONCE(atomic_read(&key->enabled) != 0);\\n\\t\\treturn;\\n\\t}\\n\\n\\tjump_label_lock();\\n\\tif (atomic_cmpxchg(&key->enabled, 1, 0) == 1)\\n\\t\\tjump_label_update(key);\\n\\tjump_label_unlock();\\n}\\nEXPORT_SYMBOL_GPL(static_key_disable_cpuslocked);\\n\\nvoid static_key_disable(struct static_key *key)\\n{\\n\\tcpus_read_lock();\\n\\tstatic_key_disable_cpuslocked(key);\\n\\tcpus_read_unlock();\\n}\\nEXPORT_SYMBOL_GPL(static_key_disable);\\n\\nstatic bool static_key_dec_not_one(struct static_key *key)\\n{\\n\\tint v;\\n\\n\\t/*\\n\\t * Go into the slow path if key::enabled is less than or equal than\\n\\t * one. One is valid to shut down the key, anything less than one\\n\\t * is an imbalance, which is handled at the call site.\\n\\t *\\n\\t * That includes the special case of \\'-1\\' which is set in\\n\\t * static_key_slow_inc_cpuslocked(), but that\\'s harmless as it is\\n\\t * fully serialized in the slow path below. By the time this task\\n\\t * acquires the jump label lock the value is back to one and the\\n\\t * retry under the lock must succeed.\\n\\t */\\n\\tv = atomic_read(&key->enabled);\\n\\tdo {\\n\\t\\t/*\\n\\t\\t * Warn about the \\'-1\\' case though; since that means a\\n\\t\\t * decrement is concurrent with a first (0->1) increment. IOW\\n\\t\\t * people are trying to disable something that wasn\\'t yet fully\\n\\t\\t * enabled. This suggests an ordering problem on the user side.\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(v < 0);\\n\\n\\t\\t/*\\n\\t\\t * Warn about underflow, and lie about success in an attempt to\\n\\t\\t * not make things worse.\\n\\t\\t */\\n\\t\\tif (WARN_ON_ONCE(v == 0))\\n\\t\\t\\treturn true;\\n\\n\\t\\tif (v <= 1)\\n\\t\\t\\treturn false;\\n\\t} while (!likely(atomic_try_cmpxchg(&key->enabled, &v, v - 1)));\\n\\n\\treturn true;\\n}\\n\\nstatic void __static_key_slow_dec_cpuslocked(struct static_key *key)\\n{\\n\\tlockdep_assert_cpus_held();\\n\\tint val;\\n\\n\\tif (static_key_dec_not_one(key))\\n\\t\\treturn;\\n\\n\\tguard(mutex)(&jump_label_mutex);\\n\\tval = atomic_read(&key->enabled);\\n\\t/*\\n\\t * It should be impossible to observe -1 with jump_label_mutex held,\\n\\t * see static_key_slow_inc_cpuslocked().\\n\\t */\\n\\tif (WARN_ON_ONCE(val == -1))\\n\\t\\treturn;\\n\\t/*\\n\\t * Cannot already be 0, something went sideways.\\n\\t */\\n\\tif (WARN_ON_ONCE(val == 0))\\n\\t\\treturn;\\n\\n\\tif (atomic_dec_and_test(&key->enabled))\\n\\t\\tjump_label_update(key);\\n}\\n\\nstatic void __static_key_slow_dec(struct static_key *key)\\n{\\n\\tcpus_read_lock();\\n\\t__static_key_slow_dec_cpuslocked(key);\\n\\tcpus_read_unlock();\\n}\\n\\nvoid jump_label_update_timeout(struct work_struct *work)\\n{\\n\\tstruct static_key_deferred *key =\\n\\t\\tcontainer_of(work, struct static_key_deferred, work.work);\\n\\t__static_key_slow_dec(&key->key);\\n}\\nEXPORT_SYMBOL_GPL(jump_label_update_timeout);\\n\\nvoid static_key_slow_dec(struct static_key *key)\\n{\\n\\tSTATIC_KEY_CHECK_USE(key);\\n\\t__static_key_slow_dec(key);\\n}\\nEXPORT_SYMBOL_GPL(static_key_slow_dec);\\n\\nvoid static_key_slow_dec_cpuslocked(struct static_key *key)\\n{\\n\\tSTATIC_KEY_CHECK_USE(key);\\n\\t__static_key_slow_dec_cpuslocked(key);\\n}\\n\\nvoid __static_key_slow_dec_deferred(struct static_key *key,\\n\\t\\t\\t\\t    struct delayed_work *work,\\n\\t\\t\\t\\t    unsigned long timeout)\\n{\\n\\tSTATIC_KEY_CHECK_USE(key);\\n\\n\\tif (static_key_dec_not_one(key))\\n\\t\\treturn;\\n\\n\\tschedule_delayed_work(work, timeout);\\n}\\nEXPORT_SYMBOL_GPL(__static_key_slow_dec_deferred);\\n\\nvoid __static_key_deferred_flush(void *key, struct delayed_work *work)\\n{\\n\\tSTATIC_KEY_CHECK_USE(key);\\n\\tflush_delayed_work(work);\\n}\\nEXPORT_SYMBOL_GPL(__static_key_deferred_flush);\\n\\nvoid jump_label_rate_limit(struct static_key_deferred *key,\\n\\t\\tunsigned long rl)\\n{\\n\\tSTATIC_KEY_CHECK_USE(key);\\n\\tkey->timeout = rl;\\n\\tINIT_DELAYED_WORK(&key->work, jump_label_update_timeout);\\n}\\nEXPORT_SYMBOL_GPL(jump_label_rate_limit);\\n\\nstatic int addr_conflict(struct jump_entry *entry, void *start, void *end)\\n{\\n\\tif (jump_entry_code(entry) <= (unsigned long)end &&\\n\\t    jump_entry_code(entry) + jump_entry_size(entry) > (unsigned long)start)\\n\\t\\treturn 1;\\n\\n\\treturn 0;\\n}\\n\\nstatic int __jump_label_text_reserved(struct jump_entry *iter_start,\\n\\t\\tstruct jump_entry *iter_stop, void *start, void *end, bool init)\\n{\\n\\tstruct jump_entry *iter;\\n\\n\\titer = iter_start;\\n\\twhile (iter < iter_stop) {\\n\\t\\tif (init || !jump_entry_is_init(iter)) {\\n\\t\\t\\tif (addr_conflict(iter, start, end))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t}\\n\\t\\titer++;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n#ifndef arch_jump_label_transform_static\\nstatic void arch_jump_label_transform_static(struct jump_entry *entry,\\n\\t\\t\\t\\t\\t     enum jump_label_type type)\\n{\\n\\t/* nothing to do on most architectures */\\n}\\n#endif\\n\\nstatic inline struct jump_entry *static_key_entries(struct static_key *key)\\n{\\n\\tWARN_ON_ONCE(key->type & JUMP_TYPE_LINKED);\\n\\treturn (struct jump_entry *)(key->type & ~JUMP_TYPE_MASK);\\n}\\n\\nstatic inline bool static_key_type(struct static_key *key)\\n{\\n\\treturn key->type & JUMP_TYPE_TRUE;\\n}\\n\\nstatic inline bool static_key_linked(struct static_key *key)\\n{\\n\\treturn key->type & JUMP_TYPE_LINKED;\\n}\\n\\nstatic inline void static_key_clear_linked(struct static_key *key)\\n{\\n\\tkey->type &= ~JUMP_TYPE_LINKED;\\n}\\n\\nstatic inline void static_key_set_linked(struct static_key *key)\\n{\\n\\tkey->type |= JUMP_TYPE_LINKED;\\n}\\n\\n/***\\n * A \\'struct static_key\\' uses a union such that it either points directly\\n * to a table of \\'struct jump_entry\\' or to a linked list of modules which in\\n * turn point to \\'struct jump_entry\\' tables.\\n *\\n * The two lower bits of the pointer are used to keep track of which pointer\\n * type is in use and to store the initial branch direction, we use an access\\n * function which preserves these bits.\\n */\\nstatic void static_key_set_entries(struct static_key *key,\\n\\t\\t\\t\\t   struct jump_entry *entries)\\n{\\n\\tunsigned long type;\\n\\n\\tWARN_ON_ONCE((unsigned long)entries & JUMP_TYPE_MASK);\\n\\ttype = key->type & JUMP_TYPE_MASK;\\n\\tkey->entries = entries;\\n\\tkey->type |= type;\\n}\\n\\nstatic enum jump_label_type jump_label_type(struct jump_entry *entry)\\n{\\n\\tstruct static_key *key = jump_entry_key(entry);\\n\\tbool enabled = static_key_enabled(key);\\n\\tbool branch = jump_entry_is_branch(entry);\\n\\n\\t/* See the comment in linux/jump_label.h */\\n\\treturn enabled ^ branch;\\n}\\n\\nstatic bool jump_label_can_update(struct jump_entry *entry, bool init)\\n{\\n\\t/*\\n\\t * Cannot update code that was in an init text area.\\n\\t */\\n\\tif (!init && jump_entry_is_init(entry))\\n\\t\\treturn false;\\n\\n\\tif (!kernel_text_address(jump_entry_code(entry))) {\\n\\t\\t/*\\n\\t\\t * This skips patching built-in __exit, which\\n\\t\\t * is part of init_section_contains() but is\\n\\t\\t * not part of kernel_text_address().\\n\\t\\t *\\n\\t\\t * Skipping built-in __exit is fine since it\\n\\t\\t * will never be executed.\\n\\t\\t */\\n\\t\\tWARN_ONCE(!jump_entry_is_init(entry),\\n\\t\\t\\t  \"can\\'t patch jump_label at %pS\",\\n\\t\\t\\t  (void *)jump_entry_code(entry));\\n\\t\\treturn false;\\n\\t}\\n\\n\\treturn true;\\n}\\n\\n#ifndef HAVE_JUMP_LABEL_BATCH\\nstatic void __jump_label_update(struct static_key *key,\\n\\t\\t\\t\\tstruct jump_entry *entry,\\n\\t\\t\\t\\tstruct jump_entry *stop,\\n\\t\\t\\t\\tbool init)\\n{\\n\\tfor (; (entry < stop) && (jump_entry_key(entry) == key); entry++) {\\n\\t\\tif (jump_label_can_update(entry, init))\\n\\t\\t\\tarch_jump_label_transform(entry, jump_label_type(entry));\\n\\t}\\n}\\n#else\\nstatic void __jump_label_update(struct static_key *key,\\n\\t\\t\\t\\tstruct jump_entry *entry,\\n\\t\\t\\t\\tstruct jump_entry *stop,\\n\\t\\t\\t\\tbool init)\\n{\\n\\tfor (; (entry < stop) && (jump_entry_key(entry) == key); entry++) {\\n\\n\\t\\tif (!jump_label_can_update(entry, init))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (!arch_jump_label_transform_queue(entry, jump_label_type(entry))) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Queue is full: Apply the current queue and try again.\\n\\t\\t\\t */\\n\\t\\t\\tarch_jump_label_transform_apply();\\n\\t\\t\\tBUG_ON(!arch_jump_label_transform_queue(entry, jump_label_type(entry)));\\n\\t\\t}\\n\\t}\\n\\tarch_jump_label_transform_apply();\\n}\\n#endif\\n\\nvoid __init jump_label_init(void)\\n{\\n\\tstruct jump_entry *iter_start = __start___jump_table;\\n\\tstruct jump_entry *iter_stop = __stop___jump_table;\\n\\tstruct static_key *key = NULL;\\n\\tstruct jump_entry *iter;\\n\\n\\t/*\\n\\t * Since we are initializing the static_key.enabled field with\\n\\t * with the \\'raw\\' int values (to avoid pulling in atomic.h) in\\n\\t * jump_label.h, let\\'s make sure that is safe. There are only two\\n\\t * cases to check since we initialize to 0 or 1.\\n\\t */\\n\\tBUILD_BUG_ON((int)ATOMIC_INIT(0) != 0);\\n\\tBUILD_BUG_ON((int)ATOMIC_INIT(1) != 1);\\n\\n\\tif (static_key_initialized)\\n\\t\\treturn;\\n\\n\\tcpus_read_lock();\\n\\tjump_label_lock();\\n\\tjump_label_sort_entries(iter_start, iter_stop);\\n\\n\\tfor (iter = iter_start; iter < iter_stop; iter++) {\\n\\t\\tstruct static_key *iterk;\\n\\t\\tbool in_init;\\n\\n\\t\\t/* rewrite NOPs */\\n\\t\\tif (jump_label_type(iter) == JUMP_LABEL_NOP)\\n\\t\\t\\tarch_jump_label_transform_static(iter, JUMP_LABEL_NOP);\\n\\n\\t\\tin_init = init_section_contains((void *)jump_entry_code(iter), 1);\\n\\t\\tjump_entry_set_init(iter, in_init);\\n\\n\\t\\titerk = jump_entry_key(iter);\\n\\t\\tif (iterk == key)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tkey = iterk;\\n\\t\\tstatic_key_set_entries(key, iter);\\n\\t}\\n\\tstatic_key_initialized = true;\\n\\tjump_label_unlock();\\n\\tcpus_read_unlock();\\n}\\n\\nstatic inline bool static_key_sealed(struct static_key *key)\\n{\\n\\treturn (key->type & JUMP_TYPE_LINKED) && !(key->type & ~JUMP_TYPE_MASK);\\n}\\n\\nstatic inline void static_key_seal(struct static_key *key)\\n{\\n\\tunsigned long type = key->type & JUMP_TYPE_TRUE;\\n\\tkey->type = JUMP_TYPE_LINKED | type;\\n}\\n\\nvoid jump_label_init_ro(void)\\n{\\n\\tstruct jump_entry *iter_start = __start___jump_table;\\n\\tstruct jump_entry *iter_stop = __stop___jump_table;\\n\\tstruct jump_entry *iter;\\n\\n\\tif (WARN_ON_ONCE(!static_key_initialized))\\n\\t\\treturn;\\n\\n\\tcpus_read_lock();\\n\\tjump_label_lock();\\n\\n\\tfor (iter = iter_start; iter < iter_stop; iter++) {\\n\\t\\tstruct static_key *iterk = jump_entry_key(iter);\\n\\n\\t\\tif (!is_kernel_ro_after_init((unsigned long)iterk))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (static_key_sealed(iterk))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tstatic_key_seal(iterk);\\n\\t}\\n\\n\\tjump_label_unlock();\\n\\tcpus_read_unlock();\\n}\\n\\n#ifdef CONFIG_MODULES\\n\\nenum jump_label_type jump_label_init_type(struct jump_entry *entry)\\n{\\n\\tstruct static_key *key = jump_entry_key(entry);\\n\\tbool type = static_key_type(key);\\n\\tbool branch = jump_entry_is_branch(entry);\\n\\n\\t/* See the comment in linux/jump_label.h */\\n\\treturn type ^ branch;\\n}\\n\\nstruct static_key_mod {\\n\\tstruct static_key_mod *next;\\n\\tstruct jump_entry *entries;\\n\\tstruct module *mod;\\n};\\n\\nstatic inline struct static_key_mod *static_key_mod(struct static_key *key)\\n{\\n\\tWARN_ON_ONCE(!static_key_linked(key));\\n\\treturn (struct static_key_mod *)(key->type & ~JUMP_TYPE_MASK);\\n}\\n\\n/***\\n * key->type and key->next are the same via union.\\n * This sets key->next and preserves the type bits.\\n *\\n * See additional comments above static_key_set_entries().\\n */\\nstatic void static_key_set_mod(struct static_key *key,\\n\\t\\t\\t       struct static_key_mod *mod)\\n{\\n\\tunsigned long type;\\n\\n\\tWARN_ON_ONCE((unsigned long)mod & JUMP_TYPE_MASK);\\n\\ttype = key->type & JUMP_TYPE_MASK;\\n\\tkey->next = mod;\\n\\tkey->type |= type;\\n}\\n\\nstatic int __jump_label_mod_text_reserved(void *start, void *end)\\n{\\n\\tstruct module *mod;\\n\\tint ret;\\n\\n\\tpreempt_disable();\\n\\tmod = __module_text_address((unsigned long)start);\\n\\tWARN_ON_ONCE(__module_text_address((unsigned long)end) != mod);\\n\\tif (!try_module_get(mod))\\n\\t\\tmod = NULL;\\n\\tpreempt_enable();\\n\\n\\tif (!mod)\\n\\t\\treturn 0;\\n\\n\\tret = __jump_label_text_reserved(mod->jump_entries,\\n\\t\\t\\t\\tmod->jump_entries + mod->num_jump_entries,\\n\\t\\t\\t\\tstart, end, mod->state == MODULE_STATE_COMING);\\n\\n\\tmodule_put(mod);\\n\\n\\treturn ret;\\n}\\n\\nstatic void __jump_label_mod_update(struct static_key *key)\\n{\\n\\tstruct static_key_mod *mod;\\n\\n\\tfor (mod = static_key_mod(key); mod; mod = mod->next) {\\n\\t\\tstruct jump_entry *stop;\\n\\t\\tstruct module *m;\\n\\n\\t\\t/*\\n\\t\\t * NULL if the static_key is defined in a module\\n\\t\\t * that does not use it\\n\\t\\t */\\n\\t\\tif (!mod->entries)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tm = mod->mod;\\n\\t\\tif (!m)\\n\\t\\t\\tstop = __stop___jump_table;\\n\\t\\telse\\n\\t\\t\\tstop = m->jump_entries + m->num_jump_entries;\\n\\t\\t__jump_label_update(key, mod->entries, stop,\\n\\t\\t\\t\\t    m && m->state == MODULE_STATE_COMING);\\n\\t}\\n}\\n\\nstatic int jump_label_add_module(struct module *mod)\\n{\\n\\tstruct jump_entry *iter_start = mod->jump_entries;\\n\\tstruct jump_entry *iter_stop = iter_start + mod->num_jump_entries;\\n\\tstruct jump_entry *iter;\\n\\tstruct static_key *key = NULL;\\n\\tstruct static_key_mod *jlm, *jlm2;\\n\\n\\t/* if the module doesn\\'t have jump label entries, just return */\\n\\tif (iter_start == iter_stop)\\n\\t\\treturn 0;\\n\\n\\tjump_label_sort_entries(iter_start, iter_stop);\\n\\n\\tfor (iter = iter_start; iter < iter_stop; iter++) {\\n\\t\\tstruct static_key *iterk;\\n\\t\\tbool in_init;\\n\\n\\t\\tin_init = within_module_init(jump_entry_code(iter), mod);\\n\\t\\tjump_entry_set_init(iter, in_init);\\n\\n\\t\\titerk = jump_entry_key(iter);\\n\\t\\tif (iterk == key)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tkey = iterk;\\n\\t\\tif (within_module((unsigned long)key, mod)) {\\n\\t\\t\\tstatic_key_set_entries(key, iter);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * If the key was sealed at init, then there\\'s no need to keep a\\n\\t\\t * reference to its module entries - just patch them now and be\\n\\t\\t * done with it.\\n\\t\\t */\\n\\t\\tif (static_key_sealed(key))\\n\\t\\t\\tgoto do_poke;\\n\\n\\t\\tjlm = kzalloc(sizeof(struct static_key_mod), GFP_KERNEL);\\n\\t\\tif (!jlm)\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\tif (!static_key_linked(key)) {\\n\\t\\t\\tjlm2 = kzalloc(sizeof(struct static_key_mod),\\n\\t\\t\\t\\t       GFP_KERNEL);\\n\\t\\t\\tif (!jlm2) {\\n\\t\\t\\t\\tkfree(jlm);\\n\\t\\t\\t\\treturn -ENOMEM;\\n\\t\\t\\t}\\n\\t\\t\\tpreempt_disable();\\n\\t\\t\\tjlm2->mod = __module_address((unsigned long)key);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tjlm2->entries = static_key_entries(key);\\n\\t\\t\\tjlm2->next = NULL;\\n\\t\\t\\tstatic_key_set_mod(key, jlm2);\\n\\t\\t\\tstatic_key_set_linked(key);\\n\\t\\t}\\n\\t\\tjlm->mod = mod;\\n\\t\\tjlm->entries = iter;\\n\\t\\tjlm->next = static_key_mod(key);\\n\\t\\tstatic_key_set_mod(key, jlm);\\n\\t\\tstatic_key_set_linked(key);\\n\\n\\t\\t/* Only update if we\\'ve changed from our initial state */\\ndo_poke:\\n\\t\\tif (jump_label_type(iter) != jump_label_init_type(iter))\\n\\t\\t\\t__jump_label_update(key, iter, iter_stop, true);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic void jump_label_del_module(struct module *mod)\\n{\\n\\tstruct jump_entry *iter_start = mod->jump_entries;\\n\\tstruct jump_entry *iter_stop = iter_start + mod->num_jump_entries;\\n\\tstruct jump_entry *iter;\\n\\tstruct static_key *key = NULL;\\n\\tstruct static_key_mod *jlm, **prev;\\n\\n\\tfor (iter = iter_start; iter < iter_stop; iter++) {\\n\\t\\tif (jump_entry_key(iter) == key)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tkey = jump_entry_key(iter);\\n\\n\\t\\tif (within_module((unsigned long)key, mod))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* No @jlm allocated because key was sealed at init. */\\n\\t\\tif (static_key_sealed(key))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* No memory during module load */\\n\\t\\tif (WARN_ON(!static_key_linked(key)))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tprev = &key->next;\\n\\t\\tjlm = static_key_mod(key);\\n\\n\\t\\twhile (jlm && jlm->mod != mod) {\\n\\t\\t\\tprev = &jlm->next;\\n\\t\\t\\tjlm = jlm->next;\\n\\t\\t}\\n\\n\\t\\t/* No memory during module load */\\n\\t\\tif (WARN_ON(!jlm))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (prev == &key->next)\\n\\t\\t\\tstatic_key_set_mod(key, jlm->next);\\n\\t\\telse\\n\\t\\t\\t*prev = jlm->next;\\n\\n\\t\\tkfree(jlm);\\n\\n\\t\\tjlm = static_key_mod(key);\\n\\t\\t/* if only one etry is left, fold it back into the static_key */\\n\\t\\tif (jlm->next == NULL) {\\n\\t\\t\\tstatic_key_set_entries(key, jlm->entries);\\n\\t\\t\\tstatic_key_clear_linked(key);\\n\\t\\t\\tkfree(jlm);\\n\\t\\t}\\n\\t}\\n}\\n\\nstatic int\\njump_label_module_notify(struct notifier_block *self, unsigned long val,\\n\\t\\t\\t void *data)\\n{\\n\\tstruct module *mod = data;\\n\\tint ret = 0;\\n\\n\\tcpus_read_lock();\\n\\tjump_label_lock();\\n\\n\\tswitch (val) {\\n\\tcase MODULE_STATE_COMING:\\n\\t\\tret = jump_label_add_module(mod);\\n\\t\\tif (ret) {\\n\\t\\t\\tWARN(1, \"Failed to allocate memory: jump_label may not work properly.\\\\n\");\\n\\t\\t\\tjump_label_del_module(mod);\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase MODULE_STATE_GOING:\\n\\t\\tjump_label_del_module(mod);\\n\\t\\tbreak;\\n\\t}\\n\\n\\tjump_label_unlock();\\n\\tcpus_read_unlock();\\n\\n\\treturn notifier_from_errno(ret);\\n}\\n\\nstatic struct notifier_block jump_label_module_nb = {\\n\\t.notifier_call = jump_label_module_notify,\\n\\t.priority = 1, /* higher than tracepoints */\\n};\\n\\nstatic __init int jump_label_init_module(void)\\n{\\n\\treturn register_module_notifier(&jump_label_module_nb);\\n}\\nearly_initcall(jump_label_init_module);\\n\\n#endif /* CONFIG_MODULES */\\n\\n/***\\n * jump_label_text_reserved - check if addr range is reserved\\n * @start: start text addr\\n * @end: end text addr\\n *\\n * checks if the text addr located between @start and @end\\n * overlaps with any of the jump label patch addresses. Code\\n * that wants to modify kernel text should first verify that\\n * it does not overlap with any of the jump label addresses.\\n * Caller must hold jump_label_mutex.\\n *\\n * returns 1 if there is an overlap, 0 otherwise\\n */\\nint jump_label_text_reserved(void *start, void *end)\\n{\\n\\tbool init = system_state < SYSTEM_RUNNING;\\n\\tint ret = __jump_label_text_reserved(__start___jump_table,\\n\\t\\t\\t__stop___jump_table, start, end, init);\\n\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n#ifdef CONFIG_MODULES\\n\\tret = __jump_label_mod_text_reserved(start, end);\\n#endif\\n\\treturn ret;\\n}\\n\\nstatic void jump_label_update(struct static_key *key)\\n{\\n\\tstruct jump_entry *stop = __stop___jump_table;\\n\\tbool init = system_state < SYSTEM_RUNNING;\\n\\tstruct jump_entry *entry;\\n#ifdef CONFIG_MODULES\\n\\tstruct module *mod;\\n\\n\\tif (static_key_linked(key)) {\\n\\t\\t__jump_label_mod_update(key);\\n\\t\\treturn;\\n\\t}\\n\\n\\tpreempt_disable();\\n\\tmod = __module_address((unsigned long)key);\\n\\tif (mod) {\\n\\t\\tstop = mod->jump_entries + mod->num_jump_entries;\\n\\t\\tinit = mod->state == MODULE_STATE_COMING;\\n\\t}\\n\\tpreempt_enable();\\n#endif\\n\\tentry = static_key_entries(key);\\n\\t/* if there are no users, entry can be NULL */\\n\\tif (entry)\\n\\t\\t__jump_label_update(key, entry, stop, init);\\n}\\n\\n#ifdef CONFIG_STATIC_KEYS_SELFTEST\\nstatic DEFINE_STATIC_KEY_TRUE(sk_true);\\nstatic DEFINE_STATIC_KEY_FALSE(sk_false);\\n\\nstatic __init int jump_label_test(void)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < 2; i++) {\\n\\t\\tWARN_ON(static_key_enabled(&sk_true.key) != true);\\n\\t\\tWARN_ON(static_key_enabled(&sk_false.key) != false);\\n\\n\\t\\tWARN_ON(!static_branch_likely(&sk_true));\\n\\t\\tWARN_ON(!static_branch_unlikely(&sk_true));\\n\\t\\tWARN_ON(static_branch_likely(&sk_false));\\n\\t\\tWARN_ON(static_branch_unlikely(&sk_false));\\n\\n\\t\\tstatic_branch_disable(&sk_true);\\n\\t\\tstatic_branch_enable(&sk_false);\\n\\n\\t\\tWARN_ON(static_key_enabled(&sk_true.key) == true);\\n\\t\\tWARN_ON(static_key_enabled(&sk_false.key) == false);\\n\\n\\t\\tWARN_ON(static_branch_likely(&sk_true));\\n\\t\\tWARN_ON(static_branch_unlikely(&sk_true));\\n\\t\\tWARN_ON(!static_branch_likely(&sk_false));\\n\\t\\tWARN_ON(!static_branch_unlikely(&sk_false));\\n\\n\\t\\tstatic_branch_enable(&sk_true);\\n\\t\\tstatic_branch_disable(&sk_false);\\n\\t}\\n\\n\\treturn 0;\\n}\\nearly_initcall(jump_label_test);\\n#endif /* STATIC_KEYS_SELFTEST */\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#define pr_fmt(fmt) \"kcov: \" fmt\\n\\n#define DISABLE_BRANCH_PROFILING\\n#include <linux/atomic.h>\\n#include <linux/compiler.h>\\n#include <linux/errno.h>\\n#include <linux/export.h>\\n#include <linux/types.h>\\n#include <linux/file.h>\\n#include <linux/fs.h>\\n#include <linux/hashtable.h>\\n#include <linux/init.h>\\n#include <linux/jiffies.h>\\n#include <linux/kmsan-checks.h>\\n#include <linux/mm.h>\\n#include <linux/preempt.h>\\n#include <linux/printk.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/spinlock.h>\\n#include <linux/vmalloc.h>\\n#include <linux/debugfs.h>\\n#include <linux/uaccess.h>\\n#include <linux/kcov.h>\\n#include <linux/refcount.h>\\n#include <linux/log2.h>\\n#include <asm/setup.h>\\n\\n#define kcov_debug(fmt, ...) pr_debug(\"%s: \" fmt, __func__, ##__VA_ARGS__)\\n\\n/* Number of 64-bit words written per one comparison: */\\n#define KCOV_WORDS_PER_CMP 4\\n\\n/*\\n * kcov descriptor (one per opened debugfs file).\\n * State transitions of the descriptor:\\n *  - initial state after open()\\n *  - then there must be a single ioctl(KCOV_INIT_TRACE) call\\n *  - then, mmap() call (several calls are allowed but not useful)\\n *  - then, ioctl(KCOV_ENABLE, arg), where arg is\\n *\\tKCOV_TRACE_PC - to trace only the PCs\\n *\\tor\\n *\\tKCOV_TRACE_CMP - to trace only the comparison operands\\n *  - then, ioctl(KCOV_DISABLE) to disable the task.\\n * Enabling/disabling ioctls can be repeated (only one task a time allowed).\\n */\\nstruct kcov {\\n\\t/*\\n\\t * Reference counter. We keep one for:\\n\\t *  - opened file descriptor\\n\\t *  - task with enabled coverage (we can\\'t unwire it from another task)\\n\\t *  - each code section for remote coverage collection\\n\\t */\\n\\trefcount_t\\t\\trefcount;\\n\\t/* The lock protects mode, size, area and t. */\\n\\tspinlock_t\\t\\tlock;\\n\\tenum kcov_mode\\t\\tmode;\\n\\t/* Size of arena (in long\\'s). */\\n\\tunsigned int\\t\\tsize;\\n\\t/* Coverage buffer shared with user space. */\\n\\tvoid\\t\\t\\t*area;\\n\\t/* Task for which we collect coverage, or NULL. */\\n\\tstruct task_struct\\t*t;\\n\\t/* Collecting coverage from remote (background) threads. */\\n\\tbool\\t\\t\\tremote;\\n\\t/* Size of remote area (in long\\'s). */\\n\\tunsigned int\\t\\tremote_size;\\n\\t/*\\n\\t * Sequence is incremented each time kcov is reenabled, used by\\n\\t * kcov_remote_stop(), see the comment there.\\n\\t */\\n\\tint\\t\\t\\tsequence;\\n};\\n\\nstruct kcov_remote_area {\\n\\tstruct list_head\\tlist;\\n\\tunsigned int\\t\\tsize;\\n};\\n\\nstruct kcov_remote {\\n\\tu64\\t\\t\\thandle;\\n\\tstruct kcov\\t\\t*kcov;\\n\\tstruct hlist_node\\thnode;\\n};\\n\\nstatic DEFINE_SPINLOCK(kcov_remote_lock);\\nstatic DEFINE_HASHTABLE(kcov_remote_map, 4);\\nstatic struct list_head kcov_remote_areas = LIST_HEAD_INIT(kcov_remote_areas);\\n\\nstruct kcov_percpu_data {\\n\\tvoid\\t\\t\\t*irq_area;\\n\\tlocal_lock_t\\t\\tlock;\\n\\n\\tunsigned int\\t\\tsaved_mode;\\n\\tunsigned int\\t\\tsaved_size;\\n\\tvoid\\t\\t\\t*saved_area;\\n\\tstruct kcov\\t\\t*saved_kcov;\\n\\tint\\t\\t\\tsaved_sequence;\\n};\\n\\nstatic DEFINE_PER_CPU(struct kcov_percpu_data, kcov_percpu_data) = {\\n\\t.lock = INIT_LOCAL_LOCK(lock),\\n};\\n\\n/* Must be called with kcov_remote_lock locked. */\\nstatic struct kcov_remote *kcov_remote_find(u64 handle)\\n{\\n\\tstruct kcov_remote *remote;\\n\\n\\thash_for_each_possible(kcov_remote_map, remote, hnode, handle) {\\n\\t\\tif (remote->handle == handle)\\n\\t\\t\\treturn remote;\\n\\t}\\n\\treturn NULL;\\n}\\n\\n/* Must be called with kcov_remote_lock locked. */\\nstatic struct kcov_remote *kcov_remote_add(struct kcov *kcov, u64 handle)\\n{\\n\\tstruct kcov_remote *remote;\\n\\n\\tif (kcov_remote_find(handle))\\n\\t\\treturn ERR_PTR(-EEXIST);\\n\\tremote = kmalloc(sizeof(*remote), GFP_ATOMIC);\\n\\tif (!remote)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\tremote->handle = handle;\\n\\tremote->kcov = kcov;\\n\\thash_add(kcov_remote_map, &remote->hnode, handle);\\n\\treturn remote;\\n}\\n\\n/* Must be called with kcov_remote_lock locked. */\\nstatic struct kcov_remote_area *kcov_remote_area_get(unsigned int size)\\n{\\n\\tstruct kcov_remote_area *area;\\n\\tstruct list_head *pos;\\n\\n\\tlist_for_each(pos, &kcov_remote_areas) {\\n\\t\\tarea = list_entry(pos, struct kcov_remote_area, list);\\n\\t\\tif (area->size == size) {\\n\\t\\t\\tlist_del(&area->list);\\n\\t\\t\\treturn area;\\n\\t\\t}\\n\\t}\\n\\treturn NULL;\\n}\\n\\n/* Must be called with kcov_remote_lock locked. */\\nstatic void kcov_remote_area_put(struct kcov_remote_area *area,\\n\\t\\t\\t\\t\\tunsigned int size)\\n{\\n\\tINIT_LIST_HEAD(&area->list);\\n\\tarea->size = size;\\n\\tlist_add(&area->list, &kcov_remote_areas);\\n\\t/*\\n\\t * KMSAN doesn\\'t instrument this file, so it may not know area->list\\n\\t * is initialized. Unpoison it explicitly to avoid reports in\\n\\t * kcov_remote_area_get().\\n\\t */\\n\\tkmsan_unpoison_memory(&area->list, sizeof(area->list));\\n}\\n\\n/*\\n * Unlike in_serving_softirq(), this function returns false when called during\\n * a hardirq or an NMI that happened in the softirq context.\\n */\\nstatic inline bool in_softirq_really(void)\\n{\\n\\treturn in_serving_softirq() && !in_hardirq() && !in_nmi();\\n}\\n\\nstatic notrace bool check_kcov_mode(enum kcov_mode needed_mode, struct task_struct *t)\\n{\\n\\tunsigned int mode;\\n\\n\\t/*\\n\\t * We are interested in code coverage as a function of a syscall inputs,\\n\\t * so we ignore code executed in interrupts, unless we are in a remote\\n\\t * coverage collection section in a softirq.\\n\\t */\\n\\tif (!in_task() && !(in_softirq_really() && t->kcov_softirq))\\n\\t\\treturn false;\\n\\tmode = READ_ONCE(t->kcov_mode);\\n\\t/*\\n\\t * There is some code that runs in interrupts but for which\\n\\t * in_interrupt() returns false (e.g. preempt_schedule_irq()).\\n\\t * READ_ONCE()/barrier() effectively provides load-acquire wrt\\n\\t * interrupts, there are paired barrier()/WRITE_ONCE() in\\n\\t * kcov_start().\\n\\t */\\n\\tbarrier();\\n\\treturn mode == needed_mode;\\n}\\n\\nstatic notrace unsigned long canonicalize_ip(unsigned long ip)\\n{\\n#ifdef CONFIG_RANDOMIZE_BASE\\n\\tip -= kaslr_offset();\\n#endif\\n\\treturn ip;\\n}\\n\\n/*\\n * Entry point from instrumented code.\\n * This is called once per basic-block/edge.\\n */\\nvoid notrace __sanitizer_cov_trace_pc(void)\\n{\\n\\tstruct task_struct *t;\\n\\tunsigned long *area;\\n\\tunsigned long ip = canonicalize_ip(_RET_IP_);\\n\\tunsigned long pos;\\n\\n\\tt = current;\\n\\tif (!check_kcov_mode(KCOV_MODE_TRACE_PC, t))\\n\\t\\treturn;\\n\\n\\tarea = t->kcov_area;\\n\\t/* The first 64-bit word is the number of subsequent PCs. */\\n\\tpos = READ_ONCE(area[0]) + 1;\\n\\tif (likely(pos < t->kcov_size)) {\\n\\t\\t/* Previously we write pc before updating pos. However, some\\n\\t\\t * early interrupt code could bypass check_kcov_mode() check\\n\\t\\t * and invoke __sanitizer_cov_trace_pc(). If such interrupt is\\n\\t\\t * raised between writing pc and updating pos, the pc could be\\n\\t\\t * overitten by the recursive __sanitizer_cov_trace_pc().\\n\\t\\t * Update pos before writing pc to avoid such interleaving.\\n\\t\\t */\\n\\t\\tWRITE_ONCE(area[0], pos);\\n\\t\\tbarrier();\\n\\t\\tarea[pos] = ip;\\n\\t}\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_pc);\\n\\n#ifdef CONFIG_KCOV_ENABLE_COMPARISONS\\nstatic void notrace write_comp_data(u64 type, u64 arg1, u64 arg2, u64 ip)\\n{\\n\\tstruct task_struct *t;\\n\\tu64 *area;\\n\\tu64 count, start_index, end_pos, max_pos;\\n\\n\\tt = current;\\n\\tif (!check_kcov_mode(KCOV_MODE_TRACE_CMP, t))\\n\\t\\treturn;\\n\\n\\tip = canonicalize_ip(ip);\\n\\n\\t/*\\n\\t * We write all comparison arguments and types as u64.\\n\\t * The buffer was allocated for t->kcov_size unsigned longs.\\n\\t */\\n\\tarea = (u64 *)t->kcov_area;\\n\\tmax_pos = t->kcov_size * sizeof(unsigned long);\\n\\n\\tcount = READ_ONCE(area[0]);\\n\\n\\t/* Every record is KCOV_WORDS_PER_CMP 64-bit words. */\\n\\tstart_index = 1 + count * KCOV_WORDS_PER_CMP;\\n\\tend_pos = (start_index + KCOV_WORDS_PER_CMP) * sizeof(u64);\\n\\tif (likely(end_pos <= max_pos)) {\\n\\t\\t/* See comment in __sanitizer_cov_trace_pc(). */\\n\\t\\tWRITE_ONCE(area[0], count + 1);\\n\\t\\tbarrier();\\n\\t\\tarea[start_index] = type;\\n\\t\\tarea[start_index + 1] = arg1;\\n\\t\\tarea[start_index + 2] = arg2;\\n\\t\\tarea[start_index + 3] = ip;\\n\\t}\\n}\\n\\nvoid notrace __sanitizer_cov_trace_cmp1(u8 arg1, u8 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(0), arg1, arg2, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_cmp1);\\n\\nvoid notrace __sanitizer_cov_trace_cmp2(u16 arg1, u16 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(1), arg1, arg2, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_cmp2);\\n\\nvoid notrace __sanitizer_cov_trace_cmp4(u32 arg1, u32 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(2), arg1, arg2, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_cmp4);\\n\\nvoid notrace __sanitizer_cov_trace_cmp8(kcov_u64 arg1, kcov_u64 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(3), arg1, arg2, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_cmp8);\\n\\nvoid notrace __sanitizer_cov_trace_const_cmp1(u8 arg1, u8 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(0) | KCOV_CMP_CONST, arg1, arg2,\\n\\t\\t\\t_RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp1);\\n\\nvoid notrace __sanitizer_cov_trace_const_cmp2(u16 arg1, u16 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(1) | KCOV_CMP_CONST, arg1, arg2,\\n\\t\\t\\t_RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp2);\\n\\nvoid notrace __sanitizer_cov_trace_const_cmp4(u32 arg1, u32 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(2) | KCOV_CMP_CONST, arg1, arg2,\\n\\t\\t\\t_RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp4);\\n\\nvoid notrace __sanitizer_cov_trace_const_cmp8(kcov_u64 arg1, kcov_u64 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(3) | KCOV_CMP_CONST, arg1, arg2,\\n\\t\\t\\t_RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp8);\\n\\nvoid notrace __sanitizer_cov_trace_switch(kcov_u64 val, void *arg)\\n{\\n\\tu64 i;\\n\\tu64 *cases = arg;\\n\\tu64 count = cases[0];\\n\\tu64 size = cases[1];\\n\\tu64 type = KCOV_CMP_CONST;\\n\\n\\tswitch (size) {\\n\\tcase 8:\\n\\t\\ttype |= KCOV_CMP_SIZE(0);\\n\\t\\tbreak;\\n\\tcase 16:\\n\\t\\ttype |= KCOV_CMP_SIZE(1);\\n\\t\\tbreak;\\n\\tcase 32:\\n\\t\\ttype |= KCOV_CMP_SIZE(2);\\n\\t\\tbreak;\\n\\tcase 64:\\n\\t\\ttype |= KCOV_CMP_SIZE(3);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn;\\n\\t}\\n\\tfor (i = 0; i < count; i++)\\n\\t\\twrite_comp_data(type, cases[i + 2], val, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_switch);\\n#endif /* ifdef CONFIG_KCOV_ENABLE_COMPARISONS */\\n\\nstatic void kcov_start(struct task_struct *t, struct kcov *kcov,\\n\\t\\t\\tunsigned int size, void *area, enum kcov_mode mode,\\n\\t\\t\\tint sequence)\\n{\\n\\tkcov_debug(\"t = %px, size = %u, area = %px\\\\n\", t, size, area);\\n\\tt->kcov = kcov;\\n\\t/* Cache in task struct for performance. */\\n\\tt->kcov_size = size;\\n\\tt->kcov_area = area;\\n\\tt->kcov_sequence = sequence;\\n\\t/* See comment in check_kcov_mode(). */\\n\\tbarrier();\\n\\tWRITE_ONCE(t->kcov_mode, mode);\\n}\\n\\nstatic void kcov_stop(struct task_struct *t)\\n{\\n\\tWRITE_ONCE(t->kcov_mode, KCOV_MODE_DISABLED);\\n\\tbarrier();\\n\\tt->kcov = NULL;\\n\\tt->kcov_size = 0;\\n\\tt->kcov_area = NULL;\\n}\\n\\nstatic void kcov_task_reset(struct task_struct *t)\\n{\\n\\tkcov_stop(t);\\n\\tt->kcov_sequence = 0;\\n\\tt->kcov_handle = 0;\\n}\\n\\nvoid kcov_task_init(struct task_struct *t)\\n{\\n\\tkcov_task_reset(t);\\n\\tt->kcov_handle = current->kcov_handle;\\n}\\n\\nstatic void kcov_reset(struct kcov *kcov)\\n{\\n\\tkcov->t = NULL;\\n\\tkcov->mode = KCOV_MODE_INIT;\\n\\tkcov->remote = false;\\n\\tkcov->remote_size = 0;\\n\\tkcov->sequence++;\\n}\\n\\nstatic void kcov_remote_reset(struct kcov *kcov)\\n{\\n\\tint bkt;\\n\\tstruct kcov_remote *remote;\\n\\tstruct hlist_node *tmp;\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&kcov_remote_lock, flags);\\n\\thash_for_each_safe(kcov_remote_map, bkt, tmp, remote, hnode) {\\n\\t\\tif (remote->kcov != kcov)\\n\\t\\t\\tcontinue;\\n\\t\\thash_del(&remote->hnode);\\n\\t\\tkfree(remote);\\n\\t}\\n\\t/* Do reset before unlock to prevent races with kcov_remote_start(). */\\n\\tkcov_reset(kcov);\\n\\tspin_unlock_irqrestore(&kcov_remote_lock, flags);\\n}\\n\\nstatic void kcov_disable(struct task_struct *t, struct kcov *kcov)\\n{\\n\\tkcov_task_reset(t);\\n\\tif (kcov->remote)\\n\\t\\tkcov_remote_reset(kcov);\\n\\telse\\n\\t\\tkcov_reset(kcov);\\n}\\n\\nstatic void kcov_get(struct kcov *kcov)\\n{\\n\\trefcount_inc(&kcov->refcount);\\n}\\n\\nstatic void kcov_put(struct kcov *kcov)\\n{\\n\\tif (refcount_dec_and_test(&kcov->refcount)) {\\n\\t\\tkcov_remote_reset(kcov);\\n\\t\\tvfree(kcov->area);\\n\\t\\tkfree(kcov);\\n\\t}\\n}\\n\\nvoid kcov_task_exit(struct task_struct *t)\\n{\\n\\tstruct kcov *kcov;\\n\\tunsigned long flags;\\n\\n\\tkcov = t->kcov;\\n\\tif (kcov == NULL)\\n\\t\\treturn;\\n\\n\\tspin_lock_irqsave(&kcov->lock, flags);\\n\\tkcov_debug(\"t = %px, kcov->t = %px\\\\n\", t, kcov->t);\\n\\t/*\\n\\t * For KCOV_ENABLE devices we want to make sure that t->kcov->t == t,\\n\\t * which comes down to:\\n\\t *        WARN_ON(!kcov->remote && kcov->t != t);\\n\\t *\\n\\t * For KCOV_REMOTE_ENABLE devices, the exiting task is either:\\n\\t *\\n\\t * 1. A remote task between kcov_remote_start() and kcov_remote_stop().\\n\\t *    In this case we should print a warning right away, since a task\\n\\t *    shouldn\\'t be exiting when it\\'s in a kcov coverage collection\\n\\t *    section. Here t points to the task that is collecting remote\\n\\t *    coverage, and t->kcov->t points to the thread that created the\\n\\t *    kcov device. Which means that to detect this case we need to\\n\\t *    check that t != t->kcov->t, and this gives us the following:\\n\\t *        WARN_ON(kcov->remote && kcov->t != t);\\n\\t *\\n\\t * 2. The task that created kcov exiting without calling KCOV_DISABLE,\\n\\t *    and then again we make sure that t->kcov->t == t:\\n\\t *        WARN_ON(kcov->remote && kcov->t != t);\\n\\t *\\n\\t * By combining all three checks into one we get:\\n\\t */\\n\\tif (WARN_ON(kcov->t != t)) {\\n\\t\\tspin_unlock_irqrestore(&kcov->lock, flags);\\n\\t\\treturn;\\n\\t}\\n\\t/* Just to not leave dangling references behind. */\\n\\tkcov_disable(t, kcov);\\n\\tspin_unlock_irqrestore(&kcov->lock, flags);\\n\\tkcov_put(kcov);\\n}\\n\\nstatic int kcov_mmap(struct file *filep, struct vm_area_struct *vma)\\n{\\n\\tint res = 0;\\n\\tstruct kcov *kcov = vma->vm_file->private_data;\\n\\tunsigned long size, off;\\n\\tstruct page *page;\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&kcov->lock, flags);\\n\\tsize = kcov->size * sizeof(unsigned long);\\n\\tif (kcov->area == NULL || vma->vm_pgoff != 0 ||\\n\\t    vma->vm_end - vma->vm_start != size) {\\n\\t\\tres = -EINVAL;\\n\\t\\tgoto exit;\\n\\t}\\n\\tspin_unlock_irqrestore(&kcov->lock, flags);\\n\\tvm_flags_set(vma, VM_DONTEXPAND);\\n\\tfor (off = 0; off < size; off += PAGE_SIZE) {\\n\\t\\tpage = vmalloc_to_page(kcov->area + off);\\n\\t\\tres = vm_insert_page(vma, vma->vm_start + off, page);\\n\\t\\tif (res) {\\n\\t\\t\\tpr_warn_once(\"kcov: vm_insert_page() failed\\\\n\");\\n\\t\\t\\treturn res;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\nexit:\\n\\tspin_unlock_irqrestore(&kcov->lock, flags);\\n\\treturn res;\\n}\\n\\nstatic int kcov_open(struct inode *inode, struct file *filep)\\n{\\n\\tstruct kcov *kcov;\\n\\n\\tkcov = kzalloc(sizeof(*kcov), GFP_KERNEL);\\n\\tif (!kcov)\\n\\t\\treturn -ENOMEM;\\n\\tkcov->mode = KCOV_MODE_DISABLED;\\n\\tkcov->sequence = 1;\\n\\trefcount_set(&kcov->refcount, 1);\\n\\tspin_lock_init(&kcov->lock);\\n\\tfilep->private_data = kcov;\\n\\treturn nonseekable_open(inode, filep);\\n}\\n\\nstatic int kcov_close(struct inode *inode, struct file *filep)\\n{\\n\\tkcov_put(filep->private_data);\\n\\treturn 0;\\n}\\n\\nstatic int kcov_get_mode(unsigned long arg)\\n{\\n\\tif (arg == KCOV_TRACE_PC)\\n\\t\\treturn KCOV_MODE_TRACE_PC;\\n\\telse if (arg == KCOV_TRACE_CMP)\\n#ifdef CONFIG_KCOV_ENABLE_COMPARISONS\\n\\t\\treturn KCOV_MODE_TRACE_CMP;\\n#else\\n\\t\\treturn -ENOTSUPP;\\n#endif\\n\\telse\\n\\t\\treturn -EINVAL;\\n}\\n\\n/*\\n * Fault in a lazily-faulted vmalloc area before it can be used by\\n * __santizer_cov_trace_pc(), to avoid recursion issues if any code on the\\n * vmalloc fault handling path is instrumented.\\n */\\nstatic void kcov_fault_in_area(struct kcov *kcov)\\n{\\n\\tunsigned long stride = PAGE_SIZE / sizeof(unsigned long);\\n\\tunsigned long *area = kcov->area;\\n\\tunsigned long offset;\\n\\n\\tfor (offset = 0; offset < kcov->size; offset += stride)\\n\\t\\tREAD_ONCE(area[offset]);\\n}\\n\\nstatic inline bool kcov_check_handle(u64 handle, bool common_valid,\\n\\t\\t\\t\\tbool uncommon_valid, bool zero_valid)\\n{\\n\\tif (handle & ~(KCOV_SUBSYSTEM_MASK | KCOV_INSTANCE_MASK))\\n\\t\\treturn false;\\n\\tswitch (handle & KCOV_SUBSYSTEM_MASK) {\\n\\tcase KCOV_SUBSYSTEM_COMMON:\\n\\t\\treturn (handle & KCOV_INSTANCE_MASK) ?\\n\\t\\t\\tcommon_valid : zero_valid;\\n\\tcase KCOV_SUBSYSTEM_USB:\\n\\t\\treturn uncommon_valid;\\n\\tdefault:\\n\\t\\treturn false;\\n\\t}\\n\\treturn false;\\n}\\n\\nstatic int kcov_ioctl_locked(struct kcov *kcov, unsigned int cmd,\\n\\t\\t\\t     unsigned long arg)\\n{\\n\\tstruct task_struct *t;\\n\\tunsigned long flags, unused;\\n\\tint mode, i;\\n\\tstruct kcov_remote_arg *remote_arg;\\n\\tstruct kcov_remote *remote;\\n\\n\\tswitch (cmd) {\\n\\tcase KCOV_ENABLE:\\n\\t\\t/*\\n\\t\\t * Enable coverage for the current task.\\n\\t\\t * At this point user must have been enabled trace mode,\\n\\t\\t * and mmapped the file. Coverage collection is disabled only\\n\\t\\t * at task exit or voluntary by KCOV_DISABLE. After that it can\\n\\t\\t * be enabled for another task.\\n\\t\\t */\\n\\t\\tif (kcov->mode != KCOV_MODE_INIT || !kcov->area)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tt = current;\\n\\t\\tif (kcov->t != NULL || t->kcov != NULL)\\n\\t\\t\\treturn -EBUSY;\\n\\t\\tmode = kcov_get_mode(arg);\\n\\t\\tif (mode < 0)\\n\\t\\t\\treturn mode;\\n\\t\\tkcov_fault_in_area(kcov);\\n\\t\\tkcov->mode = mode;\\n\\t\\tkcov_start(t, kcov, kcov->size, kcov->area, kcov->mode,\\n\\t\\t\\t\\tkcov->sequence);\\n\\t\\tkcov->t = t;\\n\\t\\t/* Put either in kcov_task_exit() or in KCOV_DISABLE. */\\n\\t\\tkcov_get(kcov);\\n\\t\\treturn 0;\\n\\tcase KCOV_DISABLE:\\n\\t\\t/* Disable coverage for the current task. */\\n\\t\\tunused = arg;\\n\\t\\tif (unused != 0 || current->kcov != kcov)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tt = current;\\n\\t\\tif (WARN_ON(kcov->t != t))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tkcov_disable(t, kcov);\\n\\t\\tkcov_put(kcov);\\n\\t\\treturn 0;\\n\\tcase KCOV_REMOTE_ENABLE:\\n\\t\\tif (kcov->mode != KCOV_MODE_INIT || !kcov->area)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tt = current;\\n\\t\\tif (kcov->t != NULL || t->kcov != NULL)\\n\\t\\t\\treturn -EBUSY;\\n\\t\\tremote_arg = (struct kcov_remote_arg *)arg;\\n\\t\\tmode = kcov_get_mode(remote_arg->trace_mode);\\n\\t\\tif (mode < 0)\\n\\t\\t\\treturn mode;\\n\\t\\tif ((unsigned long)remote_arg->area_size >\\n\\t\\t    LONG_MAX / sizeof(unsigned long))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tkcov->mode = mode;\\n\\t\\tt->kcov = kcov;\\n\\t        t->kcov_mode = KCOV_MODE_REMOTE;\\n\\t\\tkcov->t = t;\\n\\t\\tkcov->remote = true;\\n\\t\\tkcov->remote_size = remote_arg->area_size;\\n\\t\\tspin_lock_irqsave(&kcov_remote_lock, flags);\\n\\t\\tfor (i = 0; i < remote_arg->num_handles; i++) {\\n\\t\\t\\tif (!kcov_check_handle(remote_arg->handles[i],\\n\\t\\t\\t\\t\\t\\tfalse, true, false)) {\\n\\t\\t\\t\\tspin_unlock_irqrestore(&kcov_remote_lock,\\n\\t\\t\\t\\t\\t\\t\\tflags);\\n\\t\\t\\t\\tkcov_disable(t, kcov);\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\t}\\n\\t\\t\\tremote = kcov_remote_add(kcov, remote_arg->handles[i]);\\n\\t\\t\\tif (IS_ERR(remote)) {\\n\\t\\t\\t\\tspin_unlock_irqrestore(&kcov_remote_lock,\\n\\t\\t\\t\\t\\t\\t\\tflags);\\n\\t\\t\\t\\tkcov_disable(t, kcov);\\n\\t\\t\\t\\treturn PTR_ERR(remote);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif (remote_arg->common_handle) {\\n\\t\\t\\tif (!kcov_check_handle(remote_arg->common_handle,\\n\\t\\t\\t\\t\\t\\ttrue, false, false)) {\\n\\t\\t\\t\\tspin_unlock_irqrestore(&kcov_remote_lock,\\n\\t\\t\\t\\t\\t\\t\\tflags);\\n\\t\\t\\t\\tkcov_disable(t, kcov);\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\t}\\n\\t\\t\\tremote = kcov_remote_add(kcov,\\n\\t\\t\\t\\t\\tremote_arg->common_handle);\\n\\t\\t\\tif (IS_ERR(remote)) {\\n\\t\\t\\t\\tspin_unlock_irqrestore(&kcov_remote_lock,\\n\\t\\t\\t\\t\\t\\t\\tflags);\\n\\t\\t\\t\\tkcov_disable(t, kcov);\\n\\t\\t\\t\\treturn PTR_ERR(remote);\\n\\t\\t\\t}\\n\\t\\t\\tt->kcov_handle = remote_arg->common_handle;\\n\\t\\t}\\n\\t\\tspin_unlock_irqrestore(&kcov_remote_lock, flags);\\n\\t\\t/* Put either in kcov_task_exit() or in KCOV_DISABLE. */\\n\\t\\tkcov_get(kcov);\\n\\t\\treturn 0;\\n\\tdefault:\\n\\t\\treturn -ENOTTY;\\n\\t}\\n}\\n\\nstatic long kcov_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)\\n{\\n\\tstruct kcov *kcov;\\n\\tint res;\\n\\tstruct kcov_remote_arg *remote_arg = NULL;\\n\\tunsigned int remote_num_handles;\\n\\tunsigned long remote_arg_size;\\n\\tunsigned long size, flags;\\n\\tvoid *area;\\n\\n\\tkcov = filep->private_data;\\n\\tswitch (cmd) {\\n\\tcase KCOV_INIT_TRACE:\\n\\t\\t/*\\n\\t\\t * Enable kcov in trace mode and setup buffer size.\\n\\t\\t * Must happen before anything else.\\n\\t\\t *\\n\\t\\t * First check the size argument - it must be at least 2\\n\\t\\t * to hold the current position and one PC.\\n\\t\\t */\\n\\t\\tsize = arg;\\n\\t\\tif (size < 2 || size > INT_MAX / sizeof(unsigned long))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tarea = vmalloc_user(size * sizeof(unsigned long));\\n\\t\\tif (area == NULL)\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\tspin_lock_irqsave(&kcov->lock, flags);\\n\\t\\tif (kcov->mode != KCOV_MODE_DISABLED) {\\n\\t\\t\\tspin_unlock_irqrestore(&kcov->lock, flags);\\n\\t\\t\\tvfree(area);\\n\\t\\t\\treturn -EBUSY;\\n\\t\\t}\\n\\t\\tkcov->area = area;\\n\\t\\tkcov->size = size;\\n\\t\\tkcov->mode = KCOV_MODE_INIT;\\n\\t\\tspin_unlock_irqrestore(&kcov->lock, flags);\\n\\t\\treturn 0;\\n\\tcase KCOV_REMOTE_ENABLE:\\n\\t\\tif (get_user(remote_num_handles, (unsigned __user *)(arg +\\n\\t\\t\\t\\toffsetof(struct kcov_remote_arg, num_handles))))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tif (remote_num_handles > KCOV_REMOTE_MAX_HANDLES)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tremote_arg_size = struct_size(remote_arg, handles,\\n\\t\\t\\t\\t\\tremote_num_handles);\\n\\t\\tremote_arg = memdup_user((void __user *)arg, remote_arg_size);\\n\\t\\tif (IS_ERR(remote_arg))\\n\\t\\t\\treturn PTR_ERR(remote_arg);\\n\\t\\tif (remote_arg->num_handles != remote_num_handles) {\\n\\t\\t\\tkfree(remote_arg);\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\targ = (unsigned long)remote_arg;\\n\\t\\tfallthrough;\\n\\tdefault:\\n\\t\\t/*\\n\\t\\t * All other commands can be normally executed under a spin lock, so we\\n\\t\\t * obtain and release it here in order to simplify kcov_ioctl_locked().\\n\\t\\t */\\n\\t\\tspin_lock_irqsave(&kcov->lock, flags);\\n\\t\\tres = kcov_ioctl_locked(kcov, cmd, arg);\\n\\t\\tspin_unlock_irqrestore(&kcov->lock, flags);\\n\\t\\tkfree(remote_arg);\\n\\t\\treturn res;\\n\\t}\\n}\\n\\nstatic const struct file_operations kcov_fops = {\\n\\t.open\\t\\t= kcov_open,\\n\\t.unlocked_ioctl\\t= kcov_ioctl,\\n\\t.compat_ioctl\\t= kcov_ioctl,\\n\\t.mmap\\t\\t= kcov_mmap,\\n\\t.release        = kcov_close,\\n};\\n\\n/*\\n * kcov_remote_start() and kcov_remote_stop() can be used to annotate a section\\n * of code in a kernel background thread or in a softirq to allow kcov to be\\n * used to collect coverage from that part of code.\\n *\\n * The handle argument of kcov_remote_start() identifies a code section that is\\n * used for coverage collection. A userspace process passes this handle to\\n * KCOV_REMOTE_ENABLE ioctl to make the used kcov device start collecting\\n * coverage for the code section identified by this handle.\\n *\\n * The usage of these annotations in the kernel code is different depending on\\n * the type of the kernel thread whose code is being annotated.\\n *\\n * For global kernel threads that are spawned in a limited number of instances\\n * (e.g. one USB hub_event() worker thread is spawned per USB HCD) and for\\n * softirqs, each instance must be assigned a unique 4-byte instance id. The\\n * instance id is then combined with a 1-byte subsystem id to get a handle via\\n * kcov_remote_handle(subsystem_id, instance_id).\\n *\\n * For local kernel threads that are spawned from system calls handler when a\\n * user interacts with some kernel interface (e.g. vhost workers), a handle is\\n * passed from a userspace process as the common_handle field of the\\n * kcov_remote_arg struct (note, that the user must generate a handle by using\\n * kcov_remote_handle() with KCOV_SUBSYSTEM_COMMON as the subsystem id and an\\n * arbitrary 4-byte non-zero number as the instance id). This common handle\\n * then gets saved into the task_struct of the process that issued the\\n * KCOV_REMOTE_ENABLE ioctl. When this process issues system calls that spawn\\n * kernel threads, the common handle must be retrieved via kcov_common_handle()\\n * and passed to the spawned threads via custom annotations. Those kernel\\n * threads must in turn be annotated with kcov_remote_start(common_handle) and\\n * kcov_remote_stop(). All of the threads that are spawned by the same process\\n * obtain the same handle, hence the name \"common\".\\n *\\n * See Documentation/dev-tools/kcov.rst for more details.\\n *\\n * Internally, kcov_remote_start() looks up the kcov device associated with the\\n * provided handle, allocates an area for coverage collection, and saves the\\n * pointers to kcov and area into the current task_struct to allow coverage to\\n * be collected via __sanitizer_cov_trace_pc().\\n * In turns kcov_remote_stop() clears those pointers from task_struct to stop\\n * collecting coverage and copies all collected coverage into the kcov area.\\n */\\n\\nstatic inline bool kcov_mode_enabled(unsigned int mode)\\n{\\n\\treturn (mode & ~KCOV_IN_CTXSW) != KCOV_MODE_DISABLED;\\n}\\n\\nstatic void kcov_remote_softirq_start(struct task_struct *t)\\n{\\n\\tstruct kcov_percpu_data *data = this_cpu_ptr(&kcov_percpu_data);\\n\\tunsigned int mode;\\n\\n\\tmode = READ_ONCE(t->kcov_mode);\\n\\tbarrier();\\n\\tif (kcov_mode_enabled(mode)) {\\n\\t\\tdata->saved_mode = mode;\\n\\t\\tdata->saved_size = t->kcov_size;\\n\\t\\tdata->saved_area = t->kcov_area;\\n\\t\\tdata->saved_sequence = t->kcov_sequence;\\n\\t\\tdata->saved_kcov = t->kcov;\\n\\t\\tkcov_stop(t);\\n\\t}\\n}\\n\\nstatic void kcov_remote_softirq_stop(struct task_struct *t)\\n{\\n\\tstruct kcov_percpu_data *data = this_cpu_ptr(&kcov_percpu_data);\\n\\n\\tif (data->saved_kcov) {\\n\\t\\tkcov_start(t, data->saved_kcov, data->saved_size,\\n\\t\\t\\t\\tdata->saved_area, data->saved_mode,\\n\\t\\t\\t\\tdata->saved_sequence);\\n\\t\\tdata->saved_mode = 0;\\n\\t\\tdata->saved_size = 0;\\n\\t\\tdata->saved_area = NULL;\\n\\t\\tdata->saved_sequence = 0;\\n\\t\\tdata->saved_kcov = NULL;\\n\\t}\\n}\\n\\nvoid kcov_remote_start(u64 handle)\\n{\\n\\tstruct task_struct *t = current;\\n\\tstruct kcov_remote *remote;\\n\\tstruct kcov *kcov;\\n\\tunsigned int mode;\\n\\tvoid *area;\\n\\tunsigned int size;\\n\\tint sequence;\\n\\tunsigned long flags;\\n\\n\\tif (WARN_ON(!kcov_check_handle(handle, true, true, true)))\\n\\t\\treturn;\\n\\tif (!in_task() && !in_softirq_really())\\n\\t\\treturn;\\n\\n\\tlocal_lock_irqsave(&kcov_percpu_data.lock, flags);\\n\\n\\t/*\\n\\t * Check that kcov_remote_start() is not called twice in background\\n\\t * threads nor called by user tasks (with enabled kcov).\\n\\t */\\n\\tmode = READ_ONCE(t->kcov_mode);\\n\\tif (WARN_ON(in_task() && kcov_mode_enabled(mode))) {\\n\\t\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\t\\treturn;\\n\\t}\\n\\t/*\\n\\t * Check that kcov_remote_start() is not called twice in softirqs.\\n\\t * Note, that kcov_remote_start() can be called from a softirq that\\n\\t * happened while collecting coverage from a background thread.\\n\\t */\\n\\tif (WARN_ON(in_serving_softirq() && t->kcov_softirq)) {\\n\\t\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\t\\treturn;\\n\\t}\\n\\n\\tspin_lock(&kcov_remote_lock);\\n\\tremote = kcov_remote_find(handle);\\n\\tif (!remote) {\\n\\t\\tspin_unlock(&kcov_remote_lock);\\n\\t\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\t\\treturn;\\n\\t}\\n\\tkcov_debug(\"handle = %llx, context: %s\\\\n\", handle,\\n\\t\\t\\tin_task() ? \"task\" : \"softirq\");\\n\\tkcov = remote->kcov;\\n\\t/* Put in kcov_remote_stop(). */\\n\\tkcov_get(kcov);\\n\\t/*\\n\\t * Read kcov fields before unlock to prevent races with\\n\\t * KCOV_DISABLE / kcov_remote_reset().\\n\\t */\\n\\tmode = kcov->mode;\\n\\tsequence = kcov->sequence;\\n\\tif (in_task()) {\\n\\t\\tsize = kcov->remote_size;\\n\\t\\tarea = kcov_remote_area_get(size);\\n\\t} else {\\n\\t\\tsize = CONFIG_KCOV_IRQ_AREA_SIZE;\\n\\t\\tarea = this_cpu_ptr(&kcov_percpu_data)->irq_area;\\n\\t}\\n\\tspin_unlock(&kcov_remote_lock);\\n\\n\\t/* Can only happen when in_task(). */\\n\\tif (!area) {\\n\\t\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\t\\tarea = vmalloc(size * sizeof(unsigned long));\\n\\t\\tif (!area) {\\n\\t\\t\\tkcov_put(kcov);\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tlocal_lock_irqsave(&kcov_percpu_data.lock, flags);\\n\\t}\\n\\n\\t/* Reset coverage size. */\\n\\t*(u64 *)area = 0;\\n\\n\\tif (in_serving_softirq()) {\\n\\t\\tkcov_remote_softirq_start(t);\\n\\t\\tt->kcov_softirq = 1;\\n\\t}\\n\\tkcov_start(t, kcov, size, area, mode, sequence);\\n\\n\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\n}\\nEXPORT_SYMBOL(kcov_remote_start);\\n\\nstatic void kcov_move_area(enum kcov_mode mode, void *dst_area,\\n\\t\\t\\t\\tunsigned int dst_area_size, void *src_area)\\n{\\n\\tu64 word_size = sizeof(unsigned long);\\n\\tu64 count_size, entry_size_log;\\n\\tu64 dst_len, src_len;\\n\\tvoid *dst_entries, *src_entries;\\n\\tu64 dst_occupied, dst_free, bytes_to_move, entries_moved;\\n\\n\\tkcov_debug(\"%px %u <= %px %lu\\\\n\",\\n\\t\\tdst_area, dst_area_size, src_area, *(unsigned long *)src_area);\\n\\n\\tswitch (mode) {\\n\\tcase KCOV_MODE_TRACE_PC:\\n\\t\\tdst_len = READ_ONCE(*(unsigned long *)dst_area);\\n\\t\\tsrc_len = *(unsigned long *)src_area;\\n\\t\\tcount_size = sizeof(unsigned long);\\n\\t\\tentry_size_log = __ilog2_u64(sizeof(unsigned long));\\n\\t\\tbreak;\\n\\tcase KCOV_MODE_TRACE_CMP:\\n\\t\\tdst_len = READ_ONCE(*(u64 *)dst_area);\\n\\t\\tsrc_len = *(u64 *)src_area;\\n\\t\\tcount_size = sizeof(u64);\\n\\t\\tBUILD_BUG_ON(!is_power_of_2(KCOV_WORDS_PER_CMP));\\n\\t\\tentry_size_log = __ilog2_u64(sizeof(u64) * KCOV_WORDS_PER_CMP);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tWARN_ON(1);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* As arm can\\'t divide u64 integers use log of entry size. */\\n\\tif (dst_len > ((dst_area_size * word_size - count_size) >>\\n\\t\\t\\t\\tentry_size_log))\\n\\t\\treturn;\\n\\tdst_occupied = count_size + (dst_len << entry_size_log);\\n\\tdst_free = dst_area_size * word_size - dst_occupied;\\n\\tbytes_to_move = min(dst_free, src_len << entry_size_log);\\n\\tdst_entries = dst_area + dst_occupied;\\n\\tsrc_entries = src_area + count_size;\\n\\tmemcpy(dst_entries, src_entries, bytes_to_move);\\n\\tentries_moved = bytes_to_move >> entry_size_log;\\n\\n\\tswitch (mode) {\\n\\tcase KCOV_MODE_TRACE_PC:\\n\\t\\tWRITE_ONCE(*(unsigned long *)dst_area, dst_len + entries_moved);\\n\\t\\tbreak;\\n\\tcase KCOV_MODE_TRACE_CMP:\\n\\t\\tWRITE_ONCE(*(u64 *)dst_area, dst_len + entries_moved);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tbreak;\\n\\t}\\n}\\n\\n/* See the comment before kcov_remote_start() for usage details. */\\nvoid kcov_remote_stop(void)\\n{\\n\\tstruct task_struct *t = current;\\n\\tstruct kcov *kcov;\\n\\tunsigned int mode;\\n\\tvoid *area;\\n\\tunsigned int size;\\n\\tint sequence;\\n\\tunsigned long flags;\\n\\n\\tif (!in_task() && !in_softirq_really())\\n\\t\\treturn;\\n\\n\\tlocal_lock_irqsave(&kcov_percpu_data.lock, flags);\\n\\n\\tmode = READ_ONCE(t->kcov_mode);\\n\\tbarrier();\\n\\tif (!kcov_mode_enabled(mode)) {\\n\\t\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\t\\treturn;\\n\\t}\\n\\t/*\\n\\t * When in softirq, check if the corresponding kcov_remote_start()\\n\\t * actually found the remote handle and started collecting coverage.\\n\\t */\\n\\tif (in_serving_softirq() && !t->kcov_softirq) {\\n\\t\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\t\\treturn;\\n\\t}\\n\\t/* Make sure that kcov_softirq is only set when in softirq. */\\n\\tif (WARN_ON(!in_serving_softirq() && t->kcov_softirq)) {\\n\\t\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\t\\treturn;\\n\\t}\\n\\n\\tkcov = t->kcov;\\n\\tarea = t->kcov_area;\\n\\tsize = t->kcov_size;\\n\\tsequence = t->kcov_sequence;\\n\\n\\tkcov_stop(t);\\n\\tif (in_serving_softirq()) {\\n\\t\\tt->kcov_softirq = 0;\\n\\t\\tkcov_remote_softirq_stop(t);\\n\\t}\\n\\n\\tspin_lock(&kcov->lock);\\n\\t/*\\n\\t * KCOV_DISABLE could have been called between kcov_remote_start()\\n\\t * and kcov_remote_stop(), hence the sequence check.\\n\\t */\\n\\tif (sequence == kcov->sequence && kcov->remote)\\n\\t\\tkcov_move_area(kcov->mode, kcov->area, kcov->size, area);\\n\\tspin_unlock(&kcov->lock);\\n\\n\\tif (in_task()) {\\n\\t\\tspin_lock(&kcov_remote_lock);\\n\\t\\tkcov_remote_area_put(area, size);\\n\\t\\tspin_unlock(&kcov_remote_lock);\\n\\t}\\n\\n\\tlocal_unlock_irqrestore(&kcov_percpu_data.lock, flags);\\n\\n\\t/* Get in kcov_remote_start(). */\\n\\tkcov_put(kcov);\\n}\\nEXPORT_SYMBOL(kcov_remote_stop);\\n\\n/* See the comment before kcov_remote_start() for usage details. */\\nu64 kcov_common_handle(void)\\n{\\n\\tif (!in_task())\\n\\t\\treturn 0;\\n\\treturn current->kcov_handle;\\n}\\nEXPORT_SYMBOL(kcov_common_handle);\\n\\n#ifdef CONFIG_KCOV_SELFTEST\\nstatic void __init selftest(void)\\n{\\n\\tunsigned long start;\\n\\n\\tpr_err(\"running self test\\\\n\");\\n\\t/*\\n\\t * Test that interrupts don\\'t produce spurious coverage.\\n\\t * The coverage callback filters out interrupt code, but only\\n\\t * after the handler updates preempt count. Some code periodically\\n\\t * leaks out of that section and leads to spurious coverage.\\n\\t * It\\'s hard to call the actual interrupt handler directly,\\n\\t * so we just loop here for a bit waiting for a timer interrupt.\\n\\t * We set kcov_mode to enable tracing, but don\\'t setup the area,\\n\\t * so any attempt to trace will crash. Note: we must not call any\\n\\t * potentially traced functions in this region.\\n\\t */\\n\\tstart = jiffies;\\n\\tcurrent->kcov_mode = KCOV_MODE_TRACE_PC;\\n\\twhile ((jiffies - start) * MSEC_PER_SEC / HZ < 300)\\n\\t\\t;\\n\\tcurrent->kcov_mode = 0;\\n\\tpr_err(\"done running self test\\\\n\");\\n}\\n#endif\\n\\nstatic int __init kcov_init(void)\\n{\\n\\tint cpu;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tvoid *area = vmalloc_node(CONFIG_KCOV_IRQ_AREA_SIZE *\\n\\t\\t\\t\\tsizeof(unsigned long), cpu_to_node(cpu));\\n\\t\\tif (!area)\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\tper_cpu_ptr(&kcov_percpu_data, cpu)->irq_area = area;\\n\\t}\\n\\n\\t/*\\n\\t * The kcov debugfs file won\\'t ever get removed and thus,\\n\\t * there is no need to protect it against removal races. The\\n\\t * use of debugfs_create_file_unsafe() is actually safe here.\\n\\t */\\n\\tdebugfs_create_file_unsafe(\"kcov\", 0600, NULL, NULL, &kcov_fops);\\n\\n#ifdef CONFIG_KCOV_SELFTEST\\n\\tselftest();\\n#endif\\n\\n\\treturn 0;\\n}\\n\\ndevice_initcall(kcov_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/*\\n * kernel/configs.c\\n * Echo the kernel .config file used to build the kernel\\n *\\n * Copyright (C) 2002 Khalid Aziz <khalid_aziz@hp.com>\\n * Copyright (C) 2002 Randy Dunlap <rdunlap@xenotime.net>\\n * Copyright (C) 2002 Al Stone <ahs3@fc.hp.com>\\n * Copyright (C) 2002 Hewlett-Packard Company\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/module.h>\\n#include <linux/proc_fs.h>\\n#include <linux/seq_file.h>\\n#include <linux/init.h>\\n#include <linux/uaccess.h>\\n\\n/*\\n * \"IKCFG_ST\" and \"IKCFG_ED\" are used to extract the config data from\\n * a binary kernel image or a module. See scripts/extract-ikconfig.\\n */\\nasm (\\n\"\\t.pushsection .rodata, \\\\\"a\\\\\"\\t\\t\\\\n\"\\n\"\\t.ascii \\\\\"IKCFG_ST\\\\\"\\t\\t\\t\\\\n\"\\n\"\\t.global kernel_config_data\\t\\t\\\\n\"\\n\"kernel_config_data:\\t\\t\\t\\t\\\\n\"\\n\"\\t.incbin \\\\\"kernel/config_data.gz\\\\\"\\t\\\\n\"\\n\"\\t.global kernel_config_data_end\\t\\t\\\\n\"\\n\"kernel_config_data_end:\\t\\t\\t\\\\n\"\\n\"\\t.ascii \\\\\"IKCFG_ED\\\\\"\\t\\t\\t\\\\n\"\\n\"\\t.popsection\\t\\t\\t\\t\\\\n\"\\n);\\n\\n#ifdef CONFIG_IKCONFIG_PROC\\n\\nextern char kernel_config_data;\\nextern char kernel_config_data_end;\\n\\nstatic ssize_t\\nikconfig_read_current(struct file *file, char __user *buf,\\n\\t\\t      size_t len, loff_t * offset)\\n{\\n\\treturn simple_read_from_buffer(buf, len, offset,\\n\\t\\t\\t\\t       &kernel_config_data,\\n\\t\\t\\t\\t       &kernel_config_data_end -\\n\\t\\t\\t\\t       &kernel_config_data);\\n}\\n\\nstatic const struct proc_ops config_gz_proc_ops = {\\n\\t.proc_read\\t= ikconfig_read_current,\\n\\t.proc_lseek\\t= default_llseek,\\n};\\n\\nstatic int __init ikconfig_init(void)\\n{\\n\\tstruct proc_dir_entry *entry;\\n\\n\\t/* create the current config file */\\n\\tentry = proc_create(\"config.gz\", S_IFREG | S_IRUGO, NULL,\\n\\t\\t\\t    &config_gz_proc_ops);\\n\\tif (!entry)\\n\\t\\treturn -ENOMEM;\\n\\n\\tproc_set_size(entry, &kernel_config_data_end - &kernel_config_data);\\n\\n\\treturn 0;\\n}\\n\\nstatic void __exit ikconfig_cleanup(void)\\n{\\n\\tremove_proc_entry(\"config.gz\", NULL);\\n}\\n\\nmodule_init(ikconfig_init);\\nmodule_exit(ikconfig_cleanup);\\n\\n#endif /* CONFIG_IKCONFIG_PROC */\\n\\nMODULE_LICENSE(\"GPL\");\\nMODULE_AUTHOR(\"Randy Dunlap\");\\nMODULE_DESCRIPTION(\"Echo the kernel .config file used to build the kernel\");\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Simple stack backtrace regression test module\\n *\\n * (C) Copyright 2008 Intel Corporation\\n * Author: Arjan van de Ven <arjan@linux.intel.com>\\n */\\n\\n#include <linux/completion.h>\\n#include <linux/delay.h>\\n#include <linux/interrupt.h>\\n#include <linux/module.h>\\n#include <linux/sched.h>\\n#include <linux/stacktrace.h>\\n\\nstatic void backtrace_test_normal(void)\\n{\\n\\tpr_info(\"Testing a backtrace from process context.\\\\n\");\\n\\tpr_info(\"The following trace is a kernel self test and not a bug!\\\\n\");\\n\\n\\tdump_stack();\\n}\\n\\nstatic void backtrace_test_bh_workfn(struct work_struct *work)\\n{\\n\\tdump_stack();\\n}\\n\\nstatic DECLARE_WORK(backtrace_bh_work, &backtrace_test_bh_workfn);\\n\\nstatic void backtrace_test_bh(void)\\n{\\n\\tpr_info(\"Testing a backtrace from BH context.\\\\n\");\\n\\tpr_info(\"The following trace is a kernel self test and not a bug!\\\\n\");\\n\\n\\tqueue_work(system_bh_wq, &backtrace_bh_work);\\n\\tflush_work(&backtrace_bh_work);\\n}\\n\\n#ifdef CONFIG_STACKTRACE\\nstatic void backtrace_test_saved(void)\\n{\\n\\tunsigned long entries[8];\\n\\tunsigned int nr_entries;\\n\\n\\tpr_info(\"Testing a saved backtrace.\\\\n\");\\n\\tpr_info(\"The following trace is a kernel self test and not a bug!\\\\n\");\\n\\n\\tnr_entries = stack_trace_save(entries, ARRAY_SIZE(entries), 0);\\n\\tstack_trace_print(entries, nr_entries, 0);\\n}\\n#else\\nstatic void backtrace_test_saved(void)\\n{\\n\\tpr_info(\"Saved backtrace test skipped.\\\\n\");\\n}\\n#endif\\n\\nstatic int backtrace_regression_test(void)\\n{\\n\\tpr_info(\"====[ backtrace testing ]===========\\\\n\");\\n\\n\\tbacktrace_test_normal();\\n\\tbacktrace_test_bh();\\n\\tbacktrace_test_saved();\\n\\n\\tpr_info(\"====[ end of backtrace testing ]====\\\\n\");\\n\\treturn 0;\\n}\\n\\nstatic void exitf(void)\\n{\\n}\\n\\nmodule_init(backtrace_regression_test);\\nmodule_exit(exitf);\\nMODULE_DESCRIPTION(\"Simple stack backtrace regression test module\");\\nMODULE_LICENSE(\"GPL\");\\nMODULE_AUTHOR(\"Arjan van de Ven <arjan@linux.intel.com>\");\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * kallsyms.c: in-kernel printing of symbolic oopses and stack traces.\\n *\\n * Rewritten and vastly simplified by Rusty Russell for in-kernel\\n * module loader:\\n *   Copyright 2002 Rusty Russell <rusty@rustcorp.com.au> IBM Corporation\\n *\\n * ChangeLog:\\n *\\n * (25/Aug/2004) Paulo Marques <pmarques@grupopie.com>\\n *      Changed the compression method from stem compression to \"table lookup\"\\n *      compression (see scripts/kallsyms.c for a more complete description)\\n */\\n#include <linux/kallsyms.h>\\n#include <linux/init.h>\\n#include <linux/seq_file.h>\\n#include <linux/fs.h>\\n#include <linux/kdb.h>\\n#include <linux/err.h>\\n#include <linux/proc_fs.h>\\n#include <linux/sched.h>\\t/* for cond_resched */\\n#include <linux/ctype.h>\\n#include <linux/slab.h>\\n#include <linux/filter.h>\\n#include <linux/ftrace.h>\\n#include <linux/kprobes.h>\\n#include <linux/build_bug.h>\\n#include <linux/compiler.h>\\n#include <linux/module.h>\\n#include <linux/kernel.h>\\n#include <linux/bsearch.h>\\n#include <linux/btf_ids.h>\\n\\n#include \"kallsyms_internal.h\"\\n\\n/*\\n * Expand a compressed symbol data into the resulting uncompressed string,\\n * if uncompressed string is too long (>= maxlen), it will be truncated,\\n * given the offset to where the symbol is in the compressed stream.\\n */\\nstatic unsigned int kallsyms_expand_symbol(unsigned int off,\\n\\t\\t\\t\\t\\t   char *result, size_t maxlen)\\n{\\n\\tint len, skipped_first = 0;\\n\\tconst char *tptr;\\n\\tconst u8 *data;\\n\\n\\t/* Get the compressed symbol length from the first symbol byte. */\\n\\tdata = &kallsyms_names[off];\\n\\tlen = *data;\\n\\tdata++;\\n\\toff++;\\n\\n\\t/* If MSB is 1, it is a \"big\" symbol, so needs an additional byte. */\\n\\tif ((len & 0x80) != 0) {\\n\\t\\tlen = (len & 0x7F) | (*data << 7);\\n\\t\\tdata++;\\n\\t\\toff++;\\n\\t}\\n\\n\\t/*\\n\\t * Update the offset to return the offset for the next symbol on\\n\\t * the compressed stream.\\n\\t */\\n\\toff += len;\\n\\n\\t/*\\n\\t * For every byte on the compressed symbol data, copy the table\\n\\t * entry for that byte.\\n\\t */\\n\\twhile (len) {\\n\\t\\ttptr = &kallsyms_token_table[kallsyms_token_index[*data]];\\n\\t\\tdata++;\\n\\t\\tlen--;\\n\\n\\t\\twhile (*tptr) {\\n\\t\\t\\tif (skipped_first) {\\n\\t\\t\\t\\tif (maxlen <= 1)\\n\\t\\t\\t\\t\\tgoto tail;\\n\\t\\t\\t\\t*result = *tptr;\\n\\t\\t\\t\\tresult++;\\n\\t\\t\\t\\tmaxlen--;\\n\\t\\t\\t} else\\n\\t\\t\\t\\tskipped_first = 1;\\n\\t\\t\\ttptr++;\\n\\t\\t}\\n\\t}\\n\\ntail:\\n\\tif (maxlen)\\n\\t\\t*result = \\'\\\\0\\';\\n\\n\\t/* Return to offset to the next symbol. */\\n\\treturn off;\\n}\\n\\n/*\\n * Get symbol type information. This is encoded as a single char at the\\n * beginning of the symbol name.\\n */\\nstatic char kallsyms_get_symbol_type(unsigned int off)\\n{\\n\\t/*\\n\\t * Get just the first code, look it up in the token table,\\n\\t * and return the first char from this token.\\n\\t */\\n\\treturn kallsyms_token_table[kallsyms_token_index[kallsyms_names[off + 1]]];\\n}\\n\\n\\n/*\\n * Find the offset on the compressed stream given and index in the\\n * kallsyms array.\\n */\\nstatic unsigned int get_symbol_offset(unsigned long pos)\\n{\\n\\tconst u8 *name;\\n\\tint i, len;\\n\\n\\t/*\\n\\t * Use the closest marker we have. We have markers every 256 positions,\\n\\t * so that should be close enough.\\n\\t */\\n\\tname = &kallsyms_names[kallsyms_markers[pos >> 8]];\\n\\n\\t/*\\n\\t * Sequentially scan all the symbols up to the point we\\'re searching\\n\\t * for. Every symbol is stored in a [<len>][<len> bytes of data] format,\\n\\t * so we just need to add the len to the current pointer for every\\n\\t * symbol we wish to skip.\\n\\t */\\n\\tfor (i = 0; i < (pos & 0xFF); i++) {\\n\\t\\tlen = *name;\\n\\n\\t\\t/*\\n\\t\\t * If MSB is 1, it is a \"big\" symbol, so we need to look into\\n\\t\\t * the next byte (and skip it, too).\\n\\t\\t */\\n\\t\\tif ((len & 0x80) != 0)\\n\\t\\t\\tlen = ((len & 0x7F) | (name[1] << 7)) + 1;\\n\\n\\t\\tname = name + len + 1;\\n\\t}\\n\\n\\treturn name - kallsyms_names;\\n}\\n\\nunsigned long kallsyms_sym_address(int idx)\\n{\\n\\t/* values are unsigned offsets if --absolute-percpu is not in effect */\\n\\tif (!IS_ENABLED(CONFIG_KALLSYMS_ABSOLUTE_PERCPU))\\n\\t\\treturn kallsyms_relative_base + (u32)kallsyms_offsets[idx];\\n\\n\\t/* ...otherwise, positive offsets are absolute values */\\n\\tif (kallsyms_offsets[idx] >= 0)\\n\\t\\treturn kallsyms_offsets[idx];\\n\\n\\t/* ...and negative offsets are relative to kallsyms_relative_base - 1 */\\n\\treturn kallsyms_relative_base - 1 - kallsyms_offsets[idx];\\n}\\n\\nstatic unsigned int get_symbol_seq(int index)\\n{\\n\\tunsigned int i, seq = 0;\\n\\n\\tfor (i = 0; i < 3; i++)\\n\\t\\tseq = (seq << 8) | kallsyms_seqs_of_names[3 * index + i];\\n\\n\\treturn seq;\\n}\\n\\nstatic int kallsyms_lookup_names(const char *name,\\n\\t\\t\\t\\t unsigned int *start,\\n\\t\\t\\t\\t unsigned int *end)\\n{\\n\\tint ret;\\n\\tint low, mid, high;\\n\\tunsigned int seq, off;\\n\\tchar namebuf[KSYM_NAME_LEN];\\n\\n\\tlow = 0;\\n\\thigh = kallsyms_num_syms - 1;\\n\\n\\twhile (low <= high) {\\n\\t\\tmid = low + (high - low) / 2;\\n\\t\\tseq = get_symbol_seq(mid);\\n\\t\\toff = get_symbol_offset(seq);\\n\\t\\tkallsyms_expand_symbol(off, namebuf, ARRAY_SIZE(namebuf));\\n\\t\\tret = strcmp(name, namebuf);\\n\\t\\tif (ret > 0)\\n\\t\\t\\tlow = mid + 1;\\n\\t\\telse if (ret < 0)\\n\\t\\t\\thigh = mid - 1;\\n\\t\\telse\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tif (low > high)\\n\\t\\treturn -ESRCH;\\n\\n\\tlow = mid;\\n\\twhile (low) {\\n\\t\\tseq = get_symbol_seq(low - 1);\\n\\t\\toff = get_symbol_offset(seq);\\n\\t\\tkallsyms_expand_symbol(off, namebuf, ARRAY_SIZE(namebuf));\\n\\t\\tif (strcmp(name, namebuf))\\n\\t\\t\\tbreak;\\n\\t\\tlow--;\\n\\t}\\n\\t*start = low;\\n\\n\\tif (end) {\\n\\t\\thigh = mid;\\n\\t\\twhile (high < kallsyms_num_syms - 1) {\\n\\t\\t\\tseq = get_symbol_seq(high + 1);\\n\\t\\t\\toff = get_symbol_offset(seq);\\n\\t\\t\\tkallsyms_expand_symbol(off, namebuf, ARRAY_SIZE(namebuf));\\n\\t\\t\\tif (strcmp(name, namebuf))\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\thigh++;\\n\\t\\t}\\n\\t\\t*end = high;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/* Lookup the address for this symbol. Returns 0 if not found. */\\nunsigned long kallsyms_lookup_name(const char *name)\\n{\\n\\tint ret;\\n\\tunsigned int i;\\n\\n\\t/* Skip the search for empty string. */\\n\\tif (!*name)\\n\\t\\treturn 0;\\n\\n\\tret = kallsyms_lookup_names(name, &i, NULL);\\n\\tif (!ret)\\n\\t\\treturn kallsyms_sym_address(get_symbol_seq(i));\\n\\n\\treturn module_kallsyms_lookup_name(name);\\n}\\n\\n/*\\n * Iterate over all symbols in vmlinux.  For symbols from modules use\\n * module_kallsyms_on_each_symbol instead.\\n */\\nint kallsyms_on_each_symbol(int (*fn)(void *, const char *, unsigned long),\\n\\t\\t\\t    void *data)\\n{\\n\\tchar namebuf[KSYM_NAME_LEN];\\n\\tunsigned long i;\\n\\tunsigned int off;\\n\\tint ret;\\n\\n\\tfor (i = 0, off = 0; i < kallsyms_num_syms; i++) {\\n\\t\\toff = kallsyms_expand_symbol(off, namebuf, ARRAY_SIZE(namebuf));\\n\\t\\tret = fn(data, namebuf, kallsyms_sym_address(i));\\n\\t\\tif (ret != 0)\\n\\t\\t\\treturn ret;\\n\\t\\tcond_resched();\\n\\t}\\n\\treturn 0;\\n}\\n\\nint kallsyms_on_each_match_symbol(int (*fn)(void *, unsigned long),\\n\\t\\t\\t\\t  const char *name, void *data)\\n{\\n\\tint ret;\\n\\tunsigned int i, start, end;\\n\\n\\tret = kallsyms_lookup_names(name, &start, &end);\\n\\tif (ret)\\n\\t\\treturn 0;\\n\\n\\tfor (i = start; !ret && i <= end; i++) {\\n\\t\\tret = fn(data, kallsyms_sym_address(get_symbol_seq(i)));\\n\\t\\tcond_resched();\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic unsigned long get_symbol_pos(unsigned long addr,\\n\\t\\t\\t\\t    unsigned long *symbolsize,\\n\\t\\t\\t\\t    unsigned long *offset)\\n{\\n\\tunsigned long symbol_start = 0, symbol_end = 0;\\n\\tunsigned long i, low, high, mid;\\n\\n\\t/* Do a binary search on the sorted kallsyms_offsets array. */\\n\\tlow = 0;\\n\\thigh = kallsyms_num_syms;\\n\\n\\twhile (high - low > 1) {\\n\\t\\tmid = low + (high - low) / 2;\\n\\t\\tif (kallsyms_sym_address(mid) <= addr)\\n\\t\\t\\tlow = mid;\\n\\t\\telse\\n\\t\\t\\thigh = mid;\\n\\t}\\n\\n\\t/*\\n\\t * Search for the first aliased symbol. Aliased\\n\\t * symbols are symbols with the same address.\\n\\t */\\n\\twhile (low && kallsyms_sym_address(low-1) == kallsyms_sym_address(low))\\n\\t\\t--low;\\n\\n\\tsymbol_start = kallsyms_sym_address(low);\\n\\n\\t/* Search for next non-aliased symbol. */\\n\\tfor (i = low + 1; i < kallsyms_num_syms; i++) {\\n\\t\\tif (kallsyms_sym_address(i) > symbol_start) {\\n\\t\\t\\tsymbol_end = kallsyms_sym_address(i);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\t/* If we found no next symbol, we use the end of the section. */\\n\\tif (!symbol_end) {\\n\\t\\tif (is_kernel_inittext(addr))\\n\\t\\t\\tsymbol_end = (unsigned long)_einittext;\\n\\t\\telse if (IS_ENABLED(CONFIG_KALLSYMS_ALL))\\n\\t\\t\\tsymbol_end = (unsigned long)_end;\\n\\t\\telse\\n\\t\\t\\tsymbol_end = (unsigned long)_etext;\\n\\t}\\n\\n\\tif (symbolsize)\\n\\t\\t*symbolsize = symbol_end - symbol_start;\\n\\tif (offset)\\n\\t\\t*offset = addr - symbol_start;\\n\\n\\treturn low;\\n}\\n\\n/*\\n * Lookup an address but don\\'t bother to find any names.\\n */\\nint kallsyms_lookup_size_offset(unsigned long addr, unsigned long *symbolsize,\\n\\t\\t\\t\\tunsigned long *offset)\\n{\\n\\tchar namebuf[KSYM_NAME_LEN];\\n\\n\\tif (is_ksym_addr(addr)) {\\n\\t\\tget_symbol_pos(addr, symbolsize, offset);\\n\\t\\treturn 1;\\n\\t}\\n\\treturn !!module_address_lookup(addr, symbolsize, offset, NULL, NULL, namebuf) ||\\n\\t       !!__bpf_address_lookup(addr, symbolsize, offset, namebuf);\\n}\\n\\nstatic int kallsyms_lookup_buildid(unsigned long addr,\\n\\t\\t\\tunsigned long *symbolsize,\\n\\t\\t\\tunsigned long *offset, char **modname,\\n\\t\\t\\tconst unsigned char **modbuildid, char *namebuf)\\n{\\n\\tint ret;\\n\\n\\tnamebuf[KSYM_NAME_LEN - 1] = 0;\\n\\tnamebuf[0] = 0;\\n\\n\\tif (is_ksym_addr(addr)) {\\n\\t\\tunsigned long pos;\\n\\n\\t\\tpos = get_symbol_pos(addr, symbolsize, offset);\\n\\t\\t/* Grab name */\\n\\t\\tkallsyms_expand_symbol(get_symbol_offset(pos),\\n\\t\\t\\t\\t       namebuf, KSYM_NAME_LEN);\\n\\t\\tif (modname)\\n\\t\\t\\t*modname = NULL;\\n\\t\\tif (modbuildid)\\n\\t\\t\\t*modbuildid = NULL;\\n\\n\\t\\treturn strlen(namebuf);\\n\\t}\\n\\n\\t/* See if it\\'s in a module or a BPF JITed image. */\\n\\tret = module_address_lookup(addr, symbolsize, offset,\\n\\t\\t\\t\\t    modname, modbuildid, namebuf);\\n\\tif (!ret)\\n\\t\\tret = bpf_address_lookup(addr, symbolsize,\\n\\t\\t\\t\\t\\t offset, modname, namebuf);\\n\\n\\tif (!ret)\\n\\t\\tret = ftrace_mod_address_lookup(addr, symbolsize,\\n\\t\\t\\t\\t\\t\\toffset, modname, namebuf);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * Lookup an address\\n * - modname is set to NULL if it\\'s in the kernel.\\n * - We guarantee that the returned name is valid until we reschedule even if.\\n *   It resides in a module.\\n * - We also guarantee that modname will be valid until rescheduled.\\n */\\nconst char *kallsyms_lookup(unsigned long addr,\\n\\t\\t\\t    unsigned long *symbolsize,\\n\\t\\t\\t    unsigned long *offset,\\n\\t\\t\\t    char **modname, char *namebuf)\\n{\\n\\tint ret = kallsyms_lookup_buildid(addr, symbolsize, offset, modname,\\n\\t\\t\\t\\t\\t  NULL, namebuf);\\n\\n\\tif (!ret)\\n\\t\\treturn NULL;\\n\\n\\treturn namebuf;\\n}\\n\\nint lookup_symbol_name(unsigned long addr, char *symname)\\n{\\n\\tsymname[0] = \\'\\\\0\\';\\n\\tsymname[KSYM_NAME_LEN - 1] = \\'\\\\0\\';\\n\\n\\tif (is_ksym_addr(addr)) {\\n\\t\\tunsigned long pos;\\n\\n\\t\\tpos = get_symbol_pos(addr, NULL, NULL);\\n\\t\\t/* Grab name */\\n\\t\\tkallsyms_expand_symbol(get_symbol_offset(pos),\\n\\t\\t\\t\\t       symname, KSYM_NAME_LEN);\\n\\t\\treturn 0;\\n\\t}\\n\\t/* See if it\\'s in a module. */\\n\\treturn lookup_module_symbol_name(addr, symname);\\n}\\n\\n/* Look up a kernel symbol and return it in a text buffer. */\\nstatic int __sprint_symbol(char *buffer, unsigned long address,\\n\\t\\t\\t   int symbol_offset, int add_offset, int add_buildid)\\n{\\n\\tchar *modname;\\n\\tconst unsigned char *buildid;\\n\\tunsigned long offset, size;\\n\\tint len;\\n\\n\\taddress += symbol_offset;\\n\\tlen = kallsyms_lookup_buildid(address, &size, &offset, &modname, &buildid,\\n\\t\\t\\t\\t       buffer);\\n\\tif (!len)\\n\\t\\treturn sprintf(buffer, \"0x%lx\", address - symbol_offset);\\n\\n\\toffset -= symbol_offset;\\n\\n\\tif (add_offset)\\n\\t\\tlen += sprintf(buffer + len, \"+%#lx/%#lx\", offset, size);\\n\\n\\tif (modname) {\\n\\t\\tlen += sprintf(buffer + len, \" [%s\", modname);\\n#if IS_ENABLED(CONFIG_STACKTRACE_BUILD_ID)\\n\\t\\tif (add_buildid && buildid) {\\n\\t\\t\\t/* build ID should match length of sprintf */\\n#if IS_ENABLED(CONFIG_MODULES)\\n\\t\\t\\tstatic_assert(sizeof(typeof_member(struct module, build_id)) == 20);\\n#endif\\n\\t\\t\\tlen += sprintf(buffer + len, \" %20phN\", buildid);\\n\\t\\t}\\n#endif\\n\\t\\tlen += sprintf(buffer + len, \"]\");\\n\\t}\\n\\n\\treturn len;\\n}\\n\\n/**\\n * sprint_symbol - Look up a kernel symbol and return it in a text buffer\\n * @buffer: buffer to be stored\\n * @address: address to lookup\\n *\\n * This function looks up a kernel symbol with @address and stores its name,\\n * offset, size and module name to @buffer if possible. If no symbol was found,\\n * just saves its @address as is.\\n *\\n * This function returns the number of bytes stored in @buffer.\\n */\\nint sprint_symbol(char *buffer, unsigned long address)\\n{\\n\\treturn __sprint_symbol(buffer, address, 0, 1, 0);\\n}\\nEXPORT_SYMBOL_GPL(sprint_symbol);\\n\\n/**\\n * sprint_symbol_build_id - Look up a kernel symbol and return it in a text buffer\\n * @buffer: buffer to be stored\\n * @address: address to lookup\\n *\\n * This function looks up a kernel symbol with @address and stores its name,\\n * offset, size, module name and module build ID to @buffer if possible. If no\\n * symbol was found, just saves its @address as is.\\n *\\n * This function returns the number of bytes stored in @buffer.\\n */\\nint sprint_symbol_build_id(char *buffer, unsigned long address)\\n{\\n\\treturn __sprint_symbol(buffer, address, 0, 1, 1);\\n}\\nEXPORT_SYMBOL_GPL(sprint_symbol_build_id);\\n\\n/**\\n * sprint_symbol_no_offset - Look up a kernel symbol and return it in a text buffer\\n * @buffer: buffer to be stored\\n * @address: address to lookup\\n *\\n * This function looks up a kernel symbol with @address and stores its name\\n * and module name to @buffer if possible. If no symbol was found, just saves\\n * its @address as is.\\n *\\n * This function returns the number of bytes stored in @buffer.\\n */\\nint sprint_symbol_no_offset(char *buffer, unsigned long address)\\n{\\n\\treturn __sprint_symbol(buffer, address, 0, 0, 0);\\n}\\nEXPORT_SYMBOL_GPL(sprint_symbol_no_offset);\\n\\n/**\\n * sprint_backtrace - Look up a backtrace symbol and return it in a text buffer\\n * @buffer: buffer to be stored\\n * @address: address to lookup\\n *\\n * This function is for stack backtrace and does the same thing as\\n * sprint_symbol() but with modified/decreased @address. If there is a\\n * tail-call to the function marked \"noreturn\", gcc optimized out code after\\n * the call so that the stack-saved return address could point outside of the\\n * caller. This function ensures that kallsyms will find the original caller\\n * by decreasing @address.\\n *\\n * This function returns the number of bytes stored in @buffer.\\n */\\nint sprint_backtrace(char *buffer, unsigned long address)\\n{\\n\\treturn __sprint_symbol(buffer, address, -1, 1, 0);\\n}\\n\\n/**\\n * sprint_backtrace_build_id - Look up a backtrace symbol and return it in a text buffer\\n * @buffer: buffer to be stored\\n * @address: address to lookup\\n *\\n * This function is for stack backtrace and does the same thing as\\n * sprint_symbol() but with modified/decreased @address. If there is a\\n * tail-call to the function marked \"noreturn\", gcc optimized out code after\\n * the call so that the stack-saved return address could point outside of the\\n * caller. This function ensures that kallsyms will find the original caller\\n * by decreasing @address. This function also appends the module build ID to\\n * the @buffer if @address is within a kernel module.\\n *\\n * This function returns the number of bytes stored in @buffer.\\n */\\nint sprint_backtrace_build_id(char *buffer, unsigned long address)\\n{\\n\\treturn __sprint_symbol(buffer, address, -1, 1, 1);\\n}\\n\\n/* To avoid using get_symbol_offset for every symbol, we carry prefix along. */\\nstruct kallsym_iter {\\n\\tloff_t pos;\\n\\tloff_t pos_mod_end;\\n\\tloff_t pos_ftrace_mod_end;\\n\\tloff_t pos_bpf_end;\\n\\tunsigned long value;\\n\\tunsigned int nameoff; /* If iterating in core kernel symbols. */\\n\\tchar type;\\n\\tchar name[KSYM_NAME_LEN];\\n\\tchar module_name[MODULE_NAME_LEN];\\n\\tint exported;\\n\\tint show_value;\\n};\\n\\nstatic int get_ksymbol_mod(struct kallsym_iter *iter)\\n{\\n\\tint ret = module_get_kallsym(iter->pos - kallsyms_num_syms,\\n\\t\\t\\t\\t     &iter->value, &iter->type,\\n\\t\\t\\t\\t     iter->name, iter->module_name,\\n\\t\\t\\t\\t     &iter->exported);\\n\\tif (ret < 0) {\\n\\t\\titer->pos_mod_end = iter->pos;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\treturn 1;\\n}\\n\\n/*\\n * ftrace_mod_get_kallsym() may also get symbols for pages allocated for ftrace\\n * purposes. In that case \"__builtin__ftrace\" is used as a module name, even\\n * though \"__builtin__ftrace\" is not a module.\\n */\\nstatic int get_ksymbol_ftrace_mod(struct kallsym_iter *iter)\\n{\\n\\tint ret = ftrace_mod_get_kallsym(iter->pos - iter->pos_mod_end,\\n\\t\\t\\t\\t\\t &iter->value, &iter->type,\\n\\t\\t\\t\\t\\t iter->name, iter->module_name,\\n\\t\\t\\t\\t\\t &iter->exported);\\n\\tif (ret < 0) {\\n\\t\\titer->pos_ftrace_mod_end = iter->pos;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\treturn 1;\\n}\\n\\nstatic int get_ksymbol_bpf(struct kallsym_iter *iter)\\n{\\n\\tint ret;\\n\\n\\tstrscpy(iter->module_name, \"bpf\", MODULE_NAME_LEN);\\n\\titer->exported = 0;\\n\\tret = bpf_get_kallsym(iter->pos - iter->pos_ftrace_mod_end,\\n\\t\\t\\t      &iter->value, &iter->type,\\n\\t\\t\\t      iter->name);\\n\\tif (ret < 0) {\\n\\t\\titer->pos_bpf_end = iter->pos;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\treturn 1;\\n}\\n\\n/*\\n * This uses \"__builtin__kprobes\" as a module name for symbols for pages\\n * allocated for kprobes\\' purposes, even though \"__builtin__kprobes\" is not a\\n * module.\\n */\\nstatic int get_ksymbol_kprobe(struct kallsym_iter *iter)\\n{\\n\\tstrscpy(iter->module_name, \"__builtin__kprobes\", MODULE_NAME_LEN);\\n\\titer->exported = 0;\\n\\treturn kprobe_get_kallsym(iter->pos - iter->pos_bpf_end,\\n\\t\\t\\t\\t  &iter->value, &iter->type,\\n\\t\\t\\t\\t  iter->name) < 0 ? 0 : 1;\\n}\\n\\n/* Returns space to next name. */\\nstatic unsigned long get_ksymbol_core(struct kallsym_iter *iter)\\n{\\n\\tunsigned off = iter->nameoff;\\n\\n\\titer->module_name[0] = \\'\\\\0\\';\\n\\titer->value = kallsyms_sym_address(iter->pos);\\n\\n\\titer->type = kallsyms_get_symbol_type(off);\\n\\n\\toff = kallsyms_expand_symbol(off, iter->name, ARRAY_SIZE(iter->name));\\n\\n\\treturn off - iter->nameoff;\\n}\\n\\nstatic void reset_iter(struct kallsym_iter *iter, loff_t new_pos)\\n{\\n\\titer->name[0] = \\'\\\\0\\';\\n\\titer->nameoff = get_symbol_offset(new_pos);\\n\\titer->pos = new_pos;\\n\\tif (new_pos == 0) {\\n\\t\\titer->pos_mod_end = 0;\\n\\t\\titer->pos_ftrace_mod_end = 0;\\n\\t\\titer->pos_bpf_end = 0;\\n\\t}\\n}\\n\\n/*\\n * The end position (last + 1) of each additional kallsyms section is recorded\\n * in iter->pos_..._end as each section is added, and so can be used to\\n * determine which get_ksymbol_...() function to call next.\\n */\\nstatic int update_iter_mod(struct kallsym_iter *iter, loff_t pos)\\n{\\n\\titer->pos = pos;\\n\\n\\tif ((!iter->pos_mod_end || iter->pos_mod_end > pos) &&\\n\\t    get_ksymbol_mod(iter))\\n\\t\\treturn 1;\\n\\n\\tif ((!iter->pos_ftrace_mod_end || iter->pos_ftrace_mod_end > pos) &&\\n\\t    get_ksymbol_ftrace_mod(iter))\\n\\t\\treturn 1;\\n\\n\\tif ((!iter->pos_bpf_end || iter->pos_bpf_end > pos) &&\\n\\t    get_ksymbol_bpf(iter))\\n\\t\\treturn 1;\\n\\n\\treturn get_ksymbol_kprobe(iter);\\n}\\n\\n/* Returns false if pos at or past end of file. */\\nstatic int update_iter(struct kallsym_iter *iter, loff_t pos)\\n{\\n\\t/* Module symbols can be accessed randomly. */\\n\\tif (pos >= kallsyms_num_syms)\\n\\t\\treturn update_iter_mod(iter, pos);\\n\\n\\t/* If we\\'re not on the desired position, reset to new position. */\\n\\tif (pos != iter->pos)\\n\\t\\treset_iter(iter, pos);\\n\\n\\titer->nameoff += get_ksymbol_core(iter);\\n\\titer->pos++;\\n\\n\\treturn 1;\\n}\\n\\nstatic void *s_next(struct seq_file *m, void *p, loff_t *pos)\\n{\\n\\t(*pos)++;\\n\\n\\tif (!update_iter(m->private, *pos))\\n\\t\\treturn NULL;\\n\\treturn p;\\n}\\n\\nstatic void *s_start(struct seq_file *m, loff_t *pos)\\n{\\n\\tif (!update_iter(m->private, *pos))\\n\\t\\treturn NULL;\\n\\treturn m->private;\\n}\\n\\nstatic void s_stop(struct seq_file *m, void *p)\\n{\\n}\\n\\nstatic int s_show(struct seq_file *m, void *p)\\n{\\n\\tvoid *value;\\n\\tstruct kallsym_iter *iter = m->private;\\n\\n\\t/* Some debugging symbols have no name.  Ignore them. */\\n\\tif (!iter->name[0])\\n\\t\\treturn 0;\\n\\n\\tvalue = iter->show_value ? (void *)iter->value : NULL;\\n\\n\\tif (iter->module_name[0]) {\\n\\t\\tchar type;\\n\\n\\t\\t/*\\n\\t\\t * Label it \"global\" if it is exported,\\n\\t\\t * \"local\" if not exported.\\n\\t\\t */\\n\\t\\ttype = iter->exported ? toupper(iter->type) :\\n\\t\\t\\t\\t\\ttolower(iter->type);\\n\\t\\tseq_printf(m, \"%px %c %s\\\\t[%s]\\\\n\", value,\\n\\t\\t\\t   type, iter->name, iter->module_name);\\n\\t} else\\n\\t\\tseq_printf(m, \"%px %c %s\\\\n\", value,\\n\\t\\t\\t   iter->type, iter->name);\\n\\treturn 0;\\n}\\n\\nstatic const struct seq_operations kallsyms_op = {\\n\\t.start = s_start,\\n\\t.next = s_next,\\n\\t.stop = s_stop,\\n\\t.show = s_show\\n};\\n\\n#ifdef CONFIG_BPF_SYSCALL\\n\\nstruct bpf_iter__ksym {\\n\\t__bpf_md_ptr(struct bpf_iter_meta *, meta);\\n\\t__bpf_md_ptr(struct kallsym_iter *, ksym);\\n};\\n\\nstatic int ksym_prog_seq_show(struct seq_file *m, bool in_stop)\\n{\\n\\tstruct bpf_iter__ksym ctx;\\n\\tstruct bpf_iter_meta meta;\\n\\tstruct bpf_prog *prog;\\n\\n\\tmeta.seq = m;\\n\\tprog = bpf_iter_get_info(&meta, in_stop);\\n\\tif (!prog)\\n\\t\\treturn 0;\\n\\n\\tctx.meta = &meta;\\n\\tctx.ksym = m ? m->private : NULL;\\n\\treturn bpf_iter_run_prog(prog, &ctx);\\n}\\n\\nstatic int bpf_iter_ksym_seq_show(struct seq_file *m, void *p)\\n{\\n\\treturn ksym_prog_seq_show(m, false);\\n}\\n\\nstatic void bpf_iter_ksym_seq_stop(struct seq_file *m, void *p)\\n{\\n\\tif (!p)\\n\\t\\t(void) ksym_prog_seq_show(m, true);\\n\\telse\\n\\t\\ts_stop(m, p);\\n}\\n\\nstatic const struct seq_operations bpf_iter_ksym_ops = {\\n\\t.start = s_start,\\n\\t.next = s_next,\\n\\t.stop = bpf_iter_ksym_seq_stop,\\n\\t.show = bpf_iter_ksym_seq_show,\\n};\\n\\nstatic int bpf_iter_ksym_init(void *priv_data, struct bpf_iter_aux_info *aux)\\n{\\n\\tstruct kallsym_iter *iter = priv_data;\\n\\n\\treset_iter(iter, 0);\\n\\n\\t/* cache here as in kallsyms_open() case; use current process\\n\\t * credentials to tell BPF iterators if values should be shown.\\n\\t */\\n\\titer->show_value = kallsyms_show_value(current_cred());\\n\\n\\treturn 0;\\n}\\n\\nDEFINE_BPF_ITER_FUNC(ksym, struct bpf_iter_meta *meta, struct kallsym_iter *ksym)\\n\\nstatic const struct bpf_iter_seq_info ksym_iter_seq_info = {\\n\\t.seq_ops\\t\\t= &bpf_iter_ksym_ops,\\n\\t.init_seq_private\\t= bpf_iter_ksym_init,\\n\\t.fini_seq_private\\t= NULL,\\n\\t.seq_priv_size\\t\\t= sizeof(struct kallsym_iter),\\n};\\n\\nstatic struct bpf_iter_reg ksym_iter_reg_info = {\\n\\t.target                 = \"ksym\",\\n\\t.feature\\t\\t= BPF_ITER_RESCHED,\\n\\t.ctx_arg_info_size\\t= 1,\\n\\t.ctx_arg_info\\t\\t= {\\n\\t\\t{ offsetof(struct bpf_iter__ksym, ksym),\\n\\t\\t  PTR_TO_BTF_ID_OR_NULL },\\n\\t},\\n\\t.seq_info\\t\\t= &ksym_iter_seq_info,\\n};\\n\\nBTF_ID_LIST(btf_ksym_iter_id)\\nBTF_ID(struct, kallsym_iter)\\n\\nstatic int __init bpf_ksym_iter_register(void)\\n{\\n\\tksym_iter_reg_info.ctx_arg_info[0].btf_id = *btf_ksym_iter_id;\\n\\treturn bpf_iter_reg_target(&ksym_iter_reg_info);\\n}\\n\\nlate_initcall(bpf_ksym_iter_register);\\n\\n#endif /* CONFIG_BPF_SYSCALL */\\n\\nstatic int kallsyms_open(struct inode *inode, struct file *file)\\n{\\n\\t/*\\n\\t * We keep iterator in m->private, since normal case is to\\n\\t * s_start from where we left off, so we avoid doing\\n\\t * using get_symbol_offset for every symbol.\\n\\t */\\n\\tstruct kallsym_iter *iter;\\n\\titer = __seq_open_private(file, &kallsyms_op, sizeof(*iter));\\n\\tif (!iter)\\n\\t\\treturn -ENOMEM;\\n\\treset_iter(iter, 0);\\n\\n\\t/*\\n\\t * Instead of checking this on every s_show() call, cache\\n\\t * the result here at open time.\\n\\t */\\n\\titer->show_value = kallsyms_show_value(file->f_cred);\\n\\treturn 0;\\n}\\n\\n#ifdef\\tCONFIG_KGDB_KDB\\nconst char *kdb_walk_kallsyms(loff_t *pos)\\n{\\n\\tstatic struct kallsym_iter kdb_walk_kallsyms_iter;\\n\\tif (*pos == 0) {\\n\\t\\tmemset(&kdb_walk_kallsyms_iter, 0,\\n\\t\\t       sizeof(kdb_walk_kallsyms_iter));\\n\\t\\treset_iter(&kdb_walk_kallsyms_iter, 0);\\n\\t}\\n\\twhile (1) {\\n\\t\\tif (!update_iter(&kdb_walk_kallsyms_iter, *pos))\\n\\t\\t\\treturn NULL;\\n\\t\\t++*pos;\\n\\t\\t/* Some debugging symbols have no name.  Ignore them. */\\n\\t\\tif (kdb_walk_kallsyms_iter.name[0])\\n\\t\\t\\treturn kdb_walk_kallsyms_iter.name;\\n\\t}\\n}\\n#endif\\t/* CONFIG_KGDB_KDB */\\n\\nstatic const struct proc_ops kallsyms_proc_ops = {\\n\\t.proc_open\\t= kallsyms_open,\\n\\t.proc_read\\t= seq_read,\\n\\t.proc_lseek\\t= seq_lseek,\\n\\t.proc_release\\t= seq_release_private,\\n};\\n\\nstatic int __init kallsyms_init(void)\\n{\\n\\tproc_create(\"kallsyms\", 0444, NULL, &kallsyms_proc_ops);\\n\\treturn 0;\\n}\\ndevice_initcall(kallsyms_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/* audit_watch.c -- watching inodes\\n *\\n * Copyright 2003-2009 Red Hat, Inc.\\n * Copyright 2005 Hewlett-Packard Development Company, L.P.\\n * Copyright 2005 IBM Corporation\\n */\\n\\n#include <linux/file.h>\\n#include <linux/kernel.h>\\n#include <linux/audit.h>\\n#include <linux/kthread.h>\\n#include <linux/mutex.h>\\n#include <linux/fs.h>\\n#include <linux/fsnotify_backend.h>\\n#include <linux/namei.h>\\n#include <linux/netlink.h>\\n#include <linux/refcount.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/security.h>\\n#include \"audit.h\"\\n\\n/*\\n * Reference counting:\\n *\\n * audit_parent: lifetime is from audit_init_parent() to receipt of an FS_IGNORED\\n * \\tevent.  Each audit_watch holds a reference to its associated parent.\\n *\\n * audit_watch: if added to lists, lifetime is from audit_init_watch() to\\n * \\taudit_remove_watch().  Additionally, an audit_watch may exist\\n * \\ttemporarily to assist in searching existing filter data.  Each\\n * \\taudit_krule holds a reference to its associated watch.\\n */\\n\\nstruct audit_watch {\\n\\trefcount_t\\t\\tcount;\\t/* reference count */\\n\\tdev_t\\t\\t\\tdev;\\t/* associated superblock device */\\n\\tchar\\t\\t\\t*path;\\t/* insertion path */\\n\\tunsigned long\\t\\tino;\\t/* associated inode number */\\n\\tstruct audit_parent\\t*parent; /* associated parent */\\n\\tstruct list_head\\twlist;\\t/* entry in parent->watches list */\\n\\tstruct list_head\\trules;\\t/* anchor for krule->rlist */\\n};\\n\\nstruct audit_parent {\\n\\tstruct list_head\\twatches; /* anchor for audit_watch->wlist */\\n\\tstruct fsnotify_mark mark; /* fsnotify mark on the inode */\\n};\\n\\n/* fsnotify handle. */\\nstatic struct fsnotify_group *audit_watch_group;\\n\\n/* fsnotify events we care about. */\\n#define AUDIT_FS_WATCH (FS_MOVE | FS_CREATE | FS_DELETE | FS_DELETE_SELF |\\\\\\n\\t\\t\\tFS_MOVE_SELF | FS_UNMOUNT)\\n\\nstatic void audit_free_parent(struct audit_parent *parent)\\n{\\n\\tWARN_ON(!list_empty(&parent->watches));\\n\\tkfree(parent);\\n}\\n\\nstatic void audit_watch_free_mark(struct fsnotify_mark *entry)\\n{\\n\\tstruct audit_parent *parent;\\n\\n\\tparent = container_of(entry, struct audit_parent, mark);\\n\\taudit_free_parent(parent);\\n}\\n\\nstatic void audit_get_parent(struct audit_parent *parent)\\n{\\n\\tif (likely(parent))\\n\\t\\tfsnotify_get_mark(&parent->mark);\\n}\\n\\nstatic void audit_put_parent(struct audit_parent *parent)\\n{\\n\\tif (likely(parent))\\n\\t\\tfsnotify_put_mark(&parent->mark);\\n}\\n\\n/*\\n * Find and return the audit_parent on the given inode.  If found a reference\\n * is taken on this parent.\\n */\\nstatic inline struct audit_parent *audit_find_parent(struct inode *inode)\\n{\\n\\tstruct audit_parent *parent = NULL;\\n\\tstruct fsnotify_mark *entry;\\n\\n\\tentry = fsnotify_find_inode_mark(inode, audit_watch_group);\\n\\tif (entry)\\n\\t\\tparent = container_of(entry, struct audit_parent, mark);\\n\\n\\treturn parent;\\n}\\n\\nvoid audit_get_watch(struct audit_watch *watch)\\n{\\n\\trefcount_inc(&watch->count);\\n}\\n\\nvoid audit_put_watch(struct audit_watch *watch)\\n{\\n\\tif (refcount_dec_and_test(&watch->count)) {\\n\\t\\tWARN_ON(watch->parent);\\n\\t\\tWARN_ON(!list_empty(&watch->rules));\\n\\t\\tkfree(watch->path);\\n\\t\\tkfree(watch);\\n\\t}\\n}\\n\\nstatic void audit_remove_watch(struct audit_watch *watch)\\n{\\n\\tlist_del(&watch->wlist);\\n\\taudit_put_parent(watch->parent);\\n\\twatch->parent = NULL;\\n\\taudit_put_watch(watch); /* match initial get */\\n}\\n\\nchar *audit_watch_path(struct audit_watch *watch)\\n{\\n\\treturn watch->path;\\n}\\n\\nint audit_watch_compare(struct audit_watch *watch, unsigned long ino, dev_t dev)\\n{\\n\\treturn (watch->ino != AUDIT_INO_UNSET) &&\\n\\t\\t(watch->ino == ino) &&\\n\\t\\t(watch->dev == dev);\\n}\\n\\n/* Initialize a parent watch entry. */\\nstatic struct audit_parent *audit_init_parent(const struct path *path)\\n{\\n\\tstruct inode *inode = d_backing_inode(path->dentry);\\n\\tstruct audit_parent *parent;\\n\\tint ret;\\n\\n\\tparent = kzalloc(sizeof(*parent), GFP_KERNEL);\\n\\tif (unlikely(!parent))\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tINIT_LIST_HEAD(&parent->watches);\\n\\n\\tfsnotify_init_mark(&parent->mark, audit_watch_group);\\n\\tparent->mark.mask = AUDIT_FS_WATCH;\\n\\tret = fsnotify_add_inode_mark(&parent->mark, inode, 0);\\n\\tif (ret < 0) {\\n\\t\\taudit_free_parent(parent);\\n\\t\\treturn ERR_PTR(ret);\\n\\t}\\n\\n\\treturn parent;\\n}\\n\\n/* Initialize a watch entry. */\\nstatic struct audit_watch *audit_init_watch(char *path)\\n{\\n\\tstruct audit_watch *watch;\\n\\n\\twatch = kzalloc(sizeof(*watch), GFP_KERNEL);\\n\\tif (unlikely(!watch))\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tINIT_LIST_HEAD(&watch->rules);\\n\\trefcount_set(&watch->count, 1);\\n\\twatch->path = path;\\n\\twatch->dev = AUDIT_DEV_UNSET;\\n\\twatch->ino = AUDIT_INO_UNSET;\\n\\n\\treturn watch;\\n}\\n\\n/* Translate a watch string to kernel representation. */\\nint audit_to_watch(struct audit_krule *krule, char *path, int len, u32 op)\\n{\\n\\tstruct audit_watch *watch;\\n\\n\\tif (!audit_watch_group)\\n\\t\\treturn -EOPNOTSUPP;\\n\\n\\tif (path[0] != \\'/\\' || path[len-1] == \\'/\\' ||\\n\\t    (krule->listnr != AUDIT_FILTER_EXIT &&\\n\\t     krule->listnr != AUDIT_FILTER_URING_EXIT) ||\\n\\t    op != Audit_equal ||\\n\\t    krule->inode_f || krule->watch || krule->tree)\\n\\t\\treturn -EINVAL;\\n\\n\\twatch = audit_init_watch(path);\\n\\tif (IS_ERR(watch))\\n\\t\\treturn PTR_ERR(watch);\\n\\n\\tkrule->watch = watch;\\n\\n\\treturn 0;\\n}\\n\\n/* Duplicate the given audit watch.  The new watch\\'s rules list is initialized\\n * to an empty list and wlist is undefined. */\\nstatic struct audit_watch *audit_dupe_watch(struct audit_watch *old)\\n{\\n\\tchar *path;\\n\\tstruct audit_watch *new;\\n\\n\\tpath = kstrdup(old->path, GFP_KERNEL);\\n\\tif (unlikely(!path))\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tnew = audit_init_watch(path);\\n\\tif (IS_ERR(new)) {\\n\\t\\tkfree(path);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tnew->dev = old->dev;\\n\\tnew->ino = old->ino;\\n\\taudit_get_parent(old->parent);\\n\\tnew->parent = old->parent;\\n\\nout:\\n\\treturn new;\\n}\\n\\nstatic void audit_watch_log_rule_change(struct audit_krule *r, struct audit_watch *w, char *op)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\tab = audit_log_start(audit_context(), GFP_NOFS, AUDIT_CONFIG_CHANGE);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\taudit_log_session_info(ab);\\n\\taudit_log_format(ab, \"op=%s path=\", op);\\n\\taudit_log_untrustedstring(ab, w->path);\\n\\taudit_log_key(ab, r->filterkey);\\n\\taudit_log_format(ab, \" list=%d res=1\", r->listnr);\\n\\taudit_log_end(ab);\\n}\\n\\n/* Update inode info in audit rules based on filesystem event. */\\nstatic void audit_update_watch(struct audit_parent *parent,\\n\\t\\t\\t       const struct qstr *dname, dev_t dev,\\n\\t\\t\\t       unsigned long ino, unsigned invalidating)\\n{\\n\\tstruct audit_watch *owatch, *nwatch, *nextw;\\n\\tstruct audit_krule *r, *nextr;\\n\\tstruct audit_entry *oentry, *nentry;\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\t/* Run all of the watches on this parent looking for the one that\\n\\t * matches the given dname */\\n\\tlist_for_each_entry_safe(owatch, nextw, &parent->watches, wlist) {\\n\\t\\tif (audit_compare_dname_path(dname, owatch->path,\\n\\t\\t\\t\\t\\t     AUDIT_NAME_FULL))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* If the update involves invalidating rules, do the inode-based\\n\\t\\t * filtering now, so we don\\'t omit records. */\\n\\t\\tif (invalidating && !audit_dummy_context())\\n\\t\\t\\taudit_filter_inodes(current, audit_context());\\n\\n\\t\\t/* updating ino will likely change which audit_hash_list we\\n\\t\\t * are on so we need a new watch for the new list */\\n\\t\\tnwatch = audit_dupe_watch(owatch);\\n\\t\\tif (IS_ERR(nwatch)) {\\n\\t\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\t\\taudit_panic(\"error updating watch, skipping\");\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tnwatch->dev = dev;\\n\\t\\tnwatch->ino = ino;\\n\\n\\t\\tlist_for_each_entry_safe(r, nextr, &owatch->rules, rlist) {\\n\\n\\t\\t\\toentry = container_of(r, struct audit_entry, rule);\\n\\t\\t\\tlist_del(&oentry->rule.rlist);\\n\\t\\t\\tlist_del_rcu(&oentry->list);\\n\\n\\t\\t\\tnentry = audit_dupe_rule(&oentry->rule);\\n\\t\\t\\tif (IS_ERR(nentry)) {\\n\\t\\t\\t\\tlist_del(&oentry->rule.list);\\n\\t\\t\\t\\taudit_panic(\"error updating watch, removing\");\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tint h = audit_hash_ino((u32)ino);\\n\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * nentry->rule.watch == oentry->rule.watch so\\n\\t\\t\\t\\t * we must drop that reference and set it to our\\n\\t\\t\\t\\t * new watch.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\taudit_put_watch(nentry->rule.watch);\\n\\t\\t\\t\\taudit_get_watch(nwatch);\\n\\t\\t\\t\\tnentry->rule.watch = nwatch;\\n\\t\\t\\t\\tlist_add(&nentry->rule.rlist, &nwatch->rules);\\n\\t\\t\\t\\tlist_add_rcu(&nentry->list, &audit_inode_hash[h]);\\n\\t\\t\\t\\tlist_replace(&oentry->rule.list,\\n\\t\\t\\t\\t\\t     &nentry->rule.list);\\n\\t\\t\\t}\\n\\t\\t\\tif (oentry->rule.exe)\\n\\t\\t\\t\\taudit_remove_mark(oentry->rule.exe);\\n\\n\\t\\t\\tcall_rcu(&oentry->rcu, audit_free_rule_rcu);\\n\\t\\t}\\n\\n\\t\\taudit_remove_watch(owatch);\\n\\t\\tgoto add_watch_to_parent; /* event applies to a single watch */\\n\\t}\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\treturn;\\n\\nadd_watch_to_parent:\\n\\tlist_add(&nwatch->wlist, &parent->watches);\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\treturn;\\n}\\n\\n/* Remove all watches & rules associated with a parent that is going away. */\\nstatic void audit_remove_parent_watches(struct audit_parent *parent)\\n{\\n\\tstruct audit_watch *w, *nextw;\\n\\tstruct audit_krule *r, *nextr;\\n\\tstruct audit_entry *e;\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tlist_for_each_entry_safe(w, nextw, &parent->watches, wlist) {\\n\\t\\tlist_for_each_entry_safe(r, nextr, &w->rules, rlist) {\\n\\t\\t\\te = container_of(r, struct audit_entry, rule);\\n\\t\\t\\taudit_watch_log_rule_change(r, w, \"remove_rule\");\\n\\t\\t\\tif (e->rule.exe)\\n\\t\\t\\t\\taudit_remove_mark(e->rule.exe);\\n\\t\\t\\tlist_del(&r->rlist);\\n\\t\\t\\tlist_del(&r->list);\\n\\t\\t\\tlist_del_rcu(&e->list);\\n\\t\\t\\tcall_rcu(&e->rcu, audit_free_rule_rcu);\\n\\t\\t}\\n\\t\\taudit_remove_watch(w);\\n\\t}\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\tfsnotify_destroy_mark(&parent->mark, audit_watch_group);\\n}\\n\\n/* Get path information necessary for adding watches. */\\nstatic int audit_get_nd(struct audit_watch *watch, struct path *parent)\\n{\\n\\tstruct dentry *d = kern_path_locked(watch->path, parent);\\n\\tif (IS_ERR(d))\\n\\t\\treturn PTR_ERR(d);\\n\\tif (d_is_positive(d)) {\\n\\t\\t/* update watch filter fields */\\n\\t\\twatch->dev = d->d_sb->s_dev;\\n\\t\\twatch->ino = d_backing_inode(d)->i_ino;\\n\\t}\\n\\tinode_unlock(d_backing_inode(parent->dentry));\\n\\tdput(d);\\n\\treturn 0;\\n}\\n\\n/* Associate the given rule with an existing parent.\\n * Caller must hold audit_filter_mutex. */\\nstatic void audit_add_to_parent(struct audit_krule *krule,\\n\\t\\t\\t\\tstruct audit_parent *parent)\\n{\\n\\tstruct audit_watch *w, *watch = krule->watch;\\n\\tint watch_found = 0;\\n\\n\\tBUG_ON(!mutex_is_locked(&audit_filter_mutex));\\n\\n\\tlist_for_each_entry(w, &parent->watches, wlist) {\\n\\t\\tif (strcmp(watch->path, w->path))\\n\\t\\t\\tcontinue;\\n\\n\\t\\twatch_found = 1;\\n\\n\\t\\t/* put krule\\'s ref to temporary watch */\\n\\t\\taudit_put_watch(watch);\\n\\n\\t\\taudit_get_watch(w);\\n\\t\\tkrule->watch = watch = w;\\n\\n\\t\\taudit_put_parent(parent);\\n\\t\\tbreak;\\n\\t}\\n\\n\\tif (!watch_found) {\\n\\t\\twatch->parent = parent;\\n\\n\\t\\taudit_get_watch(watch);\\n\\t\\tlist_add(&watch->wlist, &parent->watches);\\n\\t}\\n\\tlist_add(&krule->rlist, &watch->rules);\\n}\\n\\n/* Find a matching watch entry, or add this one.\\n * Caller must hold audit_filter_mutex. */\\nint audit_add_watch(struct audit_krule *krule, struct list_head **list)\\n{\\n\\tstruct audit_watch *watch = krule->watch;\\n\\tstruct audit_parent *parent;\\n\\tstruct path parent_path;\\n\\tint h, ret = 0;\\n\\n\\t/*\\n\\t * When we will be calling audit_add_to_parent, krule->watch might have\\n\\t * been updated and watch might have been freed.\\n\\t * So we need to keep a reference of watch.\\n\\t */\\n\\taudit_get_watch(watch);\\n\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t/* Avoid calling path_lookup under audit_filter_mutex. */\\n\\tret = audit_get_nd(watch, &parent_path);\\n\\n\\t/* caller expects mutex locked */\\n\\tmutex_lock(&audit_filter_mutex);\\n\\n\\tif (ret) {\\n\\t\\taudit_put_watch(watch);\\n\\t\\treturn ret;\\n\\t}\\n\\n\\t/* either find an old parent or attach a new one */\\n\\tparent = audit_find_parent(d_backing_inode(parent_path.dentry));\\n\\tif (!parent) {\\n\\t\\tparent = audit_init_parent(&parent_path);\\n\\t\\tif (IS_ERR(parent)) {\\n\\t\\t\\tret = PTR_ERR(parent);\\n\\t\\t\\tgoto error;\\n\\t\\t}\\n\\t}\\n\\n\\taudit_add_to_parent(krule, parent);\\n\\n\\th = audit_hash_ino((u32)watch->ino);\\n\\t*list = &audit_inode_hash[h];\\nerror:\\n\\tpath_put(&parent_path);\\n\\taudit_put_watch(watch);\\n\\treturn ret;\\n}\\n\\nvoid audit_remove_watch_rule(struct audit_krule *krule)\\n{\\n\\tstruct audit_watch *watch = krule->watch;\\n\\tstruct audit_parent *parent = watch->parent;\\n\\n\\tlist_del(&krule->rlist);\\n\\n\\tif (list_empty(&watch->rules)) {\\n\\t\\t/*\\n\\t\\t * audit_remove_watch() drops our reference to \\'parent\\' which\\n\\t\\t * can get freed. Grab our own reference to be safe.\\n\\t\\t */\\n\\t\\taudit_get_parent(parent);\\n\\t\\taudit_remove_watch(watch);\\n\\t\\tif (list_empty(&parent->watches))\\n\\t\\t\\tfsnotify_destroy_mark(&parent->mark, audit_watch_group);\\n\\t\\taudit_put_parent(parent);\\n\\t}\\n}\\n\\n/* Update watch data in audit rules based on fsnotify events. */\\nstatic int audit_watch_handle_event(struct fsnotify_mark *inode_mark, u32 mask,\\n\\t\\t\\t\\t    struct inode *inode, struct inode *dir,\\n\\t\\t\\t\\t    const struct qstr *dname, u32 cookie)\\n{\\n\\tstruct audit_parent *parent;\\n\\n\\tparent = container_of(inode_mark, struct audit_parent, mark);\\n\\n\\tif (WARN_ON_ONCE(inode_mark->group != audit_watch_group))\\n\\t\\treturn 0;\\n\\n\\tif (mask & (FS_CREATE|FS_MOVED_TO) && inode)\\n\\t\\taudit_update_watch(parent, dname, inode->i_sb->s_dev, inode->i_ino, 0);\\n\\telse if (mask & (FS_DELETE|FS_MOVED_FROM))\\n\\t\\taudit_update_watch(parent, dname, AUDIT_DEV_UNSET, AUDIT_INO_UNSET, 1);\\n\\telse if (mask & (FS_DELETE_SELF|FS_UNMOUNT|FS_MOVE_SELF))\\n\\t\\taudit_remove_parent_watches(parent);\\n\\n\\treturn 0;\\n}\\n\\nstatic const struct fsnotify_ops audit_watch_fsnotify_ops = {\\n\\t.handle_inode_event =\\taudit_watch_handle_event,\\n\\t.free_mark =\\t\\taudit_watch_free_mark,\\n};\\n\\nstatic int __init audit_watch_init(void)\\n{\\n\\taudit_watch_group = fsnotify_alloc_group(&audit_watch_fsnotify_ops, 0);\\n\\tif (IS_ERR(audit_watch_group)) {\\n\\t\\taudit_watch_group = NULL;\\n\\t\\taudit_panic(\"cannot create audit fsnotify group\");\\n\\t}\\n\\treturn 0;\\n}\\ndevice_initcall(audit_watch_init);\\n\\nint audit_dupe_exe(struct audit_krule *new, struct audit_krule *old)\\n{\\n\\tstruct audit_fsnotify_mark *audit_mark;\\n\\tchar *pathname;\\n\\n\\tpathname = kstrdup(audit_mark_path(old->exe), GFP_KERNEL);\\n\\tif (!pathname)\\n\\t\\treturn -ENOMEM;\\n\\n\\taudit_mark = audit_alloc_mark(new, pathname, strlen(pathname));\\n\\tif (IS_ERR(audit_mark)) {\\n\\t\\tkfree(pathname);\\n\\t\\treturn PTR_ERR(audit_mark);\\n\\t}\\n\\tnew->exe = audit_mark;\\n\\n\\treturn 0;\\n}\\n\\nint audit_exe_compare(struct task_struct *tsk, struct audit_fsnotify_mark *mark)\\n{\\n\\tstruct file *exe_file;\\n\\tunsigned long ino;\\n\\tdev_t dev;\\n\\n\\t/* only do exe filtering if we are recording @current events/records */\\n\\tif (tsk != current)\\n\\t\\treturn 0;\\n\\n\\tif (!current->mm)\\n\\t\\treturn 0;\\n\\texe_file = get_mm_exe_file(current->mm);\\n\\tif (!exe_file)\\n\\t\\treturn 0;\\n\\tino = file_inode(exe_file)->i_ino;\\n\\tdev = file_inode(exe_file)->i_sb->s_dev;\\n\\tfput(exe_file);\\n\\n\\treturn audit_mark_compare(mark, ino, dev);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Handling of different ABIs (personalities).\\n *\\n * We group personalities into execution domains which have their\\n * own handlers for kernel entry points, signal mapping, etc...\\n *\\n * 2001-05-06\\tComplete rewrite,  Christoph Hellwig (hch@infradead.org)\\n */\\n\\n#include <linux/init.h>\\n#include <linux/kernel.h>\\n#include <linux/kmod.h>\\n#include <linux/module.h>\\n#include <linux/personality.h>\\n#include <linux/proc_fs.h>\\n#include <linux/sched.h>\\n#include <linux/seq_file.h>\\n#include <linux/syscalls.h>\\n#include <linux/sysctl.h>\\n#include <linux/types.h>\\n\\n#ifdef CONFIG_PROC_FS\\nstatic int execdomains_proc_show(struct seq_file *m, void *v)\\n{\\n\\tseq_puts(m, \"0-0\\\\tLinux           \\\\t[kernel]\\\\n\");\\n\\treturn 0;\\n}\\n\\nstatic int __init proc_execdomains_init(void)\\n{\\n\\tproc_create_single(\"execdomains\", 0, NULL, execdomains_proc_show);\\n\\treturn 0;\\n}\\nmodule_init(proc_execdomains_init);\\n#endif\\n\\nSYSCALL_DEFINE1(personality, unsigned int, personality)\\n{\\n\\tunsigned int old = current->personality;\\n\\n\\tif (personality != 0xffffffff)\\n\\t\\tset_personality(personality);\\n\\n\\treturn old;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  linux/kernel/compat.c\\n *\\n *  Kernel compatibililty routines for e.g. 32 bit syscall support\\n *  on 64 bit kernels.\\n *\\n *  Copyright (C) 2002-2003 Stephen Rothwell, IBM Corporation\\n */\\n\\n#include <linux/linkage.h>\\n#include <linux/compat.h>\\n#include <linux/errno.h>\\n#include <linux/time.h>\\n#include <linux/signal.h>\\n#include <linux/sched.h>\\t/* for MAX_SCHEDULE_TIMEOUT */\\n#include <linux/syscalls.h>\\n#include <linux/unistd.h>\\n#include <linux/security.h>\\n#include <linux/export.h>\\n#include <linux/migrate.h>\\n#include <linux/posix-timers.h>\\n#include <linux/times.h>\\n#include <linux/ptrace.h>\\n#include <linux/gfp.h>\\n\\n#include <linux/uaccess.h>\\n\\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\\n\\n/*\\n * sys_sigprocmask SIG_SETMASK sets the first (compat) word of the\\n * blocked set of signals to the supplied signal set\\n */\\nstatic inline void compat_sig_setmask(sigset_t *blocked, compat_sigset_word set)\\n{\\n\\tmemcpy(blocked->sig, &set, sizeof(set));\\n}\\n\\nCOMPAT_SYSCALL_DEFINE3(sigprocmask, int, how,\\n\\t\\t       compat_old_sigset_t __user *, nset,\\n\\t\\t       compat_old_sigset_t __user *, oset)\\n{\\n\\told_sigset_t old_set, new_set;\\n\\tsigset_t new_blocked;\\n\\n\\told_set = current->blocked.sig[0];\\n\\n\\tif (nset) {\\n\\t\\tif (get_user(new_set, nset))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tnew_set &= ~(sigmask(SIGKILL) | sigmask(SIGSTOP));\\n\\n\\t\\tnew_blocked = current->blocked;\\n\\n\\t\\tswitch (how) {\\n\\t\\tcase SIG_BLOCK:\\n\\t\\t\\tsigaddsetmask(&new_blocked, new_set);\\n\\t\\t\\tbreak;\\n\\t\\tcase SIG_UNBLOCK:\\n\\t\\t\\tsigdelsetmask(&new_blocked, new_set);\\n\\t\\t\\tbreak;\\n\\t\\tcase SIG_SETMASK:\\n\\t\\t\\tcompat_sig_setmask(&new_blocked, new_set);\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\n\\t\\tset_current_blocked(&new_blocked);\\n\\t}\\n\\n\\tif (oset) {\\n\\t\\tif (put_user(old_set, oset))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n#endif\\n\\nint put_compat_rusage(const struct rusage *r, struct compat_rusage __user *ru)\\n{\\n\\tstruct compat_rusage r32;\\n\\tmemset(&r32, 0, sizeof(r32));\\n\\tr32.ru_utime.tv_sec = r->ru_utime.tv_sec;\\n\\tr32.ru_utime.tv_usec = r->ru_utime.tv_usec;\\n\\tr32.ru_stime.tv_sec = r->ru_stime.tv_sec;\\n\\tr32.ru_stime.tv_usec = r->ru_stime.tv_usec;\\n\\tr32.ru_maxrss = r->ru_maxrss;\\n\\tr32.ru_ixrss = r->ru_ixrss;\\n\\tr32.ru_idrss = r->ru_idrss;\\n\\tr32.ru_isrss = r->ru_isrss;\\n\\tr32.ru_minflt = r->ru_minflt;\\n\\tr32.ru_majflt = r->ru_majflt;\\n\\tr32.ru_nswap = r->ru_nswap;\\n\\tr32.ru_inblock = r->ru_inblock;\\n\\tr32.ru_oublock = r->ru_oublock;\\n\\tr32.ru_msgsnd = r->ru_msgsnd;\\n\\tr32.ru_msgrcv = r->ru_msgrcv;\\n\\tr32.ru_nsignals = r->ru_nsignals;\\n\\tr32.ru_nvcsw = r->ru_nvcsw;\\n\\tr32.ru_nivcsw = r->ru_nivcsw;\\n\\tif (copy_to_user(ru, &r32, sizeof(r32)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\nstatic int compat_get_user_cpu_mask(compat_ulong_t __user *user_mask_ptr,\\n\\t\\t\\t\\t    unsigned len, struct cpumask *new_mask)\\n{\\n\\tunsigned long *k;\\n\\n\\tif (len < cpumask_size())\\n\\t\\tmemset(new_mask, 0, cpumask_size());\\n\\telse if (len > cpumask_size())\\n\\t\\tlen = cpumask_size();\\n\\n\\tk = cpumask_bits(new_mask);\\n\\treturn compat_get_bitmap(k, user_mask_ptr, len * 8);\\n}\\n\\nCOMPAT_SYSCALL_DEFINE3(sched_setaffinity, compat_pid_t, pid,\\n\\t\\t       unsigned int, len,\\n\\t\\t       compat_ulong_t __user *, user_mask_ptr)\\n{\\n\\tcpumask_var_t new_mask;\\n\\tint retval;\\n\\n\\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\n\\tretval = compat_get_user_cpu_mask(user_mask_ptr, len, new_mask);\\n\\tif (retval)\\n\\t\\tgoto out;\\n\\n\\tretval = sched_setaffinity(pid, new_mask);\\nout:\\n\\tfree_cpumask_var(new_mask);\\n\\treturn retval;\\n}\\n\\nCOMPAT_SYSCALL_DEFINE3(sched_getaffinity, compat_pid_t,  pid, unsigned int, len,\\n\\t\\t       compat_ulong_t __user *, user_mask_ptr)\\n{\\n\\tint ret;\\n\\tcpumask_var_t mask;\\n\\n\\tif ((len * BITS_PER_BYTE) < nr_cpu_ids)\\n\\t\\treturn -EINVAL;\\n\\tif (len & (sizeof(compat_ulong_t)-1))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (!zalloc_cpumask_var(&mask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\n\\tret = sched_getaffinity(pid, mask);\\n\\tif (ret == 0) {\\n\\t\\tunsigned int retlen = min(len, cpumask_size());\\n\\n\\t\\tif (compat_put_bitmap(user_mask_ptr, cpumask_bits(mask), retlen * 8))\\n\\t\\t\\tret = -EFAULT;\\n\\t\\telse\\n\\t\\t\\tret = retlen;\\n\\t}\\n\\tfree_cpumask_var(mask);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * We currently only need the following fields from the sigevent\\n * structure: sigev_value, sigev_signo, sig_notify and (sometimes\\n * sigev_notify_thread_id).  The others are handled in user mode.\\n * We also assume that copying sigev_value.sival_int is sufficient\\n * to keep all the bits of sigev_value.sival_ptr intact.\\n */\\nint get_compat_sigevent(struct sigevent *event,\\n\\t\\tconst struct compat_sigevent __user *u_event)\\n{\\n\\tmemset(event, 0, sizeof(*event));\\n\\treturn (!access_ok(u_event, sizeof(*u_event)) ||\\n\\t\\t__get_user(event->sigev_value.sival_int,\\n\\t\\t\\t&u_event->sigev_value.sival_int) ||\\n\\t\\t__get_user(event->sigev_signo, &u_event->sigev_signo) ||\\n\\t\\t__get_user(event->sigev_notify, &u_event->sigev_notify) ||\\n\\t\\t__get_user(event->sigev_notify_thread_id,\\n\\t\\t\\t&u_event->sigev_notify_thread_id))\\n\\t\\t? -EFAULT : 0;\\n}\\n\\nlong compat_get_bitmap(unsigned long *mask, const compat_ulong_t __user *umask,\\n\\t\\t       unsigned long bitmap_size)\\n{\\n\\tunsigned long nr_compat_longs;\\n\\n\\t/* align bitmap up to nearest compat_long_t boundary */\\n\\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\\n\\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\\n\\n\\tif (!user_read_access_begin(umask, bitmap_size / 8))\\n\\t\\treturn -EFAULT;\\n\\n\\twhile (nr_compat_longs > 1) {\\n\\t\\tcompat_ulong_t l1, l2;\\n\\t\\tunsafe_get_user(l1, umask++, Efault);\\n\\t\\tunsafe_get_user(l2, umask++, Efault);\\n\\t\\t*mask++ = ((unsigned long)l2 << BITS_PER_COMPAT_LONG) | l1;\\n\\t\\tnr_compat_longs -= 2;\\n\\t}\\n\\tif (nr_compat_longs)\\n\\t\\tunsafe_get_user(*mask, umask++, Efault);\\n\\tuser_read_access_end();\\n\\treturn 0;\\n\\nEfault:\\n\\tuser_read_access_end();\\n\\treturn -EFAULT;\\n}\\n\\nlong compat_put_bitmap(compat_ulong_t __user *umask, unsigned long *mask,\\n\\t\\t       unsigned long bitmap_size)\\n{\\n\\tunsigned long nr_compat_longs;\\n\\n\\t/* align bitmap up to nearest compat_long_t boundary */\\n\\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\\n\\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\\n\\n\\tif (!user_write_access_begin(umask, bitmap_size / 8))\\n\\t\\treturn -EFAULT;\\n\\n\\twhile (nr_compat_longs > 1) {\\n\\t\\tunsigned long m = *mask++;\\n\\t\\tunsafe_put_user((compat_ulong_t)m, umask++, Efault);\\n\\t\\tunsafe_put_user(m >> BITS_PER_COMPAT_LONG, umask++, Efault);\\n\\t\\tnr_compat_longs -= 2;\\n\\t}\\n\\tif (nr_compat_longs)\\n\\t\\tunsafe_put_user((compat_ulong_t)*mask, umask++, Efault);\\n\\tuser_write_access_end();\\n\\treturn 0;\\nEfault:\\n\\tuser_write_access_end();\\n\\treturn -EFAULT;\\n}\\n\\nint\\nget_compat_sigset(sigset_t *set, const compat_sigset_t __user *compat)\\n{\\n#ifdef __BIG_ENDIAN\\n\\tcompat_sigset_t v;\\n\\tif (copy_from_user(&v, compat, sizeof(compat_sigset_t)))\\n\\t\\treturn -EFAULT;\\n\\tswitch (_NSIG_WORDS) {\\n\\tcase 4: set->sig[3] = v.sig[6] | (((long)v.sig[7]) << 32 );\\n\\t\\tfallthrough;\\n\\tcase 3: set->sig[2] = v.sig[4] | (((long)v.sig[5]) << 32 );\\n\\t\\tfallthrough;\\n\\tcase 2: set->sig[1] = v.sig[2] | (((long)v.sig[3]) << 32 );\\n\\t\\tfallthrough;\\n\\tcase 1: set->sig[0] = v.sig[0] | (((long)v.sig[1]) << 32 );\\n\\t}\\n#else\\n\\tif (copy_from_user(set, compat, sizeof(compat_sigset_t)))\\n\\t\\treturn -EFAULT;\\n#endif\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(get_compat_sigset);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Copyright (C) 2011 Google, Inc.\\n *\\n * Author:\\n *\\tColin Cross <ccross@android.com>\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/cpu_pm.h>\\n#include <linux/module.h>\\n#include <linux/notifier.h>\\n#include <linux/spinlock.h>\\n#include <linux/syscore_ops.h>\\n\\n/*\\n * atomic_notifiers use a spinlock_t, which can block under PREEMPT_RT.\\n * Notifications for cpu_pm will be issued by the idle task itself, which can\\n * never block, IOW it requires using a raw_spinlock_t.\\n */\\nstatic struct {\\n\\tstruct raw_notifier_head chain;\\n\\traw_spinlock_t lock;\\n} cpu_pm_notifier = {\\n\\t.chain = RAW_NOTIFIER_INIT(cpu_pm_notifier.chain),\\n\\t.lock  = __RAW_SPIN_LOCK_UNLOCKED(cpu_pm_notifier.lock),\\n};\\n\\nstatic int cpu_pm_notify(enum cpu_pm_event event)\\n{\\n\\tint ret;\\n\\n\\trcu_read_lock();\\n\\tret = raw_notifier_call_chain(&cpu_pm_notifier.chain, event, NULL);\\n\\trcu_read_unlock();\\n\\n\\treturn notifier_to_errno(ret);\\n}\\n\\nstatic int cpu_pm_notify_robust(enum cpu_pm_event event_up, enum cpu_pm_event event_down)\\n{\\n\\tunsigned long flags;\\n\\tint ret;\\n\\n\\traw_spin_lock_irqsave(&cpu_pm_notifier.lock, flags);\\n\\tret = raw_notifier_call_chain_robust(&cpu_pm_notifier.chain, event_up, event_down, NULL);\\n\\traw_spin_unlock_irqrestore(&cpu_pm_notifier.lock, flags);\\n\\n\\treturn notifier_to_errno(ret);\\n}\\n\\n/**\\n * cpu_pm_register_notifier - register a driver with cpu_pm\\n * @nb: notifier block to register\\n *\\n * Add a driver to a list of drivers that are notified about\\n * CPU and CPU cluster low power entry and exit.\\n *\\n * This function has the same return conditions as raw_notifier_chain_register.\\n */\\nint cpu_pm_register_notifier(struct notifier_block *nb)\\n{\\n\\tunsigned long flags;\\n\\tint ret;\\n\\n\\traw_spin_lock_irqsave(&cpu_pm_notifier.lock, flags);\\n\\tret = raw_notifier_chain_register(&cpu_pm_notifier.chain, nb);\\n\\traw_spin_unlock_irqrestore(&cpu_pm_notifier.lock, flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(cpu_pm_register_notifier);\\n\\n/**\\n * cpu_pm_unregister_notifier - unregister a driver with cpu_pm\\n * @nb: notifier block to be unregistered\\n *\\n * Remove a driver from the CPU PM notifier list.\\n *\\n * This function has the same return conditions as raw_notifier_chain_unregister.\\n */\\nint cpu_pm_unregister_notifier(struct notifier_block *nb)\\n{\\n\\tunsigned long flags;\\n\\tint ret;\\n\\n\\traw_spin_lock_irqsave(&cpu_pm_notifier.lock, flags);\\n\\tret = raw_notifier_chain_unregister(&cpu_pm_notifier.chain, nb);\\n\\traw_spin_unlock_irqrestore(&cpu_pm_notifier.lock, flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(cpu_pm_unregister_notifier);\\n\\n/**\\n * cpu_pm_enter - CPU low power entry notifier\\n *\\n * Notifies listeners that a single CPU is entering a low power state that may\\n * cause some blocks in the same power domain as the cpu to reset.\\n *\\n * Must be called on the affected CPU with interrupts disabled.  Platform is\\n * responsible for ensuring that cpu_pm_enter is not called twice on the same\\n * CPU before cpu_pm_exit is called. Notified drivers can include VFP\\n * co-processor, interrupt controller and its PM extensions, local CPU\\n * timers context save/restore which shouldn\\'t be interrupted. Hence it\\n * must be called with interrupts disabled.\\n *\\n * Return conditions are same as __raw_notifier_call_chain.\\n */\\nint cpu_pm_enter(void)\\n{\\n\\treturn cpu_pm_notify_robust(CPU_PM_ENTER, CPU_PM_ENTER_FAILED);\\n}\\nEXPORT_SYMBOL_GPL(cpu_pm_enter);\\n\\n/**\\n * cpu_pm_exit - CPU low power exit notifier\\n *\\n * Notifies listeners that a single CPU is exiting a low power state that may\\n * have caused some blocks in the same power domain as the cpu to reset.\\n *\\n * Notified drivers can include VFP co-processor, interrupt controller\\n * and its PM extensions, local CPU timers context save/restore which\\n * shouldn\\'t be interrupted. Hence it must be called with interrupts disabled.\\n *\\n * Return conditions are same as __raw_notifier_call_chain.\\n */\\nint cpu_pm_exit(void)\\n{\\n\\treturn cpu_pm_notify(CPU_PM_EXIT);\\n}\\nEXPORT_SYMBOL_GPL(cpu_pm_exit);\\n\\n/**\\n * cpu_cluster_pm_enter - CPU cluster low power entry notifier\\n *\\n * Notifies listeners that all cpus in a power domain are entering a low power\\n * state that may cause some blocks in the same power domain to reset.\\n *\\n * Must be called after cpu_pm_enter has been called on all cpus in the power\\n * domain, and before cpu_pm_exit has been called on any cpu in the power\\n * domain. Notified drivers can include VFP co-processor, interrupt controller\\n * and its PM extensions, local CPU timers context save/restore which\\n * shouldn\\'t be interrupted. Hence it must be called with interrupts disabled.\\n *\\n * Must be called with interrupts disabled.\\n *\\n * Return conditions are same as __raw_notifier_call_chain.\\n */\\nint cpu_cluster_pm_enter(void)\\n{\\n\\treturn cpu_pm_notify_robust(CPU_CLUSTER_PM_ENTER, CPU_CLUSTER_PM_ENTER_FAILED);\\n}\\nEXPORT_SYMBOL_GPL(cpu_cluster_pm_enter);\\n\\n/**\\n * cpu_cluster_pm_exit - CPU cluster low power exit notifier\\n *\\n * Notifies listeners that all cpus in a power domain are exiting form a\\n * low power state that may have caused some blocks in the same power domain\\n * to reset.\\n *\\n * Must be called after cpu_cluster_pm_enter has been called for the power\\n * domain, and before cpu_pm_exit has been called on any cpu in the power\\n * domain. Notified drivers can include VFP co-processor, interrupt controller\\n * and its PM extensions, local CPU timers context save/restore which\\n * shouldn\\'t be interrupted. Hence it must be called with interrupts disabled.\\n *\\n * Return conditions are same as __raw_notifier_call_chain.\\n */\\nint cpu_cluster_pm_exit(void)\\n{\\n\\treturn cpu_pm_notify(CPU_CLUSTER_PM_EXIT);\\n}\\nEXPORT_SYMBOL_GPL(cpu_cluster_pm_exit);\\n\\n#ifdef CONFIG_PM\\nstatic int cpu_pm_suspend(void)\\n{\\n\\tint ret;\\n\\n\\tret = cpu_pm_enter();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tret = cpu_cluster_pm_enter();\\n\\treturn ret;\\n}\\n\\nstatic void cpu_pm_resume(void)\\n{\\n\\tcpu_cluster_pm_exit();\\n\\tcpu_pm_exit();\\n}\\n\\nstatic struct syscore_ops cpu_pm_syscore_ops = {\\n\\t.suspend = cpu_pm_suspend,\\n\\t.resume = cpu_pm_resume,\\n};\\n\\nstatic int cpu_pm_init(void)\\n{\\n\\tregister_syscore_ops(&cpu_pm_syscore_ops);\\n\\treturn 0;\\n}\\ncore_initcall(cpu_pm_init);\\n#endif\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * linux/kernel/capability.c\\n *\\n * Copyright (C) 1997  Andrew Main <zefram@fysh.org>\\n *\\n * Integrated into 2.1.97+,  Andrew G. Morgan <morgan@kernel.org>\\n * 30 May 2002:\\tCleanup, Robert M. Love <rml@tech9.net>\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/audit.h>\\n#include <linux/capability.h>\\n#include <linux/mm.h>\\n#include <linux/export.h>\\n#include <linux/security.h>\\n#include <linux/syscalls.h>\\n#include <linux/pid_namespace.h>\\n#include <linux/user_namespace.h>\\n#include <linux/uaccess.h>\\n\\nint file_caps_enabled = 1;\\n\\nstatic int __init file_caps_disable(char *str)\\n{\\n\\tfile_caps_enabled = 0;\\n\\treturn 1;\\n}\\n__setup(\"no_file_caps\", file_caps_disable);\\n\\n#ifdef CONFIG_MULTIUSER\\n/*\\n * More recent versions of libcap are available from:\\n *\\n *   http://www.kernel.org/pub/linux/libs/security/linux-privs/\\n */\\n\\nstatic void warn_legacy_capability_use(void)\\n{\\n\\tchar name[sizeof(current->comm)];\\n\\n\\tpr_info_once(\"warning: `%s\\' uses 32-bit capabilities (legacy support in use)\\\\n\",\\n\\t\\t     get_task_comm(name, current));\\n}\\n\\n/*\\n * Version 2 capabilities worked fine, but the linux/capability.h file\\n * that accompanied their introduction encouraged their use without\\n * the necessary user-space source code changes. As such, we have\\n * created a version 3 with equivalent functionality to version 2, but\\n * with a header change to protect legacy source code from using\\n * version 2 when it wanted to use version 1. If your system has code\\n * that trips the following warning, it is using version 2 specific\\n * capabilities and may be doing so insecurely.\\n *\\n * The remedy is to either upgrade your version of libcap (to 2.10+,\\n * if the application is linked against it), or recompile your\\n * application with modern kernel headers and this warning will go\\n * away.\\n */\\n\\nstatic void warn_deprecated_v2(void)\\n{\\n\\tchar name[sizeof(current->comm)];\\n\\n\\tpr_info_once(\"warning: `%s\\' uses deprecated v2 capabilities in a way that may be insecure\\\\n\",\\n\\t\\t     get_task_comm(name, current));\\n}\\n\\n/*\\n * Version check. Return the number of u32s in each capability flag\\n * array, or a negative value on error.\\n */\\nstatic int cap_validate_magic(cap_user_header_t header, unsigned *tocopy)\\n{\\n\\t__u32 version;\\n\\n\\tif (get_user(version, &header->version))\\n\\t\\treturn -EFAULT;\\n\\n\\tswitch (version) {\\n\\tcase _LINUX_CAPABILITY_VERSION_1:\\n\\t\\twarn_legacy_capability_use();\\n\\t\\t*tocopy = _LINUX_CAPABILITY_U32S_1;\\n\\t\\tbreak;\\n\\tcase _LINUX_CAPABILITY_VERSION_2:\\n\\t\\twarn_deprecated_v2();\\n\\t\\tfallthrough;\\t/* v3 is otherwise equivalent to v2 */\\n\\tcase _LINUX_CAPABILITY_VERSION_3:\\n\\t\\t*tocopy = _LINUX_CAPABILITY_U32S_3;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tif (put_user((u32)_KERNEL_CAPABILITY_VERSION, &header->version))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * The only thing that can change the capabilities of the current\\n * process is the current process. As such, we can\\'t be in this code\\n * at the same time as we are in the process of setting capabilities\\n * in this process. The net result is that we can limit our use of\\n * locks to when we are reading the caps of another process.\\n */\\nstatic inline int cap_get_target_pid(pid_t pid, kernel_cap_t *pEp,\\n\\t\\t\\t\\t     kernel_cap_t *pIp, kernel_cap_t *pPp)\\n{\\n\\tint ret;\\n\\n\\tif (pid && (pid != task_pid_vnr(current))) {\\n\\t\\tconst struct task_struct *target;\\n\\n\\t\\trcu_read_lock();\\n\\n\\t\\ttarget = find_task_by_vpid(pid);\\n\\t\\tif (!target)\\n\\t\\t\\tret = -ESRCH;\\n\\t\\telse\\n\\t\\t\\tret = security_capget(target, pEp, pIp, pPp);\\n\\n\\t\\trcu_read_unlock();\\n\\t} else\\n\\t\\tret = security_capget(current, pEp, pIp, pPp);\\n\\n\\treturn ret;\\n}\\n\\n/**\\n * sys_capget - get the capabilities of a given process.\\n * @header: pointer to struct that contains capability version and\\n *\\ttarget pid data\\n * @dataptr: pointer to struct that contains the effective, permitted,\\n *\\tand inheritable capabilities that are returned\\n *\\n * Returns 0 on success and < 0 on error.\\n */\\nSYSCALL_DEFINE2(capget, cap_user_header_t, header, cap_user_data_t, dataptr)\\n{\\n\\tint ret = 0;\\n\\tpid_t pid;\\n\\tunsigned tocopy;\\n\\tkernel_cap_t pE, pI, pP;\\n\\tstruct __user_cap_data_struct kdata[2];\\n\\n\\tret = cap_validate_magic(header, &tocopy);\\n\\tif ((dataptr == NULL) || (ret != 0))\\n\\t\\treturn ((dataptr == NULL) && (ret == -EINVAL)) ? 0 : ret;\\n\\n\\tif (get_user(pid, &header->pid))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (pid < 0)\\n\\t\\treturn -EINVAL;\\n\\n\\tret = cap_get_target_pid(pid, &pE, &pI, &pP);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/*\\n\\t * Annoying legacy format with 64-bit capabilities exposed\\n\\t * as two sets of 32-bit fields, so we need to split the\\n\\t * capability values up.\\n\\t */\\n\\tkdata[0].effective   = pE.val; kdata[1].effective   = pE.val >> 32;\\n\\tkdata[0].permitted   = pP.val; kdata[1].permitted   = pP.val >> 32;\\n\\tkdata[0].inheritable = pI.val; kdata[1].inheritable = pI.val >> 32;\\n\\n\\t/*\\n\\t * Note, in the case, tocopy < _KERNEL_CAPABILITY_U32S,\\n\\t * we silently drop the upper capabilities here. This\\n\\t * has the effect of making older libcap\\n\\t * implementations implicitly drop upper capability\\n\\t * bits when they perform a: capget/modify/capset\\n\\t * sequence.\\n\\t *\\n\\t * This behavior is considered fail-safe\\n\\t * behavior. Upgrading the application to a newer\\n\\t * version of libcap will enable access to the newer\\n\\t * capabilities.\\n\\t *\\n\\t * An alternative would be to return an error here\\n\\t * (-ERANGE), but that causes legacy applications to\\n\\t * unexpectedly fail; the capget/modify/capset aborts\\n\\t * before modification is attempted and the application\\n\\t * fails.\\n\\t */\\n\\tif (copy_to_user(dataptr, kdata, tocopy * sizeof(kdata[0])))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn 0;\\n}\\n\\nstatic kernel_cap_t mk_kernel_cap(u32 low, u32 high)\\n{\\n\\treturn (kernel_cap_t) { (low | ((u64)high << 32)) & CAP_VALID_MASK };\\n}\\n\\n/**\\n * sys_capset - set capabilities for a process or (*) a group of processes\\n * @header: pointer to struct that contains capability version and\\n *\\ttarget pid data\\n * @data: pointer to struct that contains the effective, permitted,\\n *\\tand inheritable capabilities\\n *\\n * Set capabilities for the current process only.  The ability to any other\\n * process(es) has been deprecated and removed.\\n *\\n * The restrictions on setting capabilities are specified as:\\n *\\n * I: any raised capabilities must be a subset of the old permitted\\n * P: any raised capabilities must be a subset of the old permitted\\n * E: must be set to a subset of new permitted\\n *\\n * Returns 0 on success and < 0 on error.\\n */\\nSYSCALL_DEFINE2(capset, cap_user_header_t, header, const cap_user_data_t, data)\\n{\\n\\tstruct __user_cap_data_struct kdata[2] = { { 0, }, };\\n\\tunsigned tocopy, copybytes;\\n\\tkernel_cap_t inheritable, permitted, effective;\\n\\tstruct cred *new;\\n\\tint ret;\\n\\tpid_t pid;\\n\\n\\tret = cap_validate_magic(header, &tocopy);\\n\\tif (ret != 0)\\n\\t\\treturn ret;\\n\\n\\tif (get_user(pid, &header->pid))\\n\\t\\treturn -EFAULT;\\n\\n\\t/* may only affect current now */\\n\\tif (pid != 0 && pid != task_pid_vnr(current))\\n\\t\\treturn -EPERM;\\n\\n\\tcopybytes = tocopy * sizeof(struct __user_cap_data_struct);\\n\\tif (copybytes > sizeof(kdata))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (copy_from_user(&kdata, data, copybytes))\\n\\t\\treturn -EFAULT;\\n\\n\\teffective   = mk_kernel_cap(kdata[0].effective,   kdata[1].effective);\\n\\tpermitted   = mk_kernel_cap(kdata[0].permitted,   kdata[1].permitted);\\n\\tinheritable = mk_kernel_cap(kdata[0].inheritable, kdata[1].inheritable);\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\n\\tret = security_capset(new, current_cred(),\\n\\t\\t\\t      &effective, &inheritable, &permitted);\\n\\tif (ret < 0)\\n\\t\\tgoto error;\\n\\n\\taudit_log_capset(new, current_cred());\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn ret;\\n}\\n\\n/**\\n * has_ns_capability - Does a task have a capability in a specific user ns\\n * @t: The task in question\\n * @ns: target user namespace\\n * @cap: The capability to be tested for\\n *\\n * Return true if the specified task has the given superior capability\\n * currently in effect to the specified user namespace, false if not.\\n *\\n * Note that this does not set PF_SUPERPRIV on the task.\\n */\\nbool has_ns_capability(struct task_struct *t,\\n\\t\\t       struct user_namespace *ns, int cap)\\n{\\n\\tint ret;\\n\\n\\trcu_read_lock();\\n\\tret = security_capable(__task_cred(t), ns, cap, CAP_OPT_NONE);\\n\\trcu_read_unlock();\\n\\n\\treturn (ret == 0);\\n}\\n\\n/**\\n * has_capability - Does a task have a capability in init_user_ns\\n * @t: The task in question\\n * @cap: The capability to be tested for\\n *\\n * Return true if the specified task has the given superior capability\\n * currently in effect to the initial user namespace, false if not.\\n *\\n * Note that this does not set PF_SUPERPRIV on the task.\\n */\\nbool has_capability(struct task_struct *t, int cap)\\n{\\n\\treturn has_ns_capability(t, &init_user_ns, cap);\\n}\\nEXPORT_SYMBOL(has_capability);\\n\\n/**\\n * has_ns_capability_noaudit - Does a task have a capability (unaudited)\\n * in a specific user ns.\\n * @t: The task in question\\n * @ns: target user namespace\\n * @cap: The capability to be tested for\\n *\\n * Return true if the specified task has the given superior capability\\n * currently in effect to the specified user namespace, false if not.\\n * Do not write an audit message for the check.\\n *\\n * Note that this does not set PF_SUPERPRIV on the task.\\n */\\nbool has_ns_capability_noaudit(struct task_struct *t,\\n\\t\\t\\t       struct user_namespace *ns, int cap)\\n{\\n\\tint ret;\\n\\n\\trcu_read_lock();\\n\\tret = security_capable(__task_cred(t), ns, cap, CAP_OPT_NOAUDIT);\\n\\trcu_read_unlock();\\n\\n\\treturn (ret == 0);\\n}\\n\\n/**\\n * has_capability_noaudit - Does a task have a capability (unaudited) in the\\n * initial user ns\\n * @t: The task in question\\n * @cap: The capability to be tested for\\n *\\n * Return true if the specified task has the given superior capability\\n * currently in effect to init_user_ns, false if not.  Don\\'t write an\\n * audit message for the check.\\n *\\n * Note that this does not set PF_SUPERPRIV on the task.\\n */\\nbool has_capability_noaudit(struct task_struct *t, int cap)\\n{\\n\\treturn has_ns_capability_noaudit(t, &init_user_ns, cap);\\n}\\nEXPORT_SYMBOL(has_capability_noaudit);\\n\\nstatic bool ns_capable_common(struct user_namespace *ns,\\n\\t\\t\\t      int cap,\\n\\t\\t\\t      unsigned int opts)\\n{\\n\\tint capable;\\n\\n\\tif (unlikely(!cap_valid(cap))) {\\n\\t\\tpr_crit(\"capable() called with invalid cap=%u\\\\n\", cap);\\n\\t\\tBUG();\\n\\t}\\n\\n\\tcapable = security_capable(current_cred(), ns, cap, opts);\\n\\tif (capable == 0) {\\n\\t\\tcurrent->flags |= PF_SUPERPRIV;\\n\\t\\treturn true;\\n\\t}\\n\\treturn false;\\n}\\n\\n/**\\n * ns_capable - Determine if the current task has a superior capability in effect\\n * @ns:  The usernamespace we want the capability in\\n * @cap: The capability to be tested for\\n *\\n * Return true if the current task has the given superior capability currently\\n * available for use, false if not.\\n *\\n * This sets PF_SUPERPRIV on the task if the capability is available on the\\n * assumption that it\\'s about to be used.\\n */\\nbool ns_capable(struct user_namespace *ns, int cap)\\n{\\n\\treturn ns_capable_common(ns, cap, CAP_OPT_NONE);\\n}\\nEXPORT_SYMBOL(ns_capable);\\n\\n/**\\n * ns_capable_noaudit - Determine if the current task has a superior capability\\n * (unaudited) in effect\\n * @ns:  The usernamespace we want the capability in\\n * @cap: The capability to be tested for\\n *\\n * Return true if the current task has the given superior capability currently\\n * available for use, false if not.\\n *\\n * This sets PF_SUPERPRIV on the task if the capability is available on the\\n * assumption that it\\'s about to be used.\\n */\\nbool ns_capable_noaudit(struct user_namespace *ns, int cap)\\n{\\n\\treturn ns_capable_common(ns, cap, CAP_OPT_NOAUDIT);\\n}\\nEXPORT_SYMBOL(ns_capable_noaudit);\\n\\n/**\\n * ns_capable_setid - Determine if the current task has a superior capability\\n * in effect, while signalling that this check is being done from within a\\n * setid or setgroups syscall.\\n * @ns:  The usernamespace we want the capability in\\n * @cap: The capability to be tested for\\n *\\n * Return true if the current task has the given superior capability currently\\n * available for use, false if not.\\n *\\n * This sets PF_SUPERPRIV on the task if the capability is available on the\\n * assumption that it\\'s about to be used.\\n */\\nbool ns_capable_setid(struct user_namespace *ns, int cap)\\n{\\n\\treturn ns_capable_common(ns, cap, CAP_OPT_INSETID);\\n}\\nEXPORT_SYMBOL(ns_capable_setid);\\n\\n/**\\n * capable - Determine if the current task has a superior capability in effect\\n * @cap: The capability to be tested for\\n *\\n * Return true if the current task has the given superior capability currently\\n * available for use, false if not.\\n *\\n * This sets PF_SUPERPRIV on the task if the capability is available on the\\n * assumption that it\\'s about to be used.\\n */\\nbool capable(int cap)\\n{\\n\\treturn ns_capable(&init_user_ns, cap);\\n}\\nEXPORT_SYMBOL(capable);\\n#endif /* CONFIG_MULTIUSER */\\n\\n/**\\n * file_ns_capable - Determine if the file\\'s opener had a capability in effect\\n * @file:  The file we want to check\\n * @ns:  The usernamespace we want the capability in\\n * @cap: The capability to be tested for\\n *\\n * Return true if task that opened the file had a capability in effect\\n * when the file was opened.\\n *\\n * This does not set PF_SUPERPRIV because the caller may not\\n * actually be privileged.\\n */\\nbool file_ns_capable(const struct file *file, struct user_namespace *ns,\\n\\t\\t     int cap)\\n{\\n\\n\\tif (WARN_ON_ONCE(!cap_valid(cap)))\\n\\t\\treturn false;\\n\\n\\tif (security_capable(file->f_cred, ns, cap, CAP_OPT_NONE) == 0)\\n\\t\\treturn true;\\n\\n\\treturn false;\\n}\\nEXPORT_SYMBOL(file_ns_capable);\\n\\n/**\\n * privileged_wrt_inode_uidgid - Do capabilities in the namespace work over the inode?\\n * @ns: The user namespace in question\\n * @idmap: idmap of the mount @inode was found from\\n * @inode: The inode in question\\n *\\n * Return true if the inode uid and gid are within the namespace.\\n */\\nbool privileged_wrt_inode_uidgid(struct user_namespace *ns,\\n\\t\\t\\t\\t struct mnt_idmap *idmap,\\n\\t\\t\\t\\t const struct inode *inode)\\n{\\n\\treturn vfsuid_has_mapping(ns, i_uid_into_vfsuid(idmap, inode)) &&\\n\\t       vfsgid_has_mapping(ns, i_gid_into_vfsgid(idmap, inode));\\n}\\n\\n/**\\n * capable_wrt_inode_uidgid - Check nsown_capable and uid and gid mapped\\n * @idmap: idmap of the mount @inode was found from\\n * @inode: The inode in question\\n * @cap: The capability in question\\n *\\n * Return true if the current task has the given capability targeted at\\n * its own user namespace and that the given inode\\'s uid and gid are\\n * mapped into the current user namespace.\\n */\\nbool capable_wrt_inode_uidgid(struct mnt_idmap *idmap,\\n\\t\\t\\t      const struct inode *inode, int cap)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\n\\treturn ns_capable(ns, cap) &&\\n\\t       privileged_wrt_inode_uidgid(ns, idmap, inode);\\n}\\nEXPORT_SYMBOL(capable_wrt_inode_uidgid);\\n\\n/**\\n * ptracer_capable - Determine if the ptracer holds CAP_SYS_PTRACE in the namespace\\n * @tsk: The task that may be ptraced\\n * @ns: The user namespace to search for CAP_SYS_PTRACE in\\n *\\n * Return true if the task that is ptracing the current task had CAP_SYS_PTRACE\\n * in the specified user namespace.\\n */\\nbool ptracer_capable(struct task_struct *tsk, struct user_namespace *ns)\\n{\\n\\tint ret = 0;  /* An absent tracer adds no restrictions */\\n\\tconst struct cred *cred;\\n\\n\\trcu_read_lock();\\n\\tcred = rcu_dereference(tsk->ptracer_cred);\\n\\tif (cred)\\n\\t\\tret = security_capable(cred, ns, CAP_SYS_PTRACE,\\n\\t\\t\\t\\t       CAP_OPT_NOAUDIT);\\n\\trcu_read_unlock();\\n\\treturn (ret == 0);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Context tracking: Probe on high level context boundaries such as kernel,\\n * userspace, guest or idle.\\n *\\n * This is used by RCU to remove its dependency on the timer tick while a CPU\\n * runs in idle, userspace or guest mode.\\n *\\n * User/guest tracking started by Frederic Weisbecker:\\n *\\n * Copyright (C) 2012 Red Hat, Inc., Frederic Weisbecker\\n *\\n * Many thanks to Gilad Ben-Yossef, Paul McKenney, Ingo Molnar, Andrew Morton,\\n * Steven Rostedt, Peter Zijlstra for suggestions and improvements.\\n *\\n * RCU extended quiescent state bits imported from kernel/rcu/tree.c\\n * where the relevant authorship may be found.\\n */\\n\\n#include <linux/context_tracking.h>\\n#include <linux/rcupdate.h>\\n#include <linux/sched.h>\\n#include <linux/hardirq.h>\\n#include <linux/export.h>\\n#include <linux/kprobes.h>\\n#include <trace/events/rcu.h>\\n\\n\\nDEFINE_PER_CPU(struct context_tracking, context_tracking) = {\\n#ifdef CONFIG_CONTEXT_TRACKING_IDLE\\n\\t.nesting = 1,\\n\\t.nmi_nesting = CT_NESTING_IRQ_NONIDLE,\\n#endif\\n\\t.state = ATOMIC_INIT(CT_RCU_WATCHING),\\n};\\nEXPORT_SYMBOL_GPL(context_tracking);\\n\\n#ifdef CONFIG_CONTEXT_TRACKING_IDLE\\n#define TPS(x)  tracepoint_string(x)\\n\\n/* Record the current task on exiting RCU-tasks (dyntick-idle entry). */\\nstatic __always_inline void rcu_task_exit(void)\\n{\\n#if defined(CONFIG_TASKS_RCU) && defined(CONFIG_NO_HZ_FULL)\\n\\tWRITE_ONCE(current->rcu_tasks_idle_cpu, smp_processor_id());\\n#endif /* #if defined(CONFIG_TASKS_RCU) && defined(CONFIG_NO_HZ_FULL) */\\n}\\n\\n/* Record no current task on entering RCU-tasks (dyntick-idle exit). */\\nstatic __always_inline void rcu_task_enter(void)\\n{\\n#if defined(CONFIG_TASKS_RCU) && defined(CONFIG_NO_HZ_FULL)\\n\\tWRITE_ONCE(current->rcu_tasks_idle_cpu, -1);\\n#endif /* #if defined(CONFIG_TASKS_RCU) && defined(CONFIG_NO_HZ_FULL) */\\n}\\n\\n/* Turn on heavyweight RCU tasks trace readers on kernel exit. */\\nstatic __always_inline void rcu_task_trace_heavyweight_enter(void)\\n{\\n#ifdef CONFIG_TASKS_TRACE_RCU\\n\\tif (IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB))\\n\\t\\tcurrent->trc_reader_special.b.need_mb = true;\\n#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */\\n}\\n\\n/* Turn off heavyweight RCU tasks trace readers on kernel entry. */\\nstatic __always_inline void rcu_task_trace_heavyweight_exit(void)\\n{\\n#ifdef CONFIG_TASKS_TRACE_RCU\\n\\tif (IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB))\\n\\t\\tcurrent->trc_reader_special.b.need_mb = false;\\n#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */\\n}\\n\\n/*\\n * Record entry into an extended quiescent state.  This is only to be\\n * called when not already in an extended quiescent state, that is,\\n * RCU is watching prior to the call to this function and is no longer\\n * watching upon return.\\n */\\nstatic noinstr void ct_kernel_exit_state(int offset)\\n{\\n\\tint seq;\\n\\n\\t/*\\n\\t * CPUs seeing atomic_add_return() must see prior RCU read-side\\n\\t * critical sections, and we also must force ordering with the\\n\\t * next idle sojourn.\\n\\t */\\n\\trcu_task_trace_heavyweight_enter();  // Before CT state update!\\n\\tseq = ct_state_inc(offset);\\n\\t// RCU is no longer watching.  Better be in extended quiescent state!\\n\\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && (seq & CT_RCU_WATCHING));\\n}\\n\\n/*\\n * Record exit from an extended quiescent state.  This is only to be\\n * called from an extended quiescent state, that is, RCU is not watching\\n * prior to the call to this function and is watching upon return.\\n */\\nstatic noinstr void ct_kernel_enter_state(int offset)\\n{\\n\\tint seq;\\n\\n\\t/*\\n\\t * CPUs seeing atomic_add_return() must see prior idle sojourns,\\n\\t * and we also must force ordering with the next RCU read-side\\n\\t * critical section.\\n\\t */\\n\\tseq = ct_state_inc(offset);\\n\\t// RCU is now watching.  Better not be in an extended quiescent state!\\n\\trcu_task_trace_heavyweight_exit();  // After CT state update!\\n\\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !(seq & CT_RCU_WATCHING));\\n}\\n\\n/*\\n * Enter an RCU extended quiescent state, which can be either the\\n * idle loop or adaptive-tickless usermode execution.\\n *\\n * We crowbar the ->nmi_nesting field to zero to allow for\\n * the possibility of usermode upcalls having messed up our count\\n * of interrupt nesting level during the prior busy period.\\n */\\nstatic void noinstr ct_kernel_exit(bool user, int offset)\\n{\\n\\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);\\n\\n\\tWARN_ON_ONCE(ct_nmi_nesting() != CT_NESTING_IRQ_NONIDLE);\\n\\tWRITE_ONCE(ct->nmi_nesting, 0);\\n\\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&\\n\\t\\t     ct_nesting() == 0);\\n\\tif (ct_nesting() != 1) {\\n\\t\\t// RCU will still be watching, so just do accounting and leave.\\n\\t\\tct->nesting--;\\n\\t\\treturn;\\n\\t}\\n\\n\\tinstrumentation_begin();\\n\\tlockdep_assert_irqs_disabled();\\n\\ttrace_rcu_watching(TPS(\"End\"), ct_nesting(), 0, ct_rcu_watching());\\n\\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !user && !is_idle_task(current));\\n\\trcu_preempt_deferred_qs(current);\\n\\n\\t// instrumentation for the noinstr ct_kernel_exit_state()\\n\\tinstrument_atomic_write(&ct->state, sizeof(ct->state));\\n\\n\\tinstrumentation_end();\\n\\tWRITE_ONCE(ct->nesting, 0); /* Avoid irq-access tearing. */\\n\\t// RCU is watching here ...\\n\\tct_kernel_exit_state(offset);\\n\\t// ... but is no longer watching here.\\n\\trcu_task_exit();\\n}\\n\\n/*\\n * Exit an RCU extended quiescent state, which can be either the\\n * idle loop or adaptive-tickless usermode execution.\\n *\\n * We crowbar the ->nmi_nesting field to CT_NESTING_IRQ_NONIDLE to\\n * allow for the possibility of usermode upcalls messing up our count of\\n * interrupt nesting level during the busy period that is just now starting.\\n */\\nstatic void noinstr ct_kernel_enter(bool user, int offset)\\n{\\n\\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);\\n\\tlong oldval;\\n\\n\\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !raw_irqs_disabled());\\n\\toldval = ct_nesting();\\n\\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);\\n\\tif (oldval) {\\n\\t\\t// RCU was already watching, so just do accounting and leave.\\n\\t\\tct->nesting++;\\n\\t\\treturn;\\n\\t}\\n\\trcu_task_enter();\\n\\t// RCU is not watching here ...\\n\\tct_kernel_enter_state(offset);\\n\\t// ... but is watching here.\\n\\tinstrumentation_begin();\\n\\n\\t// instrumentation for the noinstr ct_kernel_enter_state()\\n\\tinstrument_atomic_write(&ct->state, sizeof(ct->state));\\n\\n\\ttrace_rcu_watching(TPS(\"Start\"), ct_nesting(), 1, ct_rcu_watching());\\n\\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !user && !is_idle_task(current));\\n\\tWRITE_ONCE(ct->nesting, 1);\\n\\tWARN_ON_ONCE(ct_nmi_nesting());\\n\\tWRITE_ONCE(ct->nmi_nesting, CT_NESTING_IRQ_NONIDLE);\\n\\tinstrumentation_end();\\n}\\n\\n/**\\n * ct_nmi_exit - inform RCU of exit from NMI context\\n *\\n * If we are returning from the outermost NMI handler that interrupted an\\n * RCU-idle period, update ct->state and ct->nmi_nesting\\n * to let the RCU grace-period handling know that the CPU is back to\\n * being RCU-idle.\\n *\\n * If you add or remove a call to ct_nmi_exit(), be sure to test\\n * with CONFIG_RCU_EQS_DEBUG=y.\\n */\\nvoid noinstr ct_nmi_exit(void)\\n{\\n\\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);\\n\\n\\tinstrumentation_begin();\\n\\t/*\\n\\t * Check for ->nmi_nesting underflow and bad CT state.\\n\\t * (We are exiting an NMI handler, so RCU better be paying attention\\n\\t * to us!)\\n\\t */\\n\\tWARN_ON_ONCE(ct_nmi_nesting() <= 0);\\n\\tWARN_ON_ONCE(!rcu_is_watching_curr_cpu());\\n\\n\\t/*\\n\\t * If the nesting level is not 1, the CPU wasn\\'t RCU-idle, so\\n\\t * leave it in non-RCU-idle state.\\n\\t */\\n\\tif (ct_nmi_nesting() != 1) {\\n\\t\\ttrace_rcu_watching(TPS(\"--=\"), ct_nmi_nesting(), ct_nmi_nesting() - 2,\\n\\t\\t\\t\\t  ct_rcu_watching());\\n\\t\\tWRITE_ONCE(ct->nmi_nesting, /* No store tearing. */\\n\\t\\t\\t   ct_nmi_nesting() - 2);\\n\\t\\tinstrumentation_end();\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* This NMI interrupted an RCU-idle CPU, restore RCU-idleness. */\\n\\ttrace_rcu_watching(TPS(\"Endirq\"), ct_nmi_nesting(), 0, ct_rcu_watching());\\n\\tWRITE_ONCE(ct->nmi_nesting, 0); /* Avoid store tearing. */\\n\\n\\t// instrumentation for the noinstr ct_kernel_exit_state()\\n\\tinstrument_atomic_write(&ct->state, sizeof(ct->state));\\n\\tinstrumentation_end();\\n\\n\\t// RCU is watching here ...\\n\\tct_kernel_exit_state(CT_RCU_WATCHING);\\n\\t// ... but is no longer watching here.\\n\\n\\tif (!in_nmi())\\n\\t\\trcu_task_exit();\\n}\\n\\n/**\\n * ct_nmi_enter - inform RCU of entry to NMI context\\n *\\n * If the CPU was idle from RCU\\'s viewpoint, update ct->state and\\n * ct->nmi_nesting to let the RCU grace-period handling know\\n * that the CPU is active.  This implementation permits nested NMIs, as\\n * long as the nesting level does not overflow an int.  (You will probably\\n * run out of stack space first.)\\n *\\n * If you add or remove a call to ct_nmi_enter(), be sure to test\\n * with CONFIG_RCU_EQS_DEBUG=y.\\n */\\nvoid noinstr ct_nmi_enter(void)\\n{\\n\\tlong incby = 2;\\n\\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);\\n\\n\\t/* Complain about underflow. */\\n\\tWARN_ON_ONCE(ct_nmi_nesting() < 0);\\n\\n\\t/*\\n\\t * If idle from RCU viewpoint, atomically increment CT state\\n\\t * to mark non-idle and increment ->nmi_nesting by one.\\n\\t * Otherwise, increment ->nmi_nesting by two.  This means\\n\\t * if ->nmi_nesting is equal to one, we are guaranteed\\n\\t * to be in the outermost NMI handler that interrupted an RCU-idle\\n\\t * period (observation due to Andy Lutomirski).\\n\\t */\\n\\tif (!rcu_is_watching_curr_cpu()) {\\n\\n\\t\\tif (!in_nmi())\\n\\t\\t\\trcu_task_enter();\\n\\n\\t\\t// RCU is not watching here ...\\n\\t\\tct_kernel_enter_state(CT_RCU_WATCHING);\\n\\t\\t// ... but is watching here.\\n\\n\\t\\tinstrumentation_begin();\\n\\t\\t// instrumentation for the noinstr rcu_is_watching_curr_cpu()\\n\\t\\tinstrument_atomic_read(&ct->state, sizeof(ct->state));\\n\\t\\t// instrumentation for the noinstr ct_kernel_enter_state()\\n\\t\\tinstrument_atomic_write(&ct->state, sizeof(ct->state));\\n\\n\\t\\tincby = 1;\\n\\t} else if (!in_nmi()) {\\n\\t\\tinstrumentation_begin();\\n\\t\\trcu_irq_enter_check_tick();\\n\\t} else  {\\n\\t\\tinstrumentation_begin();\\n\\t}\\n\\n\\ttrace_rcu_watching(incby == 1 ? TPS(\"Startirq\") : TPS(\"++=\"),\\n\\t\\t\\t  ct_nmi_nesting(),\\n\\t\\t\\t  ct_nmi_nesting() + incby, ct_rcu_watching());\\n\\tinstrumentation_end();\\n\\tWRITE_ONCE(ct->nmi_nesting, /* Prevent store tearing. */\\n\\t\\t   ct_nmi_nesting() + incby);\\n\\tbarrier();\\n}\\n\\n/**\\n * ct_idle_enter - inform RCU that current CPU is entering idle\\n *\\n * Enter idle mode, in other words, -leave- the mode in which RCU\\n * read-side critical sections can occur.  (Though RCU read-side\\n * critical sections can occur in irq handlers in idle, a possibility\\n * handled by irq_enter() and irq_exit().)\\n *\\n * If you add or remove a call to ct_idle_enter(), be sure to test with\\n * CONFIG_RCU_EQS_DEBUG=y.\\n */\\nvoid noinstr ct_idle_enter(void)\\n{\\n\\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !raw_irqs_disabled());\\n\\tct_kernel_exit(false, CT_RCU_WATCHING + CT_STATE_IDLE);\\n}\\nEXPORT_SYMBOL_GPL(ct_idle_enter);\\n\\n/**\\n * ct_idle_exit - inform RCU that current CPU is leaving idle\\n *\\n * Exit idle mode, in other words, -enter- the mode in which RCU\\n * read-side critical sections can occur.\\n *\\n * If you add or remove a call to ct_idle_exit(), be sure to test with\\n * CONFIG_RCU_EQS_DEBUG=y.\\n */\\nvoid noinstr ct_idle_exit(void)\\n{\\n\\tunsigned long flags;\\n\\n\\traw_local_irq_save(flags);\\n\\tct_kernel_enter(false, CT_RCU_WATCHING - CT_STATE_IDLE);\\n\\traw_local_irq_restore(flags);\\n}\\nEXPORT_SYMBOL_GPL(ct_idle_exit);\\n\\n/**\\n * ct_irq_enter - inform RCU that current CPU is entering irq away from idle\\n *\\n * Enter an interrupt handler, which might possibly result in exiting\\n * idle mode, in other words, entering the mode in which read-side critical\\n * sections can occur.  The caller must have disabled interrupts.\\n *\\n * Note that the Linux kernel is fully capable of entering an interrupt\\n * handler that it never exits, for example when doing upcalls to user mode!\\n * This code assumes that the idle loop never does upcalls to user mode.\\n * If your architecture\\'s idle loop does do upcalls to user mode (or does\\n * anything else that results in unbalanced calls to the irq_enter() and\\n * irq_exit() functions), RCU will give you what you deserve, good and hard.\\n * But very infrequently and irreproducibly.\\n *\\n * Use things like work queues to work around this limitation.\\n *\\n * You have been warned.\\n *\\n * If you add or remove a call to ct_irq_enter(), be sure to test with\\n * CONFIG_RCU_EQS_DEBUG=y.\\n */\\nnoinstr void ct_irq_enter(void)\\n{\\n\\tlockdep_assert_irqs_disabled();\\n\\tct_nmi_enter();\\n}\\n\\n/**\\n * ct_irq_exit - inform RCU that current CPU is exiting irq towards idle\\n *\\n * Exit from an interrupt handler, which might possibly result in entering\\n * idle mode, in other words, leaving the mode in which read-side critical\\n * sections can occur.  The caller must have disabled interrupts.\\n *\\n * This code assumes that the idle loop never does anything that might\\n * result in unbalanced calls to irq_enter() and irq_exit().  If your\\n * architecture\\'s idle loop violates this assumption, RCU will give you what\\n * you deserve, good and hard.  But very infrequently and irreproducibly.\\n *\\n * Use things like work queues to work around this limitation.\\n *\\n * You have been warned.\\n *\\n * If you add or remove a call to ct_irq_exit(), be sure to test with\\n * CONFIG_RCU_EQS_DEBUG=y.\\n */\\nnoinstr void ct_irq_exit(void)\\n{\\n\\tlockdep_assert_irqs_disabled();\\n\\tct_nmi_exit();\\n}\\n\\n/*\\n * Wrapper for ct_irq_enter() where interrupts are enabled.\\n *\\n * If you add or remove a call to ct_irq_enter_irqson(), be sure to test\\n * with CONFIG_RCU_EQS_DEBUG=y.\\n */\\nvoid ct_irq_enter_irqson(void)\\n{\\n\\tunsigned long flags;\\n\\n\\tlocal_irq_save(flags);\\n\\tct_irq_enter();\\n\\tlocal_irq_restore(flags);\\n}\\n\\n/*\\n * Wrapper for ct_irq_exit() where interrupts are enabled.\\n *\\n * If you add or remove a call to ct_irq_exit_irqson(), be sure to test\\n * with CONFIG_RCU_EQS_DEBUG=y.\\n */\\nvoid ct_irq_exit_irqson(void)\\n{\\n\\tunsigned long flags;\\n\\n\\tlocal_irq_save(flags);\\n\\tct_irq_exit();\\n\\tlocal_irq_restore(flags);\\n}\\n#else\\nstatic __always_inline void ct_kernel_exit(bool user, int offset) { }\\nstatic __always_inline void ct_kernel_enter(bool user, int offset) { }\\n#endif /* #ifdef CONFIG_CONTEXT_TRACKING_IDLE */\\n\\n#ifdef CONFIG_CONTEXT_TRACKING_USER\\n\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/context_tracking.h>\\n\\nDEFINE_STATIC_KEY_FALSE_RO(context_tracking_key);\\nEXPORT_SYMBOL_GPL(context_tracking_key);\\n\\nstatic noinstr bool context_tracking_recursion_enter(void)\\n{\\n\\tint recursion;\\n\\n\\trecursion = __this_cpu_inc_return(context_tracking.recursion);\\n\\tif (recursion == 1)\\n\\t\\treturn true;\\n\\n\\tWARN_ONCE((recursion < 1), \"Invalid context tracking recursion value %d\\\\n\", recursion);\\n\\t__this_cpu_dec(context_tracking.recursion);\\n\\n\\treturn false;\\n}\\n\\nstatic __always_inline void context_tracking_recursion_exit(void)\\n{\\n\\t__this_cpu_dec(context_tracking.recursion);\\n}\\n\\n/**\\n * __ct_user_enter - Inform the context tracking that the CPU is going\\n *\\t\\t     to enter user or guest space mode.\\n *\\n * @state: userspace context-tracking state to enter.\\n *\\n * This function must be called right before we switch from the kernel\\n * to user or guest space, when it\\'s guaranteed the remaining kernel\\n * instructions to execute won\\'t use any RCU read side critical section\\n * because this function sets RCU in extended quiescent state.\\n */\\nvoid noinstr __ct_user_enter(enum ctx_state state)\\n{\\n\\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);\\n\\tlockdep_assert_irqs_disabled();\\n\\n\\t/* Kernel threads aren\\'t supposed to go to userspace */\\n\\tWARN_ON_ONCE(!current->mm);\\n\\n\\tif (!context_tracking_recursion_enter())\\n\\t\\treturn;\\n\\n\\tif (__ct_state() != state) {\\n\\t\\tif (ct->active) {\\n\\t\\t\\t/*\\n\\t\\t\\t * At this stage, only low level arch entry code remains and\\n\\t\\t\\t * then we\\'ll run in userspace. We can assume there won\\'t be\\n\\t\\t\\t * any RCU read-side critical section until the next call to\\n\\t\\t\\t * user_exit() or ct_irq_enter(). Let\\'s remove RCU\\'s dependency\\n\\t\\t\\t * on the tick.\\n\\t\\t\\t */\\n\\t\\t\\tif (state == CT_STATE_USER) {\\n\\t\\t\\t\\tinstrumentation_begin();\\n\\t\\t\\t\\ttrace_user_enter(0);\\n\\t\\t\\t\\tvtime_user_enter(current);\\n\\t\\t\\t\\tinstrumentation_end();\\n\\t\\t\\t}\\n\\t\\t\\t/*\\n\\t\\t\\t * Other than generic entry implementation, we may be past the last\\n\\t\\t\\t * rescheduling opportunity in the entry code. Trigger a self IPI\\n\\t\\t\\t * that will fire and reschedule once we resume in user/guest mode.\\n\\t\\t\\t */\\n\\t\\t\\trcu_irq_work_resched();\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * Enter RCU idle mode right before resuming userspace.  No use of RCU\\n\\t\\t\\t * is permitted between this call and rcu_eqs_exit(). This way the\\n\\t\\t\\t * CPU doesn\\'t need to maintain the tick for RCU maintenance purposes\\n\\t\\t\\t * when the CPU runs in userspace.\\n\\t\\t\\t */\\n\\t\\t\\tct_kernel_exit(true, CT_RCU_WATCHING + state);\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * Special case if we only track user <-> kernel transitions for tickless\\n\\t\\t\\t * cputime accounting but we don\\'t support RCU extended quiescent state.\\n\\t\\t\\t * In this we case we don\\'t care about any concurrency/ordering.\\n\\t\\t\\t */\\n\\t\\t\\tif (!IS_ENABLED(CONFIG_CONTEXT_TRACKING_IDLE))\\n\\t\\t\\t\\traw_atomic_set(&ct->state, state);\\n\\t\\t} else {\\n\\t\\t\\t/*\\n\\t\\t\\t * Even if context tracking is disabled on this CPU, because it\\'s outside\\n\\t\\t\\t * the full dynticks mask for example, we still have to keep track of the\\n\\t\\t\\t * context transitions and states to prevent inconsistency on those of\\n\\t\\t\\t * other CPUs.\\n\\t\\t\\t * If a task triggers an exception in userspace, sleep on the exception\\n\\t\\t\\t * handler and then migrate to another CPU, that new CPU must know where\\n\\t\\t\\t * the exception returns by the time we call exception_exit().\\n\\t\\t\\t * This information can only be provided by the previous CPU when it called\\n\\t\\t\\t * exception_enter().\\n\\t\\t\\t * OTOH we can spare the calls to vtime and RCU when context_tracking.active\\n\\t\\t\\t * is false because we know that CPU is not tickless.\\n\\t\\t\\t */\\n\\t\\t\\tif (!IS_ENABLED(CONFIG_CONTEXT_TRACKING_IDLE)) {\\n\\t\\t\\t\\t/* Tracking for vtime only, no concurrent RCU EQS accounting */\\n\\t\\t\\t\\traw_atomic_set(&ct->state, state);\\n\\t\\t\\t} else {\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * Tracking for vtime and RCU EQS. Make sure we don\\'t race\\n\\t\\t\\t\\t * with NMIs. OTOH we don\\'t care about ordering here since\\n\\t\\t\\t\\t * RCU only requires CT_RCU_WATCHING increments to be fully\\n\\t\\t\\t\\t * ordered.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\traw_atomic_add(state, &ct->state);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\tcontext_tracking_recursion_exit();\\n}\\nEXPORT_SYMBOL_GPL(__ct_user_enter);\\n\\n/*\\n * OBSOLETE:\\n * This function should be noinstr but the below local_irq_restore() is\\n * unsafe because it involves illegal RCU uses through tracing and lockdep.\\n * This is unlikely to be fixed as this function is obsolete. The preferred\\n * way is to call __context_tracking_enter() through user_enter_irqoff()\\n * or context_tracking_guest_enter(). It should be the arch entry code\\n * responsibility to call into context tracking with IRQs disabled.\\n */\\nvoid ct_user_enter(enum ctx_state state)\\n{\\n\\tunsigned long flags;\\n\\n\\t/*\\n\\t * Some contexts may involve an exception occuring in an irq,\\n\\t * leading to that nesting:\\n\\t * ct_irq_enter() rcu_eqs_exit(true) rcu_eqs_enter(true) ct_irq_exit()\\n\\t * This would mess up the dyntick_nesting count though. And rcu_irq_*()\\n\\t * helpers are enough to protect RCU uses inside the exception. So\\n\\t * just return immediately if we detect we are in an IRQ.\\n\\t */\\n\\tif (in_interrupt())\\n\\t\\treturn;\\n\\n\\tlocal_irq_save(flags);\\n\\t__ct_user_enter(state);\\n\\tlocal_irq_restore(flags);\\n}\\nNOKPROBE_SYMBOL(ct_user_enter);\\nEXPORT_SYMBOL_GPL(ct_user_enter);\\n\\n/**\\n * user_enter_callable() - Unfortunate ASM callable version of user_enter() for\\n *\\t\\t\\t   archs that didn\\'t manage to check the context tracking\\n *\\t\\t\\t   static key from low level code.\\n *\\n * This OBSOLETE function should be noinstr but it unsafely calls\\n * local_irq_restore(), involving illegal RCU uses through tracing and lockdep.\\n * This is unlikely to be fixed as this function is obsolete. The preferred\\n * way is to call user_enter_irqoff(). It should be the arch entry code\\n * responsibility to call into context tracking with IRQs disabled.\\n */\\nvoid user_enter_callable(void)\\n{\\n\\tuser_enter();\\n}\\nNOKPROBE_SYMBOL(user_enter_callable);\\n\\n/**\\n * __ct_user_exit - Inform the context tracking that the CPU is\\n *\\t\\t    exiting user or guest mode and entering the kernel.\\n *\\n * @state: userspace context-tracking state being exited from.\\n *\\n * This function must be called after we entered the kernel from user or\\n * guest space before any use of RCU read side critical section. This\\n * potentially include any high level kernel code like syscalls, exceptions,\\n * signal handling, etc...\\n *\\n * This call supports re-entrancy. This way it can be called from any exception\\n * handler without needing to know if we came from userspace or not.\\n */\\nvoid noinstr __ct_user_exit(enum ctx_state state)\\n{\\n\\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);\\n\\n\\tif (!context_tracking_recursion_enter())\\n\\t\\treturn;\\n\\n\\tif (__ct_state() == state) {\\n\\t\\tif (ct->active) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Exit RCU idle mode while entering the kernel because it can\\n\\t\\t\\t * run a RCU read side critical section anytime.\\n\\t\\t\\t */\\n\\t\\t\\tct_kernel_enter(true, CT_RCU_WATCHING - state);\\n\\t\\t\\tif (state == CT_STATE_USER) {\\n\\t\\t\\t\\tinstrumentation_begin();\\n\\t\\t\\t\\tvtime_user_exit(current);\\n\\t\\t\\t\\ttrace_user_exit(0);\\n\\t\\t\\t\\tinstrumentation_end();\\n\\t\\t\\t}\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * Special case if we only track user <-> kernel transitions for tickless\\n\\t\\t\\t * cputime accounting but we don\\'t support RCU extended quiescent state.\\n\\t\\t\\t * In this we case we don\\'t care about any concurrency/ordering.\\n\\t\\t\\t */\\n\\t\\t\\tif (!IS_ENABLED(CONFIG_CONTEXT_TRACKING_IDLE))\\n\\t\\t\\t\\traw_atomic_set(&ct->state, CT_STATE_KERNEL);\\n\\n\\t\\t} else {\\n\\t\\t\\tif (!IS_ENABLED(CONFIG_CONTEXT_TRACKING_IDLE)) {\\n\\t\\t\\t\\t/* Tracking for vtime only, no concurrent RCU EQS accounting */\\n\\t\\t\\t\\traw_atomic_set(&ct->state, CT_STATE_KERNEL);\\n\\t\\t\\t} else {\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * Tracking for vtime and RCU EQS. Make sure we don\\'t race\\n\\t\\t\\t\\t * with NMIs. OTOH we don\\'t care about ordering here since\\n\\t\\t\\t\\t * RCU only requires CT_RCU_WATCHING increments to be fully\\n\\t\\t\\t\\t * ordered.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\traw_atomic_sub(state, &ct->state);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\tcontext_tracking_recursion_exit();\\n}\\nEXPORT_SYMBOL_GPL(__ct_user_exit);\\n\\n/*\\n * OBSOLETE:\\n * This function should be noinstr but the below local_irq_save() is\\n * unsafe because it involves illegal RCU uses through tracing and lockdep.\\n * This is unlikely to be fixed as this function is obsolete. The preferred\\n * way is to call __context_tracking_exit() through user_exit_irqoff()\\n * or context_tracking_guest_exit(). It should be the arch entry code\\n * responsibility to call into context tracking with IRQs disabled.\\n */\\nvoid ct_user_exit(enum ctx_state state)\\n{\\n\\tunsigned long flags;\\n\\n\\tif (in_interrupt())\\n\\t\\treturn;\\n\\n\\tlocal_irq_save(flags);\\n\\t__ct_user_exit(state);\\n\\tlocal_irq_restore(flags);\\n}\\nNOKPROBE_SYMBOL(ct_user_exit);\\nEXPORT_SYMBOL_GPL(ct_user_exit);\\n\\n/**\\n * user_exit_callable() - Unfortunate ASM callable version of user_exit() for\\n *\\t\\t\\t  archs that didn\\'t manage to check the context tracking\\n *\\t\\t\\t  static key from low level code.\\n *\\n * This OBSOLETE function should be noinstr but it unsafely calls local_irq_save(),\\n * involving illegal RCU uses through tracing and lockdep. This is unlikely\\n * to be fixed as this function is obsolete. The preferred way is to call\\n * user_exit_irqoff(). It should be the arch entry code responsibility to\\n * call into context tracking with IRQs disabled.\\n */\\nvoid user_exit_callable(void)\\n{\\n\\tuser_exit();\\n}\\nNOKPROBE_SYMBOL(user_exit_callable);\\n\\nvoid __init ct_cpu_track_user(int cpu)\\n{\\n\\tstatic __initdata bool initialized = false;\\n\\n\\tif (!per_cpu(context_tracking.active, cpu)) {\\n\\t\\tper_cpu(context_tracking.active, cpu) = true;\\n\\t\\tstatic_branch_inc(&context_tracking_key);\\n\\t}\\n\\n\\tif (initialized)\\n\\t\\treturn;\\n\\n#ifdef CONFIG_HAVE_TIF_NOHZ\\n\\t/*\\n\\t * Set TIF_NOHZ to init/0 and let it propagate to all tasks through fork\\n\\t * This assumes that init is the only task at this early boot stage.\\n\\t */\\n\\tset_tsk_thread_flag(&init_task, TIF_NOHZ);\\n#endif\\n\\tWARN_ON_ONCE(!tasklist_empty());\\n\\n\\tinitialized = true;\\n}\\n\\n#ifdef CONFIG_CONTEXT_TRACKING_USER_FORCE\\nvoid __init context_tracking_init(void)\\n{\\n\\tint cpu;\\n\\n\\tfor_each_possible_cpu(cpu)\\n\\t\\tct_cpu_track_user(cpu);\\n}\\n#endif\\n\\n#endif /* #ifdef CONFIG_CONTEXT_TRACKING_USER */\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Shadow Call Stack support.\\n *\\n * Copyright (C) 2019 Google LLC\\n */\\n\\n#include <linux/cpuhotplug.h>\\n#include <linux/kasan.h>\\n#include <linux/mm.h>\\n#include <linux/scs.h>\\n#include <linux/vmalloc.h>\\n#include <linux/vmstat.h>\\n\\n#ifdef CONFIG_DYNAMIC_SCS\\nDEFINE_STATIC_KEY_FALSE(dynamic_scs_enabled);\\n#endif\\n\\nstatic void __scs_account(void *s, int account)\\n{\\n\\tstruct page *scs_page = vmalloc_to_page(s);\\n\\n\\tmod_node_page_state(page_pgdat(scs_page), NR_KERNEL_SCS_KB,\\n\\t\\t\\t    account * (SCS_SIZE / SZ_1K));\\n}\\n\\n/* Matches NR_CACHED_STACKS for VMAP_STACK */\\n#define NR_CACHED_SCS 2\\nstatic DEFINE_PER_CPU(void *, scs_cache[NR_CACHED_SCS]);\\n\\nstatic void *__scs_alloc(int node)\\n{\\n\\tint i;\\n\\tvoid *s;\\n\\n\\tfor (i = 0; i < NR_CACHED_SCS; i++) {\\n\\t\\ts = this_cpu_xchg(scs_cache[i], NULL);\\n\\t\\tif (s) {\\n\\t\\t\\ts = kasan_unpoison_vmalloc(s, SCS_SIZE,\\n\\t\\t\\t\\t\\t\\t   KASAN_VMALLOC_PROT_NORMAL);\\n\\t\\t\\tmemset(s, 0, SCS_SIZE);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\n\\ts = __vmalloc_node_range(SCS_SIZE, 1, VMALLOC_START, VMALLOC_END,\\n\\t\\t\\t\\t    GFP_SCS, PAGE_KERNEL, 0, node,\\n\\t\\t\\t\\t    __builtin_return_address(0));\\n\\nout:\\n\\treturn kasan_reset_tag(s);\\n}\\n\\nvoid *scs_alloc(int node)\\n{\\n\\tvoid *s;\\n\\n\\ts = __scs_alloc(node);\\n\\tif (!s)\\n\\t\\treturn NULL;\\n\\n\\t*__scs_magic(s) = SCS_END_MAGIC;\\n\\n\\t/*\\n\\t * Poison the allocation to catch unintentional accesses to\\n\\t * the shadow stack when KASAN is enabled.\\n\\t */\\n\\tkasan_poison_vmalloc(s, SCS_SIZE);\\n\\t__scs_account(s, 1);\\n\\treturn s;\\n}\\n\\nvoid scs_free(void *s)\\n{\\n\\tint i;\\n\\n\\t__scs_account(s, -1);\\n\\n\\t/*\\n\\t * We cannot sleep as this can be called in interrupt context,\\n\\t * so use this_cpu_cmpxchg to update the cache, and vfree_atomic\\n\\t * to free the stack.\\n\\t */\\n\\n\\tfor (i = 0; i < NR_CACHED_SCS; i++)\\n\\t\\tif (this_cpu_cmpxchg(scs_cache[i], 0, s) == NULL)\\n\\t\\t\\treturn;\\n\\n\\tkasan_unpoison_vmalloc(s, SCS_SIZE, KASAN_VMALLOC_PROT_NORMAL);\\n\\tvfree_atomic(s);\\n}\\n\\nstatic int scs_cleanup(unsigned int cpu)\\n{\\n\\tint i;\\n\\tvoid **cache = per_cpu_ptr(scs_cache, cpu);\\n\\n\\tfor (i = 0; i < NR_CACHED_SCS; i++) {\\n\\t\\tvfree(cache[i]);\\n\\t\\tcache[i] = NULL;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nvoid __init scs_init(void)\\n{\\n\\tif (!scs_is_enabled())\\n\\t\\treturn;\\n\\tcpuhp_setup_state(CPUHP_BP_PREPARE_DYN, \"scs:scs_cache\", NULL,\\n\\t\\t\\t  scs_cleanup);\\n}\\n\\nint scs_prepare(struct task_struct *tsk, int node)\\n{\\n\\tvoid *s;\\n\\n\\tif (!scs_is_enabled())\\n\\t\\treturn 0;\\n\\n\\ts = scs_alloc(node);\\n\\tif (!s)\\n\\t\\treturn -ENOMEM;\\n\\n\\ttask_scs(tsk) = task_scs_sp(tsk) = s;\\n\\treturn 0;\\n}\\n\\nstatic void scs_check_usage(struct task_struct *tsk)\\n{\\n\\tstatic unsigned long highest;\\n\\n\\tunsigned long *p, prev, curr = highest, used = 0;\\n\\n\\tif (!IS_ENABLED(CONFIG_DEBUG_STACK_USAGE))\\n\\t\\treturn;\\n\\n\\tfor (p = task_scs(tsk); p < __scs_magic(tsk); ++p) {\\n\\t\\tif (!READ_ONCE_NOCHECK(*p))\\n\\t\\t\\tbreak;\\n\\t\\tused += sizeof(*p);\\n\\t}\\n\\n\\twhile (used > curr) {\\n\\t\\tprev = cmpxchg_relaxed(&highest, curr, used);\\n\\n\\t\\tif (prev == curr) {\\n\\t\\t\\tpr_info(\"%s (%d): highest shadow stack usage: %lu bytes\\\\n\",\\n\\t\\t\\t\\ttsk->comm, task_pid_nr(tsk), used);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tcurr = prev;\\n\\t}\\n}\\n\\nvoid scs_release(struct task_struct *tsk)\\n{\\n\\tvoid *s = task_scs(tsk);\\n\\n\\tif (!scs_is_enabled() || !s)\\n\\t\\treturn;\\n\\n\\tWARN(task_scs_end_corrupted(tsk),\\n\\t     \"corrupted shadow stack detected when freeing task\\\\n\");\\n\\tscs_check_usage(tsk);\\n\\tscs_free(s);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * KUnit test of proc sysctl.\\n */\\n\\n#include <kunit/test.h>\\n#include <linux/sysctl.h>\\n\\n#define KUNIT_PROC_READ 0\\n#define KUNIT_PROC_WRITE 1\\n\\n/*\\n * Test that proc_dointvec will not try to use a NULL .data field even when the\\n * length is non-zero.\\n */\\nstatic void sysctl_test_api_dointvec_null_tbl_data(struct kunit *test)\\n{\\n\\tstruct ctl_table null_data_table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t/*\\n\\t\\t * Here we are testing that proc_dointvec behaves correctly when\\n\\t\\t * we give it a NULL .data field. Normally this would point to a\\n\\t\\t * piece of memory where the value would be stored.\\n\\t\\t */\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\t/*\\n\\t * proc_dointvec expects a buffer in user space, so we allocate one. We\\n\\t * also need to cast it to __user so sparse doesn\\'t get mad.\\n\\t */\\n\\tvoid __user *buffer = (void __user *)kunit_kzalloc(test, sizeof(int),\\n\\t\\t\\t\\t\\t\\t\\t   GFP_USER);\\n\\tsize_t len;\\n\\tloff_t pos;\\n\\n\\t/*\\n\\t * We don\\'t care what the starting length is since proc_dointvec should\\n\\t * not try to read because .data is NULL.\\n\\t */\\n\\tlen = 1234;\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&null_data_table,\\n\\t\\t\\t\\t\\t       KUNIT_PROC_READ, buffer, &len,\\n\\t\\t\\t\\t\\t       &pos));\\n\\tKUNIT_EXPECT_EQ(test, 0, len);\\n\\n\\t/*\\n\\t * See above.\\n\\t */\\n\\tlen = 1234;\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&null_data_table,\\n\\t\\t\\t\\t\\t       KUNIT_PROC_WRITE, buffer, &len,\\n\\t\\t\\t\\t\\t       &pos));\\n\\tKUNIT_EXPECT_EQ(test, 0, len);\\n}\\n\\n/*\\n * Similar to the previous test, we create a struct ctrl_table that has a .data\\n * field that proc_dointvec cannot do anything with; however, this time it is\\n * because we tell proc_dointvec that the size is 0.\\n */\\nstatic void sysctl_test_api_dointvec_table_maxlen_unset(struct kunit *test)\\n{\\n\\tint data = 0;\\n\\tstruct ctl_table data_maxlen_unset_table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t/*\\n\\t\\t * So .data is no longer NULL, but we tell proc_dointvec its\\n\\t\\t * length is 0, so it still shouldn\\'t try to use it.\\n\\t\\t */\\n\\t\\t.maxlen\\t\\t= 0,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tvoid __user *buffer = (void __user *)kunit_kzalloc(test, sizeof(int),\\n\\t\\t\\t\\t\\t\\t\\t   GFP_USER);\\n\\tsize_t len;\\n\\tloff_t pos;\\n\\n\\t/*\\n\\t * As before, we don\\'t care what buffer length is because proc_dointvec\\n\\t * cannot do anything because its internal .data buffer has zero length.\\n\\t */\\n\\tlen = 1234;\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&data_maxlen_unset_table,\\n\\t\\t\\t\\t\\t       KUNIT_PROC_READ, buffer, &len,\\n\\t\\t\\t\\t\\t       &pos));\\n\\tKUNIT_EXPECT_EQ(test, 0, len);\\n\\n\\t/*\\n\\t * See previous comment.\\n\\t */\\n\\tlen = 1234;\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&data_maxlen_unset_table,\\n\\t\\t\\t\\t\\t       KUNIT_PROC_WRITE, buffer, &len,\\n\\t\\t\\t\\t\\t       &pos));\\n\\tKUNIT_EXPECT_EQ(test, 0, len);\\n}\\n\\n/*\\n * Here we provide a valid struct ctl_table, but we try to read and write from\\n * it using a buffer of zero length, so it should still fail in a similar way as\\n * before.\\n */\\nstatic void sysctl_test_api_dointvec_table_len_is_zero(struct kunit *test)\\n{\\n\\tint data = 0;\\n\\t/* Good table. */\\n\\tstruct ctl_table table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tvoid __user *buffer = (void __user *)kunit_kzalloc(test, sizeof(int),\\n\\t\\t\\t\\t\\t\\t\\t   GFP_USER);\\n\\t/*\\n\\t * However, now our read/write buffer has zero length.\\n\\t */\\n\\tsize_t len = 0;\\n\\tloff_t pos;\\n\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&table, KUNIT_PROC_READ, buffer,\\n\\t\\t\\t\\t\\t       &len, &pos));\\n\\tKUNIT_EXPECT_EQ(test, 0, len);\\n\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&table, KUNIT_PROC_WRITE, buffer,\\n\\t\\t\\t\\t\\t       &len, &pos));\\n\\tKUNIT_EXPECT_EQ(test, 0, len);\\n}\\n\\n/*\\n * Test that proc_dointvec refuses to read when the file position is non-zero.\\n */\\nstatic void sysctl_test_api_dointvec_table_read_but_position_set(\\n\\t\\tstruct kunit *test)\\n{\\n\\tint data = 0;\\n\\t/* Good table. */\\n\\tstruct ctl_table table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tvoid __user *buffer = (void __user *)kunit_kzalloc(test, sizeof(int),\\n\\t\\t\\t\\t\\t\\t\\t   GFP_USER);\\n\\t/*\\n\\t * We don\\'t care about our buffer length because we start off with a\\n\\t * non-zero file position.\\n\\t */\\n\\tsize_t len = 1234;\\n\\t/*\\n\\t * proc_dointvec should refuse to read into the buffer since the file\\n\\t * pos is non-zero.\\n\\t */\\n\\tloff_t pos = 1;\\n\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&table, KUNIT_PROC_READ, buffer,\\n\\t\\t\\t\\t\\t       &len, &pos));\\n\\tKUNIT_EXPECT_EQ(test, 0, len);\\n}\\n\\n/*\\n * Test that we can read a two digit number in a sufficiently size buffer.\\n * Nothing fancy.\\n */\\nstatic void sysctl_test_dointvec_read_happy_single_positive(struct kunit *test)\\n{\\n\\tint data = 0;\\n\\t/* Good table. */\\n\\tstruct ctl_table table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tsize_t len = 4;\\n\\tloff_t pos = 0;\\n\\tchar *buffer = kunit_kzalloc(test, len, GFP_USER);\\n\\tchar __user *user_buffer = (char __user *)buffer;\\n\\t/* Store 13 in the data field. */\\n\\t*((int *)table.data) = 13;\\n\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&table, KUNIT_PROC_READ,\\n\\t\\t\\t\\t\\t       user_buffer, &len, &pos));\\n\\tKUNIT_ASSERT_EQ(test, 3, len);\\n\\tbuffer[len] = \\'\\\\0\\';\\n\\t/* And we read 13 back out. */\\n\\tKUNIT_EXPECT_STREQ(test, \"13\\\\n\", buffer);\\n}\\n\\n/*\\n * Same as previous test, just now with negative numbers.\\n */\\nstatic void sysctl_test_dointvec_read_happy_single_negative(struct kunit *test)\\n{\\n\\tint data = 0;\\n\\t/* Good table. */\\n\\tstruct ctl_table table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tsize_t len = 5;\\n\\tloff_t pos = 0;\\n\\tchar *buffer = kunit_kzalloc(test, len, GFP_USER);\\n\\tchar __user *user_buffer = (char __user *)buffer;\\n\\t*((int *)table.data) = -16;\\n\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&table, KUNIT_PROC_READ,\\n\\t\\t\\t\\t\\t       user_buffer, &len, &pos));\\n\\tKUNIT_ASSERT_EQ(test, 4, len);\\n\\tbuffer[len] = \\'\\\\0\\';\\n\\tKUNIT_EXPECT_STREQ(test, \"-16\\\\n\", buffer);\\n}\\n\\n/*\\n * Test that a simple positive write works.\\n */\\nstatic void sysctl_test_dointvec_write_happy_single_positive(struct kunit *test)\\n{\\n\\tint data = 0;\\n\\t/* Good table. */\\n\\tstruct ctl_table table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tchar input[] = \"9\";\\n\\tsize_t len = sizeof(input) - 1;\\n\\tloff_t pos = 0;\\n\\tchar *buffer = kunit_kzalloc(test, len, GFP_USER);\\n\\tchar __user *user_buffer = (char __user *)buffer;\\n\\n\\tmemcpy(buffer, input, len);\\n\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&table, KUNIT_PROC_WRITE,\\n\\t\\t\\t\\t\\t       user_buffer, &len, &pos));\\n\\tKUNIT_EXPECT_EQ(test, sizeof(input) - 1, len);\\n\\tKUNIT_EXPECT_EQ(test, sizeof(input) - 1, pos);\\n\\tKUNIT_EXPECT_EQ(test, 9, *((int *)table.data));\\n}\\n\\n/*\\n * Same as previous test, but now with negative numbers.\\n */\\nstatic void sysctl_test_dointvec_write_happy_single_negative(struct kunit *test)\\n{\\n\\tint data = 0;\\n\\tstruct ctl_table table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tchar input[] = \"-9\";\\n\\tsize_t len = sizeof(input) - 1;\\n\\tloff_t pos = 0;\\n\\tchar *buffer = kunit_kzalloc(test, len, GFP_USER);\\n\\tchar __user *user_buffer = (char __user *)buffer;\\n\\n\\tmemcpy(buffer, input, len);\\n\\n\\tKUNIT_EXPECT_EQ(test, 0, proc_dointvec(&table, KUNIT_PROC_WRITE,\\n\\t\\t\\t\\t\\t       user_buffer, &len, &pos));\\n\\tKUNIT_EXPECT_EQ(test, sizeof(input) - 1, len);\\n\\tKUNIT_EXPECT_EQ(test, sizeof(input) - 1, pos);\\n\\tKUNIT_EXPECT_EQ(test, -9, *((int *)table.data));\\n}\\n\\n/*\\n * Test that writing a value smaller than the minimum possible value is not\\n * allowed.\\n */\\nstatic void sysctl_test_api_dointvec_write_single_less_int_min(\\n\\t\\tstruct kunit *test)\\n{\\n\\tint data = 0;\\n\\tstruct ctl_table table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tsize_t max_len = 32, len = max_len;\\n\\tloff_t pos = 0;\\n\\tchar *buffer = kunit_kzalloc(test, max_len, GFP_USER);\\n\\tchar __user *user_buffer = (char __user *)buffer;\\n\\tunsigned long abs_of_less_than_min = (unsigned long)INT_MAX\\n\\t\\t\\t\\t\\t     - (INT_MAX + INT_MIN) + 1;\\n\\n\\t/*\\n\\t * We use this rigmarole to create a string that contains a value one\\n\\t * less than the minimum accepted value.\\n\\t */\\n\\tKUNIT_ASSERT_LT(test,\\n\\t\\t\\t(size_t)snprintf(buffer, max_len, \"-%lu\",\\n\\t\\t\\t\\t\\t abs_of_less_than_min),\\n\\t\\t\\tmax_len);\\n\\n\\tKUNIT_EXPECT_EQ(test, -EINVAL, proc_dointvec(&table, KUNIT_PROC_WRITE,\\n\\t\\t\\t\\t\\t\\t     user_buffer, &len, &pos));\\n\\tKUNIT_EXPECT_EQ(test, max_len, len);\\n\\tKUNIT_EXPECT_EQ(test, 0, *((int *)table.data));\\n}\\n\\n/*\\n * Test that writing the maximum possible value works.\\n */\\nstatic void sysctl_test_api_dointvec_write_single_greater_int_max(\\n\\t\\tstruct kunit *test)\\n{\\n\\tint data = 0;\\n\\tstruct ctl_table table = {\\n\\t\\t.procname = \"foo\",\\n\\t\\t.data\\t\\t= &data,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE_HUNDRED,\\n\\t};\\n\\tsize_t max_len = 32, len = max_len;\\n\\tloff_t pos = 0;\\n\\tchar *buffer = kunit_kzalloc(test, max_len, GFP_USER);\\n\\tchar __user *user_buffer = (char __user *)buffer;\\n\\tunsigned long greater_than_max = (unsigned long)INT_MAX + 1;\\n\\n\\tKUNIT_ASSERT_GT(test, greater_than_max, (unsigned long)INT_MAX);\\n\\tKUNIT_ASSERT_LT(test, (size_t)snprintf(buffer, max_len, \"%lu\",\\n\\t\\t\\t\\t\\t       greater_than_max),\\n\\t\\t\\tmax_len);\\n\\tKUNIT_EXPECT_EQ(test, -EINVAL, proc_dointvec(&table, KUNIT_PROC_WRITE,\\n\\t\\t\\t\\t\\t\\t     user_buffer, &len, &pos));\\n\\tKUNIT_ASSERT_EQ(test, max_len, len);\\n\\tKUNIT_EXPECT_EQ(test, 0, *((int *)table.data));\\n}\\n\\n/*\\n * Test that registering an invalid extra value is not allowed.\\n */\\nstatic void sysctl_test_register_sysctl_sz_invalid_extra_value(\\n\\t\\tstruct kunit *test)\\n{\\n\\tunsigned char data = 0;\\n\\tstruct ctl_table table_foo[] = {\\n\\t\\t{\\n\\t\\t\\t.procname\\t= \"foo\",\\n\\t\\t\\t.data\\t\\t= &data,\\n\\t\\t\\t.maxlen\\t\\t= sizeof(u8),\\n\\t\\t\\t.mode\\t\\t= 0644,\\n\\t\\t\\t.proc_handler\\t= proc_dou8vec_minmax,\\n\\t\\t\\t.extra1\\t\\t= SYSCTL_FOUR,\\n\\t\\t\\t.extra2\\t\\t= SYSCTL_ONE_THOUSAND,\\n\\t\\t},\\n\\t};\\n\\n\\tstruct ctl_table table_bar[] = {\\n\\t\\t{\\n\\t\\t\\t.procname\\t= \"bar\",\\n\\t\\t\\t.data\\t\\t= &data,\\n\\t\\t\\t.maxlen\\t\\t= sizeof(u8),\\n\\t\\t\\t.mode\\t\\t= 0644,\\n\\t\\t\\t.proc_handler\\t= proc_dou8vec_minmax,\\n\\t\\t\\t.extra1\\t\\t= SYSCTL_NEG_ONE,\\n\\t\\t\\t.extra2\\t\\t= SYSCTL_ONE_HUNDRED,\\n\\t\\t},\\n\\t};\\n\\n\\tstruct ctl_table table_qux[] = {\\n\\t\\t{\\n\\t\\t\\t.procname\\t= \"qux\",\\n\\t\\t\\t.data\\t\\t= &data,\\n\\t\\t\\t.maxlen\\t\\t= sizeof(u8),\\n\\t\\t\\t.mode\\t\\t= 0644,\\n\\t\\t\\t.proc_handler\\t= proc_dou8vec_minmax,\\n\\t\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t\\t.extra2\\t\\t= SYSCTL_TWO_HUNDRED,\\n\\t\\t},\\n\\t};\\n\\n\\tKUNIT_EXPECT_NULL(test, register_sysctl(\"foo\", table_foo));\\n\\tKUNIT_EXPECT_NULL(test, register_sysctl(\"foo\", table_bar));\\n\\tKUNIT_EXPECT_NOT_NULL(test, register_sysctl(\"foo\", table_qux));\\n}\\n\\nstatic struct kunit_case sysctl_test_cases[] = {\\n\\tKUNIT_CASE(sysctl_test_api_dointvec_null_tbl_data),\\n\\tKUNIT_CASE(sysctl_test_api_dointvec_table_maxlen_unset),\\n\\tKUNIT_CASE(sysctl_test_api_dointvec_table_len_is_zero),\\n\\tKUNIT_CASE(sysctl_test_api_dointvec_table_read_but_position_set),\\n\\tKUNIT_CASE(sysctl_test_dointvec_read_happy_single_positive),\\n\\tKUNIT_CASE(sysctl_test_dointvec_read_happy_single_negative),\\n\\tKUNIT_CASE(sysctl_test_dointvec_write_happy_single_positive),\\n\\tKUNIT_CASE(sysctl_test_dointvec_write_happy_single_negative),\\n\\tKUNIT_CASE(sysctl_test_api_dointvec_write_single_less_int_min),\\n\\tKUNIT_CASE(sysctl_test_api_dointvec_write_single_greater_int_max),\\n\\tKUNIT_CASE(sysctl_test_register_sysctl_sz_invalid_extra_value),\\n\\t{}\\n};\\n\\nstatic struct kunit_suite sysctl_test_suite = {\\n\\t.name = \"sysctl_test\",\\n\\t.test_cases = sysctl_test_cases,\\n};\\n\\nkunit_test_suites(&sysctl_test_suite);\\n\\nMODULE_DESCRIPTION(\"KUnit test of proc sysctl\");\\nMODULE_LICENSE(\"GPL v2\");\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  linux/kernel/signal.c\\n *\\n *  Copyright (C) 1991, 1992  Linus Torvalds\\n *\\n *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson\\n *\\n *  2003-06-02  Jim Houston - Concurrent Computer Corp.\\n *\\t\\tChanges to use preallocated sigqueue structures\\n *\\t\\tto allow signals to be sent reliably.\\n */\\n\\n#include <linux/slab.h>\\n#include <linux/export.h>\\n#include <linux/init.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/user.h>\\n#include <linux/sched/debug.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/task_stack.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/file.h>\\n#include <linux/fs.h>\\n#include <linux/mm.h>\\n#include <linux/proc_fs.h>\\n#include <linux/tty.h>\\n#include <linux/binfmts.h>\\n#include <linux/coredump.h>\\n#include <linux/security.h>\\n#include <linux/syscalls.h>\\n#include <linux/ptrace.h>\\n#include <linux/signal.h>\\n#include <linux/signalfd.h>\\n#include <linux/ratelimit.h>\\n#include <linux/task_work.h>\\n#include <linux/capability.h>\\n#include <linux/freezer.h>\\n#include <linux/pid_namespace.h>\\n#include <linux/nsproxy.h>\\n#include <linux/user_namespace.h>\\n#include <linux/uprobes.h>\\n#include <linux/compat.h>\\n#include <linux/cn_proc.h>\\n#include <linux/compiler.h>\\n#include <linux/posix-timers.h>\\n#include <linux/cgroup.h>\\n#include <linux/audit.h>\\n#include <linux/sysctl.h>\\n#include <uapi/linux/pidfd.h>\\n\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/signal.h>\\n\\n#include <asm/param.h>\\n#include <linux/uaccess.h>\\n#include <asm/unistd.h>\\n#include <asm/siginfo.h>\\n#include <asm/cacheflush.h>\\n#include <asm/syscall.h>\\t/* for syscall_get_* */\\n\\n#include \"time/posix-timers.h\"\\n\\n/*\\n * SLAB caches for signal bits.\\n */\\n\\nstatic struct kmem_cache *sigqueue_cachep;\\n\\nint print_fatal_signals __read_mostly;\\n\\nstatic void __user *sig_handler(struct task_struct *t, int sig)\\n{\\n\\treturn t->sighand->action[sig - 1].sa.sa_handler;\\n}\\n\\nstatic inline bool sig_handler_ignored(void __user *handler, int sig)\\n{\\n\\t/* Is it explicitly or implicitly ignored? */\\n\\treturn handler == SIG_IGN ||\\n\\t       (handler == SIG_DFL && sig_kernel_ignore(sig));\\n}\\n\\nstatic bool sig_task_ignored(struct task_struct *t, int sig, bool force)\\n{\\n\\tvoid __user *handler;\\n\\n\\thandler = sig_handler(t, sig);\\n\\n\\t/* SIGKILL and SIGSTOP may not be sent to the global init */\\n\\tif (unlikely(is_global_init(t) && sig_kernel_only(sig)))\\n\\t\\treturn true;\\n\\n\\tif (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&\\n\\t    handler == SIG_DFL && !(force && sig_kernel_only(sig)))\\n\\t\\treturn true;\\n\\n\\t/* Only allow kernel generated signals to this kthread */\\n\\tif (unlikely((t->flags & PF_KTHREAD) &&\\n\\t\\t     (handler == SIG_KTHREAD_KERNEL) && !force))\\n\\t\\treturn true;\\n\\n\\treturn sig_handler_ignored(handler, sig);\\n}\\n\\nstatic bool sig_ignored(struct task_struct *t, int sig, bool force)\\n{\\n\\t/*\\n\\t * Blocked signals are never ignored, since the\\n\\t * signal handler may change by the time it is\\n\\t * unblocked.\\n\\t */\\n\\tif (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))\\n\\t\\treturn false;\\n\\n\\t/*\\n\\t * Tracers may want to know about even ignored signal unless it\\n\\t * is SIGKILL which can\\'t be reported anyway but can be ignored\\n\\t * by SIGNAL_UNKILLABLE task.\\n\\t */\\n\\tif (t->ptrace && sig != SIGKILL)\\n\\t\\treturn false;\\n\\n\\treturn sig_task_ignored(t, sig, force);\\n}\\n\\n/*\\n * Re-calculate pending state from the set of locally pending\\n * signals, globally pending signals, and blocked signals.\\n */\\nstatic inline bool has_pending_signals(sigset_t *signal, sigset_t *blocked)\\n{\\n\\tunsigned long ready;\\n\\tlong i;\\n\\n\\tswitch (_NSIG_WORDS) {\\n\\tdefault:\\n\\t\\tfor (i = _NSIG_WORDS, ready = 0; --i >= 0 ;)\\n\\t\\t\\tready |= signal->sig[i] &~ blocked->sig[i];\\n\\t\\tbreak;\\n\\n\\tcase 4: ready  = signal->sig[3] &~ blocked->sig[3];\\n\\t\\tready |= signal->sig[2] &~ blocked->sig[2];\\n\\t\\tready |= signal->sig[1] &~ blocked->sig[1];\\n\\t\\tready |= signal->sig[0] &~ blocked->sig[0];\\n\\t\\tbreak;\\n\\n\\tcase 2: ready  = signal->sig[1] &~ blocked->sig[1];\\n\\t\\tready |= signal->sig[0] &~ blocked->sig[0];\\n\\t\\tbreak;\\n\\n\\tcase 1: ready  = signal->sig[0] &~ blocked->sig[0];\\n\\t}\\n\\treturn ready !=\\t0;\\n}\\n\\n#define PENDING(p,b) has_pending_signals(&(p)->signal, (b))\\n\\nstatic bool recalc_sigpending_tsk(struct task_struct *t)\\n{\\n\\tif ((t->jobctl & (JOBCTL_PENDING_MASK | JOBCTL_TRAP_FREEZE)) ||\\n\\t    PENDING(&t->pending, &t->blocked) ||\\n\\t    PENDING(&t->signal->shared_pending, &t->blocked) ||\\n\\t    cgroup_task_frozen(t)) {\\n\\t\\tset_tsk_thread_flag(t, TIF_SIGPENDING);\\n\\t\\treturn true;\\n\\t}\\n\\n\\t/*\\n\\t * We must never clear the flag in another thread, or in current\\n\\t * when it\\'s possible the current syscall is returning -ERESTART*.\\n\\t * So we don\\'t clear it here, and only callers who know they should do.\\n\\t */\\n\\treturn false;\\n}\\n\\nvoid recalc_sigpending(void)\\n{\\n\\tif (!recalc_sigpending_tsk(current) && !freezing(current))\\n\\t\\tclear_thread_flag(TIF_SIGPENDING);\\n\\n}\\nEXPORT_SYMBOL(recalc_sigpending);\\n\\nvoid calculate_sigpending(void)\\n{\\n\\t/* Have any signals or users of TIF_SIGPENDING been delayed\\n\\t * until after fork?\\n\\t */\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\tset_tsk_thread_flag(current, TIF_SIGPENDING);\\n\\trecalc_sigpending();\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n}\\n\\n/* Given the mask, find the first available signal that should be serviced. */\\n\\n#define SYNCHRONOUS_MASK \\\\\\n\\t(sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \\\\\\n\\t sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))\\n\\nint next_signal(struct sigpending *pending, sigset_t *mask)\\n{\\n\\tunsigned long i, *s, *m, x;\\n\\tint sig = 0;\\n\\n\\ts = pending->signal.sig;\\n\\tm = mask->sig;\\n\\n\\t/*\\n\\t * Handle the first word specially: it contains the\\n\\t * synchronous signals that need to be dequeued first.\\n\\t */\\n\\tx = *s &~ *m;\\n\\tif (x) {\\n\\t\\tif (x & SYNCHRONOUS_MASK)\\n\\t\\t\\tx &= SYNCHRONOUS_MASK;\\n\\t\\tsig = ffz(~x) + 1;\\n\\t\\treturn sig;\\n\\t}\\n\\n\\tswitch (_NSIG_WORDS) {\\n\\tdefault:\\n\\t\\tfor (i = 1; i < _NSIG_WORDS; ++i) {\\n\\t\\t\\tx = *++s &~ *++m;\\n\\t\\t\\tif (!x)\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\tsig = ffz(~x) + i*_NSIG_BPW + 1;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tbreak;\\n\\n\\tcase 2:\\n\\t\\tx = s[1] &~ m[1];\\n\\t\\tif (!x)\\n\\t\\t\\tbreak;\\n\\t\\tsig = ffz(~x) + _NSIG_BPW + 1;\\n\\t\\tbreak;\\n\\n\\tcase 1:\\n\\t\\t/* Nothing to do */\\n\\t\\tbreak;\\n\\t}\\n\\n\\treturn sig;\\n}\\n\\nstatic inline void print_dropped_signal(int sig)\\n{\\n\\tstatic DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);\\n\\n\\tif (!print_fatal_signals)\\n\\t\\treturn;\\n\\n\\tif (!__ratelimit(&ratelimit_state))\\n\\t\\treturn;\\n\\n\\tpr_info(\"%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\\\\n\",\\n\\t\\t\\t\\tcurrent->comm, current->pid, sig);\\n}\\n\\n/**\\n * task_set_jobctl_pending - set jobctl pending bits\\n * @task: target task\\n * @mask: pending bits to set\\n *\\n * Clear @mask from @task->jobctl.  @mask must be subset of\\n * %JOBCTL_PENDING_MASK | %JOBCTL_STOP_CONSUME | %JOBCTL_STOP_SIGMASK |\\n * %JOBCTL_TRAPPING.  If stop signo is being set, the existing signo is\\n * cleared.  If @task is already being killed or exiting, this function\\n * becomes noop.\\n *\\n * CONTEXT:\\n * Must be called with @task->sighand->siglock held.\\n *\\n * RETURNS:\\n * %true if @mask is set, %false if made noop because @task was dying.\\n */\\nbool task_set_jobctl_pending(struct task_struct *task, unsigned long mask)\\n{\\n\\tBUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |\\n\\t\\t\\tJOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));\\n\\tBUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));\\n\\n\\tif (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))\\n\\t\\treturn false;\\n\\n\\tif (mask & JOBCTL_STOP_SIGMASK)\\n\\t\\ttask->jobctl &= ~JOBCTL_STOP_SIGMASK;\\n\\n\\ttask->jobctl |= mask;\\n\\treturn true;\\n}\\n\\n/**\\n * task_clear_jobctl_trapping - clear jobctl trapping bit\\n * @task: target task\\n *\\n * If JOBCTL_TRAPPING is set, a ptracer is waiting for us to enter TRACED.\\n * Clear it and wake up the ptracer.  Note that we don\\'t need any further\\n * locking.  @task->siglock guarantees that @task->parent points to the\\n * ptracer.\\n *\\n * CONTEXT:\\n * Must be called with @task->sighand->siglock held.\\n */\\nvoid task_clear_jobctl_trapping(struct task_struct *task)\\n{\\n\\tif (unlikely(task->jobctl & JOBCTL_TRAPPING)) {\\n\\t\\ttask->jobctl &= ~JOBCTL_TRAPPING;\\n\\t\\tsmp_mb();\\t/* advised by wake_up_bit() */\\n\\t\\twake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);\\n\\t}\\n}\\n\\n/**\\n * task_clear_jobctl_pending - clear jobctl pending bits\\n * @task: target task\\n * @mask: pending bits to clear\\n *\\n * Clear @mask from @task->jobctl.  @mask must be subset of\\n * %JOBCTL_PENDING_MASK.  If %JOBCTL_STOP_PENDING is being cleared, other\\n * STOP bits are cleared together.\\n *\\n * If clearing of @mask leaves no stop or trap pending, this function calls\\n * task_clear_jobctl_trapping().\\n *\\n * CONTEXT:\\n * Must be called with @task->sighand->siglock held.\\n */\\nvoid task_clear_jobctl_pending(struct task_struct *task, unsigned long mask)\\n{\\n\\tBUG_ON(mask & ~JOBCTL_PENDING_MASK);\\n\\n\\tif (mask & JOBCTL_STOP_PENDING)\\n\\t\\tmask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;\\n\\n\\ttask->jobctl &= ~mask;\\n\\n\\tif (!(task->jobctl & JOBCTL_PENDING_MASK))\\n\\t\\ttask_clear_jobctl_trapping(task);\\n}\\n\\n/**\\n * task_participate_group_stop - participate in a group stop\\n * @task: task participating in a group stop\\n *\\n * @task has %JOBCTL_STOP_PENDING set and is participating in a group stop.\\n * Group stop states are cleared and the group stop count is consumed if\\n * %JOBCTL_STOP_CONSUME was set.  If the consumption completes the group\\n * stop, the appropriate `SIGNAL_*` flags are set.\\n *\\n * CONTEXT:\\n * Must be called with @task->sighand->siglock held.\\n *\\n * RETURNS:\\n * %true if group stop completion should be notified to the parent, %false\\n * otherwise.\\n */\\nstatic bool task_participate_group_stop(struct task_struct *task)\\n{\\n\\tstruct signal_struct *sig = task->signal;\\n\\tbool consume = task->jobctl & JOBCTL_STOP_CONSUME;\\n\\n\\tWARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));\\n\\n\\ttask_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);\\n\\n\\tif (!consume)\\n\\t\\treturn false;\\n\\n\\tif (!WARN_ON_ONCE(sig->group_stop_count == 0))\\n\\t\\tsig->group_stop_count--;\\n\\n\\t/*\\n\\t * Tell the caller to notify completion iff we are entering into a\\n\\t * fresh group stop.  Read comment in do_signal_stop() for details.\\n\\t */\\n\\tif (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {\\n\\t\\tsignal_set_stop_flags(sig, SIGNAL_STOP_STOPPED);\\n\\t\\treturn true;\\n\\t}\\n\\treturn false;\\n}\\n\\nvoid task_join_group_stop(struct task_struct *task)\\n{\\n\\tunsigned long mask = current->jobctl & JOBCTL_STOP_SIGMASK;\\n\\tstruct signal_struct *sig = current->signal;\\n\\n\\tif (sig->group_stop_count) {\\n\\t\\tsig->group_stop_count++;\\n\\t\\tmask |= JOBCTL_STOP_CONSUME;\\n\\t} else if (!(sig->flags & SIGNAL_STOP_STOPPED))\\n\\t\\treturn;\\n\\n\\t/* Have the new thread join an on-going signal group stop */\\n\\ttask_set_jobctl_pending(task, mask | JOBCTL_STOP_PENDING);\\n}\\n\\nstatic struct ucounts *sig_get_ucounts(struct task_struct *t, int sig,\\n\\t\\t\\t\\t       int override_rlimit)\\n{\\n\\tstruct ucounts *ucounts;\\n\\tlong sigpending;\\n\\n\\t/*\\n\\t * Protect access to @t credentials. This can go away when all\\n\\t * callers hold rcu read lock.\\n\\t *\\n\\t * NOTE! A pending signal will hold on to the user refcount,\\n\\t * and we get/put the refcount only when the sigpending count\\n\\t * changes from/to zero.\\n\\t */\\n\\trcu_read_lock();\\n\\tucounts = task_ucounts(t);\\n\\tsigpending = inc_rlimit_get_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING,\\n\\t\\t\\t\\t\\t    override_rlimit);\\n\\trcu_read_unlock();\\n\\tif (!sigpending)\\n\\t\\treturn NULL;\\n\\n\\tif (unlikely(!override_rlimit && sigpending > task_rlimit(t, RLIMIT_SIGPENDING))) {\\n\\t\\tdec_rlimit_put_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING);\\n\\t\\tprint_dropped_signal(sig);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\treturn ucounts;\\n}\\n\\nstatic void __sigqueue_init(struct sigqueue *q, struct ucounts *ucounts,\\n\\t\\t\\t    const unsigned int sigqueue_flags)\\n{\\n\\tINIT_LIST_HEAD(&q->list);\\n\\tq->flags = sigqueue_flags;\\n\\tq->ucounts = ucounts;\\n}\\n\\n/*\\n * allocate a new signal queue record\\n * - this may be called without locks if and only if t == current, otherwise an\\n *   appropriate lock must be held to stop the target task from exiting\\n */\\nstatic struct sigqueue *sigqueue_alloc(int sig, struct task_struct *t, gfp_t gfp_flags,\\n\\t\\t\\t\\t       int override_rlimit)\\n{\\n\\tstruct ucounts *ucounts = sig_get_ucounts(t, sig, override_rlimit);\\n\\tstruct sigqueue *q;\\n\\n\\tif (!ucounts)\\n\\t\\treturn NULL;\\n\\n\\tq = kmem_cache_alloc(sigqueue_cachep, gfp_flags);\\n\\tif (!q) {\\n\\t\\tdec_rlimit_put_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t__sigqueue_init(q, ucounts, 0);\\n\\treturn q;\\n}\\n\\nstatic void __sigqueue_free(struct sigqueue *q)\\n{\\n\\tif (q->flags & SIGQUEUE_PREALLOC) {\\n\\t\\tposixtimer_sigqueue_putref(q);\\n\\t\\treturn;\\n\\t}\\n\\tif (q->ucounts) {\\n\\t\\tdec_rlimit_put_ucounts(q->ucounts, UCOUNT_RLIMIT_SIGPENDING);\\n\\t\\tq->ucounts = NULL;\\n\\t}\\n\\tkmem_cache_free(sigqueue_cachep, q);\\n}\\n\\nvoid flush_sigqueue(struct sigpending *queue)\\n{\\n\\tstruct sigqueue *q;\\n\\n\\tsigemptyset(&queue->signal);\\n\\twhile (!list_empty(&queue->list)) {\\n\\t\\tq = list_entry(queue->list.next, struct sigqueue , list);\\n\\t\\tlist_del_init(&q->list);\\n\\t\\t__sigqueue_free(q);\\n\\t}\\n}\\n\\n/*\\n * Flush all pending signals for this kthread.\\n */\\nvoid flush_signals(struct task_struct *t)\\n{\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&t->sighand->siglock, flags);\\n\\tclear_tsk_thread_flag(t, TIF_SIGPENDING);\\n\\tflush_sigqueue(&t->pending);\\n\\tflush_sigqueue(&t->signal->shared_pending);\\n\\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\\n}\\nEXPORT_SYMBOL(flush_signals);\\n\\nvoid ignore_signals(struct task_struct *t)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < _NSIG; ++i)\\n\\t\\tt->sighand->action[i].sa.sa_handler = SIG_IGN;\\n\\n\\tflush_signals(t);\\n}\\n\\n/*\\n * Flush all handlers for a task.\\n */\\n\\nvoid\\nflush_signal_handlers(struct task_struct *t, int force_default)\\n{\\n\\tint i;\\n\\tstruct k_sigaction *ka = &t->sighand->action[0];\\n\\tfor (i = _NSIG ; i != 0 ; i--) {\\n\\t\\tif (force_default || ka->sa.sa_handler != SIG_IGN)\\n\\t\\t\\tka->sa.sa_handler = SIG_DFL;\\n\\t\\tka->sa.sa_flags = 0;\\n#ifdef __ARCH_HAS_SA_RESTORER\\n\\t\\tka->sa.sa_restorer = NULL;\\n#endif\\n\\t\\tsigemptyset(&ka->sa.sa_mask);\\n\\t\\tka++;\\n\\t}\\n}\\n\\nbool unhandled_signal(struct task_struct *tsk, int sig)\\n{\\n\\tvoid __user *handler = tsk->sighand->action[sig-1].sa.sa_handler;\\n\\tif (is_global_init(tsk))\\n\\t\\treturn true;\\n\\n\\tif (handler != SIG_IGN && handler != SIG_DFL)\\n\\t\\treturn false;\\n\\n\\t/* If dying, we handle all new signals by ignoring them */\\n\\tif (fatal_signal_pending(tsk))\\n\\t\\treturn false;\\n\\n\\t/* if ptraced, let the tracer determine */\\n\\treturn !tsk->ptrace;\\n}\\n\\nstatic void collect_signal(int sig, struct sigpending *list, kernel_siginfo_t *info,\\n\\t\\t\\t   struct sigqueue **timer_sigq)\\n{\\n\\tstruct sigqueue *q, *first = NULL;\\n\\n\\t/*\\n\\t * Collect the siginfo appropriate to this signal.  Check if\\n\\t * there is another siginfo for the same signal.\\n\\t*/\\n\\tlist_for_each_entry(q, &list->list, list) {\\n\\t\\tif (q->info.si_signo == sig) {\\n\\t\\t\\tif (first)\\n\\t\\t\\t\\tgoto still_pending;\\n\\t\\t\\tfirst = q;\\n\\t\\t}\\n\\t}\\n\\n\\tsigdelset(&list->signal, sig);\\n\\n\\tif (first) {\\nstill_pending:\\n\\t\\tlist_del_init(&first->list);\\n\\t\\tcopy_siginfo(info, &first->info);\\n\\n\\t\\t/*\\n\\t\\t * posix-timer signals are preallocated and freed when the last\\n\\t\\t * reference count is dropped in posixtimer_deliver_signal() or\\n\\t\\t * immediately on timer deletion when the signal is not pending.\\n\\t\\t * Spare the extra round through __sigqueue_free() which is\\n\\t\\t * ignoring preallocated signals.\\n\\t\\t */\\n\\t\\tif (unlikely((first->flags & SIGQUEUE_PREALLOC) && (info->si_code == SI_TIMER)))\\n\\t\\t\\t*timer_sigq = first;\\n\\t\\telse\\n\\t\\t\\t__sigqueue_free(first);\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * Ok, it wasn\\'t in the queue.  This must be\\n\\t\\t * a fast-pathed signal or we must have been\\n\\t\\t * out of queue space.  So zero out the info.\\n\\t\\t */\\n\\t\\tclear_siginfo(info);\\n\\t\\tinfo->si_signo = sig;\\n\\t\\tinfo->si_errno = 0;\\n\\t\\tinfo->si_code = SI_USER;\\n\\t\\tinfo->si_pid = 0;\\n\\t\\tinfo->si_uid = 0;\\n\\t}\\n}\\n\\nstatic int __dequeue_signal(struct sigpending *pending, sigset_t *mask,\\n\\t\\t\\t    kernel_siginfo_t *info, struct sigqueue **timer_sigq)\\n{\\n\\tint sig = next_signal(pending, mask);\\n\\n\\tif (sig)\\n\\t\\tcollect_signal(sig, pending, info, timer_sigq);\\n\\treturn sig;\\n}\\n\\n/*\\n * Try to dequeue a signal. If a deliverable signal is found fill in the\\n * caller provided siginfo and return the signal number. Otherwise return\\n * 0.\\n */\\nint dequeue_signal(sigset_t *mask, kernel_siginfo_t *info, enum pid_type *type)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\tstruct sigqueue *timer_sigq;\\n\\tint signr;\\n\\n\\tlockdep_assert_held(&tsk->sighand->siglock);\\n\\nagain:\\n\\t*type = PIDTYPE_PID;\\n\\ttimer_sigq = NULL;\\n\\tsignr = __dequeue_signal(&tsk->pending, mask, info, &timer_sigq);\\n\\tif (!signr) {\\n\\t\\t*type = PIDTYPE_TGID;\\n\\t\\tsignr = __dequeue_signal(&tsk->signal->shared_pending,\\n\\t\\t\\t\\t\\t mask, info, &timer_sigq);\\n\\n\\t\\tif (unlikely(signr == SIGALRM))\\n\\t\\t\\tposixtimer_rearm_itimer(tsk);\\n\\t}\\n\\n\\trecalc_sigpending();\\n\\tif (!signr)\\n\\t\\treturn 0;\\n\\n\\tif (unlikely(sig_kernel_stop(signr))) {\\n\\t\\t/*\\n\\t\\t * Set a marker that we have dequeued a stop signal.  Our\\n\\t\\t * caller might release the siglock and then the pending\\n\\t\\t * stop signal it is about to process is no longer in the\\n\\t\\t * pending bitmasks, but must still be cleared by a SIGCONT\\n\\t\\t * (and overruled by a SIGKILL).  So those cases clear this\\n\\t\\t * shared flag after we\\'ve set it.  Note that this flag may\\n\\t\\t * remain set after the signal we return is ignored or\\n\\t\\t * handled.  That doesn\\'t matter because its only purpose\\n\\t\\t * is to alert stop-signal processing code when another\\n\\t\\t * processor has come along and cleared the flag.\\n\\t\\t */\\n\\t\\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\\n\\t}\\n\\n\\tif (IS_ENABLED(CONFIG_POSIX_TIMERS) && unlikely(timer_sigq)) {\\n\\t\\tif (!posixtimer_deliver_signal(info, timer_sigq))\\n\\t\\t\\tgoto again;\\n\\t}\\n\\n\\treturn signr;\\n}\\nEXPORT_SYMBOL_GPL(dequeue_signal);\\n\\nstatic int dequeue_synchronous_signal(kernel_siginfo_t *info)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\tstruct sigpending *pending = &tsk->pending;\\n\\tstruct sigqueue *q, *sync = NULL;\\n\\n\\t/*\\n\\t * Might a synchronous signal be in the queue?\\n\\t */\\n\\tif (!((pending->signal.sig[0] & ~tsk->blocked.sig[0]) & SYNCHRONOUS_MASK))\\n\\t\\treturn 0;\\n\\n\\t/*\\n\\t * Return the first synchronous signal in the queue.\\n\\t */\\n\\tlist_for_each_entry(q, &pending->list, list) {\\n\\t\\t/* Synchronous signals have a positive si_code */\\n\\t\\tif ((q->info.si_code > SI_USER) &&\\n\\t\\t    (sigmask(q->info.si_signo) & SYNCHRONOUS_MASK)) {\\n\\t\\t\\tsync = q;\\n\\t\\t\\tgoto next;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\nnext:\\n\\t/*\\n\\t * Check if there is another siginfo for the same signal.\\n\\t */\\n\\tlist_for_each_entry_continue(q, &pending->list, list) {\\n\\t\\tif (q->info.si_signo == sync->info.si_signo)\\n\\t\\t\\tgoto still_pending;\\n\\t}\\n\\n\\tsigdelset(&pending->signal, sync->info.si_signo);\\n\\trecalc_sigpending();\\nstill_pending:\\n\\tlist_del_init(&sync->list);\\n\\tcopy_siginfo(info, &sync->info);\\n\\t__sigqueue_free(sync);\\n\\treturn info->si_signo;\\n}\\n\\n/*\\n * Tell a process that it has a new active signal..\\n *\\n * NOTE! we rely on the previous spin_lock to\\n * lock interrupts for us! We can only be called with\\n * \"siglock\" held, and the local interrupt must\\n * have been disabled when that got acquired!\\n *\\n * No need to set need_resched since signal event passing\\n * goes through ->blocked\\n */\\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\\n{\\n\\tlockdep_assert_held(&t->sighand->siglock);\\n\\n\\tset_tsk_thread_flag(t, TIF_SIGPENDING);\\n\\n\\t/*\\n\\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\\n\\t * case. We don\\'t check t->state here because there is a race with it\\n\\t * executing another processor and just now entering stopped state.\\n\\t * By using wake_up_state, we ensure the process will wake up and\\n\\t * handle its death signal.\\n\\t */\\n\\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\\n\\t\\tkick_process(t);\\n}\\n\\nstatic inline void posixtimer_sig_ignore(struct task_struct *tsk, struct sigqueue *q);\\n\\nstatic void sigqueue_free_ignored(struct task_struct *tsk, struct sigqueue *q)\\n{\\n\\tif (likely(!(q->flags & SIGQUEUE_PREALLOC) || q->info.si_code != SI_TIMER))\\n\\t\\t__sigqueue_free(q);\\n\\telse\\n\\t\\tposixtimer_sig_ignore(tsk, q);\\n}\\n\\n/* Remove signals in mask from the pending set and queue. */\\nstatic void flush_sigqueue_mask(struct task_struct *p, sigset_t *mask, struct sigpending *s)\\n{\\n\\tstruct sigqueue *q, *n;\\n\\tsigset_t m;\\n\\n\\tlockdep_assert_held(&p->sighand->siglock);\\n\\n\\tsigandsets(&m, mask, &s->signal);\\n\\tif (sigisemptyset(&m))\\n\\t\\treturn;\\n\\n\\tsigandnsets(&s->signal, &s->signal, mask);\\n\\tlist_for_each_entry_safe(q, n, &s->list, list) {\\n\\t\\tif (sigismember(mask, q->info.si_signo)) {\\n\\t\\t\\tlist_del_init(&q->list);\\n\\t\\t\\tsigqueue_free_ignored(p, q);\\n\\t\\t}\\n\\t}\\n}\\n\\nstatic inline int is_si_special(const struct kernel_siginfo *info)\\n{\\n\\treturn info <= SEND_SIG_PRIV;\\n}\\n\\nstatic inline bool si_fromuser(const struct kernel_siginfo *info)\\n{\\n\\treturn info == SEND_SIG_NOINFO ||\\n\\t\\t(!is_si_special(info) && SI_FROMUSER(info));\\n}\\n\\n/*\\n * called with RCU read lock from check_kill_permission()\\n */\\nstatic bool kill_ok_by_cred(struct task_struct *t)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tconst struct cred *tcred = __task_cred(t);\\n\\n\\treturn uid_eq(cred->euid, tcred->suid) ||\\n\\t       uid_eq(cred->euid, tcred->uid) ||\\n\\t       uid_eq(cred->uid, tcred->suid) ||\\n\\t       uid_eq(cred->uid, tcred->uid) ||\\n\\t       ns_capable(tcred->user_ns, CAP_KILL);\\n}\\n\\n/*\\n * Bad permissions for sending the signal\\n * - the caller must hold the RCU read lock\\n */\\nstatic int check_kill_permission(int sig, struct kernel_siginfo *info,\\n\\t\\t\\t\\t struct task_struct *t)\\n{\\n\\tstruct pid *sid;\\n\\tint error;\\n\\n\\tif (!valid_signal(sig))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (!si_fromuser(info))\\n\\t\\treturn 0;\\n\\n\\terror = audit_signal_info(sig, t); /* Let audit system see the signal */\\n\\tif (error)\\n\\t\\treturn error;\\n\\n\\tif (!same_thread_group(current, t) &&\\n\\t    !kill_ok_by_cred(t)) {\\n\\t\\tswitch (sig) {\\n\\t\\tcase SIGCONT:\\n\\t\\t\\tsid = task_session(t);\\n\\t\\t\\t/*\\n\\t\\t\\t * We don\\'t return the error if sid == NULL. The\\n\\t\\t\\t * task was unhashed, the caller must notice this.\\n\\t\\t\\t */\\n\\t\\t\\tif (!sid || sid == task_session(current))\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tfallthrough;\\n\\t\\tdefault:\\n\\t\\t\\treturn -EPERM;\\n\\t\\t}\\n\\t}\\n\\n\\treturn security_task_kill(t, info, sig, NULL);\\n}\\n\\n/**\\n * ptrace_trap_notify - schedule trap to notify ptracer\\n * @t: tracee wanting to notify tracer\\n *\\n * This function schedules sticky ptrace trap which is cleared on the next\\n * TRAP_STOP to notify ptracer of an event.  @t must have been seized by\\n * ptracer.\\n *\\n * If @t is running, STOP trap will be taken.  If trapped for STOP and\\n * ptracer is listening for events, tracee is woken up so that it can\\n * re-trap for the new event.  If trapped otherwise, STOP trap will be\\n * eventually taken without returning to userland after the existing traps\\n * are finished by PTRACE_CONT.\\n *\\n * CONTEXT:\\n * Must be called with @task->sighand->siglock held.\\n */\\nstatic void ptrace_trap_notify(struct task_struct *t)\\n{\\n\\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\\n\\tlockdep_assert_held(&t->sighand->siglock);\\n\\n\\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\\n\\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\\n}\\n\\n/*\\n * Handle magic process-wide effects of stop/continue signals. Unlike\\n * the signal actions, these happen immediately at signal-generation\\n * time regardless of blocking, ignoring, or handling.  This does the\\n * actual continuing for SIGCONT, but not the actual stopping for stop\\n * signals. The process stop is done as a signal action for SIG_DFL.\\n *\\n * Returns true if the signal should be actually delivered, otherwise\\n * it should be dropped.\\n */\\nstatic bool prepare_signal(int sig, struct task_struct *p, bool force)\\n{\\n\\tstruct signal_struct *signal = p->signal;\\n\\tstruct task_struct *t;\\n\\tsigset_t flush;\\n\\n\\tif (signal->flags & SIGNAL_GROUP_EXIT) {\\n\\t\\tif (signal->core_state)\\n\\t\\t\\treturn sig == SIGKILL;\\n\\t\\t/*\\n\\t\\t * The process is in the middle of dying, drop the signal.\\n\\t\\t */\\n\\t\\treturn false;\\n\\t} else if (sig_kernel_stop(sig)) {\\n\\t\\t/*\\n\\t\\t * This is a stop signal.  Remove SIGCONT from all queues.\\n\\t\\t */\\n\\t\\tsiginitset(&flush, sigmask(SIGCONT));\\n\\t\\tflush_sigqueue_mask(p, &flush, &signal->shared_pending);\\n\\t\\tfor_each_thread(p, t)\\n\\t\\t\\tflush_sigqueue_mask(p, &flush, &t->pending);\\n\\t} else if (sig == SIGCONT) {\\n\\t\\tunsigned int why;\\n\\t\\t/*\\n\\t\\t * Remove all stop signals from all queues, wake all threads.\\n\\t\\t */\\n\\t\\tsiginitset(&flush, SIG_KERNEL_STOP_MASK);\\n\\t\\tflush_sigqueue_mask(p, &flush, &signal->shared_pending);\\n\\t\\tfor_each_thread(p, t) {\\n\\t\\t\\tflush_sigqueue_mask(p, &flush, &t->pending);\\n\\t\\t\\ttask_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);\\n\\t\\t\\tif (likely(!(t->ptrace & PT_SEIZED))) {\\n\\t\\t\\t\\tt->jobctl &= ~JOBCTL_STOPPED;\\n\\t\\t\\t\\twake_up_state(t, __TASK_STOPPED);\\n\\t\\t\\t} else\\n\\t\\t\\t\\tptrace_trap_notify(t);\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Notify the parent with CLD_CONTINUED if we were stopped.\\n\\t\\t *\\n\\t\\t * If we were in the middle of a group stop, we pretend it\\n\\t\\t * was already finished, and then continued. Since SIGCHLD\\n\\t\\t * doesn\\'t queue we report only CLD_STOPPED, as if the next\\n\\t\\t * CLD_CONTINUED was dropped.\\n\\t\\t */\\n\\t\\twhy = 0;\\n\\t\\tif (signal->flags & SIGNAL_STOP_STOPPED)\\n\\t\\t\\twhy |= SIGNAL_CLD_CONTINUED;\\n\\t\\telse if (signal->group_stop_count)\\n\\t\\t\\twhy |= SIGNAL_CLD_STOPPED;\\n\\n\\t\\tif (why) {\\n\\t\\t\\t/*\\n\\t\\t\\t * The first thread which returns from do_signal_stop()\\n\\t\\t\\t * will take ->siglock, notice SIGNAL_CLD_MASK, and\\n\\t\\t\\t * notify its parent. See get_signal().\\n\\t\\t\\t */\\n\\t\\t\\tsignal_set_stop_flags(signal, why | SIGNAL_STOP_CONTINUED);\\n\\t\\t\\tsignal->group_stop_count = 0;\\n\\t\\t\\tsignal->group_exit_code = 0;\\n\\t\\t}\\n\\t}\\n\\n\\treturn !sig_ignored(p, sig, force);\\n}\\n\\n/*\\n * Test if P wants to take SIG.  After we\\'ve checked all threads with this,\\n * it\\'s equivalent to finding no threads not blocking SIG.  Any threads not\\n * blocking SIG were ruled out because they are not running and already\\n * have pending signals.  Such threads will dequeue from the shared queue\\n * as soon as they\\'re available, so putting the signal on the shared queue\\n * will be equivalent to sending it to one such thread.\\n */\\nstatic inline bool wants_signal(int sig, struct task_struct *p)\\n{\\n\\tif (sigismember(&p->blocked, sig))\\n\\t\\treturn false;\\n\\n\\tif (p->flags & PF_EXITING)\\n\\t\\treturn false;\\n\\n\\tif (sig == SIGKILL)\\n\\t\\treturn true;\\n\\n\\tif (task_is_stopped_or_traced(p))\\n\\t\\treturn false;\\n\\n\\treturn task_curr(p) || !task_sigpending(p);\\n}\\n\\nstatic void complete_signal(int sig, struct task_struct *p, enum pid_type type)\\n{\\n\\tstruct signal_struct *signal = p->signal;\\n\\tstruct task_struct *t;\\n\\n\\t/*\\n\\t * Now find a thread we can wake up to take the signal off the queue.\\n\\t *\\n\\t * Try the suggested task first (may or may not be the main thread).\\n\\t */\\n\\tif (wants_signal(sig, p))\\n\\t\\tt = p;\\n\\telse if ((type == PIDTYPE_PID) || thread_group_empty(p))\\n\\t\\t/*\\n\\t\\t * There is just one thread and it does not need to be woken.\\n\\t\\t * It will dequeue unblocked signals before it runs again.\\n\\t\\t */\\n\\t\\treturn;\\n\\telse {\\n\\t\\t/*\\n\\t\\t * Otherwise try to find a suitable thread.\\n\\t\\t */\\n\\t\\tt = signal->curr_target;\\n\\t\\twhile (!wants_signal(sig, t)) {\\n\\t\\t\\tt = next_thread(t);\\n\\t\\t\\tif (t == signal->curr_target)\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * No thread needs to be woken.\\n\\t\\t\\t\\t * Any eligible threads will see\\n\\t\\t\\t\\t * the signal in the queue soon.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tsignal->curr_target = t;\\n\\t}\\n\\n\\t/*\\n\\t * Found a killable thread.  If the signal will be fatal,\\n\\t * then start taking the whole group down immediately.\\n\\t */\\n\\tif (sig_fatal(p, sig) &&\\n\\t    (signal->core_state || !(signal->flags & SIGNAL_GROUP_EXIT)) &&\\n\\t    !sigismember(&t->real_blocked, sig) &&\\n\\t    (sig == SIGKILL || !p->ptrace)) {\\n\\t\\t/*\\n\\t\\t * This signal will be fatal to the whole group.\\n\\t\\t */\\n\\t\\tif (!sig_kernel_coredump(sig)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Start a group exit and wake everybody up.\\n\\t\\t\\t * This way we don\\'t have other threads\\n\\t\\t\\t * running and doing things after a slower\\n\\t\\t\\t * thread has the fatal signal pending.\\n\\t\\t\\t */\\n\\t\\t\\tsignal->flags = SIGNAL_GROUP_EXIT;\\n\\t\\t\\tsignal->group_exit_code = sig;\\n\\t\\t\\tsignal->group_stop_count = 0;\\n\\t\\t\\t__for_each_thread(signal, t) {\\n\\t\\t\\t\\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\\n\\t\\t\\t\\tsigaddset(&t->pending.signal, SIGKILL);\\n\\t\\t\\t\\tsignal_wake_up(t, 1);\\n\\t\\t\\t}\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * The signal is already in the shared-pending queue.\\n\\t * Tell the chosen thread to wake up and dequeue it.\\n\\t */\\n\\tsignal_wake_up(t, sig == SIGKILL);\\n\\treturn;\\n}\\n\\nstatic inline bool legacy_queue(struct sigpending *signals, int sig)\\n{\\n\\treturn (sig < SIGRTMIN) && sigismember(&signals->signal, sig);\\n}\\n\\nstatic int __send_signal_locked(int sig, struct kernel_siginfo *info,\\n\\t\\t\\t\\tstruct task_struct *t, enum pid_type type, bool force)\\n{\\n\\tstruct sigpending *pending;\\n\\tstruct sigqueue *q;\\n\\tint override_rlimit;\\n\\tint ret = 0, result;\\n\\n\\tlockdep_assert_held(&t->sighand->siglock);\\n\\n\\tresult = TRACE_SIGNAL_IGNORED;\\n\\tif (!prepare_signal(sig, t, force))\\n\\t\\tgoto ret;\\n\\n\\tpending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\\n\\t/*\\n\\t * Short-circuit ignored signals and support queuing\\n\\t * exactly one non-rt signal, so that we can get more\\n\\t * detailed information about the cause of the signal.\\n\\t */\\n\\tresult = TRACE_SIGNAL_ALREADY_PENDING;\\n\\tif (legacy_queue(pending, sig))\\n\\t\\tgoto ret;\\n\\n\\tresult = TRACE_SIGNAL_DELIVERED;\\n\\t/*\\n\\t * Skip useless siginfo allocation for SIGKILL and kernel threads.\\n\\t */\\n\\tif ((sig == SIGKILL) || (t->flags & PF_KTHREAD))\\n\\t\\tgoto out_set;\\n\\n\\t/*\\n\\t * Real-time signals must be queued if sent by sigqueue, or\\n\\t * some other real-time mechanism.  It is implementation\\n\\t * defined whether kill() does so.  We attempt to do so, on\\n\\t * the principle of least surprise, but since kill is not\\n\\t * allowed to fail with EAGAIN when low on memory we just\\n\\t * make sure at least one signal gets delivered and don\\'t\\n\\t * pass on the info struct.\\n\\t */\\n\\tif (sig < SIGRTMIN)\\n\\t\\toverride_rlimit = (is_si_special(info) || info->si_code >= 0);\\n\\telse\\n\\t\\toverride_rlimit = 0;\\n\\n\\tq = sigqueue_alloc(sig, t, GFP_ATOMIC, override_rlimit);\\n\\n\\tif (q) {\\n\\t\\tlist_add_tail(&q->list, &pending->list);\\n\\t\\tswitch ((unsigned long) info) {\\n\\t\\tcase (unsigned long) SEND_SIG_NOINFO:\\n\\t\\t\\tclear_siginfo(&q->info);\\n\\t\\t\\tq->info.si_signo = sig;\\n\\t\\t\\tq->info.si_errno = 0;\\n\\t\\t\\tq->info.si_code = SI_USER;\\n\\t\\t\\tq->info.si_pid = task_tgid_nr_ns(current,\\n\\t\\t\\t\\t\\t\\t\\ttask_active_pid_ns(t));\\n\\t\\t\\trcu_read_lock();\\n\\t\\t\\tq->info.si_uid =\\n\\t\\t\\t\\tfrom_kuid_munged(task_cred_xxx(t, user_ns),\\n\\t\\t\\t\\t\\t\\t current_uid());\\n\\t\\t\\trcu_read_unlock();\\n\\t\\t\\tbreak;\\n\\t\\tcase (unsigned long) SEND_SIG_PRIV:\\n\\t\\t\\tclear_siginfo(&q->info);\\n\\t\\t\\tq->info.si_signo = sig;\\n\\t\\t\\tq->info.si_errno = 0;\\n\\t\\t\\tq->info.si_code = SI_KERNEL;\\n\\t\\t\\tq->info.si_pid = 0;\\n\\t\\t\\tq->info.si_uid = 0;\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\tcopy_siginfo(&q->info, info);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t} else if (!is_si_special(info) &&\\n\\t\\t   sig >= SIGRTMIN && info->si_code != SI_USER) {\\n\\t\\t/*\\n\\t\\t * Queue overflow, abort.  We may abort if the\\n\\t\\t * signal was rt and sent by user using something\\n\\t\\t * other than kill().\\n\\t\\t */\\n\\t\\tresult = TRACE_SIGNAL_OVERFLOW_FAIL;\\n\\t\\tret = -EAGAIN;\\n\\t\\tgoto ret;\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * This is a silent loss of information.  We still\\n\\t\\t * send the signal, but the *info bits are lost.\\n\\t\\t */\\n\\t\\tresult = TRACE_SIGNAL_LOSE_INFO;\\n\\t}\\n\\nout_set:\\n\\tsignalfd_notify(t, sig);\\n\\tsigaddset(&pending->signal, sig);\\n\\n\\t/* Let multiprocess signals appear after on-going forks */\\n\\tif (type > PIDTYPE_TGID) {\\n\\t\\tstruct multiprocess_signals *delayed;\\n\\t\\thlist_for_each_entry(delayed, &t->signal->multiprocess, node) {\\n\\t\\t\\tsigset_t *signal = &delayed->signal;\\n\\t\\t\\t/* Can\\'t queue both a stop and a continue signal */\\n\\t\\t\\tif (sig == SIGCONT)\\n\\t\\t\\t\\tsigdelsetmask(signal, SIG_KERNEL_STOP_MASK);\\n\\t\\t\\telse if (sig_kernel_stop(sig))\\n\\t\\t\\t\\tsigdelset(signal, SIGCONT);\\n\\t\\t\\tsigaddset(signal, sig);\\n\\t\\t}\\n\\t}\\n\\n\\tcomplete_signal(sig, t, type);\\nret:\\n\\ttrace_signal_generate(sig, info, t, type != PIDTYPE_PID, result);\\n\\treturn ret;\\n}\\n\\nstatic inline bool has_si_pid_and_uid(struct kernel_siginfo *info)\\n{\\n\\tbool ret = false;\\n\\tswitch (siginfo_layout(info->si_signo, info->si_code)) {\\n\\tcase SIL_KILL:\\n\\tcase SIL_CHLD:\\n\\tcase SIL_RT:\\n\\t\\tret = true;\\n\\t\\tbreak;\\n\\tcase SIL_TIMER:\\n\\tcase SIL_POLL:\\n\\tcase SIL_FAULT:\\n\\tcase SIL_FAULT_TRAPNO:\\n\\tcase SIL_FAULT_MCEERR:\\n\\tcase SIL_FAULT_BNDERR:\\n\\tcase SIL_FAULT_PKUERR:\\n\\tcase SIL_FAULT_PERF_EVENT:\\n\\tcase SIL_SYS:\\n\\t\\tret = false;\\n\\t\\tbreak;\\n\\t}\\n\\treturn ret;\\n}\\n\\nint send_signal_locked(int sig, struct kernel_siginfo *info,\\n\\t\\t       struct task_struct *t, enum pid_type type)\\n{\\n\\t/* Should SIGKILL or SIGSTOP be received by a pid namespace init? */\\n\\tbool force = false;\\n\\n\\tif (info == SEND_SIG_NOINFO) {\\n\\t\\t/* Force if sent from an ancestor pid namespace */\\n\\t\\tforce = !task_pid_nr_ns(current, task_active_pid_ns(t));\\n\\t} else if (info == SEND_SIG_PRIV) {\\n\\t\\t/* Don\\'t ignore kernel generated signals */\\n\\t\\tforce = true;\\n\\t} else if (has_si_pid_and_uid(info)) {\\n\\t\\t/* SIGKILL and SIGSTOP is special or has ids */\\n\\t\\tstruct user_namespace *t_user_ns;\\n\\n\\t\\trcu_read_lock();\\n\\t\\tt_user_ns = task_cred_xxx(t, user_ns);\\n\\t\\tif (current_user_ns() != t_user_ns) {\\n\\t\\t\\tkuid_t uid = make_kuid(current_user_ns(), info->si_uid);\\n\\t\\t\\tinfo->si_uid = from_kuid_munged(t_user_ns, uid);\\n\\t\\t}\\n\\t\\trcu_read_unlock();\\n\\n\\t\\t/* A kernel generated signal? */\\n\\t\\tforce = (info->si_code == SI_KERNEL);\\n\\n\\t\\t/* From an ancestor pid namespace? */\\n\\t\\tif (!task_pid_nr_ns(current, task_active_pid_ns(t))) {\\n\\t\\t\\tinfo->si_pid = 0;\\n\\t\\t\\tforce = true;\\n\\t\\t}\\n\\t}\\n\\treturn __send_signal_locked(sig, info, t, type, force);\\n}\\n\\nstatic void print_fatal_signal(int signr)\\n{\\n\\tstruct pt_regs *regs = task_pt_regs(current);\\n\\tstruct file *exe_file;\\n\\n\\texe_file = get_task_exe_file(current);\\n\\tif (exe_file) {\\n\\t\\tpr_info(\"%pD: %s: potentially unexpected fatal signal %d.\\\\n\",\\n\\t\\t\\texe_file, current->comm, signr);\\n\\t\\tfput(exe_file);\\n\\t} else {\\n\\t\\tpr_info(\"%s: potentially unexpected fatal signal %d.\\\\n\",\\n\\t\\t\\tcurrent->comm, signr);\\n\\t}\\n\\n#if defined(__i386__) && !defined(__arch_um__)\\n\\tpr_info(\"code at %08lx: \", regs->ip);\\n\\t{\\n\\t\\tint i;\\n\\t\\tfor (i = 0; i < 16; i++) {\\n\\t\\t\\tunsigned char insn;\\n\\n\\t\\t\\tif (get_user(insn, (unsigned char *)(regs->ip + i)))\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tpr_cont(\"%02x \", insn);\\n\\t\\t}\\n\\t}\\n\\tpr_cont(\"\\\\n\");\\n#endif\\n\\tpreempt_disable();\\n\\tshow_regs(regs);\\n\\tpreempt_enable();\\n}\\n\\nstatic int __init setup_print_fatal_signals(char *str)\\n{\\n\\tget_option (&str, &print_fatal_signals);\\n\\n\\treturn 1;\\n}\\n\\n__setup(\"print-fatal-signals=\", setup_print_fatal_signals);\\n\\nint do_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p,\\n\\t\\t\\tenum pid_type type)\\n{\\n\\tunsigned long flags;\\n\\tint ret = -ESRCH;\\n\\n\\tif (lock_task_sighand(p, &flags)) {\\n\\t\\tret = send_signal_locked(sig, info, p, type);\\n\\t\\tunlock_task_sighand(p, &flags);\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nenum sig_handler {\\n\\tHANDLER_CURRENT, /* If reachable use the current handler */\\n\\tHANDLER_SIG_DFL, /* Always use SIG_DFL handler semantics */\\n\\tHANDLER_EXIT,\\t /* Only visible as the process exit code */\\n};\\n\\n/*\\n * Force a signal that the process can\\'t ignore: if necessary\\n * we unblock the signal and change any SIG_IGN to SIG_DFL.\\n *\\n * Note: If we unblock the signal, we always reset it to SIG_DFL,\\n * since we do not want to have a signal handler that was blocked\\n * be invoked when user space had explicitly blocked it.\\n *\\n * We don\\'t want to have recursive SIGSEGV\\'s etc, for example,\\n * that is why we also clear SIGNAL_UNKILLABLE.\\n */\\nstatic int\\nforce_sig_info_to_task(struct kernel_siginfo *info, struct task_struct *t,\\n\\tenum sig_handler handler)\\n{\\n\\tunsigned long int flags;\\n\\tint ret, blocked, ignored;\\n\\tstruct k_sigaction *action;\\n\\tint sig = info->si_signo;\\n\\n\\tspin_lock_irqsave(&t->sighand->siglock, flags);\\n\\taction = &t->sighand->action[sig-1];\\n\\tignored = action->sa.sa_handler == SIG_IGN;\\n\\tblocked = sigismember(&t->blocked, sig);\\n\\tif (blocked || ignored || (handler != HANDLER_CURRENT)) {\\n\\t\\taction->sa.sa_handler = SIG_DFL;\\n\\t\\tif (handler == HANDLER_EXIT)\\n\\t\\t\\taction->sa.sa_flags |= SA_IMMUTABLE;\\n\\t\\tif (blocked)\\n\\t\\t\\tsigdelset(&t->blocked, sig);\\n\\t}\\n\\t/*\\n\\t * Don\\'t clear SIGNAL_UNKILLABLE for traced tasks, users won\\'t expect\\n\\t * debugging to leave init killable. But HANDLER_EXIT is always fatal.\\n\\t */\\n\\tif (action->sa.sa_handler == SIG_DFL &&\\n\\t    (!t->ptrace || (handler == HANDLER_EXIT)))\\n\\t\\tt->signal->flags &= ~SIGNAL_UNKILLABLE;\\n\\tret = send_signal_locked(sig, info, t, PIDTYPE_PID);\\n\\t/* This can happen if the signal was already pending and blocked */\\n\\tif (!task_sigpending(t))\\n\\t\\tsignal_wake_up(t, 0);\\n\\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\\n\\n\\treturn ret;\\n}\\n\\nint force_sig_info(struct kernel_siginfo *info)\\n{\\n\\treturn force_sig_info_to_task(info, current, HANDLER_CURRENT);\\n}\\n\\n/*\\n * Nuke all other threads in the group.\\n */\\nint zap_other_threads(struct task_struct *p)\\n{\\n\\tstruct task_struct *t;\\n\\tint count = 0;\\n\\n\\tp->signal->group_stop_count = 0;\\n\\n\\tfor_other_threads(p, t) {\\n\\t\\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\\n\\t\\tcount++;\\n\\n\\t\\t/* Don\\'t bother with already dead threads */\\n\\t\\tif (t->exit_state)\\n\\t\\t\\tcontinue;\\n\\t\\tsigaddset(&t->pending.signal, SIGKILL);\\n\\t\\tsignal_wake_up(t, 1);\\n\\t}\\n\\n\\treturn count;\\n}\\n\\nstruct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\\n\\t\\t\\t\\t\\t   unsigned long *flags)\\n{\\n\\tstruct sighand_struct *sighand;\\n\\n\\trcu_read_lock();\\n\\tfor (;;) {\\n\\t\\tsighand = rcu_dereference(tsk->sighand);\\n\\t\\tif (unlikely(sighand == NULL))\\n\\t\\t\\tbreak;\\n\\n\\t\\t/*\\n\\t\\t * This sighand can be already freed and even reused, but\\n\\t\\t * we rely on SLAB_TYPESAFE_BY_RCU and sighand_ctor() which\\n\\t\\t * initializes ->siglock: this slab can\\'t go away, it has\\n\\t\\t * the same object type, ->siglock can\\'t be reinitialized.\\n\\t\\t *\\n\\t\\t * We need to ensure that tsk->sighand is still the same\\n\\t\\t * after we take the lock, we can race with de_thread() or\\n\\t\\t * __exit_signal(). In the latter case the next iteration\\n\\t\\t * must see ->sighand == NULL.\\n\\t\\t */\\n\\t\\tspin_lock_irqsave(&sighand->siglock, *flags);\\n\\t\\tif (likely(sighand == rcu_access_pointer(tsk->sighand)))\\n\\t\\t\\tbreak;\\n\\t\\tspin_unlock_irqrestore(&sighand->siglock, *flags);\\n\\t}\\n\\trcu_read_unlock();\\n\\n\\treturn sighand;\\n}\\n\\n#ifdef CONFIG_LOCKDEP\\nvoid lockdep_assert_task_sighand_held(struct task_struct *task)\\n{\\n\\tstruct sighand_struct *sighand;\\n\\n\\trcu_read_lock();\\n\\tsighand = rcu_dereference(task->sighand);\\n\\tif (sighand)\\n\\t\\tlockdep_assert_held(&sighand->siglock);\\n\\telse\\n\\t\\tWARN_ON_ONCE(1);\\n\\trcu_read_unlock();\\n}\\n#endif\\n\\n/*\\n * send signal info to all the members of a thread group or to the\\n * individual thread if type == PIDTYPE_PID.\\n */\\nint group_send_sig_info(int sig, struct kernel_siginfo *info,\\n\\t\\t\\tstruct task_struct *p, enum pid_type type)\\n{\\n\\tint ret;\\n\\n\\trcu_read_lock();\\n\\tret = check_kill_permission(sig, info, p);\\n\\trcu_read_unlock();\\n\\n\\tif (!ret && sig)\\n\\t\\tret = do_send_sig_info(sig, info, p, type);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * __kill_pgrp_info() sends a signal to a process group: this is what the tty\\n * control characters do (^C, ^Z etc)\\n * - the caller must hold at least a readlock on tasklist_lock\\n */\\nint __kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp)\\n{\\n\\tstruct task_struct *p = NULL;\\n\\tint ret = -ESRCH;\\n\\n\\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\\n\\t\\tint err = group_send_sig_info(sig, info, p, PIDTYPE_PGID);\\n\\t\\t/*\\n\\t\\t * If group_send_sig_info() succeeds at least once ret\\n\\t\\t * becomes 0 and after that the code below has no effect.\\n\\t\\t * Otherwise we return the last err or -ESRCH if this\\n\\t\\t * process group is empty.\\n\\t\\t */\\n\\t\\tif (ret)\\n\\t\\t\\tret = err;\\n\\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\\n\\n\\treturn ret;\\n}\\n\\nstatic int kill_pid_info_type(int sig, struct kernel_siginfo *info,\\n\\t\\t\\t\\tstruct pid *pid, enum pid_type type)\\n{\\n\\tint error = -ESRCH;\\n\\tstruct task_struct *p;\\n\\n\\tfor (;;) {\\n\\t\\trcu_read_lock();\\n\\t\\tp = pid_task(pid, PIDTYPE_PID);\\n\\t\\tif (p)\\n\\t\\t\\terror = group_send_sig_info(sig, info, p, type);\\n\\t\\trcu_read_unlock();\\n\\t\\tif (likely(!p || error != -ESRCH))\\n\\t\\t\\treturn error;\\n\\t\\t/*\\n\\t\\t * The task was unhashed in between, try again.  If it\\n\\t\\t * is dead, pid_task() will return NULL, if we race with\\n\\t\\t * de_thread() it will find the new leader.\\n\\t\\t */\\n\\t}\\n}\\n\\nint kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid)\\n{\\n\\treturn kill_pid_info_type(sig, info, pid, PIDTYPE_TGID);\\n}\\n\\nstatic int kill_proc_info(int sig, struct kernel_siginfo *info, pid_t pid)\\n{\\n\\tint error;\\n\\trcu_read_lock();\\n\\terror = kill_pid_info(sig, info, find_vpid(pid));\\n\\trcu_read_unlock();\\n\\treturn error;\\n}\\n\\nstatic inline bool kill_as_cred_perm(const struct cred *cred,\\n\\t\\t\\t\\t     struct task_struct *target)\\n{\\n\\tconst struct cred *pcred = __task_cred(target);\\n\\n\\treturn uid_eq(cred->euid, pcred->suid) ||\\n\\t       uid_eq(cred->euid, pcred->uid) ||\\n\\t       uid_eq(cred->uid, pcred->suid) ||\\n\\t       uid_eq(cred->uid, pcred->uid);\\n}\\n\\n/*\\n * The usb asyncio usage of siginfo is wrong.  The glibc support\\n * for asyncio which uses SI_ASYNCIO assumes the layout is SIL_RT.\\n * AKA after the generic fields:\\n *\\tkernel_pid_t\\tsi_pid;\\n *\\tkernel_uid32_t\\tsi_uid;\\n *\\tsigval_t\\tsi_value;\\n *\\n * Unfortunately when usb generates SI_ASYNCIO it assumes the layout\\n * after the generic fields is:\\n *\\tvoid __user \\t*si_addr;\\n *\\n * This is a practical problem when there is a 64bit big endian kernel\\n * and a 32bit userspace.  As the 32bit address will encoded in the low\\n * 32bits of the pointer.  Those low 32bits will be stored at higher\\n * address than appear in a 32 bit pointer.  So userspace will not\\n * see the address it was expecting for it\\'s completions.\\n *\\n * There is nothing in the encoding that can allow\\n * copy_siginfo_to_user32 to detect this confusion of formats, so\\n * handle this by requiring the caller of kill_pid_usb_asyncio to\\n * notice when this situration takes place and to store the 32bit\\n * pointer in sival_int, instead of sival_addr of the sigval_t addr\\n * parameter.\\n */\\nint kill_pid_usb_asyncio(int sig, int errno, sigval_t addr,\\n\\t\\t\\t struct pid *pid, const struct cred *cred)\\n{\\n\\tstruct kernel_siginfo info;\\n\\tstruct task_struct *p;\\n\\tunsigned long flags;\\n\\tint ret = -EINVAL;\\n\\n\\tif (!valid_signal(sig))\\n\\t\\treturn ret;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = errno;\\n\\tinfo.si_code = SI_ASYNCIO;\\n\\t*((sigval_t *)&info.si_pid) = addr;\\n\\n\\trcu_read_lock();\\n\\tp = pid_task(pid, PIDTYPE_PID);\\n\\tif (!p) {\\n\\t\\tret = -ESRCH;\\n\\t\\tgoto out_unlock;\\n\\t}\\n\\tif (!kill_as_cred_perm(cred, p)) {\\n\\t\\tret = -EPERM;\\n\\t\\tgoto out_unlock;\\n\\t}\\n\\tret = security_task_kill(p, &info, sig, cred);\\n\\tif (ret)\\n\\t\\tgoto out_unlock;\\n\\n\\tif (sig) {\\n\\t\\tif (lock_task_sighand(p, &flags)) {\\n\\t\\t\\tret = __send_signal_locked(sig, &info, p, PIDTYPE_TGID, false);\\n\\t\\t\\tunlock_task_sighand(p, &flags);\\n\\t\\t} else\\n\\t\\t\\tret = -ESRCH;\\n\\t}\\nout_unlock:\\n\\trcu_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(kill_pid_usb_asyncio);\\n\\n/*\\n * kill_something_info() interprets pid in interesting ways just like kill(2).\\n *\\n * POSIX specifies that kill(-1,sig) is unspecified, but what we have\\n * is probably wrong.  Should make it like BSD or SYSV.\\n */\\n\\nstatic int kill_something_info(int sig, struct kernel_siginfo *info, pid_t pid)\\n{\\n\\tint ret;\\n\\n\\tif (pid > 0)\\n\\t\\treturn kill_proc_info(sig, info, pid);\\n\\n\\t/* -INT_MIN is undefined.  Exclude this case to avoid a UBSAN warning */\\n\\tif (pid == INT_MIN)\\n\\t\\treturn -ESRCH;\\n\\n\\tread_lock(&tasklist_lock);\\n\\tif (pid != -1) {\\n\\t\\tret = __kill_pgrp_info(sig, info,\\n\\t\\t\\t\\tpid ? find_vpid(-pid) : task_pgrp(current));\\n\\t} else {\\n\\t\\tint retval = 0, count = 0;\\n\\t\\tstruct task_struct * p;\\n\\n\\t\\tfor_each_process(p) {\\n\\t\\t\\tif (task_pid_vnr(p) > 1 &&\\n\\t\\t\\t\\t\\t!same_thread_group(p, current)) {\\n\\t\\t\\t\\tint err = group_send_sig_info(sig, info, p,\\n\\t\\t\\t\\t\\t\\t\\t      PIDTYPE_MAX);\\n\\t\\t\\t\\t++count;\\n\\t\\t\\t\\tif (err != -EPERM)\\n\\t\\t\\t\\t\\tretval = err;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tret = count ? retval : -ESRCH;\\n\\t}\\n\\tread_unlock(&tasklist_lock);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * These are for backward compatibility with the rest of the kernel source.\\n */\\n\\nint send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p)\\n{\\n\\t/*\\n\\t * Make sure legacy kernel users don\\'t send in bad values\\n\\t * (normal paths check this in check_kill_permission).\\n\\t */\\n\\tif (!valid_signal(sig))\\n\\t\\treturn -EINVAL;\\n\\n\\treturn do_send_sig_info(sig, info, p, PIDTYPE_PID);\\n}\\nEXPORT_SYMBOL(send_sig_info);\\n\\n#define __si_special(priv) \\\\\\n\\t((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)\\n\\nint\\nsend_sig(int sig, struct task_struct *p, int priv)\\n{\\n\\treturn send_sig_info(sig, __si_special(priv), p);\\n}\\nEXPORT_SYMBOL(send_sig);\\n\\nvoid force_sig(int sig)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code = SI_KERNEL;\\n\\tinfo.si_pid = 0;\\n\\tinfo.si_uid = 0;\\n\\tforce_sig_info(&info);\\n}\\nEXPORT_SYMBOL(force_sig);\\n\\nvoid force_fatal_sig(int sig)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code = SI_KERNEL;\\n\\tinfo.si_pid = 0;\\n\\tinfo.si_uid = 0;\\n\\tforce_sig_info_to_task(&info, current, HANDLER_SIG_DFL);\\n}\\n\\nvoid force_exit_sig(int sig)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code = SI_KERNEL;\\n\\tinfo.si_pid = 0;\\n\\tinfo.si_uid = 0;\\n\\tforce_sig_info_to_task(&info, current, HANDLER_EXIT);\\n}\\n\\n/*\\n * When things go south during signal handling, we\\n * will force a SIGSEGV. And if the signal that caused\\n * the problem was already a SIGSEGV, we\\'ll want to\\n * make sure we don\\'t even try to deliver the signal..\\n */\\nvoid force_sigsegv(int sig)\\n{\\n\\tif (sig == SIGSEGV)\\n\\t\\tforce_fatal_sig(SIGSEGV);\\n\\telse\\n\\t\\tforce_sig(SIGSEGV);\\n}\\n\\nint force_sig_fault_to_task(int sig, int code, void __user *addr,\\n\\t\\t\\t    struct task_struct *t)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code  = code;\\n\\tinfo.si_addr  = addr;\\n\\treturn force_sig_info_to_task(&info, t, HANDLER_CURRENT);\\n}\\n\\nint force_sig_fault(int sig, int code, void __user *addr)\\n{\\n\\treturn force_sig_fault_to_task(sig, code, addr, current);\\n}\\n\\nint send_sig_fault(int sig, int code, void __user *addr, struct task_struct *t)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code  = code;\\n\\tinfo.si_addr  = addr;\\n\\treturn send_sig_info(info.si_signo, &info, t);\\n}\\n\\nint force_sig_mceerr(int code, void __user *addr, short lsb)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tWARN_ON((code != BUS_MCEERR_AO) && (code != BUS_MCEERR_AR));\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = SIGBUS;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code = code;\\n\\tinfo.si_addr = addr;\\n\\tinfo.si_addr_lsb = lsb;\\n\\treturn force_sig_info(&info);\\n}\\n\\nint send_sig_mceerr(int code, void __user *addr, short lsb, struct task_struct *t)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tWARN_ON((code != BUS_MCEERR_AO) && (code != BUS_MCEERR_AR));\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = SIGBUS;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code = code;\\n\\tinfo.si_addr = addr;\\n\\tinfo.si_addr_lsb = lsb;\\n\\treturn send_sig_info(info.si_signo, &info, t);\\n}\\nEXPORT_SYMBOL(send_sig_mceerr);\\n\\nint force_sig_bnderr(void __user *addr, void __user *lower, void __user *upper)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = SIGSEGV;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code  = SEGV_BNDERR;\\n\\tinfo.si_addr  = addr;\\n\\tinfo.si_lower = lower;\\n\\tinfo.si_upper = upper;\\n\\treturn force_sig_info(&info);\\n}\\n\\n#ifdef SEGV_PKUERR\\nint force_sig_pkuerr(void __user *addr, u32 pkey)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = SIGSEGV;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code  = SEGV_PKUERR;\\n\\tinfo.si_addr  = addr;\\n\\tinfo.si_pkey  = pkey;\\n\\treturn force_sig_info(&info);\\n}\\n#endif\\n\\nint send_sig_perf(void __user *addr, u32 type, u64 sig_data)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo     = SIGTRAP;\\n\\tinfo.si_errno     = 0;\\n\\tinfo.si_code      = TRAP_PERF;\\n\\tinfo.si_addr      = addr;\\n\\tinfo.si_perf_data = sig_data;\\n\\tinfo.si_perf_type = type;\\n\\n\\t/*\\n\\t * Signals generated by perf events should not terminate the whole\\n\\t * process if SIGTRAP is blocked, however, delivering the signal\\n\\t * asynchronously is better than not delivering at all. But tell user\\n\\t * space if the signal was asynchronous, so it can clearly be\\n\\t * distinguished from normal synchronous ones.\\n\\t */\\n\\tinfo.si_perf_flags = sigismember(&current->blocked, info.si_signo) ?\\n\\t\\t\\t\\t     TRAP_PERF_FLAG_ASYNC :\\n\\t\\t\\t\\t     0;\\n\\n\\treturn send_sig_info(info.si_signo, &info, current);\\n}\\n\\n/**\\n * force_sig_seccomp - signals the task to allow in-process syscall emulation\\n * @syscall: syscall number to send to userland\\n * @reason: filter-supplied reason code to send to userland (via si_errno)\\n * @force_coredump: true to trigger a coredump\\n *\\n * Forces a SIGSYS with a code of SYS_SECCOMP and related sigsys info.\\n */\\nint force_sig_seccomp(int syscall, int reason, bool force_coredump)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = SIGSYS;\\n\\tinfo.si_code = SYS_SECCOMP;\\n\\tinfo.si_call_addr = (void __user *)KSTK_EIP(current);\\n\\tinfo.si_errno = reason;\\n\\tinfo.si_arch = syscall_get_arch(current);\\n\\tinfo.si_syscall = syscall;\\n\\treturn force_sig_info_to_task(&info, current,\\n\\t\\tforce_coredump ? HANDLER_EXIT : HANDLER_CURRENT);\\n}\\n\\n/* For the crazy architectures that include trap information in\\n * the errno field, instead of an actual errno value.\\n */\\nint force_sig_ptrace_errno_trap(int errno, void __user *addr)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = SIGTRAP;\\n\\tinfo.si_errno = errno;\\n\\tinfo.si_code  = TRAP_HWBKPT;\\n\\tinfo.si_addr  = addr;\\n\\treturn force_sig_info(&info);\\n}\\n\\n/* For the rare architectures that include trap information using\\n * si_trapno.\\n */\\nint force_sig_fault_trapno(int sig, int code, void __user *addr, int trapno)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code  = code;\\n\\tinfo.si_addr  = addr;\\n\\tinfo.si_trapno = trapno;\\n\\treturn force_sig_info(&info);\\n}\\n\\n/* For the rare architectures that include trap information using\\n * si_trapno.\\n */\\nint send_sig_fault_trapno(int sig, int code, void __user *addr, int trapno,\\n\\t\\t\\t  struct task_struct *t)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = 0;\\n\\tinfo.si_code  = code;\\n\\tinfo.si_addr  = addr;\\n\\tinfo.si_trapno = trapno;\\n\\treturn send_sig_info(info.si_signo, &info, t);\\n}\\n\\nstatic int kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp)\\n{\\n\\tint ret;\\n\\tread_lock(&tasklist_lock);\\n\\tret = __kill_pgrp_info(sig, info, pgrp);\\n\\tread_unlock(&tasklist_lock);\\n\\treturn ret;\\n}\\n\\nint kill_pgrp(struct pid *pid, int sig, int priv)\\n{\\n\\treturn kill_pgrp_info(sig, __si_special(priv), pid);\\n}\\nEXPORT_SYMBOL(kill_pgrp);\\n\\nint kill_pid(struct pid *pid, int sig, int priv)\\n{\\n\\treturn kill_pid_info(sig, __si_special(priv), pid);\\n}\\nEXPORT_SYMBOL(kill_pid);\\n\\n#ifdef CONFIG_POSIX_TIMERS\\n/*\\n * These functions handle POSIX timer signals. POSIX timers use\\n * preallocated sigqueue structs for sending signals.\\n */\\nstatic void __flush_itimer_signals(struct sigpending *pending)\\n{\\n\\tsigset_t signal, retain;\\n\\tstruct sigqueue *q, *n;\\n\\n\\tsignal = pending->signal;\\n\\tsigemptyset(&retain);\\n\\n\\tlist_for_each_entry_safe(q, n, &pending->list, list) {\\n\\t\\tint sig = q->info.si_signo;\\n\\n\\t\\tif (likely(q->info.si_code != SI_TIMER)) {\\n\\t\\t\\tsigaddset(&retain, sig);\\n\\t\\t} else {\\n\\t\\t\\tsigdelset(&signal, sig);\\n\\t\\t\\tlist_del_init(&q->list);\\n\\t\\t\\t__sigqueue_free(q);\\n\\t\\t}\\n\\t}\\n\\n\\tsigorsets(&pending->signal, &signal, &retain);\\n}\\n\\nvoid flush_itimer_signals(void)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\n\\tguard(spinlock_irqsave)(&tsk->sighand->siglock);\\n\\t__flush_itimer_signals(&tsk->pending);\\n\\t__flush_itimer_signals(&tsk->signal->shared_pending);\\n}\\n\\nbool posixtimer_init_sigqueue(struct sigqueue *q)\\n{\\n\\tstruct ucounts *ucounts = sig_get_ucounts(current, -1, 0);\\n\\n\\tif (!ucounts)\\n\\t\\treturn false;\\n\\tclear_siginfo(&q->info);\\n\\t__sigqueue_init(q, ucounts, SIGQUEUE_PREALLOC);\\n\\treturn true;\\n}\\n\\nstatic void posixtimer_queue_sigqueue(struct sigqueue *q, struct task_struct *t, enum pid_type type)\\n{\\n\\tstruct sigpending *pending;\\n\\tint sig = q->info.si_signo;\\n\\n\\tsignalfd_notify(t, sig);\\n\\tpending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\\n\\tlist_add_tail(&q->list, &pending->list);\\n\\tsigaddset(&pending->signal, sig);\\n\\tcomplete_signal(sig, t, type);\\n}\\n\\n/*\\n * This function is used by POSIX timers to deliver a timer signal.\\n * Where type is PIDTYPE_PID (such as for timers with SIGEV_THREAD_ID\\n * set), the signal must be delivered to the specific thread (queues\\n * into t->pending).\\n *\\n * Where type is not PIDTYPE_PID, signals must be delivered to the\\n * process. In this case, prefer to deliver to current if it is in\\n * the same thread group as the target process and its sighand is\\n * stable, which avoids unnecessarily waking up a potentially idle task.\\n */\\nstatic inline struct task_struct *posixtimer_get_target(struct k_itimer *tmr)\\n{\\n\\tstruct task_struct *t = pid_task(tmr->it_pid, tmr->it_pid_type);\\n\\n\\tif (t && tmr->it_pid_type != PIDTYPE_PID &&\\n\\t    same_thread_group(t, current) && !current->exit_state)\\n\\t\\tt = current;\\n\\treturn t;\\n}\\n\\nvoid posixtimer_send_sigqueue(struct k_itimer *tmr)\\n{\\n\\tstruct sigqueue *q = &tmr->sigq;\\n\\tint sig = q->info.si_signo;\\n\\tstruct task_struct *t;\\n\\tunsigned long flags;\\n\\tint result;\\n\\n\\tguard(rcu)();\\n\\n\\tt = posixtimer_get_target(tmr);\\n\\tif (!t)\\n\\t\\treturn;\\n\\n\\tif (!likely(lock_task_sighand(t, &flags)))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Update @tmr::sigqueue_seq for posix timer signals with sighand\\n\\t * locked to prevent a race against dequeue_signal().\\n\\t */\\n\\ttmr->it_sigqueue_seq = tmr->it_signal_seq;\\n\\n\\t/*\\n\\t * Set the signal delivery status under sighand lock, so that the\\n\\t * ignored signal handling can distinguish between a periodic and a\\n\\t * non-periodic timer.\\n\\t */\\n\\ttmr->it_sig_periodic = tmr->it_status == POSIX_TIMER_REQUEUE_PENDING;\\n\\n\\tif (!prepare_signal(sig, t, false)) {\\n\\t\\tresult = TRACE_SIGNAL_IGNORED;\\n\\n\\t\\tif (!list_empty(&q->list)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * If task group is exiting with the signal already pending,\\n\\t\\t\\t * wait for __exit_signal() to do its job. Otherwise if\\n\\t\\t\\t * ignored, it\\'s not supposed to be queued. Try to survive.\\n\\t\\t\\t */\\n\\t\\t\\tWARN_ON_ONCE(!(t->signal->flags & SIGNAL_GROUP_EXIT));\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\n\\t\\t/* Periodic timers with SIG_IGN are queued on the ignored list */\\n\\t\\tif (tmr->it_sig_periodic) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Already queued means the timer was rearmed after\\n\\t\\t\\t * the previous expiry got it on the ignore list.\\n\\t\\t\\t * Nothing to do for that case.\\n\\t\\t\\t */\\n\\t\\t\\tif (hlist_unhashed(&tmr->ignored_list)) {\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * Take a signal reference and queue it on\\n\\t\\t\\t\\t * the ignored list.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\tposixtimer_sigqueue_getref(q);\\n\\t\\t\\t\\tposixtimer_sig_ignore(t, q);\\n\\t\\t\\t}\\n\\t\\t} else if (!hlist_unhashed(&tmr->ignored_list)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Covers the case where a timer was periodic and\\n\\t\\t\\t * then the signal was ignored. Later it was rearmed\\n\\t\\t\\t * as oneshot timer. The previous signal is invalid\\n\\t\\t\\t * now, and this oneshot signal has to be dropped.\\n\\t\\t\\t * Remove it from the ignored list and drop the\\n\\t\\t\\t * reference count as the signal is not longer\\n\\t\\t\\t * queued.\\n\\t\\t\\t */\\n\\t\\t\\thlist_del_init(&tmr->ignored_list);\\n\\t\\t\\tposixtimer_putref(tmr);\\n\\t\\t}\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t/* This should never happen and leaks a reference count */\\n\\tif (WARN_ON_ONCE(!hlist_unhashed(&tmr->ignored_list)))\\n\\t\\thlist_del_init(&tmr->ignored_list);\\n\\n\\tif (unlikely(!list_empty(&q->list))) {\\n\\t\\t/* This holds a reference count already */\\n\\t\\tresult = TRACE_SIGNAL_ALREADY_PENDING;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tposixtimer_sigqueue_getref(q);\\n\\tposixtimer_queue_sigqueue(q, t, tmr->it_pid_type);\\n\\tresult = TRACE_SIGNAL_DELIVERED;\\nout:\\n\\ttrace_signal_generate(sig, &q->info, t, tmr->it_pid_type != PIDTYPE_PID, result);\\n\\tunlock_task_sighand(t, &flags);\\n}\\n\\nstatic inline void posixtimer_sig_ignore(struct task_struct *tsk, struct sigqueue *q)\\n{\\n\\tstruct k_itimer *tmr = container_of(q, struct k_itimer, sigq);\\n\\n\\t/*\\n\\t * If the timer is marked deleted already or the signal originates\\n\\t * from a non-periodic timer, then just drop the reference\\n\\t * count. Otherwise queue it on the ignored list.\\n\\t */\\n\\tif (tmr->it_signal && tmr->it_sig_periodic)\\n\\t\\thlist_add_head(&tmr->ignored_list, &tsk->signal->ignored_posix_timers);\\n\\telse\\n\\t\\tposixtimer_putref(tmr);\\n}\\n\\nstatic void posixtimer_sig_unignore(struct task_struct *tsk, int sig)\\n{\\n\\tstruct hlist_head *head = &tsk->signal->ignored_posix_timers;\\n\\tstruct hlist_node *tmp;\\n\\tstruct k_itimer *tmr;\\n\\n\\tif (likely(hlist_empty(head)))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Rearming a timer with sighand lock held is not possible due to\\n\\t * lock ordering vs. tmr::it_lock. Just stick the sigqueue back and\\n\\t * let the signal delivery path deal with it whether it needs to be\\n\\t * rearmed or not. This cannot be decided here w/o dropping sighand\\n\\t * lock and creating a loop retry horror show.\\n\\t */\\n\\thlist_for_each_entry_safe(tmr, tmp , head, ignored_list) {\\n\\t\\tstruct task_struct *target;\\n\\n\\t\\t/*\\n\\t\\t * tmr::sigq.info.si_signo is immutable, so accessing it\\n\\t\\t * without holding tmr::it_lock is safe.\\n\\t\\t */\\n\\t\\tif (tmr->sigq.info.si_signo != sig)\\n\\t\\t\\tcontinue;\\n\\n\\t\\thlist_del_init(&tmr->ignored_list);\\n\\n\\t\\t/* This should never happen and leaks a reference count */\\n\\t\\tif (WARN_ON_ONCE(!list_empty(&tmr->sigq.list)))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * Get the target for the signal. If target is a thread and\\n\\t\\t * has exited by now, drop the reference count.\\n\\t\\t */\\n\\t\\tguard(rcu)();\\n\\t\\ttarget = posixtimer_get_target(tmr);\\n\\t\\tif (target)\\n\\t\\t\\tposixtimer_queue_sigqueue(&tmr->sigq, target, tmr->it_pid_type);\\n\\t\\telse\\n\\t\\t\\tposixtimer_putref(tmr);\\n\\t}\\n}\\n#else /* CONFIG_POSIX_TIMERS */\\nstatic inline void posixtimer_sig_ignore(struct task_struct *tsk, struct sigqueue *q) { }\\nstatic inline void posixtimer_sig_unignore(struct task_struct *tsk, int sig) { }\\n#endif /* !CONFIG_POSIX_TIMERS */\\n\\nvoid do_notify_pidfd(struct task_struct *task)\\n{\\n\\tstruct pid *pid = task_pid(task);\\n\\n\\tWARN_ON(task->exit_state == 0);\\n\\n\\t__wake_up(&pid->wait_pidfd, TASK_NORMAL, 0,\\n\\t\\t\\tpoll_to_key(EPOLLIN | EPOLLRDNORM));\\n}\\n\\n/*\\n * Let a parent know about the death of a child.\\n * For a stopped/continued status change, use do_notify_parent_cldstop instead.\\n *\\n * Returns true if our parent ignored us and so we\\'ve switched to\\n * self-reaping.\\n */\\nbool do_notify_parent(struct task_struct *tsk, int sig)\\n{\\n\\tstruct kernel_siginfo info;\\n\\tunsigned long flags;\\n\\tstruct sighand_struct *psig;\\n\\tbool autoreap = false;\\n\\tu64 utime, stime;\\n\\n\\tWARN_ON_ONCE(sig == -1);\\n\\n\\t/* do_notify_parent_cldstop should have been called instead.  */\\n\\tWARN_ON_ONCE(task_is_stopped_or_traced(tsk));\\n\\n\\tWARN_ON_ONCE(!tsk->ptrace &&\\n\\t       (tsk->group_leader != tsk || !thread_group_empty(tsk)));\\n\\t/*\\n\\t * tsk is a group leader and has no threads, wake up the\\n\\t * non-PIDFD_THREAD waiters.\\n\\t */\\n\\tif (thread_group_empty(tsk))\\n\\t\\tdo_notify_pidfd(tsk);\\n\\n\\tif (sig != SIGCHLD) {\\n\\t\\t/*\\n\\t\\t * This is only possible if parent == real_parent.\\n\\t\\t * Check if it has changed security domain.\\n\\t\\t */\\n\\t\\tif (tsk->parent_exec_id != READ_ONCE(tsk->parent->self_exec_id))\\n\\t\\t\\tsig = SIGCHLD;\\n\\t}\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = sig;\\n\\tinfo.si_errno = 0;\\n\\t/*\\n\\t * We are under tasklist_lock here so our parent is tied to\\n\\t * us and cannot change.\\n\\t *\\n\\t * task_active_pid_ns will always return the same pid namespace\\n\\t * until a task passes through release_task.\\n\\t *\\n\\t * write_lock() currently calls preempt_disable() which is the\\n\\t * same as rcu_read_lock(), but according to Oleg, this is not\\n\\t * correct to rely on this\\n\\t */\\n\\trcu_read_lock();\\n\\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));\\n\\tinfo.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),\\n\\t\\t\\t\\t       task_uid(tsk));\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\tinfo.si_utime = nsec_to_clock_t(utime + tsk->signal->utime);\\n\\tinfo.si_stime = nsec_to_clock_t(stime + tsk->signal->stime);\\n\\n\\tinfo.si_status = tsk->exit_code & 0x7f;\\n\\tif (tsk->exit_code & 0x80)\\n\\t\\tinfo.si_code = CLD_DUMPED;\\n\\telse if (tsk->exit_code & 0x7f)\\n\\t\\tinfo.si_code = CLD_KILLED;\\n\\telse {\\n\\t\\tinfo.si_code = CLD_EXITED;\\n\\t\\tinfo.si_status = tsk->exit_code >> 8;\\n\\t}\\n\\n\\tpsig = tsk->parent->sighand;\\n\\tspin_lock_irqsave(&psig->siglock, flags);\\n\\tif (!tsk->ptrace && sig == SIGCHLD &&\\n\\t    (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||\\n\\t     (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {\\n\\t\\t/*\\n\\t\\t * We are exiting and our parent doesn\\'t care.  POSIX.1\\n\\t\\t * defines special semantics for setting SIGCHLD to SIG_IGN\\n\\t\\t * or setting the SA_NOCLDWAIT flag: we should be reaped\\n\\t\\t * automatically and not left for our parent\\'s wait4 call.\\n\\t\\t * Rather than having the parent do it as a magic kind of\\n\\t\\t * signal handler, we just set this to tell do_exit that we\\n\\t\\t * can be cleaned up without becoming a zombie.  Note that\\n\\t\\t * we still call __wake_up_parent in this case, because a\\n\\t\\t * blocked sys_wait4 might now return -ECHILD.\\n\\t\\t *\\n\\t\\t * Whether we send SIGCHLD or not for SA_NOCLDWAIT\\n\\t\\t * is implementation-defined: we do (if you don\\'t want\\n\\t\\t * it, just use SIG_IGN instead).\\n\\t\\t */\\n\\t\\tautoreap = true;\\n\\t\\tif (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)\\n\\t\\t\\tsig = 0;\\n\\t}\\n\\t/*\\n\\t * Send with __send_signal as si_pid and si_uid are in the\\n\\t * parent\\'s namespaces.\\n\\t */\\n\\tif (valid_signal(sig) && sig)\\n\\t\\t__send_signal_locked(sig, &info, tsk->parent, PIDTYPE_TGID, false);\\n\\t__wake_up_parent(tsk, tsk->parent);\\n\\tspin_unlock_irqrestore(&psig->siglock, flags);\\n\\n\\treturn autoreap;\\n}\\n\\n/**\\n * do_notify_parent_cldstop - notify parent of stopped/continued state change\\n * @tsk: task reporting the state change\\n * @for_ptracer: the notification is for ptracer\\n * @why: CLD_{CONTINUED|STOPPED|TRAPPED} to report\\n *\\n * Notify @tsk\\'s parent that the stopped/continued state has changed.  If\\n * @for_ptracer is %false, @tsk\\'s group leader notifies to its real parent.\\n * If %true, @tsk reports to @tsk->parent which should be the ptracer.\\n *\\n * CONTEXT:\\n * Must be called with tasklist_lock at least read locked.\\n */\\nstatic void do_notify_parent_cldstop(struct task_struct *tsk,\\n\\t\\t\\t\\t     bool for_ptracer, int why)\\n{\\n\\tstruct kernel_siginfo info;\\n\\tunsigned long flags;\\n\\tstruct task_struct *parent;\\n\\tstruct sighand_struct *sighand;\\n\\tu64 utime, stime;\\n\\n\\tif (for_ptracer) {\\n\\t\\tparent = tsk->parent;\\n\\t} else {\\n\\t\\ttsk = tsk->group_leader;\\n\\t\\tparent = tsk->real_parent;\\n\\t}\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = SIGCHLD;\\n\\tinfo.si_errno = 0;\\n\\t/*\\n\\t * see comment in do_notify_parent() about the following 4 lines\\n\\t */\\n\\trcu_read_lock();\\n\\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));\\n\\tinfo.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\tinfo.si_utime = nsec_to_clock_t(utime);\\n\\tinfo.si_stime = nsec_to_clock_t(stime);\\n\\n \\tinfo.si_code = why;\\n \\tswitch (why) {\\n \\tcase CLD_CONTINUED:\\n \\t\\tinfo.si_status = SIGCONT;\\n \\t\\tbreak;\\n \\tcase CLD_STOPPED:\\n \\t\\tinfo.si_status = tsk->signal->group_exit_code & 0x7f;\\n \\t\\tbreak;\\n \\tcase CLD_TRAPPED:\\n \\t\\tinfo.si_status = tsk->exit_code & 0x7f;\\n \\t\\tbreak;\\n \\tdefault:\\n \\t\\tBUG();\\n \\t}\\n\\n\\tsighand = parent->sighand;\\n\\tspin_lock_irqsave(&sighand->siglock, flags);\\n\\tif (sighand->action[SIGCHLD-1].sa.sa_handler != SIG_IGN &&\\n\\t    !(sighand->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))\\n\\t\\tsend_signal_locked(SIGCHLD, &info, parent, PIDTYPE_TGID);\\n\\t/*\\n\\t * Even if SIGCHLD is not generated, we must wake up wait4 calls.\\n\\t */\\n\\t__wake_up_parent(tsk, parent);\\n\\tspin_unlock_irqrestore(&sighand->siglock, flags);\\n}\\n\\n/*\\n * This must be called with current->sighand->siglock held.\\n *\\n * This should be the path for all ptrace stops.\\n * We always set current->last_siginfo while stopped here.\\n * That makes it a way to test a stopped process for\\n * being ptrace-stopped vs being job-control-stopped.\\n *\\n * Returns the signal the ptracer requested the code resume\\n * with.  If the code did not stop because the tracer is gone,\\n * the stop signal remains unchanged unless clear_code.\\n */\\nstatic int ptrace_stop(int exit_code, int why, unsigned long message,\\n\\t\\t       kernel_siginfo_t *info)\\n\\t__releases(&current->sighand->siglock)\\n\\t__acquires(&current->sighand->siglock)\\n{\\n\\tbool gstop_done = false;\\n\\n\\tif (arch_ptrace_stop_needed()) {\\n\\t\\t/*\\n\\t\\t * The arch code has something special to do before a\\n\\t\\t * ptrace stop.  This is allowed to block, e.g. for faults\\n\\t\\t * on user stack pages.  We can\\'t keep the siglock while\\n\\t\\t * calling arch_ptrace_stop, so we must release it now.\\n\\t\\t * To preserve proper semantics, we must do this before\\n\\t\\t * any signal bookkeeping like checking group_stop_count.\\n\\t\\t */\\n\\t\\tspin_unlock_irq(&current->sighand->siglock);\\n\\t\\tarch_ptrace_stop();\\n\\t\\tspin_lock_irq(&current->sighand->siglock);\\n\\t}\\n\\n\\t/*\\n\\t * After this point ptrace_signal_wake_up or signal_wake_up\\n\\t * will clear TASK_TRACED if ptrace_unlink happens or a fatal\\n\\t * signal comes in.  Handle previous ptrace_unlinks and fatal\\n\\t * signals here to prevent ptrace_stop sleeping in schedule.\\n\\t */\\n\\tif (!current->ptrace || __fatal_signal_pending(current))\\n\\t\\treturn exit_code;\\n\\n\\tset_special_state(TASK_TRACED);\\n\\tcurrent->jobctl |= JOBCTL_TRACED;\\n\\n\\t/*\\n\\t * We\\'re committing to trapping.  TRACED should be visible before\\n\\t * TRAPPING is cleared; otherwise, the tracer might fail do_wait().\\n\\t * Also, transition to TRACED and updates to ->jobctl should be\\n\\t * atomic with respect to siglock and should be done after the arch\\n\\t * hook as siglock is released and regrabbed across it.\\n\\t *\\n\\t *     TRACER\\t\\t\\t\\t    TRACEE\\n\\t *\\n\\t *     ptrace_attach()\\n\\t * [L]   wait_on_bit(JOBCTL_TRAPPING)\\t[S] set_special_state(TRACED)\\n\\t *     do_wait()\\n\\t *       set_current_state()                smp_wmb();\\n\\t *       ptrace_do_wait()\\n\\t *         wait_task_stopped()\\n\\t *           task_stopped_code()\\n\\t * [L]         task_is_traced()\\t\\t[S] task_clear_jobctl_trapping();\\n\\t */\\n\\tsmp_wmb();\\n\\n\\tcurrent->ptrace_message = message;\\n\\tcurrent->last_siginfo = info;\\n\\tcurrent->exit_code = exit_code;\\n\\n\\t/*\\n\\t * If @why is CLD_STOPPED, we\\'re trapping to participate in a group\\n\\t * stop.  Do the bookkeeping.  Note that if SIGCONT was delievered\\n\\t * across siglock relocks since INTERRUPT was scheduled, PENDING\\n\\t * could be clear now.  We act as if SIGCONT is received after\\n\\t * TASK_TRACED is entered - ignore it.\\n\\t */\\n\\tif (why == CLD_STOPPED && (current->jobctl & JOBCTL_STOP_PENDING))\\n\\t\\tgstop_done = task_participate_group_stop(current);\\n\\n\\t/* any trap clears pending STOP trap, STOP trap clears NOTIFY */\\n\\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_STOP);\\n\\tif (info && info->si_code >> 8 == PTRACE_EVENT_STOP)\\n\\t\\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY);\\n\\n\\t/* entering a trap, clear TRAPPING */\\n\\ttask_clear_jobctl_trapping(current);\\n\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\tread_lock(&tasklist_lock);\\n\\t/*\\n\\t * Notify parents of the stop.\\n\\t *\\n\\t * While ptraced, there are two parents - the ptracer and\\n\\t * the real_parent of the group_leader.  The ptracer should\\n\\t * know about every stop while the real parent is only\\n\\t * interested in the completion of group stop.  The states\\n\\t * for the two don\\'t interact with each other.  Notify\\n\\t * separately unless they\\'re gonna be duplicates.\\n\\t */\\n\\tif (current->ptrace)\\n\\t\\tdo_notify_parent_cldstop(current, true, why);\\n\\tif (gstop_done && (!current->ptrace || ptrace_reparented(current)))\\n\\t\\tdo_notify_parent_cldstop(current, false, why);\\n\\n\\t/*\\n\\t * The previous do_notify_parent_cldstop() invocation woke ptracer.\\n\\t * One a PREEMPTION kernel this can result in preemption requirement\\n\\t * which will be fulfilled after read_unlock() and the ptracer will be\\n\\t * put on the CPU.\\n\\t * The ptracer is in wait_task_inactive(, __TASK_TRACED) waiting for\\n\\t * this task wait in schedule(). If this task gets preempted then it\\n\\t * remains enqueued on the runqueue. The ptracer will observe this and\\n\\t * then sleep for a delay of one HZ tick. In the meantime this task\\n\\t * gets scheduled, enters schedule() and will wait for the ptracer.\\n\\t *\\n\\t * This preemption point is not bad from a correctness point of\\n\\t * view but extends the runtime by one HZ tick time due to the\\n\\t * ptracer\\'s sleep.  The preempt-disable section ensures that there\\n\\t * will be no preemption between unlock and schedule() and so\\n\\t * improving the performance since the ptracer will observe that\\n\\t * the tracee is scheduled out once it gets on the CPU.\\n\\t *\\n\\t * On PREEMPT_RT locking tasklist_lock does not disable preemption.\\n\\t * Therefore the task can be preempted after do_notify_parent_cldstop()\\n\\t * before unlocking tasklist_lock so there is no benefit in doing this.\\n\\t *\\n\\t * In fact disabling preemption is harmful on PREEMPT_RT because\\n\\t * the spinlock_t in cgroup_enter_frozen() must not be acquired\\n\\t * with preemption disabled due to the \\'sleeping\\' spinlock\\n\\t * substitution of RT.\\n\\t */\\n\\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\\n\\t\\tpreempt_disable();\\n\\tread_unlock(&tasklist_lock);\\n\\tcgroup_enter_frozen();\\n\\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\\n\\t\\tpreempt_enable_no_resched();\\n\\tschedule();\\n\\tcgroup_leave_frozen(true);\\n\\n\\t/*\\n\\t * We are back.  Now reacquire the siglock before touching\\n\\t * last_siginfo, so that we are sure to have synchronized with\\n\\t * any signal-sending on another CPU that wants to examine it.\\n\\t */\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\texit_code = current->exit_code;\\n\\tcurrent->last_siginfo = NULL;\\n\\tcurrent->ptrace_message = 0;\\n\\tcurrent->exit_code = 0;\\n\\n\\t/* LISTENING can be set only during STOP traps, clear it */\\n\\tcurrent->jobctl &= ~(JOBCTL_LISTENING | JOBCTL_PTRACE_FROZEN);\\n\\n\\t/*\\n\\t * Queued signals ignored us while we were stopped for tracing.\\n\\t * So check for any that we should take before resuming user mode.\\n\\t * This sets TIF_SIGPENDING, but never clears it.\\n\\t */\\n\\trecalc_sigpending_tsk(current);\\n\\treturn exit_code;\\n}\\n\\nstatic int ptrace_do_notify(int signr, int exit_code, int why, unsigned long message)\\n{\\n\\tkernel_siginfo_t info;\\n\\n\\tclear_siginfo(&info);\\n\\tinfo.si_signo = signr;\\n\\tinfo.si_code = exit_code;\\n\\tinfo.si_pid = task_pid_vnr(current);\\n\\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\\n\\n\\t/* Let the debugger run.  */\\n\\treturn ptrace_stop(exit_code, why, message, &info);\\n}\\n\\nint ptrace_notify(int exit_code, unsigned long message)\\n{\\n\\tint signr;\\n\\n\\tBUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);\\n\\tif (unlikely(task_work_pending(current)))\\n\\t\\ttask_work_run();\\n\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\tsignr = ptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED, message);\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\treturn signr;\\n}\\n\\n/**\\n * do_signal_stop - handle group stop for SIGSTOP and other stop signals\\n * @signr: signr causing group stop if initiating\\n *\\n * If %JOBCTL_STOP_PENDING is not set yet, initiate group stop with @signr\\n * and participate in it.  If already set, participate in the existing\\n * group stop.  If participated in a group stop (and thus slept), %true is\\n * returned with siglock released.\\n *\\n * If ptraced, this function doesn\\'t handle stop itself.  Instead,\\n * %JOBCTL_TRAP_STOP is scheduled and %false is returned with siglock\\n * untouched.  The caller must ensure that INTERRUPT trap handling takes\\n * places afterwards.\\n *\\n * CONTEXT:\\n * Must be called with @current->sighand->siglock held, which is released\\n * on %true return.\\n *\\n * RETURNS:\\n * %false if group stop is already cancelled or ptrace trap is scheduled.\\n * %true if participated in group stop.\\n */\\nstatic bool do_signal_stop(int signr)\\n\\t__releases(&current->sighand->siglock)\\n{\\n\\tstruct signal_struct *sig = current->signal;\\n\\n\\tif (!(current->jobctl & JOBCTL_STOP_PENDING)) {\\n\\t\\tunsigned long gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\\n\\t\\tstruct task_struct *t;\\n\\n\\t\\t/* signr will be recorded in task->jobctl for retries */\\n\\t\\tWARN_ON_ONCE(signr & ~JOBCTL_STOP_SIGMASK);\\n\\n\\t\\tif (!likely(current->jobctl & JOBCTL_STOP_DEQUEUED) ||\\n\\t\\t    unlikely(sig->flags & SIGNAL_GROUP_EXIT) ||\\n\\t\\t    unlikely(sig->group_exec_task))\\n\\t\\t\\treturn false;\\n\\t\\t/*\\n\\t\\t * There is no group stop already in progress.  We must\\n\\t\\t * initiate one now.\\n\\t\\t *\\n\\t\\t * While ptraced, a task may be resumed while group stop is\\n\\t\\t * still in effect and then receive a stop signal and\\n\\t\\t * initiate another group stop.  This deviates from the\\n\\t\\t * usual behavior as two consecutive stop signals can\\'t\\n\\t\\t * cause two group stops when !ptraced.  That is why we\\n\\t\\t * also check !task_is_stopped(t) below.\\n\\t\\t *\\n\\t\\t * The condition can be distinguished by testing whether\\n\\t\\t * SIGNAL_STOP_STOPPED is already set.  Don\\'t generate\\n\\t\\t * group_exit_code in such case.\\n\\t\\t *\\n\\t\\t * This is not necessary for SIGNAL_STOP_CONTINUED because\\n\\t\\t * an intervening stop signal is required to cause two\\n\\t\\t * continued events regardless of ptrace.\\n\\t\\t */\\n\\t\\tif (!(sig->flags & SIGNAL_STOP_STOPPED))\\n\\t\\t\\tsig->group_exit_code = signr;\\n\\n\\t\\tsig->group_stop_count = 0;\\n\\t\\tif (task_set_jobctl_pending(current, signr | gstop))\\n\\t\\t\\tsig->group_stop_count++;\\n\\n\\t\\tfor_other_threads(current, t) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Setting state to TASK_STOPPED for a group\\n\\t\\t\\t * stop is always done with the siglock held,\\n\\t\\t\\t * so this check has no races.\\n\\t\\t\\t */\\n\\t\\t\\tif (!task_is_stopped(t) &&\\n\\t\\t\\t    task_set_jobctl_pending(t, signr | gstop)) {\\n\\t\\t\\t\\tsig->group_stop_count++;\\n\\t\\t\\t\\tif (likely(!(t->ptrace & PT_SEIZED)))\\n\\t\\t\\t\\t\\tsignal_wake_up(t, 0);\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tptrace_trap_notify(t);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\tif (likely(!current->ptrace)) {\\n\\t\\tint notify = 0;\\n\\n\\t\\t/*\\n\\t\\t * If there are no other threads in the group, or if there\\n\\t\\t * is a group stop in progress and we are the last to stop,\\n\\t\\t * report to the parent.\\n\\t\\t */\\n\\t\\tif (task_participate_group_stop(current))\\n\\t\\t\\tnotify = CLD_STOPPED;\\n\\n\\t\\tcurrent->jobctl |= JOBCTL_STOPPED;\\n\\t\\tset_special_state(TASK_STOPPED);\\n\\t\\tspin_unlock_irq(&current->sighand->siglock);\\n\\n\\t\\t/*\\n\\t\\t * Notify the parent of the group stop completion.  Because\\n\\t\\t * we\\'re not holding either the siglock or tasklist_lock\\n\\t\\t * here, ptracer may attach inbetween; however, this is for\\n\\t\\t * group stop and should always be delivered to the real\\n\\t\\t * parent of the group leader.  The new ptracer will get\\n\\t\\t * its notification when this task transitions into\\n\\t\\t * TASK_TRACED.\\n\\t\\t */\\n\\t\\tif (notify) {\\n\\t\\t\\tread_lock(&tasklist_lock);\\n\\t\\t\\tdo_notify_parent_cldstop(current, false, notify);\\n\\t\\t\\tread_unlock(&tasklist_lock);\\n\\t\\t}\\n\\n\\t\\t/* Now we don\\'t run again until woken by SIGCONT or SIGKILL */\\n\\t\\tcgroup_enter_frozen();\\n\\t\\tschedule();\\n\\t\\treturn true;\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * While ptraced, group stop is handled by STOP trap.\\n\\t\\t * Schedule it and let the caller deal with it.\\n\\t\\t */\\n\\t\\ttask_set_jobctl_pending(current, JOBCTL_TRAP_STOP);\\n\\t\\treturn false;\\n\\t}\\n}\\n\\n/**\\n * do_jobctl_trap - take care of ptrace jobctl traps\\n *\\n * When PT_SEIZED, it\\'s used for both group stop and explicit\\n * SEIZE/INTERRUPT traps.  Both generate PTRACE_EVENT_STOP trap with\\n * accompanying siginfo.  If stopped, lower eight bits of exit_code contain\\n * the stop signal; otherwise, %SIGTRAP.\\n *\\n * When !PT_SEIZED, it\\'s used only for group stop trap with stop signal\\n * number as exit_code and no siginfo.\\n *\\n * CONTEXT:\\n * Must be called with @current->sighand->siglock held, which may be\\n * released and re-acquired before returning with intervening sleep.\\n */\\nstatic void do_jobctl_trap(void)\\n{\\n\\tstruct signal_struct *signal = current->signal;\\n\\tint signr = current->jobctl & JOBCTL_STOP_SIGMASK;\\n\\n\\tif (current->ptrace & PT_SEIZED) {\\n\\t\\tif (!signal->group_stop_count &&\\n\\t\\t    !(signal->flags & SIGNAL_STOP_STOPPED))\\n\\t\\t\\tsignr = SIGTRAP;\\n\\t\\tWARN_ON_ONCE(!signr);\\n\\t\\tptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),\\n\\t\\t\\t\\t CLD_STOPPED, 0);\\n\\t} else {\\n\\t\\tWARN_ON_ONCE(!signr);\\n\\t\\tptrace_stop(signr, CLD_STOPPED, 0, NULL);\\n\\t}\\n}\\n\\n/**\\n * do_freezer_trap - handle the freezer jobctl trap\\n *\\n * Puts the task into frozen state, if only the task is not about to quit.\\n * In this case it drops JOBCTL_TRAP_FREEZE.\\n *\\n * CONTEXT:\\n * Must be called with @current->sighand->siglock held,\\n * which is always released before returning.\\n */\\nstatic void do_freezer_trap(void)\\n\\t__releases(&current->sighand->siglock)\\n{\\n\\t/*\\n\\t * If there are other trap bits pending except JOBCTL_TRAP_FREEZE,\\n\\t * let\\'s make another loop to give it a chance to be handled.\\n\\t * In any case, we\\'ll return back.\\n\\t */\\n\\tif ((current->jobctl & (JOBCTL_PENDING_MASK | JOBCTL_TRAP_FREEZE)) !=\\n\\t     JOBCTL_TRAP_FREEZE) {\\n\\t\\tspin_unlock_irq(&current->sighand->siglock);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/*\\n\\t * Now we\\'re sure that there is no pending fatal signal and no\\n\\t * pending traps. Clear TIF_SIGPENDING to not get out of schedule()\\n\\t * immediately (if there is a non-fatal signal pending), and\\n\\t * put the task into sleep.\\n\\t */\\n\\t__set_current_state(TASK_INTERRUPTIBLE|TASK_FREEZABLE);\\n\\tclear_thread_flag(TIF_SIGPENDING);\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\tcgroup_enter_frozen();\\n\\tschedule();\\n\\n\\t/*\\n\\t * We could\\'ve been woken by task_work, run it to clear\\n\\t * TIF_NOTIFY_SIGNAL. The caller will retry if necessary.\\n\\t */\\n\\tclear_notify_signal();\\n\\tif (unlikely(task_work_pending(current)))\\n\\t\\ttask_work_run();\\n}\\n\\nstatic int ptrace_signal(int signr, kernel_siginfo_t *info, enum pid_type type)\\n{\\n\\t/*\\n\\t * We do not check sig_kernel_stop(signr) but set this marker\\n\\t * unconditionally because we do not know whether debugger will\\n\\t * change signr. This flag has no meaning unless we are going\\n\\t * to stop after return from ptrace_stop(). In this case it will\\n\\t * be checked in do_signal_stop(), we should only stop if it was\\n\\t * not cleared by SIGCONT while we were sleeping. See also the\\n\\t * comment in dequeue_signal().\\n\\t */\\n\\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\\n\\tsignr = ptrace_stop(signr, CLD_TRAPPED, 0, info);\\n\\n\\t/* We\\'re back.  Did the debugger cancel the sig?  */\\n\\tif (signr == 0)\\n\\t\\treturn signr;\\n\\n\\t/*\\n\\t * Update the siginfo structure if the signal has\\n\\t * changed.  If the debugger wanted something\\n\\t * specific in the siginfo structure then it should\\n\\t * have updated *info via PTRACE_SETSIGINFO.\\n\\t */\\n\\tif (signr != info->si_signo) {\\n\\t\\tclear_siginfo(info);\\n\\t\\tinfo->si_signo = signr;\\n\\t\\tinfo->si_errno = 0;\\n\\t\\tinfo->si_code = SI_USER;\\n\\t\\trcu_read_lock();\\n\\t\\tinfo->si_pid = task_pid_vnr(current->parent);\\n\\t\\tinfo->si_uid = from_kuid_munged(current_user_ns(),\\n\\t\\t\\t\\t\\t\\ttask_uid(current->parent));\\n\\t\\trcu_read_unlock();\\n\\t}\\n\\n\\t/* If the (new) signal is now blocked, requeue it.  */\\n\\tif (sigismember(&current->blocked, signr) ||\\n\\t    fatal_signal_pending(current)) {\\n\\t\\tsend_signal_locked(signr, info, current, type);\\n\\t\\tsignr = 0;\\n\\t}\\n\\n\\treturn signr;\\n}\\n\\nstatic void hide_si_addr_tag_bits(struct ksignal *ksig)\\n{\\n\\tswitch (siginfo_layout(ksig->sig, ksig->info.si_code)) {\\n\\tcase SIL_FAULT:\\n\\tcase SIL_FAULT_TRAPNO:\\n\\tcase SIL_FAULT_MCEERR:\\n\\tcase SIL_FAULT_BNDERR:\\n\\tcase SIL_FAULT_PKUERR:\\n\\tcase SIL_FAULT_PERF_EVENT:\\n\\t\\tksig->info.si_addr = arch_untagged_si_addr(\\n\\t\\t\\tksig->info.si_addr, ksig->sig, ksig->info.si_code);\\n\\t\\tbreak;\\n\\tcase SIL_KILL:\\n\\tcase SIL_TIMER:\\n\\tcase SIL_POLL:\\n\\tcase SIL_CHLD:\\n\\tcase SIL_RT:\\n\\tcase SIL_SYS:\\n\\t\\tbreak;\\n\\t}\\n}\\n\\nbool get_signal(struct ksignal *ksig)\\n{\\n\\tstruct sighand_struct *sighand = current->sighand;\\n\\tstruct signal_struct *signal = current->signal;\\n\\tint signr;\\n\\n\\tclear_notify_signal();\\n\\tif (unlikely(task_work_pending(current)))\\n\\t\\ttask_work_run();\\n\\n\\tif (!task_sigpending(current))\\n\\t\\treturn false;\\n\\n\\tif (unlikely(uprobe_deny_signal()))\\n\\t\\treturn false;\\n\\n\\t/*\\n\\t * Do this once, we can\\'t return to user-mode if freezing() == T.\\n\\t * do_signal_stop() and ptrace_stop() do freezable_schedule() and\\n\\t * thus do not need another check after return.\\n\\t */\\n\\ttry_to_freeze();\\n\\nrelock:\\n\\tspin_lock_irq(&sighand->siglock);\\n\\n\\t/*\\n\\t * Every stopped thread goes here after wakeup. Check to see if\\n\\t * we should notify the parent, prepare_signal(SIGCONT) encodes\\n\\t * the CLD_ si_code into SIGNAL_CLD_MASK bits.\\n\\t */\\n\\tif (unlikely(signal->flags & SIGNAL_CLD_MASK)) {\\n\\t\\tint why;\\n\\n\\t\\tif (signal->flags & SIGNAL_CLD_CONTINUED)\\n\\t\\t\\twhy = CLD_CONTINUED;\\n\\t\\telse\\n\\t\\t\\twhy = CLD_STOPPED;\\n\\n\\t\\tsignal->flags &= ~SIGNAL_CLD_MASK;\\n\\n\\t\\tspin_unlock_irq(&sighand->siglock);\\n\\n\\t\\t/*\\n\\t\\t * Notify the parent that we\\'re continuing.  This event is\\n\\t\\t * always per-process and doesn\\'t make whole lot of sense\\n\\t\\t * for ptracers, who shouldn\\'t consume the state via\\n\\t\\t * wait(2) either, but, for backward compatibility, notify\\n\\t\\t * the ptracer of the group leader too unless it\\'s gonna be\\n\\t\\t * a duplicate.\\n\\t\\t */\\n\\t\\tread_lock(&tasklist_lock);\\n\\t\\tdo_notify_parent_cldstop(current, false, why);\\n\\n\\t\\tif (ptrace_reparented(current->group_leader))\\n\\t\\t\\tdo_notify_parent_cldstop(current->group_leader,\\n\\t\\t\\t\\t\\t\\ttrue, why);\\n\\t\\tread_unlock(&tasklist_lock);\\n\\n\\t\\tgoto relock;\\n\\t}\\n\\n\\tfor (;;) {\\n\\t\\tstruct k_sigaction *ka;\\n\\t\\tenum pid_type type;\\n\\n\\t\\t/* Has this task already been marked for death? */\\n\\t\\tif ((signal->flags & SIGNAL_GROUP_EXIT) ||\\n\\t\\t     signal->group_exec_task) {\\n\\t\\t\\tsignr = SIGKILL;\\n\\t\\t\\tsigdelset(&current->pending.signal, SIGKILL);\\n\\t\\t\\ttrace_signal_deliver(SIGKILL, SEND_SIG_NOINFO,\\n\\t\\t\\t\\t\\t     &sighand->action[SIGKILL-1]);\\n\\t\\t\\trecalc_sigpending();\\n\\t\\t\\t/*\\n\\t\\t\\t * implies do_group_exit() or return to PF_USER_WORKER,\\n\\t\\t\\t * no need to initialize ksig->info/etc.\\n\\t\\t\\t */\\n\\t\\t\\tgoto fatal;\\n\\t\\t}\\n\\n\\t\\tif (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&\\n\\t\\t    do_signal_stop(0))\\n\\t\\t\\tgoto relock;\\n\\n\\t\\tif (unlikely(current->jobctl &\\n\\t\\t\\t     (JOBCTL_TRAP_MASK | JOBCTL_TRAP_FREEZE))) {\\n\\t\\t\\tif (current->jobctl & JOBCTL_TRAP_MASK) {\\n\\t\\t\\t\\tdo_jobctl_trap();\\n\\t\\t\\t\\tspin_unlock_irq(&sighand->siglock);\\n\\t\\t\\t} else if (current->jobctl & JOBCTL_TRAP_FREEZE)\\n\\t\\t\\t\\tdo_freezer_trap();\\n\\n\\t\\t\\tgoto relock;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * If the task is leaving the frozen state, let\\'s update\\n\\t\\t * cgroup counters and reset the frozen bit.\\n\\t\\t */\\n\\t\\tif (unlikely(cgroup_task_frozen(current))) {\\n\\t\\t\\tspin_unlock_irq(&sighand->siglock);\\n\\t\\t\\tcgroup_leave_frozen(false);\\n\\t\\t\\tgoto relock;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Signals generated by the execution of an instruction\\n\\t\\t * need to be delivered before any other pending signals\\n\\t\\t * so that the instruction pointer in the signal stack\\n\\t\\t * frame points to the faulting instruction.\\n\\t\\t */\\n\\t\\ttype = PIDTYPE_PID;\\n\\t\\tsignr = dequeue_synchronous_signal(&ksig->info);\\n\\t\\tif (!signr)\\n\\t\\t\\tsignr = dequeue_signal(&current->blocked, &ksig->info, &type);\\n\\n\\t\\tif (!signr)\\n\\t\\t\\tbreak; /* will return 0 */\\n\\n\\t\\tif (unlikely(current->ptrace) && (signr != SIGKILL) &&\\n\\t\\t    !(sighand->action[signr -1].sa.sa_flags & SA_IMMUTABLE)) {\\n\\t\\t\\tsignr = ptrace_signal(signr, &ksig->info, type);\\n\\t\\t\\tif (!signr)\\n\\t\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tka = &sighand->action[signr-1];\\n\\n\\t\\t/* Trace actually delivered signals. */\\n\\t\\ttrace_signal_deliver(signr, &ksig->info, ka);\\n\\n\\t\\tif (ka->sa.sa_handler == SIG_IGN) /* Do nothing.  */\\n\\t\\t\\tcontinue;\\n\\t\\tif (ka->sa.sa_handler != SIG_DFL) {\\n\\t\\t\\t/* Run the handler.  */\\n\\t\\t\\tksig->ka = *ka;\\n\\n\\t\\t\\tif (ka->sa.sa_flags & SA_ONESHOT)\\n\\t\\t\\t\\tka->sa.sa_handler = SIG_DFL;\\n\\n\\t\\t\\tbreak; /* will return non-zero \"signr\" value */\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Now we are doing the default action for this signal.\\n\\t\\t */\\n\\t\\tif (sig_kernel_ignore(signr)) /* Default is nothing. */\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * Global init gets no signals it doesn\\'t want.\\n\\t\\t * Container-init gets no signals it doesn\\'t want from same\\n\\t\\t * container.\\n\\t\\t *\\n\\t\\t * Note that if global/container-init sees a sig_kernel_only()\\n\\t\\t * signal here, the signal must have been generated internally\\n\\t\\t * or must have come from an ancestor namespace. In either\\n\\t\\t * case, the signal cannot be dropped.\\n\\t\\t */\\n\\t\\tif (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&\\n\\t\\t\\t\\t!sig_kernel_only(signr))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (sig_kernel_stop(signr)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * The default action is to stop all threads in\\n\\t\\t\\t * the thread group.  The job control signals\\n\\t\\t\\t * do nothing in an orphaned pgrp, but SIGSTOP\\n\\t\\t\\t * always works.  Note that siglock needs to be\\n\\t\\t\\t * dropped during the call to is_orphaned_pgrp()\\n\\t\\t\\t * because of lock ordering with tasklist_lock.\\n\\t\\t\\t * This allows an intervening SIGCONT to be posted.\\n\\t\\t\\t * We need to check for that and bail out if necessary.\\n\\t\\t\\t */\\n\\t\\t\\tif (signr != SIGSTOP) {\\n\\t\\t\\t\\tspin_unlock_irq(&sighand->siglock);\\n\\n\\t\\t\\t\\t/* signals can be posted during this window */\\n\\n\\t\\t\\t\\tif (is_current_pgrp_orphaned())\\n\\t\\t\\t\\t\\tgoto relock;\\n\\n\\t\\t\\t\\tspin_lock_irq(&sighand->siglock);\\n\\t\\t\\t}\\n\\n\\t\\t\\tif (likely(do_signal_stop(signr))) {\\n\\t\\t\\t\\t/* It released the siglock.  */\\n\\t\\t\\t\\tgoto relock;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * We didn\\'t actually stop, due to a race\\n\\t\\t\\t * with SIGCONT or something like that.\\n\\t\\t\\t */\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\tfatal:\\n\\t\\tspin_unlock_irq(&sighand->siglock);\\n\\t\\tif (unlikely(cgroup_task_frozen(current)))\\n\\t\\t\\tcgroup_leave_frozen(true);\\n\\n\\t\\t/*\\n\\t\\t * Anything else is fatal, maybe with a core dump.\\n\\t\\t */\\n\\t\\tcurrent->flags |= PF_SIGNALED;\\n\\n\\t\\tif (sig_kernel_coredump(signr)) {\\n\\t\\t\\tif (print_fatal_signals)\\n\\t\\t\\t\\tprint_fatal_signal(signr);\\n\\t\\t\\tproc_coredump_connector(current);\\n\\t\\t\\t/*\\n\\t\\t\\t * If it was able to dump core, this kills all\\n\\t\\t\\t * other threads in the group and synchronizes with\\n\\t\\t\\t * their demise.  If we lost the race with another\\n\\t\\t\\t * thread getting here, it set group_exit_code\\n\\t\\t\\t * first and our do_group_exit call below will use\\n\\t\\t\\t * that value and ignore the one we pass it.\\n\\t\\t\\t */\\n\\t\\t\\tdo_coredump(&ksig->info);\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * PF_USER_WORKER threads will catch and exit on fatal signals\\n\\t\\t * themselves. They have cleanup that must be performed, so we\\n\\t\\t * cannot call do_exit() on their behalf. Note that ksig won\\'t\\n\\t\\t * be properly initialized, PF_USER_WORKER\\'s shouldn\\'t use it.\\n\\t\\t */\\n\\t\\tif (current->flags & PF_USER_WORKER)\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/*\\n\\t\\t * Death signals, no core dump.\\n\\t\\t */\\n\\t\\tdo_group_exit(signr);\\n\\t\\t/* NOTREACHED */\\n\\t}\\n\\tspin_unlock_irq(&sighand->siglock);\\n\\n\\tksig->sig = signr;\\n\\n\\tif (signr && !(ksig->ka.sa.sa_flags & SA_EXPOSE_TAGBITS))\\n\\t\\thide_si_addr_tag_bits(ksig);\\nout:\\n\\treturn signr > 0;\\n}\\n\\n/**\\n * signal_delivered - called after signal delivery to update blocked signals\\n * @ksig:\\t\\tkernel signal struct\\n * @stepping:\\t\\tnonzero if debugger single-step or block-step in use\\n *\\n * This function should be called when a signal has successfully been\\n * delivered. It updates the blocked signals accordingly (@ksig->ka.sa.sa_mask\\n * is always blocked), and the signal itself is blocked unless %SA_NODEFER\\n * is set in @ksig->ka.sa.sa_flags.  Tracing is notified.\\n */\\nstatic void signal_delivered(struct ksignal *ksig, int stepping)\\n{\\n\\tsigset_t blocked;\\n\\n\\t/* A signal was successfully delivered, and the\\n\\t   saved sigmask was stored on the signal frame,\\n\\t   and will be restored by sigreturn.  So we can\\n\\t   simply clear the restore sigmask flag.  */\\n\\tclear_restore_sigmask();\\n\\n\\tsigorsets(&blocked, &current->blocked, &ksig->ka.sa.sa_mask);\\n\\tif (!(ksig->ka.sa.sa_flags & SA_NODEFER))\\n\\t\\tsigaddset(&blocked, ksig->sig);\\n\\tset_current_blocked(&blocked);\\n\\tif (current->sas_ss_flags & SS_AUTODISARM)\\n\\t\\tsas_ss_reset(current);\\n\\tif (stepping)\\n\\t\\tptrace_notify(SIGTRAP, 0);\\n}\\n\\nvoid signal_setup_done(int failed, struct ksignal *ksig, int stepping)\\n{\\n\\tif (failed)\\n\\t\\tforce_sigsegv(ksig->sig);\\n\\telse\\n\\t\\tsignal_delivered(ksig, stepping);\\n}\\n\\n/*\\n * It could be that complete_signal() picked us to notify about the\\n * group-wide signal. Other threads should be notified now to take\\n * the shared signals in @which since we will not.\\n */\\nstatic void retarget_shared_pending(struct task_struct *tsk, sigset_t *which)\\n{\\n\\tsigset_t retarget;\\n\\tstruct task_struct *t;\\n\\n\\tsigandsets(&retarget, &tsk->signal->shared_pending.signal, which);\\n\\tif (sigisemptyset(&retarget))\\n\\t\\treturn;\\n\\n\\tfor_other_threads(tsk, t) {\\n\\t\\tif (t->flags & PF_EXITING)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (!has_pending_signals(&retarget, &t->blocked))\\n\\t\\t\\tcontinue;\\n\\t\\t/* Remove the signals this thread can handle. */\\n\\t\\tsigandsets(&retarget, &retarget, &t->blocked);\\n\\n\\t\\tif (!task_sigpending(t))\\n\\t\\t\\tsignal_wake_up(t, 0);\\n\\n\\t\\tif (sigisemptyset(&retarget))\\n\\t\\t\\tbreak;\\n\\t}\\n}\\n\\nvoid exit_signals(struct task_struct *tsk)\\n{\\n\\tint group_stop = 0;\\n\\tsigset_t unblocked;\\n\\n\\t/*\\n\\t * @tsk is about to have PF_EXITING set - lock out users which\\n\\t * expect stable threadgroup.\\n\\t */\\n\\tcgroup_threadgroup_change_begin(tsk);\\n\\n\\tif (thread_group_empty(tsk) || (tsk->signal->flags & SIGNAL_GROUP_EXIT)) {\\n\\t\\tsched_mm_cid_exit_signals(tsk);\\n\\t\\ttsk->flags |= PF_EXITING;\\n\\t\\tcgroup_threadgroup_change_end(tsk);\\n\\t\\treturn;\\n\\t}\\n\\n\\tspin_lock_irq(&tsk->sighand->siglock);\\n\\t/*\\n\\t * From now this task is not visible for group-wide signals,\\n\\t * see wants_signal(), do_signal_stop().\\n\\t */\\n\\tsched_mm_cid_exit_signals(tsk);\\n\\ttsk->flags |= PF_EXITING;\\n\\n\\tcgroup_threadgroup_change_end(tsk);\\n\\n\\tif (!task_sigpending(tsk))\\n\\t\\tgoto out;\\n\\n\\tunblocked = tsk->blocked;\\n\\tsignotset(&unblocked);\\n\\tretarget_shared_pending(tsk, &unblocked);\\n\\n\\tif (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&\\n\\t    task_participate_group_stop(tsk))\\n\\t\\tgroup_stop = CLD_STOPPED;\\nout:\\n\\tspin_unlock_irq(&tsk->sighand->siglock);\\n\\n\\t/*\\n\\t * If group stop has completed, deliver the notification.  This\\n\\t * should always go to the real parent of the group leader.\\n\\t */\\n\\tif (unlikely(group_stop)) {\\n\\t\\tread_lock(&tasklist_lock);\\n\\t\\tdo_notify_parent_cldstop(tsk, false, group_stop);\\n\\t\\tread_unlock(&tasklist_lock);\\n\\t}\\n}\\n\\n/*\\n * System call entry points.\\n */\\n\\n/**\\n *  sys_restart_syscall - restart a system call\\n */\\nSYSCALL_DEFINE0(restart_syscall)\\n{\\n\\tstruct restart_block *restart = &current->restart_block;\\n\\treturn restart->fn(restart);\\n}\\n\\nlong do_no_restart_syscall(struct restart_block *param)\\n{\\n\\treturn -EINTR;\\n}\\n\\nstatic void __set_task_blocked(struct task_struct *tsk, const sigset_t *newset)\\n{\\n\\tif (task_sigpending(tsk) && !thread_group_empty(tsk)) {\\n\\t\\tsigset_t newblocked;\\n\\t\\t/* A set of now blocked but previously unblocked signals. */\\n\\t\\tsigandnsets(&newblocked, newset, &current->blocked);\\n\\t\\tretarget_shared_pending(tsk, &newblocked);\\n\\t}\\n\\ttsk->blocked = *newset;\\n\\trecalc_sigpending();\\n}\\n\\n/**\\n * set_current_blocked - change current->blocked mask\\n * @newset: new mask\\n *\\n * It is wrong to change ->blocked directly, this helper should be used\\n * to ensure the process can\\'t miss a shared signal we are going to block.\\n */\\nvoid set_current_blocked(sigset_t *newset)\\n{\\n\\tsigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));\\n\\t__set_current_blocked(newset);\\n}\\n\\nvoid __set_current_blocked(const sigset_t *newset)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\n\\t/*\\n\\t * In case the signal mask hasn\\'t changed, there is nothing we need\\n\\t * to do. The current->blocked shouldn\\'t be modified by other task.\\n\\t */\\n\\tif (sigequalsets(&tsk->blocked, newset))\\n\\t\\treturn;\\n\\n\\tspin_lock_irq(&tsk->sighand->siglock);\\n\\t__set_task_blocked(tsk, newset);\\n\\tspin_unlock_irq(&tsk->sighand->siglock);\\n}\\n\\n/*\\n * This is also useful for kernel threads that want to temporarily\\n * (or permanently) block certain signals.\\n *\\n * NOTE! Unlike the user-mode sys_sigprocmask(), the kernel\\n * interface happily blocks \"unblockable\" signals like SIGKILL\\n * and friends.\\n */\\nint sigprocmask(int how, sigset_t *set, sigset_t *oldset)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\tsigset_t newset;\\n\\n\\t/* Lockless, only current can change ->blocked, never from irq */\\n\\tif (oldset)\\n\\t\\t*oldset = tsk->blocked;\\n\\n\\tswitch (how) {\\n\\tcase SIG_BLOCK:\\n\\t\\tsigorsets(&newset, &tsk->blocked, set);\\n\\t\\tbreak;\\n\\tcase SIG_UNBLOCK:\\n\\t\\tsigandnsets(&newset, &tsk->blocked, set);\\n\\t\\tbreak;\\n\\tcase SIG_SETMASK:\\n\\t\\tnewset = *set;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\t__set_current_blocked(&newset);\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(sigprocmask);\\n\\n/*\\n * The api helps set app-provided sigmasks.\\n *\\n * This is useful for syscalls such as ppoll, pselect, io_pgetevents and\\n * epoll_pwait where a new sigmask is passed from userland for the syscalls.\\n *\\n * Note that it does set_restore_sigmask() in advance, so it must be always\\n * paired with restore_saved_sigmask_unless() before return from syscall.\\n */\\nint set_user_sigmask(const sigset_t __user *umask, size_t sigsetsize)\\n{\\n\\tsigset_t kmask;\\n\\n\\tif (!umask)\\n\\t\\treturn 0;\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\tif (copy_from_user(&kmask, umask, sizeof(sigset_t)))\\n\\t\\treturn -EFAULT;\\n\\n\\tset_restore_sigmask();\\n\\tcurrent->saved_sigmask = current->blocked;\\n\\tset_current_blocked(&kmask);\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nint set_compat_user_sigmask(const compat_sigset_t __user *umask,\\n\\t\\t\\t    size_t sigsetsize)\\n{\\n\\tsigset_t kmask;\\n\\n\\tif (!umask)\\n\\t\\treturn 0;\\n\\tif (sigsetsize != sizeof(compat_sigset_t))\\n\\t\\treturn -EINVAL;\\n\\tif (get_compat_sigset(&kmask, umask))\\n\\t\\treturn -EFAULT;\\n\\n\\tset_restore_sigmask();\\n\\tcurrent->saved_sigmask = current->blocked;\\n\\tset_current_blocked(&kmask);\\n\\n\\treturn 0;\\n}\\n#endif\\n\\n/**\\n *  sys_rt_sigprocmask - change the list of currently blocked signals\\n *  @how: whether to add, remove, or set signals\\n *  @nset: stores pending signals\\n *  @oset: previous value of signal mask if non-null\\n *  @sigsetsize: size of sigset_t type\\n */\\nSYSCALL_DEFINE4(rt_sigprocmask, int, how, sigset_t __user *, nset,\\n\\t\\tsigset_t __user *, oset, size_t, sigsetsize)\\n{\\n\\tsigset_t old_set, new_set;\\n\\tint error;\\n\\n\\t/* XXX: Don\\'t preclude handling different sized sigset_t\\'s.  */\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\told_set = current->blocked;\\n\\n\\tif (nset) {\\n\\t\\tif (copy_from_user(&new_set, nset, sizeof(sigset_t)))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\\n\\n\\t\\terror = sigprocmask(how, &new_set, NULL);\\n\\t\\tif (error)\\n\\t\\t\\treturn error;\\n\\t}\\n\\n\\tif (oset) {\\n\\t\\tif (copy_to_user(oset, &old_set, sizeof(sigset_t)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE4(rt_sigprocmask, int, how, compat_sigset_t __user *, nset,\\n\\t\\tcompat_sigset_t __user *, oset, compat_size_t, sigsetsize)\\n{\\n\\tsigset_t old_set = current->blocked;\\n\\n\\t/* XXX: Don\\'t preclude handling different sized sigset_t\\'s.  */\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (nset) {\\n\\t\\tsigset_t new_set;\\n\\t\\tint error;\\n\\t\\tif (get_compat_sigset(&new_set, nset))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\\n\\n\\t\\terror = sigprocmask(how, &new_set, NULL);\\n\\t\\tif (error)\\n\\t\\t\\treturn error;\\n\\t}\\n\\treturn oset ? put_compat_sigset(oset, &old_set, sizeof(*oset)) : 0;\\n}\\n#endif\\n\\nstatic void do_sigpending(sigset_t *set)\\n{\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\tsigorsets(set, &current->pending.signal,\\n\\t\\t  &current->signal->shared_pending.signal);\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\n\\t/* Outside the lock because only this thread touches it.  */\\n\\tsigandsets(set, &current->blocked, set);\\n}\\n\\n/**\\n *  sys_rt_sigpending - examine a pending signal that has been raised\\n *\\t\\t\\twhile blocked\\n *  @uset: stores pending signals\\n *  @sigsetsize: size of sigset_t type or larger\\n */\\nSYSCALL_DEFINE2(rt_sigpending, sigset_t __user *, uset, size_t, sigsetsize)\\n{\\n\\tsigset_t set;\\n\\n\\tif (sigsetsize > sizeof(*uset))\\n\\t\\treturn -EINVAL;\\n\\n\\tdo_sigpending(&set);\\n\\n\\tif (copy_to_user(uset, &set, sigsetsize))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE2(rt_sigpending, compat_sigset_t __user *, uset,\\n\\t\\tcompat_size_t, sigsetsize)\\n{\\n\\tsigset_t set;\\n\\n\\tif (sigsetsize > sizeof(*uset))\\n\\t\\treturn -EINVAL;\\n\\n\\tdo_sigpending(&set);\\n\\n\\treturn put_compat_sigset(uset, &set, sigsetsize);\\n}\\n#endif\\n\\nstatic const struct {\\n\\tunsigned char limit, layout;\\n} sig_sicodes[] = {\\n\\t[SIGILL]  = { NSIGILL,  SIL_FAULT },\\n\\t[SIGFPE]  = { NSIGFPE,  SIL_FAULT },\\n\\t[SIGSEGV] = { NSIGSEGV, SIL_FAULT },\\n\\t[SIGBUS]  = { NSIGBUS,  SIL_FAULT },\\n\\t[SIGTRAP] = { NSIGTRAP, SIL_FAULT },\\n#if defined(SIGEMT)\\n\\t[SIGEMT]  = { NSIGEMT,  SIL_FAULT },\\n#endif\\n\\t[SIGCHLD] = { NSIGCHLD, SIL_CHLD },\\n\\t[SIGPOLL] = { NSIGPOLL, SIL_POLL },\\n\\t[SIGSYS]  = { NSIGSYS,  SIL_SYS },\\n};\\n\\nstatic bool known_siginfo_layout(unsigned sig, int si_code)\\n{\\n\\tif (si_code == SI_KERNEL)\\n\\t\\treturn true;\\n\\telse if ((si_code > SI_USER)) {\\n\\t\\tif (sig_specific_sicodes(sig)) {\\n\\t\\t\\tif (si_code <= sig_sicodes[sig].limit)\\n\\t\\t\\t\\treturn true;\\n\\t\\t}\\n\\t\\telse if (si_code <= NSIGPOLL)\\n\\t\\t\\treturn true;\\n\\t}\\n\\telse if (si_code >= SI_DETHREAD)\\n\\t\\treturn true;\\n\\telse if (si_code == SI_ASYNCNL)\\n\\t\\treturn true;\\n\\treturn false;\\n}\\n\\nenum siginfo_layout siginfo_layout(unsigned sig, int si_code)\\n{\\n\\tenum siginfo_layout layout = SIL_KILL;\\n\\tif ((si_code > SI_USER) && (si_code < SI_KERNEL)) {\\n\\t\\tif ((sig < ARRAY_SIZE(sig_sicodes)) &&\\n\\t\\t    (si_code <= sig_sicodes[sig].limit)) {\\n\\t\\t\\tlayout = sig_sicodes[sig].layout;\\n\\t\\t\\t/* Handle the exceptions */\\n\\t\\t\\tif ((sig == SIGBUS) &&\\n\\t\\t\\t    (si_code >= BUS_MCEERR_AR) && (si_code <= BUS_MCEERR_AO))\\n\\t\\t\\t\\tlayout = SIL_FAULT_MCEERR;\\n\\t\\t\\telse if ((sig == SIGSEGV) && (si_code == SEGV_BNDERR))\\n\\t\\t\\t\\tlayout = SIL_FAULT_BNDERR;\\n#ifdef SEGV_PKUERR\\n\\t\\t\\telse if ((sig == SIGSEGV) && (si_code == SEGV_PKUERR))\\n\\t\\t\\t\\tlayout = SIL_FAULT_PKUERR;\\n#endif\\n\\t\\t\\telse if ((sig == SIGTRAP) && (si_code == TRAP_PERF))\\n\\t\\t\\t\\tlayout = SIL_FAULT_PERF_EVENT;\\n\\t\\t\\telse if (IS_ENABLED(CONFIG_SPARC) &&\\n\\t\\t\\t\\t (sig == SIGILL) && (si_code == ILL_ILLTRP))\\n\\t\\t\\t\\tlayout = SIL_FAULT_TRAPNO;\\n\\t\\t\\telse if (IS_ENABLED(CONFIG_ALPHA) &&\\n\\t\\t\\t\\t ((sig == SIGFPE) ||\\n\\t\\t\\t\\t  ((sig == SIGTRAP) && (si_code == TRAP_UNK))))\\n\\t\\t\\t\\tlayout = SIL_FAULT_TRAPNO;\\n\\t\\t}\\n\\t\\telse if (si_code <= NSIGPOLL)\\n\\t\\t\\tlayout = SIL_POLL;\\n\\t} else {\\n\\t\\tif (si_code == SI_TIMER)\\n\\t\\t\\tlayout = SIL_TIMER;\\n\\t\\telse if (si_code == SI_SIGIO)\\n\\t\\t\\tlayout = SIL_POLL;\\n\\t\\telse if (si_code < 0)\\n\\t\\t\\tlayout = SIL_RT;\\n\\t}\\n\\treturn layout;\\n}\\n\\nstatic inline char __user *si_expansion(const siginfo_t __user *info)\\n{\\n\\treturn ((char __user *)info) + sizeof(struct kernel_siginfo);\\n}\\n\\nint copy_siginfo_to_user(siginfo_t __user *to, const kernel_siginfo_t *from)\\n{\\n\\tchar __user *expansion = si_expansion(to);\\n\\tif (copy_to_user(to, from , sizeof(struct kernel_siginfo)))\\n\\t\\treturn -EFAULT;\\n\\tif (clear_user(expansion, SI_EXPANSION_SIZE))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\nstatic int post_copy_siginfo_from_user(kernel_siginfo_t *info,\\n\\t\\t\\t\\t       const siginfo_t __user *from)\\n{\\n\\tif (unlikely(!known_siginfo_layout(info->si_signo, info->si_code))) {\\n\\t\\tchar __user *expansion = si_expansion(from);\\n\\t\\tchar buf[SI_EXPANSION_SIZE];\\n\\t\\tint i;\\n\\t\\t/*\\n\\t\\t * An unknown si_code might need more than\\n\\t\\t * sizeof(struct kernel_siginfo) bytes.  Verify all of the\\n\\t\\t * extra bytes are 0.  This guarantees copy_siginfo_to_user\\n\\t\\t * will return this data to userspace exactly.\\n\\t\\t */\\n\\t\\tif (copy_from_user(&buf, expansion, SI_EXPANSION_SIZE))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tfor (i = 0; i < SI_EXPANSION_SIZE; i++) {\\n\\t\\t\\tif (buf[i] != 0)\\n\\t\\t\\t\\treturn -E2BIG;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int __copy_siginfo_from_user(int signo, kernel_siginfo_t *to,\\n\\t\\t\\t\\t    const siginfo_t __user *from)\\n{\\n\\tif (copy_from_user(to, from, sizeof(struct kernel_siginfo)))\\n\\t\\treturn -EFAULT;\\n\\tto->si_signo = signo;\\n\\treturn post_copy_siginfo_from_user(to, from);\\n}\\n\\nint copy_siginfo_from_user(kernel_siginfo_t *to, const siginfo_t __user *from)\\n{\\n\\tif (copy_from_user(to, from, sizeof(struct kernel_siginfo)))\\n\\t\\treturn -EFAULT;\\n\\treturn post_copy_siginfo_from_user(to, from);\\n}\\n\\n#ifdef CONFIG_COMPAT\\n/**\\n * copy_siginfo_to_external32 - copy a kernel siginfo into a compat user siginfo\\n * @to: compat siginfo destination\\n * @from: kernel siginfo source\\n *\\n * Note: This function does not work properly for the SIGCHLD on x32, but\\n * fortunately it doesn\\'t have to.  The only valid callers for this function are\\n * copy_siginfo_to_user32, which is overriden for x32 and the coredump code.\\n * The latter does not care because SIGCHLD will never cause a coredump.\\n */\\nvoid copy_siginfo_to_external32(struct compat_siginfo *to,\\n\\t\\tconst struct kernel_siginfo *from)\\n{\\n\\tmemset(to, 0, sizeof(*to));\\n\\n\\tto->si_signo = from->si_signo;\\n\\tto->si_errno = from->si_errno;\\n\\tto->si_code  = from->si_code;\\n\\tswitch(siginfo_layout(from->si_signo, from->si_code)) {\\n\\tcase SIL_KILL:\\n\\t\\tto->si_pid = from->si_pid;\\n\\t\\tto->si_uid = from->si_uid;\\n\\t\\tbreak;\\n\\tcase SIL_TIMER:\\n\\t\\tto->si_tid     = from->si_tid;\\n\\t\\tto->si_overrun = from->si_overrun;\\n\\t\\tto->si_int     = from->si_int;\\n\\t\\tbreak;\\n\\tcase SIL_POLL:\\n\\t\\tto->si_band = from->si_band;\\n\\t\\tto->si_fd   = from->si_fd;\\n\\t\\tbreak;\\n\\tcase SIL_FAULT:\\n\\t\\tto->si_addr = ptr_to_compat(from->si_addr);\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_TRAPNO:\\n\\t\\tto->si_addr = ptr_to_compat(from->si_addr);\\n\\t\\tto->si_trapno = from->si_trapno;\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_MCEERR:\\n\\t\\tto->si_addr = ptr_to_compat(from->si_addr);\\n\\t\\tto->si_addr_lsb = from->si_addr_lsb;\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_BNDERR:\\n\\t\\tto->si_addr = ptr_to_compat(from->si_addr);\\n\\t\\tto->si_lower = ptr_to_compat(from->si_lower);\\n\\t\\tto->si_upper = ptr_to_compat(from->si_upper);\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_PKUERR:\\n\\t\\tto->si_addr = ptr_to_compat(from->si_addr);\\n\\t\\tto->si_pkey = from->si_pkey;\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_PERF_EVENT:\\n\\t\\tto->si_addr = ptr_to_compat(from->si_addr);\\n\\t\\tto->si_perf_data = from->si_perf_data;\\n\\t\\tto->si_perf_type = from->si_perf_type;\\n\\t\\tto->si_perf_flags = from->si_perf_flags;\\n\\t\\tbreak;\\n\\tcase SIL_CHLD:\\n\\t\\tto->si_pid = from->si_pid;\\n\\t\\tto->si_uid = from->si_uid;\\n\\t\\tto->si_status = from->si_status;\\n\\t\\tto->si_utime = from->si_utime;\\n\\t\\tto->si_stime = from->si_stime;\\n\\t\\tbreak;\\n\\tcase SIL_RT:\\n\\t\\tto->si_pid = from->si_pid;\\n\\t\\tto->si_uid = from->si_uid;\\n\\t\\tto->si_int = from->si_int;\\n\\t\\tbreak;\\n\\tcase SIL_SYS:\\n\\t\\tto->si_call_addr = ptr_to_compat(from->si_call_addr);\\n\\t\\tto->si_syscall   = from->si_syscall;\\n\\t\\tto->si_arch      = from->si_arch;\\n\\t\\tbreak;\\n\\t}\\n}\\n\\nint __copy_siginfo_to_user32(struct compat_siginfo __user *to,\\n\\t\\t\\t   const struct kernel_siginfo *from)\\n{\\n\\tstruct compat_siginfo new;\\n\\n\\tcopy_siginfo_to_external32(&new, from);\\n\\tif (copy_to_user(to, &new, sizeof(struct compat_siginfo)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\nstatic int post_copy_siginfo_from_user32(kernel_siginfo_t *to,\\n\\t\\t\\t\\t\\t const struct compat_siginfo *from)\\n{\\n\\tclear_siginfo(to);\\n\\tto->si_signo = from->si_signo;\\n\\tto->si_errno = from->si_errno;\\n\\tto->si_code  = from->si_code;\\n\\tswitch(siginfo_layout(from->si_signo, from->si_code)) {\\n\\tcase SIL_KILL:\\n\\t\\tto->si_pid = from->si_pid;\\n\\t\\tto->si_uid = from->si_uid;\\n\\t\\tbreak;\\n\\tcase SIL_TIMER:\\n\\t\\tto->si_tid     = from->si_tid;\\n\\t\\tto->si_overrun = from->si_overrun;\\n\\t\\tto->si_int     = from->si_int;\\n\\t\\tbreak;\\n\\tcase SIL_POLL:\\n\\t\\tto->si_band = from->si_band;\\n\\t\\tto->si_fd   = from->si_fd;\\n\\t\\tbreak;\\n\\tcase SIL_FAULT:\\n\\t\\tto->si_addr = compat_ptr(from->si_addr);\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_TRAPNO:\\n\\t\\tto->si_addr = compat_ptr(from->si_addr);\\n\\t\\tto->si_trapno = from->si_trapno;\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_MCEERR:\\n\\t\\tto->si_addr = compat_ptr(from->si_addr);\\n\\t\\tto->si_addr_lsb = from->si_addr_lsb;\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_BNDERR:\\n\\t\\tto->si_addr = compat_ptr(from->si_addr);\\n\\t\\tto->si_lower = compat_ptr(from->si_lower);\\n\\t\\tto->si_upper = compat_ptr(from->si_upper);\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_PKUERR:\\n\\t\\tto->si_addr = compat_ptr(from->si_addr);\\n\\t\\tto->si_pkey = from->si_pkey;\\n\\t\\tbreak;\\n\\tcase SIL_FAULT_PERF_EVENT:\\n\\t\\tto->si_addr = compat_ptr(from->si_addr);\\n\\t\\tto->si_perf_data = from->si_perf_data;\\n\\t\\tto->si_perf_type = from->si_perf_type;\\n\\t\\tto->si_perf_flags = from->si_perf_flags;\\n\\t\\tbreak;\\n\\tcase SIL_CHLD:\\n\\t\\tto->si_pid    = from->si_pid;\\n\\t\\tto->si_uid    = from->si_uid;\\n\\t\\tto->si_status = from->si_status;\\n#ifdef CONFIG_X86_X32_ABI\\n\\t\\tif (in_x32_syscall()) {\\n\\t\\t\\tto->si_utime = from->_sifields._sigchld_x32._utime;\\n\\t\\t\\tto->si_stime = from->_sifields._sigchld_x32._stime;\\n\\t\\t} else\\n#endif\\n\\t\\t{\\n\\t\\t\\tto->si_utime = from->si_utime;\\n\\t\\t\\tto->si_stime = from->si_stime;\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase SIL_RT:\\n\\t\\tto->si_pid = from->si_pid;\\n\\t\\tto->si_uid = from->si_uid;\\n\\t\\tto->si_int = from->si_int;\\n\\t\\tbreak;\\n\\tcase SIL_SYS:\\n\\t\\tto->si_call_addr = compat_ptr(from->si_call_addr);\\n\\t\\tto->si_syscall   = from->si_syscall;\\n\\t\\tto->si_arch      = from->si_arch;\\n\\t\\tbreak;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int __copy_siginfo_from_user32(int signo, struct kernel_siginfo *to,\\n\\t\\t\\t\\t      const struct compat_siginfo __user *ufrom)\\n{\\n\\tstruct compat_siginfo from;\\n\\n\\tif (copy_from_user(&from, ufrom, sizeof(struct compat_siginfo)))\\n\\t\\treturn -EFAULT;\\n\\n\\tfrom.si_signo = signo;\\n\\treturn post_copy_siginfo_from_user32(to, &from);\\n}\\n\\nint copy_siginfo_from_user32(struct kernel_siginfo *to,\\n\\t\\t\\t     const struct compat_siginfo __user *ufrom)\\n{\\n\\tstruct compat_siginfo from;\\n\\n\\tif (copy_from_user(&from, ufrom, sizeof(struct compat_siginfo)))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn post_copy_siginfo_from_user32(to, &from);\\n}\\n#endif /* CONFIG_COMPAT */\\n\\n/**\\n *  do_sigtimedwait - wait for queued signals specified in @which\\n *  @which: queued signals to wait for\\n *  @info: if non-null, the signal\\'s siginfo is returned here\\n *  @ts: upper bound on process time suspension\\n */\\nstatic int do_sigtimedwait(const sigset_t *which, kernel_siginfo_t *info,\\n\\t\\t    const struct timespec64 *ts)\\n{\\n\\tktime_t *to = NULL, timeout = KTIME_MAX;\\n\\tstruct task_struct *tsk = current;\\n\\tsigset_t mask = *which;\\n\\tenum pid_type type;\\n\\tint sig, ret = 0;\\n\\n\\tif (ts) {\\n\\t\\tif (!timespec64_valid(ts))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\ttimeout = timespec64_to_ktime(*ts);\\n\\t\\tto = &timeout;\\n\\t}\\n\\n\\t/*\\n\\t * Invert the set of allowed signals to get those we want to block.\\n\\t */\\n\\tsigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));\\n\\tsignotset(&mask);\\n\\n\\tspin_lock_irq(&tsk->sighand->siglock);\\n\\tsig = dequeue_signal(&mask, info, &type);\\n\\tif (!sig && timeout) {\\n\\t\\t/*\\n\\t\\t * None ready, temporarily unblock those we\\'re interested\\n\\t\\t * while we are sleeping in so that we\\'ll be awakened when\\n\\t\\t * they arrive. Unblocking is always fine, we can avoid\\n\\t\\t * set_current_blocked().\\n\\t\\t */\\n\\t\\ttsk->real_blocked = tsk->blocked;\\n\\t\\tsigandsets(&tsk->blocked, &tsk->blocked, &mask);\\n\\t\\trecalc_sigpending();\\n\\t\\tspin_unlock_irq(&tsk->sighand->siglock);\\n\\n\\t\\t__set_current_state(TASK_INTERRUPTIBLE|TASK_FREEZABLE);\\n\\t\\tret = schedule_hrtimeout_range(to, tsk->timer_slack_ns,\\n\\t\\t\\t\\t\\t       HRTIMER_MODE_REL);\\n\\t\\tspin_lock_irq(&tsk->sighand->siglock);\\n\\t\\t__set_task_blocked(tsk, &tsk->real_blocked);\\n\\t\\tsigemptyset(&tsk->real_blocked);\\n\\t\\tsig = dequeue_signal(&mask, info, &type);\\n\\t}\\n\\tspin_unlock_irq(&tsk->sighand->siglock);\\n\\n\\tif (sig)\\n\\t\\treturn sig;\\n\\treturn ret ? -EINTR : -EAGAIN;\\n}\\n\\n/**\\n *  sys_rt_sigtimedwait - synchronously wait for queued signals specified\\n *\\t\\t\\tin @uthese\\n *  @uthese: queued signals to wait for\\n *  @uinfo: if non-null, the signal\\'s siginfo is returned here\\n *  @uts: upper bound on process time suspension\\n *  @sigsetsize: size of sigset_t type\\n */\\nSYSCALL_DEFINE4(rt_sigtimedwait, const sigset_t __user *, uthese,\\n\\t\\tsiginfo_t __user *, uinfo,\\n\\t\\tconst struct __kernel_timespec __user *, uts,\\n\\t\\tsize_t, sigsetsize)\\n{\\n\\tsigset_t these;\\n\\tstruct timespec64 ts;\\n\\tkernel_siginfo_t info;\\n\\tint ret;\\n\\n\\t/* XXX: Don\\'t preclude handling different sized sigset_t\\'s.  */\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (copy_from_user(&these, uthese, sizeof(these)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (uts) {\\n\\t\\tif (get_timespec64(&ts, uts))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\\n\\n\\tif (ret > 0 && uinfo) {\\n\\t\\tif (copy_siginfo_to_user(uinfo, &info))\\n\\t\\t\\tret = -EFAULT;\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_COMPAT_32BIT_TIME\\nSYSCALL_DEFINE4(rt_sigtimedwait_time32, const sigset_t __user *, uthese,\\n\\t\\tsiginfo_t __user *, uinfo,\\n\\t\\tconst struct old_timespec32 __user *, uts,\\n\\t\\tsize_t, sigsetsize)\\n{\\n\\tsigset_t these;\\n\\tstruct timespec64 ts;\\n\\tkernel_siginfo_t info;\\n\\tint ret;\\n\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (copy_from_user(&these, uthese, sizeof(these)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (uts) {\\n\\t\\tif (get_old_timespec32(&ts, uts))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\\n\\n\\tif (ret > 0 && uinfo) {\\n\\t\\tif (copy_siginfo_to_user(uinfo, &info))\\n\\t\\t\\tret = -EFAULT;\\n\\t}\\n\\n\\treturn ret;\\n}\\n#endif\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE4(rt_sigtimedwait_time64, compat_sigset_t __user *, uthese,\\n\\t\\tstruct compat_siginfo __user *, uinfo,\\n\\t\\tstruct __kernel_timespec __user *, uts, compat_size_t, sigsetsize)\\n{\\n\\tsigset_t s;\\n\\tstruct timespec64 t;\\n\\tkernel_siginfo_t info;\\n\\tlong ret;\\n\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (get_compat_sigset(&s, uthese))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (uts) {\\n\\t\\tif (get_timespec64(&t, uts))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\tret = do_sigtimedwait(&s, &info, uts ? &t : NULL);\\n\\n\\tif (ret > 0 && uinfo) {\\n\\t\\tif (copy_siginfo_to_user32(uinfo, &info))\\n\\t\\t\\tret = -EFAULT;\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_COMPAT_32BIT_TIME\\nCOMPAT_SYSCALL_DEFINE4(rt_sigtimedwait_time32, compat_sigset_t __user *, uthese,\\n\\t\\tstruct compat_siginfo __user *, uinfo,\\n\\t\\tstruct old_timespec32 __user *, uts, compat_size_t, sigsetsize)\\n{\\n\\tsigset_t s;\\n\\tstruct timespec64 t;\\n\\tkernel_siginfo_t info;\\n\\tlong ret;\\n\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (get_compat_sigset(&s, uthese))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (uts) {\\n\\t\\tif (get_old_timespec32(&t, uts))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\tret = do_sigtimedwait(&s, &info, uts ? &t : NULL);\\n\\n\\tif (ret > 0 && uinfo) {\\n\\t\\tif (copy_siginfo_to_user32(uinfo, &info))\\n\\t\\t\\tret = -EFAULT;\\n\\t}\\n\\n\\treturn ret;\\n}\\n#endif\\n#endif\\n\\nstatic void prepare_kill_siginfo(int sig, struct kernel_siginfo *info,\\n\\t\\t\\t\\t enum pid_type type)\\n{\\n\\tclear_siginfo(info);\\n\\tinfo->si_signo = sig;\\n\\tinfo->si_errno = 0;\\n\\tinfo->si_code = (type == PIDTYPE_PID) ? SI_TKILL : SI_USER;\\n\\tinfo->si_pid = task_tgid_vnr(current);\\n\\tinfo->si_uid = from_kuid_munged(current_user_ns(), current_uid());\\n}\\n\\n/**\\n *  sys_kill - send a signal to a process\\n *  @pid: the PID of the process\\n *  @sig: signal to be sent\\n */\\nSYSCALL_DEFINE2(kill, pid_t, pid, int, sig)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tprepare_kill_siginfo(sig, &info, PIDTYPE_TGID);\\n\\n\\treturn kill_something_info(sig, &info, pid);\\n}\\n\\n/*\\n * Verify that the signaler and signalee either are in the same pid namespace\\n * or that the signaler\\'s pid namespace is an ancestor of the signalee\\'s pid\\n * namespace.\\n */\\nstatic bool access_pidfd_pidns(struct pid *pid)\\n{\\n\\tstruct pid_namespace *active = task_active_pid_ns(current);\\n\\tstruct pid_namespace *p = ns_of_pid(pid);\\n\\n\\tfor (;;) {\\n\\t\\tif (!p)\\n\\t\\t\\treturn false;\\n\\t\\tif (p == active)\\n\\t\\t\\tbreak;\\n\\t\\tp = p->parent;\\n\\t}\\n\\n\\treturn true;\\n}\\n\\nstatic int copy_siginfo_from_user_any(kernel_siginfo_t *kinfo,\\n\\t\\tsiginfo_t __user *info)\\n{\\n#ifdef CONFIG_COMPAT\\n\\t/*\\n\\t * Avoid hooking up compat syscalls and instead handle necessary\\n\\t * conversions here. Note, this is a stop-gap measure and should not be\\n\\t * considered a generic solution.\\n\\t */\\n\\tif (in_compat_syscall())\\n\\t\\treturn copy_siginfo_from_user32(\\n\\t\\t\\tkinfo, (struct compat_siginfo __user *)info);\\n#endif\\n\\treturn copy_siginfo_from_user(kinfo, info);\\n}\\n\\nstatic struct pid *pidfd_to_pid(const struct file *file)\\n{\\n\\tstruct pid *pid;\\n\\n\\tpid = pidfd_pid(file);\\n\\tif (!IS_ERR(pid))\\n\\t\\treturn pid;\\n\\n\\treturn tgid_pidfd_to_pid(file);\\n}\\n\\n#define PIDFD_SEND_SIGNAL_FLAGS                            \\\\\\n\\t(PIDFD_SIGNAL_THREAD | PIDFD_SIGNAL_THREAD_GROUP | \\\\\\n\\t PIDFD_SIGNAL_PROCESS_GROUP)\\n\\n/**\\n * sys_pidfd_send_signal - Signal a process through a pidfd\\n * @pidfd:  file descriptor of the process\\n * @sig:    signal to send\\n * @info:   signal info\\n * @flags:  future flags\\n *\\n * Send the signal to the thread group or to the individual thread depending\\n * on PIDFD_THREAD.\\n * In the future extension to @flags may be used to override the default scope\\n * of @pidfd.\\n *\\n * Return: 0 on success, negative errno on failure\\n */\\nSYSCALL_DEFINE4(pidfd_send_signal, int, pidfd, int, sig,\\n\\t\\tsiginfo_t __user *, info, unsigned int, flags)\\n{\\n\\tint ret;\\n\\tstruct pid *pid;\\n\\tkernel_siginfo_t kinfo;\\n\\tenum pid_type type;\\n\\n\\t/* Enforce flags be set to 0 until we add an extension. */\\n\\tif (flags & ~PIDFD_SEND_SIGNAL_FLAGS)\\n\\t\\treturn -EINVAL;\\n\\n\\t/* Ensure that only a single signal scope determining flag is set. */\\n\\tif (hweight32(flags & PIDFD_SEND_SIGNAL_FLAGS) > 1)\\n\\t\\treturn -EINVAL;\\n\\n\\tCLASS(fd, f)(pidfd);\\n\\tif (fd_empty(f))\\n\\t\\treturn -EBADF;\\n\\n\\t/* Is this a pidfd? */\\n\\tpid = pidfd_to_pid(fd_file(f));\\n\\tif (IS_ERR(pid))\\n\\t\\treturn PTR_ERR(pid);\\n\\n\\tif (!access_pidfd_pidns(pid))\\n\\t\\treturn -EINVAL;\\n\\n\\tswitch (flags) {\\n\\tcase 0:\\n\\t\\t/* Infer scope from the type of pidfd. */\\n\\t\\tif (fd_file(f)->f_flags & PIDFD_THREAD)\\n\\t\\t\\ttype = PIDTYPE_PID;\\n\\t\\telse\\n\\t\\t\\ttype = PIDTYPE_TGID;\\n\\t\\tbreak;\\n\\tcase PIDFD_SIGNAL_THREAD:\\n\\t\\ttype = PIDTYPE_PID;\\n\\t\\tbreak;\\n\\tcase PIDFD_SIGNAL_THREAD_GROUP:\\n\\t\\ttype = PIDTYPE_TGID;\\n\\t\\tbreak;\\n\\tcase PIDFD_SIGNAL_PROCESS_GROUP:\\n\\t\\ttype = PIDTYPE_PGID;\\n\\t\\tbreak;\\n\\t}\\n\\n\\tif (info) {\\n\\t\\tret = copy_siginfo_from_user_any(&kinfo, info);\\n\\t\\tif (unlikely(ret))\\n\\t\\t\\treturn ret;\\n\\n\\t\\tif (unlikely(sig != kinfo.si_signo))\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\t/* Only allow sending arbitrary signals to yourself. */\\n\\t\\tif ((task_pid(current) != pid || type > PIDTYPE_TGID) &&\\n\\t\\t    (kinfo.si_code >= 0 || kinfo.si_code == SI_TKILL))\\n\\t\\t\\treturn -EPERM;\\n\\t} else {\\n\\t\\tprepare_kill_siginfo(sig, &kinfo, type);\\n\\t}\\n\\n\\tif (type == PIDTYPE_PGID)\\n\\t\\treturn kill_pgrp_info(sig, &kinfo, pid);\\n\\telse\\n\\t\\treturn kill_pid_info_type(sig, &kinfo, pid, type);\\n}\\n\\nstatic int\\ndo_send_specific(pid_t tgid, pid_t pid, int sig, struct kernel_siginfo *info)\\n{\\n\\tstruct task_struct *p;\\n\\tint error = -ESRCH;\\n\\n\\trcu_read_lock();\\n\\tp = find_task_by_vpid(pid);\\n\\tif (p && (tgid <= 0 || task_tgid_vnr(p) == tgid)) {\\n\\t\\terror = check_kill_permission(sig, info, p);\\n\\t\\t/*\\n\\t\\t * The null signal is a permissions and process existence\\n\\t\\t * probe.  No signal is actually delivered.\\n\\t\\t */\\n\\t\\tif (!error && sig) {\\n\\t\\t\\terror = do_send_sig_info(sig, info, p, PIDTYPE_PID);\\n\\t\\t\\t/*\\n\\t\\t\\t * If lock_task_sighand() failed we pretend the task\\n\\t\\t\\t * dies after receiving the signal. The window is tiny,\\n\\t\\t\\t * and the signal is private anyway.\\n\\t\\t\\t */\\n\\t\\t\\tif (unlikely(error == -ESRCH))\\n\\t\\t\\t\\terror = 0;\\n\\t\\t}\\n\\t}\\n\\trcu_read_unlock();\\n\\n\\treturn error;\\n}\\n\\nstatic int do_tkill(pid_t tgid, pid_t pid, int sig)\\n{\\n\\tstruct kernel_siginfo info;\\n\\n\\tprepare_kill_siginfo(sig, &info, PIDTYPE_PID);\\n\\n\\treturn do_send_specific(tgid, pid, sig, &info);\\n}\\n\\n/**\\n *  sys_tgkill - send signal to one specific thread\\n *  @tgid: the thread group ID of the thread\\n *  @pid: the PID of the thread\\n *  @sig: signal to be sent\\n *\\n *  This syscall also checks the @tgid and returns -ESRCH even if the PID\\n *  exists but it\\'s not belonging to the target process anymore. This\\n *  method solves the problem of threads exiting and PIDs getting reused.\\n */\\nSYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid, int, sig)\\n{\\n\\t/* This is only valid for single tasks */\\n\\tif (pid <= 0 || tgid <= 0)\\n\\t\\treturn -EINVAL;\\n\\n\\treturn do_tkill(tgid, pid, sig);\\n}\\n\\n/**\\n *  sys_tkill - send signal to one specific task\\n *  @pid: the PID of the task\\n *  @sig: signal to be sent\\n *\\n *  Send a signal to only one task, even if it\\'s a CLONE_THREAD task.\\n */\\nSYSCALL_DEFINE2(tkill, pid_t, pid, int, sig)\\n{\\n\\t/* This is only valid for single tasks */\\n\\tif (pid <= 0)\\n\\t\\treturn -EINVAL;\\n\\n\\treturn do_tkill(0, pid, sig);\\n}\\n\\nstatic int do_rt_sigqueueinfo(pid_t pid, int sig, kernel_siginfo_t *info)\\n{\\n\\t/* Not even root can pretend to send signals from the kernel.\\n\\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\\n\\t */\\n\\tif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\\n\\t    (task_pid_vnr(current) != pid))\\n\\t\\treturn -EPERM;\\n\\n\\t/* POSIX.1b doesn\\'t mention process groups.  */\\n\\treturn kill_proc_info(sig, info, pid);\\n}\\n\\n/**\\n *  sys_rt_sigqueueinfo - send signal information to a signal\\n *  @pid: the PID of the thread\\n *  @sig: signal to be sent\\n *  @uinfo: signal info to be sent\\n */\\nSYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid, int, sig,\\n\\t\\tsiginfo_t __user *, uinfo)\\n{\\n\\tkernel_siginfo_t info;\\n\\tint ret = __copy_siginfo_from_user(sig, &info, uinfo);\\n\\tif (unlikely(ret))\\n\\t\\treturn ret;\\n\\treturn do_rt_sigqueueinfo(pid, sig, &info);\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE3(rt_sigqueueinfo,\\n\\t\\t\\tcompat_pid_t, pid,\\n\\t\\t\\tint, sig,\\n\\t\\t\\tstruct compat_siginfo __user *, uinfo)\\n{\\n\\tkernel_siginfo_t info;\\n\\tint ret = __copy_siginfo_from_user32(sig, &info, uinfo);\\n\\tif (unlikely(ret))\\n\\t\\treturn ret;\\n\\treturn do_rt_sigqueueinfo(pid, sig, &info);\\n}\\n#endif\\n\\nstatic int do_rt_tgsigqueueinfo(pid_t tgid, pid_t pid, int sig, kernel_siginfo_t *info)\\n{\\n\\t/* This is only valid for single tasks */\\n\\tif (pid <= 0 || tgid <= 0)\\n\\t\\treturn -EINVAL;\\n\\n\\t/* Not even root can pretend to send signals from the kernel.\\n\\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\\n\\t */\\n\\tif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\\n\\t    (task_pid_vnr(current) != pid))\\n\\t\\treturn -EPERM;\\n\\n\\treturn do_send_specific(tgid, pid, sig, info);\\n}\\n\\nSYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid, int, sig,\\n\\t\\tsiginfo_t __user *, uinfo)\\n{\\n\\tkernel_siginfo_t info;\\n\\tint ret = __copy_siginfo_from_user(sig, &info, uinfo);\\n\\tif (unlikely(ret))\\n\\t\\treturn ret;\\n\\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE4(rt_tgsigqueueinfo,\\n\\t\\t\\tcompat_pid_t, tgid,\\n\\t\\t\\tcompat_pid_t, pid,\\n\\t\\t\\tint, sig,\\n\\t\\t\\tstruct compat_siginfo __user *, uinfo)\\n{\\n\\tkernel_siginfo_t info;\\n\\tint ret = __copy_siginfo_from_user32(sig, &info, uinfo);\\n\\tif (unlikely(ret))\\n\\t\\treturn ret;\\n\\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\\n}\\n#endif\\n\\n/*\\n * For kthreads only, must not be used if cloned with CLONE_SIGHAND\\n */\\nvoid kernel_sigaction(int sig, __sighandler_t action)\\n{\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\tcurrent->sighand->action[sig - 1].sa.sa_handler = action;\\n\\tif (action == SIG_IGN) {\\n\\t\\tsigset_t mask;\\n\\n\\t\\tsigemptyset(&mask);\\n\\t\\tsigaddset(&mask, sig);\\n\\n\\t\\tflush_sigqueue_mask(current, &mask, &current->signal->shared_pending);\\n\\t\\tflush_sigqueue_mask(current, &mask, &current->pending);\\n\\t\\trecalc_sigpending();\\n\\t}\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n}\\nEXPORT_SYMBOL(kernel_sigaction);\\n\\nvoid __weak sigaction_compat_abi(struct k_sigaction *act,\\n\\t\\tstruct k_sigaction *oact)\\n{\\n}\\n\\nint do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)\\n{\\n\\tstruct task_struct *p = current, *t;\\n\\tstruct k_sigaction *k;\\n\\tsigset_t mask;\\n\\n\\tif (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))\\n\\t\\treturn -EINVAL;\\n\\n\\tk = &p->sighand->action[sig-1];\\n\\n\\tspin_lock_irq(&p->sighand->siglock);\\n\\tif (k->sa.sa_flags & SA_IMMUTABLE) {\\n\\t\\tspin_unlock_irq(&p->sighand->siglock);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\tif (oact)\\n\\t\\t*oact = *k;\\n\\n\\t/*\\n\\t * Make sure that we never accidentally claim to support SA_UNSUPPORTED,\\n\\t * e.g. by having an architecture use the bit in their uapi.\\n\\t */\\n\\tBUILD_BUG_ON(UAPI_SA_FLAGS & SA_UNSUPPORTED);\\n\\n\\t/*\\n\\t * Clear unknown flag bits in order to allow userspace to detect missing\\n\\t * support for flag bits and to allow the kernel to use non-uapi bits\\n\\t * internally.\\n\\t */\\n\\tif (act)\\n\\t\\tact->sa.sa_flags &= UAPI_SA_FLAGS;\\n\\tif (oact)\\n\\t\\toact->sa.sa_flags &= UAPI_SA_FLAGS;\\n\\n\\tsigaction_compat_abi(act, oact);\\n\\n\\tif (act) {\\n\\t\\tbool was_ignored = k->sa.sa_handler == SIG_IGN;\\n\\n\\t\\tsigdelsetmask(&act->sa.sa_mask,\\n\\t\\t\\t      sigmask(SIGKILL) | sigmask(SIGSTOP));\\n\\t\\t*k = *act;\\n\\t\\t/*\\n\\t\\t * POSIX 3.3.1.3:\\n\\t\\t *  \"Setting a signal action to SIG_IGN for a signal that is\\n\\t\\t *   pending shall cause the pending signal to be discarded,\\n\\t\\t *   whether or not it is blocked.\"\\n\\t\\t *\\n\\t\\t *  \"Setting a signal action to SIG_DFL for a signal that is\\n\\t\\t *   pending and whose default action is to ignore the signal\\n\\t\\t *   (for example, SIGCHLD), shall cause the pending signal to\\n\\t\\t *   be discarded, whether or not it is blocked\"\\n\\t\\t */\\n\\t\\tif (sig_handler_ignored(sig_handler(p, sig), sig)) {\\n\\t\\t\\tsigemptyset(&mask);\\n\\t\\t\\tsigaddset(&mask, sig);\\n\\t\\t\\tflush_sigqueue_mask(p, &mask, &p->signal->shared_pending);\\n\\t\\t\\tfor_each_thread(p, t)\\n\\t\\t\\t\\tflush_sigqueue_mask(p, &mask, &t->pending);\\n\\t\\t} else if (was_ignored) {\\n\\t\\t\\tposixtimer_sig_unignore(p, sig);\\n\\t\\t}\\n\\t}\\n\\n\\tspin_unlock_irq(&p->sighand->siglock);\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_DYNAMIC_SIGFRAME\\nstatic inline void sigaltstack_lock(void)\\n\\t__acquires(&current->sighand->siglock)\\n{\\n\\tspin_lock_irq(&current->sighand->siglock);\\n}\\n\\nstatic inline void sigaltstack_unlock(void)\\n\\t__releases(&current->sighand->siglock)\\n{\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n}\\n#else\\nstatic inline void sigaltstack_lock(void) { }\\nstatic inline void sigaltstack_unlock(void) { }\\n#endif\\n\\nstatic int\\ndo_sigaltstack (const stack_t *ss, stack_t *oss, unsigned long sp,\\n\\t\\tsize_t min_ss_size)\\n{\\n\\tstruct task_struct *t = current;\\n\\tint ret = 0;\\n\\n\\tif (oss) {\\n\\t\\tmemset(oss, 0, sizeof(stack_t));\\n\\t\\toss->ss_sp = (void __user *) t->sas_ss_sp;\\n\\t\\toss->ss_size = t->sas_ss_size;\\n\\t\\toss->ss_flags = sas_ss_flags(sp) |\\n\\t\\t\\t(current->sas_ss_flags & SS_FLAG_BITS);\\n\\t}\\n\\n\\tif (ss) {\\n\\t\\tvoid __user *ss_sp = ss->ss_sp;\\n\\t\\tsize_t ss_size = ss->ss_size;\\n\\t\\tunsigned ss_flags = ss->ss_flags;\\n\\t\\tint ss_mode;\\n\\n\\t\\tif (unlikely(on_sig_stack(sp)))\\n\\t\\t\\treturn -EPERM;\\n\\n\\t\\tss_mode = ss_flags & ~SS_FLAG_BITS;\\n\\t\\tif (unlikely(ss_mode != SS_DISABLE && ss_mode != SS_ONSTACK &&\\n\\t\\t\\t\\tss_mode != 0))\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\t/*\\n\\t\\t * Return before taking any locks if no actual\\n\\t\\t * sigaltstack changes were requested.\\n\\t\\t */\\n\\t\\tif (t->sas_ss_sp == (unsigned long)ss_sp &&\\n\\t\\t    t->sas_ss_size == ss_size &&\\n\\t\\t    t->sas_ss_flags == ss_flags)\\n\\t\\t\\treturn 0;\\n\\n\\t\\tsigaltstack_lock();\\n\\t\\tif (ss_mode == SS_DISABLE) {\\n\\t\\t\\tss_size = 0;\\n\\t\\t\\tss_sp = NULL;\\n\\t\\t} else {\\n\\t\\t\\tif (unlikely(ss_size < min_ss_size))\\n\\t\\t\\t\\tret = -ENOMEM;\\n\\t\\t\\tif (!sigaltstack_size_valid(ss_size))\\n\\t\\t\\t\\tret = -ENOMEM;\\n\\t\\t}\\n\\t\\tif (!ret) {\\n\\t\\t\\tt->sas_ss_sp = (unsigned long) ss_sp;\\n\\t\\t\\tt->sas_ss_size = ss_size;\\n\\t\\t\\tt->sas_ss_flags = ss_flags;\\n\\t\\t}\\n\\t\\tsigaltstack_unlock();\\n\\t}\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE2(sigaltstack,const stack_t __user *,uss, stack_t __user *,uoss)\\n{\\n\\tstack_t new, old;\\n\\tint err;\\n\\tif (uss && copy_from_user(&new, uss, sizeof(stack_t)))\\n\\t\\treturn -EFAULT;\\n\\terr = do_sigaltstack(uss ? &new : NULL, uoss ? &old : NULL,\\n\\t\\t\\t      current_user_stack_pointer(),\\n\\t\\t\\t      MINSIGSTKSZ);\\n\\tif (!err && uoss && copy_to_user(uoss, &old, sizeof(stack_t)))\\n\\t\\terr = -EFAULT;\\n\\treturn err;\\n}\\n\\nint restore_altstack(const stack_t __user *uss)\\n{\\n\\tstack_t new;\\n\\tif (copy_from_user(&new, uss, sizeof(stack_t)))\\n\\t\\treturn -EFAULT;\\n\\t(void)do_sigaltstack(&new, NULL, current_user_stack_pointer(),\\n\\t\\t\\t     MINSIGSTKSZ);\\n\\t/* squash all but EFAULT for now */\\n\\treturn 0;\\n}\\n\\nint __save_altstack(stack_t __user *uss, unsigned long sp)\\n{\\n\\tstruct task_struct *t = current;\\n\\tint err = __put_user((void __user *)t->sas_ss_sp, &uss->ss_sp) |\\n\\t\\t__put_user(t->sas_ss_flags, &uss->ss_flags) |\\n\\t\\t__put_user(t->sas_ss_size, &uss->ss_size);\\n\\treturn err;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nstatic int do_compat_sigaltstack(const compat_stack_t __user *uss_ptr,\\n\\t\\t\\t\\t compat_stack_t __user *uoss_ptr)\\n{\\n\\tstack_t uss, uoss;\\n\\tint ret;\\n\\n\\tif (uss_ptr) {\\n\\t\\tcompat_stack_t uss32;\\n\\t\\tif (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tuss.ss_sp = compat_ptr(uss32.ss_sp);\\n\\t\\tuss.ss_flags = uss32.ss_flags;\\n\\t\\tuss.ss_size = uss32.ss_size;\\n\\t}\\n\\tret = do_sigaltstack(uss_ptr ? &uss : NULL, &uoss,\\n\\t\\t\\t     compat_user_stack_pointer(),\\n\\t\\t\\t     COMPAT_MINSIGSTKSZ);\\n\\tif (ret >= 0 && uoss_ptr)  {\\n\\t\\tcompat_stack_t old;\\n\\t\\tmemset(&old, 0, sizeof(old));\\n\\t\\told.ss_sp = ptr_to_compat(uoss.ss_sp);\\n\\t\\told.ss_flags = uoss.ss_flags;\\n\\t\\told.ss_size = uoss.ss_size;\\n\\t\\tif (copy_to_user(uoss_ptr, &old, sizeof(compat_stack_t)))\\n\\t\\t\\tret = -EFAULT;\\n\\t}\\n\\treturn ret;\\n}\\n\\nCOMPAT_SYSCALL_DEFINE2(sigaltstack,\\n\\t\\t\\tconst compat_stack_t __user *, uss_ptr,\\n\\t\\t\\tcompat_stack_t __user *, uoss_ptr)\\n{\\n\\treturn do_compat_sigaltstack(uss_ptr, uoss_ptr);\\n}\\n\\nint compat_restore_altstack(const compat_stack_t __user *uss)\\n{\\n\\tint err = do_compat_sigaltstack(uss, NULL);\\n\\t/* squash all but -EFAULT for now */\\n\\treturn err == -EFAULT ? err : 0;\\n}\\n\\nint __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)\\n{\\n\\tint err;\\n\\tstruct task_struct *t = current;\\n\\terr = __put_user(ptr_to_compat((void __user *)t->sas_ss_sp),\\n\\t\\t\\t &uss->ss_sp) |\\n\\t\\t__put_user(t->sas_ss_flags, &uss->ss_flags) |\\n\\t\\t__put_user(t->sas_ss_size, &uss->ss_size);\\n\\treturn err;\\n}\\n#endif\\n\\n#ifdef __ARCH_WANT_SYS_SIGPENDING\\n\\n/**\\n *  sys_sigpending - examine pending signals\\n *  @uset: where mask of pending signal is returned\\n */\\nSYSCALL_DEFINE1(sigpending, old_sigset_t __user *, uset)\\n{\\n\\tsigset_t set;\\n\\n\\tif (sizeof(old_sigset_t) > sizeof(*uset))\\n\\t\\treturn -EINVAL;\\n\\n\\tdo_sigpending(&set);\\n\\n\\tif (copy_to_user(uset, &set, sizeof(old_sigset_t)))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE1(sigpending, compat_old_sigset_t __user *, set32)\\n{\\n\\tsigset_t set;\\n\\n\\tdo_sigpending(&set);\\n\\n\\treturn put_user(set.sig[0], set32);\\n}\\n#endif\\n\\n#endif\\n\\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\\n/**\\n *  sys_sigprocmask - examine and change blocked signals\\n *  @how: whether to add, remove, or set signals\\n *  @nset: signals to add or remove (if non-null)\\n *  @oset: previous value of signal mask if non-null\\n *\\n * Some platforms have their own version with special arguments;\\n * others support only sys_rt_sigprocmask.\\n */\\n\\nSYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,\\n\\t\\told_sigset_t __user *, oset)\\n{\\n\\told_sigset_t old_set, new_set;\\n\\tsigset_t new_blocked;\\n\\n\\told_set = current->blocked.sig[0];\\n\\n\\tif (nset) {\\n\\t\\tif (copy_from_user(&new_set, nset, sizeof(*nset)))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\tnew_blocked = current->blocked;\\n\\n\\t\\tswitch (how) {\\n\\t\\tcase SIG_BLOCK:\\n\\t\\t\\tsigaddsetmask(&new_blocked, new_set);\\n\\t\\t\\tbreak;\\n\\t\\tcase SIG_UNBLOCK:\\n\\t\\t\\tsigdelsetmask(&new_blocked, new_set);\\n\\t\\t\\tbreak;\\n\\t\\tcase SIG_SETMASK:\\n\\t\\t\\tnew_blocked.sig[0] = new_set;\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\n\\t\\tset_current_blocked(&new_blocked);\\n\\t}\\n\\n\\tif (oset) {\\n\\t\\tif (copy_to_user(oset, &old_set, sizeof(*oset)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\treturn 0;\\n}\\n#endif /* __ARCH_WANT_SYS_SIGPROCMASK */\\n\\n#ifndef CONFIG_ODD_RT_SIGACTION\\n/**\\n *  sys_rt_sigaction - alter an action taken by a process\\n *  @sig: signal to be sent\\n *  @act: new sigaction\\n *  @oact: used to save the previous sigaction\\n *  @sigsetsize: size of sigset_t type\\n */\\nSYSCALL_DEFINE4(rt_sigaction, int, sig,\\n\\t\\tconst struct sigaction __user *, act,\\n\\t\\tstruct sigaction __user *, oact,\\n\\t\\tsize_t, sigsetsize)\\n{\\n\\tstruct k_sigaction new_sa, old_sa;\\n\\tint ret;\\n\\n\\t/* XXX: Don\\'t preclude handling different sized sigset_t\\'s.  */\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (act && copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))\\n\\t\\treturn -EFAULT;\\n\\n\\tret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (oact && copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn 0;\\n}\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE4(rt_sigaction, int, sig,\\n\\t\\tconst struct compat_sigaction __user *, act,\\n\\t\\tstruct compat_sigaction __user *, oact,\\n\\t\\tcompat_size_t, sigsetsize)\\n{\\n\\tstruct k_sigaction new_ka, old_ka;\\n#ifdef __ARCH_HAS_SA_RESTORER\\n\\tcompat_uptr_t restorer;\\n#endif\\n\\tint ret;\\n\\n\\t/* XXX: Don\\'t preclude handling different sized sigset_t\\'s.  */\\n\\tif (sigsetsize != sizeof(compat_sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (act) {\\n\\t\\tcompat_uptr_t handler;\\n\\t\\tret = get_user(handler, &act->sa_handler);\\n\\t\\tnew_ka.sa.sa_handler = compat_ptr(handler);\\n#ifdef __ARCH_HAS_SA_RESTORER\\n\\t\\tret |= get_user(restorer, &act->sa_restorer);\\n\\t\\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\\n#endif\\n\\t\\tret |= get_compat_sigset(&new_ka.sa.sa_mask, &act->sa_mask);\\n\\t\\tret |= get_user(new_ka.sa.sa_flags, &act->sa_flags);\\n\\t\\tif (ret)\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\\n\\tif (!ret && oact) {\\n\\t\\tret = put_user(ptr_to_compat(old_ka.sa.sa_handler), \\n\\t\\t\\t       &oact->sa_handler);\\n\\t\\tret |= put_compat_sigset(&oact->sa_mask, &old_ka.sa.sa_mask,\\n\\t\\t\\t\\t\\t sizeof(oact->sa_mask));\\n\\t\\tret |= put_user(old_ka.sa.sa_flags, &oact->sa_flags);\\n#ifdef __ARCH_HAS_SA_RESTORER\\n\\t\\tret |= put_user(ptr_to_compat(old_ka.sa.sa_restorer),\\n\\t\\t\\t\\t&oact->sa_restorer);\\n#endif\\n\\t}\\n\\treturn ret;\\n}\\n#endif\\n#endif /* !CONFIG_ODD_RT_SIGACTION */\\n\\n#ifdef CONFIG_OLD_SIGACTION\\nSYSCALL_DEFINE3(sigaction, int, sig,\\n\\t\\tconst struct old_sigaction __user *, act,\\n\\t        struct old_sigaction __user *, oact)\\n{\\n\\tstruct k_sigaction new_ka, old_ka;\\n\\tint ret;\\n\\n\\tif (act) {\\n\\t\\told_sigset_t mask;\\n\\t\\tif (!access_ok(act, sizeof(*act)) ||\\n\\t\\t    __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||\\n\\t\\t    __get_user(new_ka.sa.sa_restorer, &act->sa_restorer) ||\\n\\t\\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\\n\\t\\t    __get_user(mask, &act->sa_mask))\\n\\t\\t\\treturn -EFAULT;\\n#ifdef __ARCH_HAS_KA_RESTORER\\n\\t\\tnew_ka.ka_restorer = NULL;\\n#endif\\n\\t\\tsiginitset(&new_ka.sa.sa_mask, mask);\\n\\t}\\n\\n\\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\\n\\n\\tif (!ret && oact) {\\n\\t\\tif (!access_ok(oact, sizeof(*oact)) ||\\n\\t\\t    __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||\\n\\t\\t    __put_user(old_ka.sa.sa_restorer, &oact->sa_restorer) ||\\n\\t\\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\\n\\t\\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\treturn ret;\\n}\\n#endif\\n#ifdef CONFIG_COMPAT_OLD_SIGACTION\\nCOMPAT_SYSCALL_DEFINE3(sigaction, int, sig,\\n\\t\\tconst struct compat_old_sigaction __user *, act,\\n\\t        struct compat_old_sigaction __user *, oact)\\n{\\n\\tstruct k_sigaction new_ka, old_ka;\\n\\tint ret;\\n\\tcompat_old_sigset_t mask;\\n\\tcompat_uptr_t handler, restorer;\\n\\n\\tif (act) {\\n\\t\\tif (!access_ok(act, sizeof(*act)) ||\\n\\t\\t    __get_user(handler, &act->sa_handler) ||\\n\\t\\t    __get_user(restorer, &act->sa_restorer) ||\\n\\t\\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\\n\\t\\t    __get_user(mask, &act->sa_mask))\\n\\t\\t\\treturn -EFAULT;\\n\\n#ifdef __ARCH_HAS_KA_RESTORER\\n\\t\\tnew_ka.ka_restorer = NULL;\\n#endif\\n\\t\\tnew_ka.sa.sa_handler = compat_ptr(handler);\\n\\t\\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\\n\\t\\tsiginitset(&new_ka.sa.sa_mask, mask);\\n\\t}\\n\\n\\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\\n\\n\\tif (!ret && oact) {\\n\\t\\tif (!access_ok(oact, sizeof(*oact)) ||\\n\\t\\t    __put_user(ptr_to_compat(old_ka.sa.sa_handler),\\n\\t\\t\\t       &oact->sa_handler) ||\\n\\t\\t    __put_user(ptr_to_compat(old_ka.sa.sa_restorer),\\n\\t\\t\\t       &oact->sa_restorer) ||\\n\\t\\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\\n\\t\\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\treturn ret;\\n}\\n#endif\\n\\n#ifdef CONFIG_SGETMASK_SYSCALL\\n\\n/*\\n * For backwards compatibility.  Functionality superseded by sigprocmask.\\n */\\nSYSCALL_DEFINE0(sgetmask)\\n{\\n\\t/* SMP safe */\\n\\treturn current->blocked.sig[0];\\n}\\n\\nSYSCALL_DEFINE1(ssetmask, int, newmask)\\n{\\n\\tint old = current->blocked.sig[0];\\n\\tsigset_t newset;\\n\\n\\tsiginitset(&newset, newmask);\\n\\tset_current_blocked(&newset);\\n\\n\\treturn old;\\n}\\n#endif /* CONFIG_SGETMASK_SYSCALL */\\n\\n#ifdef __ARCH_WANT_SYS_SIGNAL\\n/*\\n * For backwards compatibility.  Functionality superseded by sigaction.\\n */\\nSYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)\\n{\\n\\tstruct k_sigaction new_sa, old_sa;\\n\\tint ret;\\n\\n\\tnew_sa.sa.sa_handler = handler;\\n\\tnew_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;\\n\\tsigemptyset(&new_sa.sa.sa_mask);\\n\\n\\tret = do_sigaction(sig, &new_sa, &old_sa);\\n\\n\\treturn ret ? ret : (unsigned long)old_sa.sa.sa_handler;\\n}\\n#endif /* __ARCH_WANT_SYS_SIGNAL */\\n\\n#ifdef __ARCH_WANT_SYS_PAUSE\\n\\nSYSCALL_DEFINE0(pause)\\n{\\n\\twhile (!signal_pending(current)) {\\n\\t\\t__set_current_state(TASK_INTERRUPTIBLE);\\n\\t\\tschedule();\\n\\t}\\n\\treturn -ERESTARTNOHAND;\\n}\\n\\n#endif\\n\\nstatic int sigsuspend(sigset_t *set)\\n{\\n\\tcurrent->saved_sigmask = current->blocked;\\n\\tset_current_blocked(set);\\n\\n\\twhile (!signal_pending(current)) {\\n\\t\\t__set_current_state(TASK_INTERRUPTIBLE);\\n\\t\\tschedule();\\n\\t}\\n\\tset_restore_sigmask();\\n\\treturn -ERESTARTNOHAND;\\n}\\n\\n/**\\n *  sys_rt_sigsuspend - replace the signal mask for a value with the\\n *\\t@unewset value until a signal is received\\n *  @unewset: new signal mask value\\n *  @sigsetsize: size of sigset_t type\\n */\\nSYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)\\n{\\n\\tsigset_t newset;\\n\\n\\t/* XXX: Don\\'t preclude handling different sized sigset_t\\'s.  */\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (copy_from_user(&newset, unewset, sizeof(newset)))\\n\\t\\treturn -EFAULT;\\n\\treturn sigsuspend(&newset);\\n}\\n \\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE2(rt_sigsuspend, compat_sigset_t __user *, unewset, compat_size_t, sigsetsize)\\n{\\n\\tsigset_t newset;\\n\\n\\t/* XXX: Don\\'t preclude handling different sized sigset_t\\'s.  */\\n\\tif (sigsetsize != sizeof(sigset_t))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (get_compat_sigset(&newset, unewset))\\n\\t\\treturn -EFAULT;\\n\\treturn sigsuspend(&newset);\\n}\\n#endif\\n\\n#ifdef CONFIG_OLD_SIGSUSPEND\\nSYSCALL_DEFINE1(sigsuspend, old_sigset_t, mask)\\n{\\n\\tsigset_t blocked;\\n\\tsiginitset(&blocked, mask);\\n\\treturn sigsuspend(&blocked);\\n}\\n#endif\\n#ifdef CONFIG_OLD_SIGSUSPEND3\\nSYSCALL_DEFINE3(sigsuspend, int, unused1, int, unused2, old_sigset_t, mask)\\n{\\n\\tsigset_t blocked;\\n\\tsiginitset(&blocked, mask);\\n\\treturn sigsuspend(&blocked);\\n}\\n#endif\\n\\n__weak const char *arch_vma_name(struct vm_area_struct *vma)\\n{\\n\\treturn NULL;\\n}\\n\\nstatic inline void siginfo_buildtime_checks(void)\\n{\\n\\tBUILD_BUG_ON(sizeof(struct siginfo) != SI_MAX_SIZE);\\n\\n\\t/* Verify the offsets in the two siginfos match */\\n#define CHECK_OFFSET(field) \\\\\\n\\tBUILD_BUG_ON(offsetof(siginfo_t, field) != offsetof(kernel_siginfo_t, field))\\n\\n\\t/* kill */\\n\\tCHECK_OFFSET(si_pid);\\n\\tCHECK_OFFSET(si_uid);\\n\\n\\t/* timer */\\n\\tCHECK_OFFSET(si_tid);\\n\\tCHECK_OFFSET(si_overrun);\\n\\tCHECK_OFFSET(si_value);\\n\\n\\t/* rt */\\n\\tCHECK_OFFSET(si_pid);\\n\\tCHECK_OFFSET(si_uid);\\n\\tCHECK_OFFSET(si_value);\\n\\n\\t/* sigchld */\\n\\tCHECK_OFFSET(si_pid);\\n\\tCHECK_OFFSET(si_uid);\\n\\tCHECK_OFFSET(si_status);\\n\\tCHECK_OFFSET(si_utime);\\n\\tCHECK_OFFSET(si_stime);\\n\\n\\t/* sigfault */\\n\\tCHECK_OFFSET(si_addr);\\n\\tCHECK_OFFSET(si_trapno);\\n\\tCHECK_OFFSET(si_addr_lsb);\\n\\tCHECK_OFFSET(si_lower);\\n\\tCHECK_OFFSET(si_upper);\\n\\tCHECK_OFFSET(si_pkey);\\n\\tCHECK_OFFSET(si_perf_data);\\n\\tCHECK_OFFSET(si_perf_type);\\n\\tCHECK_OFFSET(si_perf_flags);\\n\\n\\t/* sigpoll */\\n\\tCHECK_OFFSET(si_band);\\n\\tCHECK_OFFSET(si_fd);\\n\\n\\t/* sigsys */\\n\\tCHECK_OFFSET(si_call_addr);\\n\\tCHECK_OFFSET(si_syscall);\\n\\tCHECK_OFFSET(si_arch);\\n#undef CHECK_OFFSET\\n\\n\\t/* usb asyncio */\\n\\tBUILD_BUG_ON(offsetof(struct siginfo, si_pid) !=\\n\\t\\t     offsetof(struct siginfo, si_addr));\\n\\tif (sizeof(int) == sizeof(void __user *)) {\\n\\t\\tBUILD_BUG_ON(sizeof_field(struct siginfo, si_pid) !=\\n\\t\\t\\t     sizeof(void __user *));\\n\\t} else {\\n\\t\\tBUILD_BUG_ON((sizeof_field(struct siginfo, si_pid) +\\n\\t\\t\\t      sizeof_field(struct siginfo, si_uid)) !=\\n\\t\\t\\t     sizeof(void __user *));\\n\\t\\tBUILD_BUG_ON(offsetofend(struct siginfo, si_pid) !=\\n\\t\\t\\t     offsetof(struct siginfo, si_uid));\\n\\t}\\n#ifdef CONFIG_COMPAT\\n\\tBUILD_BUG_ON(offsetof(struct compat_siginfo, si_pid) !=\\n\\t\\t     offsetof(struct compat_siginfo, si_addr));\\n\\tBUILD_BUG_ON(sizeof_field(struct compat_siginfo, si_pid) !=\\n\\t\\t     sizeof(compat_uptr_t));\\n\\tBUILD_BUG_ON(sizeof_field(struct compat_siginfo, si_pid) !=\\n\\t\\t     sizeof_field(struct siginfo, si_pid));\\n#endif\\n}\\n\\n#if defined(CONFIG_SYSCTL)\\nstatic struct ctl_table signal_debug_table[] = {\\n#ifdef CONFIG_SYSCTL_EXCEPTION_TRACE\\n\\t{\\n\\t\\t.procname\\t= \"exception-trace\",\\n\\t\\t.data\\t\\t= &show_unhandled_signals,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec\\n\\t},\\n#endif\\n};\\n\\nstatic int __init init_signal_sysctls(void)\\n{\\n\\tregister_sysctl_init(\"debug\", signal_debug_table);\\n\\treturn 0;\\n}\\nearly_initcall(init_signal_sysctls);\\n#endif /* CONFIG_SYSCTL */\\n\\nvoid __init signals_init(void)\\n{\\n\\tsiginfo_buildtime_checks();\\n\\n\\tsigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC | SLAB_ACCOUNT);\\n}\\n\\n#ifdef CONFIG_KGDB_KDB\\n#include <linux/kdb.h>\\n/*\\n * kdb_send_sig - Allows kdb to send signals without exposing\\n * signal internals.  This function checks if the required locks are\\n * available before calling the main signal code, to avoid kdb\\n * deadlocks.\\n */\\nvoid kdb_send_sig(struct task_struct *t, int sig)\\n{\\n\\tstatic struct task_struct *kdb_prev_t;\\n\\tint new_t, ret;\\n\\tif (!spin_trylock(&t->sighand->siglock)) {\\n\\t\\tkdb_printf(\"Can\\'t do kill command now.\\\\n\"\\n\\t\\t\\t   \"The sigmask lock is held somewhere else in \"\\n\\t\\t\\t   \"kernel, try again later\\\\n\");\\n\\t\\treturn;\\n\\t}\\n\\tnew_t = kdb_prev_t != t;\\n\\tkdb_prev_t = t;\\n\\tif (!task_is_running(t) && new_t) {\\n\\t\\tspin_unlock(&t->sighand->siglock);\\n\\t\\tkdb_printf(\"Process is not RUNNING, sending a signal from \"\\n\\t\\t\\t   \"kdb risks deadlock\\\\n\"\\n\\t\\t\\t   \"on the run queue locks. \"\\n\\t\\t\\t   \"The signal has _not_ been sent.\\\\n\"\\n\\t\\t\\t   \"Reissue the kill command if you want to risk \"\\n\\t\\t\\t   \"the deadlock.\\\\n\");\\n\\t\\treturn;\\n\\t}\\n\\tret = send_signal_locked(sig, SEND_SIG_PRIV, t, PIDTYPE_PID);\\n\\tspin_unlock(&t->sighand->siglock);\\n\\tif (ret)\\n\\t\\tkdb_printf(\"Fail to deliver Signal %d to process %d.\\\\n\",\\n\\t\\t\\t   sig, t->pid);\\n\\telse\\n\\t\\tkdb_printf(\"Signal %d is sent to process %d.\\\\n\", sig, t->pid);\\n}\\n#endif\\t/* CONFIG_KGDB_KDB */\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * linux/kernel/seccomp.c\\n *\\n * Copyright 2004-2005  Andrea Arcangeli <andrea@cpushare.com>\\n *\\n * Copyright (C) 2012 Google, Inc.\\n * Will Drewry <wad@chromium.org>\\n *\\n * This defines a simple but solid secure-computing facility.\\n *\\n * Mode 1 uses a fixed list of allowed system calls.\\n * Mode 2 allows user-defined system call filters in the form\\n *        of Berkeley Packet Filters/Linux Socket Filters.\\n */\\n#define pr_fmt(fmt) \"seccomp: \" fmt\\n\\n#include <linux/refcount.h>\\n#include <linux/audit.h>\\n#include <linux/compat.h>\\n#include <linux/coredump.h>\\n#include <linux/kmemleak.h>\\n#include <linux/nospec.h>\\n#include <linux/prctl.h>\\n#include <linux/sched.h>\\n#include <linux/sched/task_stack.h>\\n#include <linux/seccomp.h>\\n#include <linux/slab.h>\\n#include <linux/syscalls.h>\\n#include <linux/sysctl.h>\\n\\n/* Not exposed in headers: strictly internal use only. */\\n#define SECCOMP_MODE_DEAD\\t(SECCOMP_MODE_FILTER + 1)\\n\\n#ifdef CONFIG_HAVE_ARCH_SECCOMP_FILTER\\n#include <asm/syscall.h>\\n#endif\\n\\n#ifdef CONFIG_SECCOMP_FILTER\\n#include <linux/file.h>\\n#include <linux/filter.h>\\n#include <linux/pid.h>\\n#include <linux/ptrace.h>\\n#include <linux/capability.h>\\n#include <linux/uaccess.h>\\n#include <linux/anon_inodes.h>\\n#include <linux/lockdep.h>\\n\\n/*\\n * When SECCOMP_IOCTL_NOTIF_ID_VALID was first introduced, it had the\\n * wrong direction flag in the ioctl number. This is the broken one,\\n * which the kernel needs to keep supporting until all userspaces stop\\n * using the wrong command number.\\n */\\n#define SECCOMP_IOCTL_NOTIF_ID_VALID_WRONG_DIR\\tSECCOMP_IOR(2, __u64)\\n\\nenum notify_state {\\n\\tSECCOMP_NOTIFY_INIT,\\n\\tSECCOMP_NOTIFY_SENT,\\n\\tSECCOMP_NOTIFY_REPLIED,\\n};\\n\\nstruct seccomp_knotif {\\n\\t/* The struct pid of the task whose filter triggered the notification */\\n\\tstruct task_struct *task;\\n\\n\\t/* The \"cookie\" for this request; this is unique for this filter. */\\n\\tu64 id;\\n\\n\\t/*\\n\\t * The seccomp data. This pointer is valid the entire time this\\n\\t * notification is active, since it comes from __seccomp_filter which\\n\\t * eclipses the entire lifecycle here.\\n\\t */\\n\\tconst struct seccomp_data *data;\\n\\n\\t/*\\n\\t * Notification states. When SECCOMP_RET_USER_NOTIF is returned, a\\n\\t * struct seccomp_knotif is created and starts out in INIT. Once the\\n\\t * handler reads the notification off of an FD, it transitions to SENT.\\n\\t * If a signal is received the state transitions back to INIT and\\n\\t * another message is sent. When the userspace handler replies, state\\n\\t * transitions to REPLIED.\\n\\t */\\n\\tenum notify_state state;\\n\\n\\t/* The return values, only valid when in SECCOMP_NOTIFY_REPLIED */\\n\\tint error;\\n\\tlong val;\\n\\tu32 flags;\\n\\n\\t/*\\n\\t * Signals when this has changed states, such as the listener\\n\\t * dying, a new seccomp addfd message, or changing to REPLIED\\n\\t */\\n\\tstruct completion ready;\\n\\n\\tstruct list_head list;\\n\\n\\t/* outstanding addfd requests */\\n\\tstruct list_head addfd;\\n};\\n\\n/**\\n * struct seccomp_kaddfd - container for seccomp_addfd ioctl messages\\n *\\n * @file: A reference to the file to install in the other task\\n * @fd: The fd number to install it at. If the fd number is -1, it means the\\n *      installing process should allocate the fd as normal.\\n * @flags: The flags for the new file descriptor. At the moment, only O_CLOEXEC\\n *         is allowed.\\n * @ioctl_flags: The flags used for the seccomp_addfd ioctl.\\n * @setfd: whether or not SECCOMP_ADDFD_FLAG_SETFD was set during notify_addfd\\n * @ret: The return value of the installing process. It is set to the fd num\\n *       upon success (>= 0).\\n * @completion: Indicates that the installing process has completed fd\\n *              installation, or gone away (either due to successful\\n *              reply, or signal)\\n * @list: list_head for chaining seccomp_kaddfd together.\\n *\\n */\\nstruct seccomp_kaddfd {\\n\\tstruct file *file;\\n\\tint fd;\\n\\tunsigned int flags;\\n\\t__u32 ioctl_flags;\\n\\n\\tunion {\\n\\t\\tbool setfd;\\n\\t\\t/* To only be set on reply */\\n\\t\\tint ret;\\n\\t};\\n\\tstruct completion completion;\\n\\tstruct list_head list;\\n};\\n\\n/**\\n * struct notification - container for seccomp userspace notifications. Since\\n * most seccomp filters will not have notification listeners attached and this\\n * structure is fairly large, we store the notification-specific stuff in a\\n * separate structure.\\n *\\n * @requests: A semaphore that users of this notification can wait on for\\n *            changes. Actual reads and writes are still controlled with\\n *            filter->notify_lock.\\n * @flags: A set of SECCOMP_USER_NOTIF_FD_* flags.\\n * @next_id: The id of the next request.\\n * @notifications: A list of struct seccomp_knotif elements.\\n */\\n\\nstruct notification {\\n\\tatomic_t requests;\\n\\tu32 flags;\\n\\tu64 next_id;\\n\\tstruct list_head notifications;\\n};\\n\\n#ifdef SECCOMP_ARCH_NATIVE\\n/**\\n * struct action_cache - per-filter cache of seccomp actions per\\n * arch/syscall pair\\n *\\n * @allow_native: A bitmap where each bit represents whether the\\n *\\t\\t  filter will always allow the syscall, for the\\n *\\t\\t  native architecture.\\n * @allow_compat: A bitmap where each bit represents whether the\\n *\\t\\t  filter will always allow the syscall, for the\\n *\\t\\t  compat architecture.\\n */\\nstruct action_cache {\\n\\tDECLARE_BITMAP(allow_native, SECCOMP_ARCH_NATIVE_NR);\\n#ifdef SECCOMP_ARCH_COMPAT\\n\\tDECLARE_BITMAP(allow_compat, SECCOMP_ARCH_COMPAT_NR);\\n#endif\\n};\\n#else\\nstruct action_cache { };\\n\\nstatic inline bool seccomp_cache_check_allow(const struct seccomp_filter *sfilter,\\n\\t\\t\\t\\t\\t     const struct seccomp_data *sd)\\n{\\n\\treturn false;\\n}\\n\\nstatic inline void seccomp_cache_prepare(struct seccomp_filter *sfilter)\\n{\\n}\\n#endif /* SECCOMP_ARCH_NATIVE */\\n\\n/**\\n * struct seccomp_filter - container for seccomp BPF programs\\n *\\n * @refs: Reference count to manage the object lifetime.\\n *\\t  A filter\\'s reference count is incremented for each directly\\n *\\t  attached task, once for the dependent filter, and if\\n *\\t  requested for the user notifier. When @refs reaches zero,\\n *\\t  the filter can be freed.\\n * @users: A filter\\'s @users count is incremented for each directly\\n *         attached task (filter installation, fork(), thread_sync),\\n *\\t   and once for the dependent filter (tracked in filter->prev).\\n *\\t   When it reaches zero it indicates that no direct or indirect\\n *\\t   users of that filter exist. No new tasks can get associated with\\n *\\t   this filter after reaching 0. The @users count is always smaller\\n *\\t   or equal to @refs. Hence, reaching 0 for @users does not mean\\n *\\t   the filter can be freed.\\n * @cache: cache of arch/syscall mappings to actions\\n * @log: true if all actions except for SECCOMP_RET_ALLOW should be logged\\n * @wait_killable_recv: Put notifying process in killable state once the\\n *\\t\\t\\tnotification is received by the userspace listener.\\n * @prev: points to a previously installed, or inherited, filter\\n * @prog: the BPF program to evaluate\\n * @notif: the struct that holds all notification related information\\n * @notify_lock: A lock for all notification-related accesses.\\n * @wqh: A wait queue for poll if a notifier is in use.\\n *\\n * seccomp_filter objects are organized in a tree linked via the @prev\\n * pointer.  For any task, it appears to be a singly-linked list starting\\n * with current->seccomp.filter, the most recently attached or inherited filter.\\n * However, multiple filters may share a @prev node, by way of fork(), which\\n * results in a unidirectional tree existing in memory.  This is similar to\\n * how namespaces work.\\n *\\n * seccomp_filter objects should never be modified after being attached\\n * to a task_struct (other than @refs).\\n */\\nstruct seccomp_filter {\\n\\trefcount_t refs;\\n\\trefcount_t users;\\n\\tbool log;\\n\\tbool wait_killable_recv;\\n\\tstruct action_cache cache;\\n\\tstruct seccomp_filter *prev;\\n\\tstruct bpf_prog *prog;\\n\\tstruct notification *notif;\\n\\tstruct mutex notify_lock;\\n\\twait_queue_head_t wqh;\\n};\\n\\n/* Limit any path through the tree to 256KB worth of instructions. */\\n#define MAX_INSNS_PER_PATH ((1 << 18) / sizeof(struct sock_filter))\\n\\n/*\\n * Endianness is explicitly ignored and left for BPF program authors to manage\\n * as per the specific architecture.\\n */\\nstatic void populate_seccomp_data(struct seccomp_data *sd)\\n{\\n\\t/*\\n\\t * Instead of using current_pt_reg(), we\\'re already doing the work\\n\\t * to safely fetch \"current\", so just use \"task\" everywhere below.\\n\\t */\\n\\tstruct task_struct *task = current;\\n\\tstruct pt_regs *regs = task_pt_regs(task);\\n\\tunsigned long args[6];\\n\\n\\tsd->nr = syscall_get_nr(task, regs);\\n\\tsd->arch = syscall_get_arch(task);\\n\\tsyscall_get_arguments(task, regs, args);\\n\\tsd->args[0] = args[0];\\n\\tsd->args[1] = args[1];\\n\\tsd->args[2] = args[2];\\n\\tsd->args[3] = args[3];\\n\\tsd->args[4] = args[4];\\n\\tsd->args[5] = args[5];\\n\\tsd->instruction_pointer = KSTK_EIP(task);\\n}\\n\\n/**\\n *\\tseccomp_check_filter - verify seccomp filter code\\n *\\t@filter: filter to verify\\n *\\t@flen: length of filter\\n *\\n * Takes a previously checked filter (by bpf_check_classic) and\\n * redirects all filter code that loads struct sk_buff data\\n * and related data through seccomp_bpf_load.  It also\\n * enforces length and alignment checking of those loads.\\n *\\n * Returns 0 if the rule set is legal or -EINVAL if not.\\n */\\nstatic int seccomp_check_filter(struct sock_filter *filter, unsigned int flen)\\n{\\n\\tint pc;\\n\\tfor (pc = 0; pc < flen; pc++) {\\n\\t\\tstruct sock_filter *ftest = &filter[pc];\\n\\t\\tu16 code = ftest->code;\\n\\t\\tu32 k = ftest->k;\\n\\n\\t\\tswitch (code) {\\n\\t\\tcase BPF_LD | BPF_W | BPF_ABS:\\n\\t\\t\\tftest->code = BPF_LDX | BPF_W | BPF_ABS;\\n\\t\\t\\t/* 32-bit aligned and not out of bounds. */\\n\\t\\t\\tif (k >= sizeof(struct seccomp_data) || k & 3)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tcontinue;\\n\\t\\tcase BPF_LD | BPF_W | BPF_LEN:\\n\\t\\t\\tftest->code = BPF_LD | BPF_IMM;\\n\\t\\t\\tftest->k = sizeof(struct seccomp_data);\\n\\t\\t\\tcontinue;\\n\\t\\tcase BPF_LDX | BPF_W | BPF_LEN:\\n\\t\\t\\tftest->code = BPF_LDX | BPF_IMM;\\n\\t\\t\\tftest->k = sizeof(struct seccomp_data);\\n\\t\\t\\tcontinue;\\n\\t\\t/* Explicitly include allowed calls. */\\n\\t\\tcase BPF_RET | BPF_K:\\n\\t\\tcase BPF_RET | BPF_A:\\n\\t\\tcase BPF_ALU | BPF_ADD | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_ADD | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_SUB | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_SUB | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_MUL | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_MUL | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_DIV | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_DIV | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_AND | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_AND | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_OR | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_OR | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_XOR | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_XOR | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_LSH | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_LSH | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_RSH | BPF_K:\\n\\t\\tcase BPF_ALU | BPF_RSH | BPF_X:\\n\\t\\tcase BPF_ALU | BPF_NEG:\\n\\t\\tcase BPF_LD | BPF_IMM:\\n\\t\\tcase BPF_LDX | BPF_IMM:\\n\\t\\tcase BPF_MISC | BPF_TAX:\\n\\t\\tcase BPF_MISC | BPF_TXA:\\n\\t\\tcase BPF_LD | BPF_MEM:\\n\\t\\tcase BPF_LDX | BPF_MEM:\\n\\t\\tcase BPF_ST:\\n\\t\\tcase BPF_STX:\\n\\t\\tcase BPF_JMP | BPF_JA:\\n\\t\\tcase BPF_JMP | BPF_JEQ | BPF_K:\\n\\t\\tcase BPF_JMP | BPF_JEQ | BPF_X:\\n\\t\\tcase BPF_JMP | BPF_JGE | BPF_K:\\n\\t\\tcase BPF_JMP | BPF_JGE | BPF_X:\\n\\t\\tcase BPF_JMP | BPF_JGT | BPF_K:\\n\\t\\tcase BPF_JMP | BPF_JGT | BPF_X:\\n\\t\\tcase BPF_JMP | BPF_JSET | BPF_K:\\n\\t\\tcase BPF_JMP | BPF_JSET | BPF_X:\\n\\t\\t\\tcontinue;\\n\\t\\tdefault:\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\n#ifdef SECCOMP_ARCH_NATIVE\\nstatic inline bool seccomp_cache_check_allow_bitmap(const void *bitmap,\\n\\t\\t\\t\\t\\t\\t    size_t bitmap_size,\\n\\t\\t\\t\\t\\t\\t    int syscall_nr)\\n{\\n\\tif (unlikely(syscall_nr < 0 || syscall_nr >= bitmap_size))\\n\\t\\treturn false;\\n\\tsyscall_nr = array_index_nospec(syscall_nr, bitmap_size);\\n\\n\\treturn test_bit(syscall_nr, bitmap);\\n}\\n\\n/**\\n * seccomp_cache_check_allow - lookup seccomp cache\\n * @sfilter: The seccomp filter\\n * @sd: The seccomp data to lookup the cache with\\n *\\n * Returns true if the seccomp_data is cached and allowed.\\n */\\nstatic inline bool seccomp_cache_check_allow(const struct seccomp_filter *sfilter,\\n\\t\\t\\t\\t\\t     const struct seccomp_data *sd)\\n{\\n\\tint syscall_nr = sd->nr;\\n\\tconst struct action_cache *cache = &sfilter->cache;\\n\\n#ifndef SECCOMP_ARCH_COMPAT\\n\\t/* A native-only architecture doesn\\'t need to check sd->arch. */\\n\\treturn seccomp_cache_check_allow_bitmap(cache->allow_native,\\n\\t\\t\\t\\t\\t\\tSECCOMP_ARCH_NATIVE_NR,\\n\\t\\t\\t\\t\\t\\tsyscall_nr);\\n#else\\n\\tif (likely(sd->arch == SECCOMP_ARCH_NATIVE))\\n\\t\\treturn seccomp_cache_check_allow_bitmap(cache->allow_native,\\n\\t\\t\\t\\t\\t\\t\\tSECCOMP_ARCH_NATIVE_NR,\\n\\t\\t\\t\\t\\t\\t\\tsyscall_nr);\\n\\tif (likely(sd->arch == SECCOMP_ARCH_COMPAT))\\n\\t\\treturn seccomp_cache_check_allow_bitmap(cache->allow_compat,\\n\\t\\t\\t\\t\\t\\t\\tSECCOMP_ARCH_COMPAT_NR,\\n\\t\\t\\t\\t\\t\\t\\tsyscall_nr);\\n#endif /* SECCOMP_ARCH_COMPAT */\\n\\n\\tWARN_ON_ONCE(true);\\n\\treturn false;\\n}\\n#endif /* SECCOMP_ARCH_NATIVE */\\n\\n#define ACTION_ONLY(ret) ((s32)((ret) & (SECCOMP_RET_ACTION_FULL)))\\n/**\\n * seccomp_run_filters - evaluates all seccomp filters against @sd\\n * @sd: optional seccomp data to be passed to filters\\n * @match: stores struct seccomp_filter that resulted in the return value,\\n *         unless filter returned SECCOMP_RET_ALLOW, in which case it will\\n *         be unchanged.\\n *\\n * Returns valid seccomp BPF response codes.\\n */\\nstatic u32 seccomp_run_filters(const struct seccomp_data *sd,\\n\\t\\t\\t       struct seccomp_filter **match)\\n{\\n\\tu32 ret = SECCOMP_RET_ALLOW;\\n\\t/* Make sure cross-thread synced filter points somewhere sane. */\\n\\tstruct seccomp_filter *f =\\n\\t\\t\\tREAD_ONCE(current->seccomp.filter);\\n\\n\\t/* Ensure unexpected behavior doesn\\'t result in failing open. */\\n\\tif (WARN_ON(f == NULL))\\n\\t\\treturn SECCOMP_RET_KILL_PROCESS;\\n\\n\\tif (seccomp_cache_check_allow(f, sd))\\n\\t\\treturn SECCOMP_RET_ALLOW;\\n\\n\\t/*\\n\\t * All filters in the list are evaluated and the lowest BPF return\\n\\t * value always takes priority (ignoring the DATA).\\n\\t */\\n\\tfor (; f; f = f->prev) {\\n\\t\\tu32 cur_ret = bpf_prog_run_pin_on_cpu(f->prog, sd);\\n\\n\\t\\tif (ACTION_ONLY(cur_ret) < ACTION_ONLY(ret)) {\\n\\t\\t\\tret = cur_ret;\\n\\t\\t\\t*match = f;\\n\\t\\t}\\n\\t}\\n\\treturn ret;\\n}\\n#endif /* CONFIG_SECCOMP_FILTER */\\n\\nstatic inline bool seccomp_may_assign_mode(unsigned long seccomp_mode)\\n{\\n\\tassert_spin_locked(&current->sighand->siglock);\\n\\n\\tif (current->seccomp.mode && current->seccomp.mode != seccomp_mode)\\n\\t\\treturn false;\\n\\n\\treturn true;\\n}\\n\\nvoid __weak arch_seccomp_spec_mitigate(struct task_struct *task) { }\\n\\nstatic inline void seccomp_assign_mode(struct task_struct *task,\\n\\t\\t\\t\\t       unsigned long seccomp_mode,\\n\\t\\t\\t\\t       unsigned long flags)\\n{\\n\\tassert_spin_locked(&task->sighand->siglock);\\n\\n\\ttask->seccomp.mode = seccomp_mode;\\n\\t/*\\n\\t * Make sure SYSCALL_WORK_SECCOMP cannot be set before the mode (and\\n\\t * filter) is set.\\n\\t */\\n\\tsmp_mb__before_atomic();\\n\\t/* Assume default seccomp processes want spec flaw mitigation. */\\n\\tif ((flags & SECCOMP_FILTER_FLAG_SPEC_ALLOW) == 0)\\n\\t\\tarch_seccomp_spec_mitigate(task);\\n\\tset_task_syscall_work(task, SECCOMP);\\n}\\n\\n#ifdef CONFIG_SECCOMP_FILTER\\n/* Returns 1 if the parent is an ancestor of the child. */\\nstatic int is_ancestor(struct seccomp_filter *parent,\\n\\t\\t       struct seccomp_filter *child)\\n{\\n\\t/* NULL is the root ancestor. */\\n\\tif (parent == NULL)\\n\\t\\treturn 1;\\n\\tfor (; child; child = child->prev)\\n\\t\\tif (child == parent)\\n\\t\\t\\treturn 1;\\n\\treturn 0;\\n}\\n\\n/**\\n * seccomp_can_sync_threads: checks if all threads can be synchronized\\n *\\n * Expects sighand and cred_guard_mutex locks to be held.\\n *\\n * Returns 0 on success, -ve on error, or the pid of a thread which was\\n * either not in the correct seccomp mode or did not have an ancestral\\n * seccomp filter.\\n */\\nstatic inline pid_t seccomp_can_sync_threads(void)\\n{\\n\\tstruct task_struct *thread, *caller;\\n\\n\\tBUG_ON(!mutex_is_locked(&current->signal->cred_guard_mutex));\\n\\tassert_spin_locked(&current->sighand->siglock);\\n\\n\\t/* Validate all threads being eligible for synchronization. */\\n\\tcaller = current;\\n\\tfor_each_thread(caller, thread) {\\n\\t\\tpid_t failed;\\n\\n\\t\\t/* Skip current, since it is initiating the sync. */\\n\\t\\tif (thread == caller)\\n\\t\\t\\tcontinue;\\n\\t\\t/* Skip exited threads. */\\n\\t\\tif (thread->flags & PF_EXITING)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (thread->seccomp.mode == SECCOMP_MODE_DISABLED ||\\n\\t\\t    (thread->seccomp.mode == SECCOMP_MODE_FILTER &&\\n\\t\\t     is_ancestor(thread->seccomp.filter,\\n\\t\\t\\t\\t caller->seccomp.filter)))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* Return the first thread that cannot be synchronized. */\\n\\t\\tfailed = task_pid_vnr(thread);\\n\\t\\t/* If the pid cannot be resolved, then return -ESRCH */\\n\\t\\tif (WARN_ON(failed == 0))\\n\\t\\t\\tfailed = -ESRCH;\\n\\t\\treturn failed;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic inline void seccomp_filter_free(struct seccomp_filter *filter)\\n{\\n\\tif (filter) {\\n\\t\\tbpf_prog_destroy(filter->prog);\\n\\t\\tkfree(filter);\\n\\t}\\n}\\n\\nstatic void __seccomp_filter_orphan(struct seccomp_filter *orig)\\n{\\n\\twhile (orig && refcount_dec_and_test(&orig->users)) {\\n\\t\\tif (waitqueue_active(&orig->wqh))\\n\\t\\t\\twake_up_poll(&orig->wqh, EPOLLHUP);\\n\\t\\torig = orig->prev;\\n\\t}\\n}\\n\\nstatic void __put_seccomp_filter(struct seccomp_filter *orig)\\n{\\n\\t/* Clean up single-reference branches iteratively. */\\n\\twhile (orig && refcount_dec_and_test(&orig->refs)) {\\n\\t\\tstruct seccomp_filter *freeme = orig;\\n\\t\\torig = orig->prev;\\n\\t\\tseccomp_filter_free(freeme);\\n\\t}\\n}\\n\\nstatic void __seccomp_filter_release(struct seccomp_filter *orig)\\n{\\n\\t/* Notify about any unused filters in the task\\'s former filter tree. */\\n\\t__seccomp_filter_orphan(orig);\\n\\t/* Finally drop all references to the task\\'s former tree. */\\n\\t__put_seccomp_filter(orig);\\n}\\n\\n/**\\n * seccomp_filter_release - Detach the task from its filter tree,\\n *\\t\\t\\t    drop its reference count, and notify\\n *\\t\\t\\t    about unused filters\\n *\\n * @tsk: task the filter should be released from.\\n *\\n * This function should only be called when the task is exiting as\\n * it detaches it from its filter tree. PF_EXITING has to be set\\n * for the task.\\n */\\nvoid seccomp_filter_release(struct task_struct *tsk)\\n{\\n\\tstruct seccomp_filter *orig;\\n\\n\\tif (WARN_ON((tsk->flags & PF_EXITING) == 0))\\n\\t\\treturn;\\n\\n\\tspin_lock_irq(&tsk->sighand->siglock);\\n\\torig = tsk->seccomp.filter;\\n\\t/* Detach task from its filter tree. */\\n\\ttsk->seccomp.filter = NULL;\\n\\tspin_unlock_irq(&tsk->sighand->siglock);\\n\\t__seccomp_filter_release(orig);\\n}\\n\\n/**\\n * seccomp_sync_threads: sets all threads to use current\\'s filter\\n *\\n * @flags: SECCOMP_FILTER_FLAG_* flags to set during sync.\\n *\\n * Expects sighand and cred_guard_mutex locks to be held, and for\\n * seccomp_can_sync_threads() to have returned success already\\n * without dropping the locks.\\n *\\n */\\nstatic inline void seccomp_sync_threads(unsigned long flags)\\n{\\n\\tstruct task_struct *thread, *caller;\\n\\n\\tBUG_ON(!mutex_is_locked(&current->signal->cred_guard_mutex));\\n\\tassert_spin_locked(&current->sighand->siglock);\\n\\n\\t/* Synchronize all threads. */\\n\\tcaller = current;\\n\\tfor_each_thread(caller, thread) {\\n\\t\\t/* Skip current, since it needs no changes. */\\n\\t\\tif (thread == caller)\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * Skip exited threads. seccomp_filter_release could have\\n\\t\\t * been already called for this task.\\n\\t\\t */\\n\\t\\tif (thread->flags & PF_EXITING)\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* Get a task reference for the new leaf node. */\\n\\t\\tget_seccomp_filter(caller);\\n\\n\\t\\t/*\\n\\t\\t * Drop the task reference to the shared ancestor since\\n\\t\\t * current\\'s path will hold a reference.  (This also\\n\\t\\t * allows a put before the assignment.)\\n\\t\\t */\\n\\t\\t__seccomp_filter_release(thread->seccomp.filter);\\n\\n\\t\\t/* Make our new filter tree visible. */\\n\\t\\tsmp_store_release(&thread->seccomp.filter,\\n\\t\\t\\t\\t  caller->seccomp.filter);\\n\\t\\tatomic_set(&thread->seccomp.filter_count,\\n\\t\\t\\t   atomic_read(&caller->seccomp.filter_count));\\n\\n\\t\\t/*\\n\\t\\t * Don\\'t let an unprivileged task work around\\n\\t\\t * the no_new_privs restriction by creating\\n\\t\\t * a thread that sets it up, enters seccomp,\\n\\t\\t * then dies.\\n\\t\\t */\\n\\t\\tif (task_no_new_privs(caller))\\n\\t\\t\\ttask_set_no_new_privs(thread);\\n\\n\\t\\t/*\\n\\t\\t * Opt the other thread into seccomp if needed.\\n\\t\\t * As threads are considered to be trust-realm\\n\\t\\t * equivalent (see ptrace_may_access), it is safe to\\n\\t\\t * allow one thread to transition the other.\\n\\t\\t */\\n\\t\\tif (thread->seccomp.mode == SECCOMP_MODE_DISABLED)\\n\\t\\t\\tseccomp_assign_mode(thread, SECCOMP_MODE_FILTER,\\n\\t\\t\\t\\t\\t    flags);\\n\\t}\\n}\\n\\n/**\\n * seccomp_prepare_filter: Prepares a seccomp filter for use.\\n * @fprog: BPF program to install\\n *\\n * Returns filter on success or an ERR_PTR on failure.\\n */\\nstatic struct seccomp_filter *seccomp_prepare_filter(struct sock_fprog *fprog)\\n{\\n\\tstruct seccomp_filter *sfilter;\\n\\tint ret;\\n\\tconst bool save_orig =\\n#if defined(CONFIG_CHECKPOINT_RESTORE) || defined(SECCOMP_ARCH_NATIVE)\\n\\t\\ttrue;\\n#else\\n\\t\\tfalse;\\n#endif\\n\\n\\tif (fprog->len == 0 || fprog->len > BPF_MAXINSNS)\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\tBUG_ON(INT_MAX / fprog->len < sizeof(struct sock_filter));\\n\\n\\t/*\\n\\t * Installing a seccomp filter requires that the task has\\n\\t * CAP_SYS_ADMIN in its namespace or be running with no_new_privs.\\n\\t * This avoids scenarios where unprivileged tasks can affect the\\n\\t * behavior of privileged children.\\n\\t */\\n\\tif (!task_no_new_privs(current) &&\\n\\t\\t\\t!ns_capable_noaudit(current_user_ns(), CAP_SYS_ADMIN))\\n\\t\\treturn ERR_PTR(-EACCES);\\n\\n\\t/* Allocate a new seccomp_filter */\\n\\tsfilter = kzalloc(sizeof(*sfilter), GFP_KERNEL | __GFP_NOWARN);\\n\\tif (!sfilter)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tmutex_init(&sfilter->notify_lock);\\n\\tret = bpf_prog_create_from_user(&sfilter->prog, fprog,\\n\\t\\t\\t\\t\\tseccomp_check_filter, save_orig);\\n\\tif (ret < 0) {\\n\\t\\tkfree(sfilter);\\n\\t\\treturn ERR_PTR(ret);\\n\\t}\\n\\n\\trefcount_set(&sfilter->refs, 1);\\n\\trefcount_set(&sfilter->users, 1);\\n\\tinit_waitqueue_head(&sfilter->wqh);\\n\\n\\treturn sfilter;\\n}\\n\\n/**\\n * seccomp_prepare_user_filter - prepares a user-supplied sock_fprog\\n * @user_filter: pointer to the user data containing a sock_fprog.\\n *\\n * Returns 0 on success and non-zero otherwise.\\n */\\nstatic struct seccomp_filter *\\nseccomp_prepare_user_filter(const char __user *user_filter)\\n{\\n\\tstruct sock_fprog fprog;\\n\\tstruct seccomp_filter *filter = ERR_PTR(-EFAULT);\\n\\n#ifdef CONFIG_COMPAT\\n\\tif (in_compat_syscall()) {\\n\\t\\tstruct compat_sock_fprog fprog32;\\n\\t\\tif (copy_from_user(&fprog32, user_filter, sizeof(fprog32)))\\n\\t\\t\\tgoto out;\\n\\t\\tfprog.len = fprog32.len;\\n\\t\\tfprog.filter = compat_ptr(fprog32.filter);\\n\\t} else /* falls through to the if below. */\\n#endif\\n\\tif (copy_from_user(&fprog, user_filter, sizeof(fprog)))\\n\\t\\tgoto out;\\n\\tfilter = seccomp_prepare_filter(&fprog);\\nout:\\n\\treturn filter;\\n}\\n\\n#ifdef SECCOMP_ARCH_NATIVE\\n/**\\n * seccomp_is_const_allow - check if filter is constant allow with given data\\n * @fprog: The BPF programs\\n * @sd: The seccomp data to check against, only syscall number and arch\\n *      number are considered constant.\\n */\\nstatic bool seccomp_is_const_allow(struct sock_fprog_kern *fprog,\\n\\t\\t\\t\\t   struct seccomp_data *sd)\\n{\\n\\tunsigned int reg_value = 0;\\n\\tunsigned int pc;\\n\\tbool op_res;\\n\\n\\tif (WARN_ON_ONCE(!fprog))\\n\\t\\treturn false;\\n\\n\\tfor (pc = 0; pc < fprog->len; pc++) {\\n\\t\\tstruct sock_filter *insn = &fprog->filter[pc];\\n\\t\\tu16 code = insn->code;\\n\\t\\tu32 k = insn->k;\\n\\n\\t\\tswitch (code) {\\n\\t\\tcase BPF_LD | BPF_W | BPF_ABS:\\n\\t\\t\\tswitch (k) {\\n\\t\\t\\tcase offsetof(struct seccomp_data, nr):\\n\\t\\t\\t\\treg_value = sd->nr;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase offsetof(struct seccomp_data, arch):\\n\\t\\t\\t\\treg_value = sd->arch;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tdefault:\\n\\t\\t\\t\\t/* can\\'t optimize (non-constant value load) */\\n\\t\\t\\t\\treturn false;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase BPF_RET | BPF_K:\\n\\t\\t\\t/* reached return with constant values only, check allow */\\n\\t\\t\\treturn k == SECCOMP_RET_ALLOW;\\n\\t\\tcase BPF_JMP | BPF_JA:\\n\\t\\t\\tpc += insn->k;\\n\\t\\t\\tbreak;\\n\\t\\tcase BPF_JMP | BPF_JEQ | BPF_K:\\n\\t\\tcase BPF_JMP | BPF_JGE | BPF_K:\\n\\t\\tcase BPF_JMP | BPF_JGT | BPF_K:\\n\\t\\tcase BPF_JMP | BPF_JSET | BPF_K:\\n\\t\\t\\tswitch (BPF_OP(code)) {\\n\\t\\t\\tcase BPF_JEQ:\\n\\t\\t\\t\\top_res = reg_value == k;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase BPF_JGE:\\n\\t\\t\\t\\top_res = reg_value >= k;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase BPF_JGT:\\n\\t\\t\\t\\top_res = reg_value > k;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase BPF_JSET:\\n\\t\\t\\t\\top_res = !!(reg_value & k);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tdefault:\\n\\t\\t\\t\\t/* can\\'t optimize (unknown jump) */\\n\\t\\t\\t\\treturn false;\\n\\t\\t\\t}\\n\\n\\t\\t\\tpc += op_res ? insn->jt : insn->jf;\\n\\t\\t\\tbreak;\\n\\t\\tcase BPF_ALU | BPF_AND | BPF_K:\\n\\t\\t\\treg_value &= k;\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\t/* can\\'t optimize (unknown insn) */\\n\\t\\t\\treturn false;\\n\\t\\t}\\n\\t}\\n\\n\\t/* ran off the end of the filter?! */\\n\\tWARN_ON(1);\\n\\treturn false;\\n}\\n\\nstatic void seccomp_cache_prepare_bitmap(struct seccomp_filter *sfilter,\\n\\t\\t\\t\\t\\t void *bitmap, const void *bitmap_prev,\\n\\t\\t\\t\\t\\t size_t bitmap_size, int arch)\\n{\\n\\tstruct sock_fprog_kern *fprog = sfilter->prog->orig_prog;\\n\\tstruct seccomp_data sd;\\n\\tint nr;\\n\\n\\tif (bitmap_prev) {\\n\\t\\t/* The new filter must be as restrictive as the last. */\\n\\t\\tbitmap_copy(bitmap, bitmap_prev, bitmap_size);\\n\\t} else {\\n\\t\\t/* Before any filters, all syscalls are always allowed. */\\n\\t\\tbitmap_fill(bitmap, bitmap_size);\\n\\t}\\n\\n\\tfor (nr = 0; nr < bitmap_size; nr++) {\\n\\t\\t/* No bitmap change: not a cacheable action. */\\n\\t\\tif (!test_bit(nr, bitmap))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tsd.nr = nr;\\n\\t\\tsd.arch = arch;\\n\\n\\t\\t/* No bitmap change: continue to always allow. */\\n\\t\\tif (seccomp_is_const_allow(fprog, &sd))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * Not a cacheable action: always run filters.\\n\\t\\t * atomic clear_bit() not needed, filter not visible yet.\\n\\t\\t */\\n\\t\\t__clear_bit(nr, bitmap);\\n\\t}\\n}\\n\\n/**\\n * seccomp_cache_prepare - emulate the filter to find cacheable syscalls\\n * @sfilter: The seccomp filter\\n *\\n * Returns 0 if successful or -errno if error occurred.\\n */\\nstatic void seccomp_cache_prepare(struct seccomp_filter *sfilter)\\n{\\n\\tstruct action_cache *cache = &sfilter->cache;\\n\\tconst struct action_cache *cache_prev =\\n\\t\\tsfilter->prev ? &sfilter->prev->cache : NULL;\\n\\n\\tseccomp_cache_prepare_bitmap(sfilter, cache->allow_native,\\n\\t\\t\\t\\t     cache_prev ? cache_prev->allow_native : NULL,\\n\\t\\t\\t\\t     SECCOMP_ARCH_NATIVE_NR,\\n\\t\\t\\t\\t     SECCOMP_ARCH_NATIVE);\\n\\n#ifdef SECCOMP_ARCH_COMPAT\\n\\tseccomp_cache_prepare_bitmap(sfilter, cache->allow_compat,\\n\\t\\t\\t\\t     cache_prev ? cache_prev->allow_compat : NULL,\\n\\t\\t\\t\\t     SECCOMP_ARCH_COMPAT_NR,\\n\\t\\t\\t\\t     SECCOMP_ARCH_COMPAT);\\n#endif /* SECCOMP_ARCH_COMPAT */\\n}\\n#endif /* SECCOMP_ARCH_NATIVE */\\n\\n/**\\n * seccomp_attach_filter: validate and attach filter\\n * @flags:  flags to change filter behavior\\n * @filter: seccomp filter to add to the current process\\n *\\n * Caller must be holding current->sighand->siglock lock.\\n *\\n * Returns 0 on success, -ve on error, or\\n *   - in TSYNC mode: the pid of a thread which was either not in the correct\\n *     seccomp mode or did not have an ancestral seccomp filter\\n *   - in NEW_LISTENER mode: the fd of the new listener\\n */\\nstatic long seccomp_attach_filter(unsigned int flags,\\n\\t\\t\\t\\t  struct seccomp_filter *filter)\\n{\\n\\tunsigned long total_insns;\\n\\tstruct seccomp_filter *walker;\\n\\n\\tassert_spin_locked(&current->sighand->siglock);\\n\\n\\t/* Validate resulting filter length. */\\n\\ttotal_insns = filter->prog->len;\\n\\tfor (walker = current->seccomp.filter; walker; walker = walker->prev)\\n\\t\\ttotal_insns += walker->prog->len + 4;  /* 4 instr penalty */\\n\\tif (total_insns > MAX_INSNS_PER_PATH)\\n\\t\\treturn -ENOMEM;\\n\\n\\t/* If thread sync has been requested, check that it is possible. */\\n\\tif (flags & SECCOMP_FILTER_FLAG_TSYNC) {\\n\\t\\tint ret;\\n\\n\\t\\tret = seccomp_can_sync_threads();\\n\\t\\tif (ret) {\\n\\t\\t\\tif (flags & SECCOMP_FILTER_FLAG_TSYNC_ESRCH)\\n\\t\\t\\t\\treturn -ESRCH;\\n\\t\\t\\telse\\n\\t\\t\\t\\treturn ret;\\n\\t\\t}\\n\\t}\\n\\n\\t/* Set log flag, if present. */\\n\\tif (flags & SECCOMP_FILTER_FLAG_LOG)\\n\\t\\tfilter->log = true;\\n\\n\\t/* Set wait killable flag, if present. */\\n\\tif (flags & SECCOMP_FILTER_FLAG_WAIT_KILLABLE_RECV)\\n\\t\\tfilter->wait_killable_recv = true;\\n\\n\\t/*\\n\\t * If there is an existing filter, make it the prev and don\\'t drop its\\n\\t * task reference.\\n\\t */\\n\\tfilter->prev = current->seccomp.filter;\\n\\tseccomp_cache_prepare(filter);\\n\\tcurrent->seccomp.filter = filter;\\n\\tatomic_inc(&current->seccomp.filter_count);\\n\\n\\t/* Now that the new filter is in place, synchronize to all threads. */\\n\\tif (flags & SECCOMP_FILTER_FLAG_TSYNC)\\n\\t\\tseccomp_sync_threads(flags);\\n\\n\\treturn 0;\\n}\\n\\nstatic void __get_seccomp_filter(struct seccomp_filter *filter)\\n{\\n\\trefcount_inc(&filter->refs);\\n}\\n\\n/* get_seccomp_filter - increments the reference count of the filter on @tsk */\\nvoid get_seccomp_filter(struct task_struct *tsk)\\n{\\n\\tstruct seccomp_filter *orig = tsk->seccomp.filter;\\n\\tif (!orig)\\n\\t\\treturn;\\n\\t__get_seccomp_filter(orig);\\n\\trefcount_inc(&orig->users);\\n}\\n\\n#endif\\t/* CONFIG_SECCOMP_FILTER */\\n\\n/* For use with seccomp_actions_logged */\\n#define SECCOMP_LOG_KILL_PROCESS\\t(1 << 0)\\n#define SECCOMP_LOG_KILL_THREAD\\t\\t(1 << 1)\\n#define SECCOMP_LOG_TRAP\\t\\t(1 << 2)\\n#define SECCOMP_LOG_ERRNO\\t\\t(1 << 3)\\n#define SECCOMP_LOG_TRACE\\t\\t(1 << 4)\\n#define SECCOMP_LOG_LOG\\t\\t\\t(1 << 5)\\n#define SECCOMP_LOG_ALLOW\\t\\t(1 << 6)\\n#define SECCOMP_LOG_USER_NOTIF\\t\\t(1 << 7)\\n\\nstatic u32 seccomp_actions_logged = SECCOMP_LOG_KILL_PROCESS |\\n\\t\\t\\t\\t    SECCOMP_LOG_KILL_THREAD  |\\n\\t\\t\\t\\t    SECCOMP_LOG_TRAP  |\\n\\t\\t\\t\\t    SECCOMP_LOG_ERRNO |\\n\\t\\t\\t\\t    SECCOMP_LOG_USER_NOTIF |\\n\\t\\t\\t\\t    SECCOMP_LOG_TRACE |\\n\\t\\t\\t\\t    SECCOMP_LOG_LOG;\\n\\nstatic inline void seccomp_log(unsigned long syscall, long signr, u32 action,\\n\\t\\t\\t       bool requested)\\n{\\n\\tbool log = false;\\n\\n\\tswitch (action) {\\n\\tcase SECCOMP_RET_ALLOW:\\n\\t\\tbreak;\\n\\tcase SECCOMP_RET_TRAP:\\n\\t\\tlog = requested && seccomp_actions_logged & SECCOMP_LOG_TRAP;\\n\\t\\tbreak;\\n\\tcase SECCOMP_RET_ERRNO:\\n\\t\\tlog = requested && seccomp_actions_logged & SECCOMP_LOG_ERRNO;\\n\\t\\tbreak;\\n\\tcase SECCOMP_RET_TRACE:\\n\\t\\tlog = requested && seccomp_actions_logged & SECCOMP_LOG_TRACE;\\n\\t\\tbreak;\\n\\tcase SECCOMP_RET_USER_NOTIF:\\n\\t\\tlog = requested && seccomp_actions_logged & SECCOMP_LOG_USER_NOTIF;\\n\\t\\tbreak;\\n\\tcase SECCOMP_RET_LOG:\\n\\t\\tlog = seccomp_actions_logged & SECCOMP_LOG_LOG;\\n\\t\\tbreak;\\n\\tcase SECCOMP_RET_KILL_THREAD:\\n\\t\\tlog = seccomp_actions_logged & SECCOMP_LOG_KILL_THREAD;\\n\\t\\tbreak;\\n\\tcase SECCOMP_RET_KILL_PROCESS:\\n\\tdefault:\\n\\t\\tlog = seccomp_actions_logged & SECCOMP_LOG_KILL_PROCESS;\\n\\t}\\n\\n\\t/*\\n\\t * Emit an audit message when the action is RET_KILL_*, RET_LOG, or the\\n\\t * FILTER_FLAG_LOG bit was set. The admin has the ability to silence\\n\\t * any action from being logged by removing the action name from the\\n\\t * seccomp_actions_logged sysctl.\\n\\t */\\n\\tif (!log)\\n\\t\\treturn;\\n\\n\\taudit_seccomp(syscall, signr, action);\\n}\\n\\n/*\\n * Secure computing mode 1 allows only read/write/exit/sigreturn.\\n * To be fully secure this must be combined with rlimit\\n * to limit the stack allocations too.\\n */\\nstatic const int mode1_syscalls[] = {\\n\\t__NR_seccomp_read, __NR_seccomp_write, __NR_seccomp_exit, __NR_seccomp_sigreturn,\\n\\t-1, /* negative terminated */\\n};\\n\\nstatic void __secure_computing_strict(int this_syscall)\\n{\\n\\tconst int *allowed_syscalls = mode1_syscalls;\\n#ifdef CONFIG_COMPAT\\n\\tif (in_compat_syscall())\\n\\t\\tallowed_syscalls = get_compat_mode1_syscalls();\\n#endif\\n\\tdo {\\n\\t\\tif (*allowed_syscalls == this_syscall)\\n\\t\\t\\treturn;\\n\\t} while (*++allowed_syscalls != -1);\\n\\n#ifdef SECCOMP_DEBUG\\n\\tdump_stack();\\n#endif\\n\\tcurrent->seccomp.mode = SECCOMP_MODE_DEAD;\\n\\tseccomp_log(this_syscall, SIGKILL, SECCOMP_RET_KILL_THREAD, true);\\n\\tdo_exit(SIGKILL);\\n}\\n\\n#ifndef CONFIG_HAVE_ARCH_SECCOMP_FILTER\\nvoid secure_computing_strict(int this_syscall)\\n{\\n\\tint mode = current->seccomp.mode;\\n\\n\\tif (IS_ENABLED(CONFIG_CHECKPOINT_RESTORE) &&\\n\\t    unlikely(current->ptrace & PT_SUSPEND_SECCOMP))\\n\\t\\treturn;\\n\\n\\tif (mode == SECCOMP_MODE_DISABLED)\\n\\t\\treturn;\\n\\telse if (mode == SECCOMP_MODE_STRICT)\\n\\t\\t__secure_computing_strict(this_syscall);\\n\\telse\\n\\t\\tBUG();\\n}\\n#else\\n\\n#ifdef CONFIG_SECCOMP_FILTER\\nstatic u64 seccomp_next_notify_id(struct seccomp_filter *filter)\\n{\\n\\t/*\\n\\t * Note: overflow is ok here, the id just needs to be unique per\\n\\t * filter.\\n\\t */\\n\\tlockdep_assert_held(&filter->notify_lock);\\n\\treturn filter->notif->next_id++;\\n}\\n\\nstatic void seccomp_handle_addfd(struct seccomp_kaddfd *addfd, struct seccomp_knotif *n)\\n{\\n\\tint fd;\\n\\n\\t/*\\n\\t * Remove the notification, and reset the list pointers, indicating\\n\\t * that it has been handled.\\n\\t */\\n\\tlist_del_init(&addfd->list);\\n\\tif (!addfd->setfd)\\n\\t\\tfd = receive_fd(addfd->file, NULL, addfd->flags);\\n\\telse\\n\\t\\tfd = receive_fd_replace(addfd->fd, addfd->file, addfd->flags);\\n\\taddfd->ret = fd;\\n\\n\\tif (addfd->ioctl_flags & SECCOMP_ADDFD_FLAG_SEND) {\\n\\t\\t/* If we fail reset and return an error to the notifier */\\n\\t\\tif (fd < 0) {\\n\\t\\t\\tn->state = SECCOMP_NOTIFY_SENT;\\n\\t\\t} else {\\n\\t\\t\\t/* Return the FD we just added */\\n\\t\\t\\tn->flags = 0;\\n\\t\\t\\tn->error = 0;\\n\\t\\t\\tn->val = fd;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * Mark the notification as completed. From this point, addfd mem\\n\\t * might be invalidated and we can\\'t safely read it anymore.\\n\\t */\\n\\tcomplete(&addfd->completion);\\n}\\n\\nstatic bool should_sleep_killable(struct seccomp_filter *match,\\n\\t\\t\\t\\t  struct seccomp_knotif *n)\\n{\\n\\treturn match->wait_killable_recv && n->state == SECCOMP_NOTIFY_SENT;\\n}\\n\\nstatic int seccomp_do_user_notification(int this_syscall,\\n\\t\\t\\t\\t\\tstruct seccomp_filter *match,\\n\\t\\t\\t\\t\\tconst struct seccomp_data *sd)\\n{\\n\\tint err;\\n\\tu32 flags = 0;\\n\\tlong ret = 0;\\n\\tstruct seccomp_knotif n = {};\\n\\tstruct seccomp_kaddfd *addfd, *tmp;\\n\\n\\tmutex_lock(&match->notify_lock);\\n\\terr = -ENOSYS;\\n\\tif (!match->notif)\\n\\t\\tgoto out;\\n\\n\\tn.task = current;\\n\\tn.state = SECCOMP_NOTIFY_INIT;\\n\\tn.data = sd;\\n\\tn.id = seccomp_next_notify_id(match);\\n\\tinit_completion(&n.ready);\\n\\tlist_add_tail(&n.list, &match->notif->notifications);\\n\\tINIT_LIST_HEAD(&n.addfd);\\n\\n\\tatomic_inc(&match->notif->requests);\\n\\tif (match->notif->flags & SECCOMP_USER_NOTIF_FD_SYNC_WAKE_UP)\\n\\t\\twake_up_poll_on_current_cpu(&match->wqh, EPOLLIN | EPOLLRDNORM);\\n\\telse\\n\\t\\twake_up_poll(&match->wqh, EPOLLIN | EPOLLRDNORM);\\n\\n\\t/*\\n\\t * This is where we wait for a reply from userspace.\\n\\t */\\n\\tdo {\\n\\t\\tbool wait_killable = should_sleep_killable(match, &n);\\n\\n\\t\\tmutex_unlock(&match->notify_lock);\\n\\t\\tif (wait_killable)\\n\\t\\t\\terr = wait_for_completion_killable(&n.ready);\\n\\t\\telse\\n\\t\\t\\terr = wait_for_completion_interruptible(&n.ready);\\n\\t\\tmutex_lock(&match->notify_lock);\\n\\n\\t\\tif (err != 0) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Check to see if the notifcation got picked up and\\n\\t\\t\\t * whether we should switch to wait killable.\\n\\t\\t\\t */\\n\\t\\t\\tif (!wait_killable && should_sleep_killable(match, &n))\\n\\t\\t\\t\\tcontinue;\\n\\n\\t\\t\\tgoto interrupted;\\n\\t\\t}\\n\\n\\t\\taddfd = list_first_entry_or_null(&n.addfd,\\n\\t\\t\\t\\t\\t\\t struct seccomp_kaddfd, list);\\n\\t\\t/* Check if we were woken up by a addfd message */\\n\\t\\tif (addfd)\\n\\t\\t\\tseccomp_handle_addfd(addfd, &n);\\n\\n\\t}  while (n.state != SECCOMP_NOTIFY_REPLIED);\\n\\n\\tret = n.val;\\n\\terr = n.error;\\n\\tflags = n.flags;\\n\\ninterrupted:\\n\\t/* If there were any pending addfd calls, clear them out */\\n\\tlist_for_each_entry_safe(addfd, tmp, &n.addfd, list) {\\n\\t\\t/* The process went away before we got a chance to handle it */\\n\\t\\taddfd->ret = -ESRCH;\\n\\t\\tlist_del_init(&addfd->list);\\n\\t\\tcomplete(&addfd->completion);\\n\\t}\\n\\n\\t/*\\n\\t * Note that it\\'s possible the listener died in between the time when\\n\\t * we were notified of a response (or a signal) and when we were able to\\n\\t * re-acquire the lock, so only delete from the list if the\\n\\t * notification actually exists.\\n\\t *\\n\\t * Also note that this test is only valid because there\\'s no way to\\n\\t * *reattach* to a notifier right now. If one is added, we\\'ll need to\\n\\t * keep track of the notif itself and make sure they match here.\\n\\t */\\n\\tif (match->notif)\\n\\t\\tlist_del(&n.list);\\nout:\\n\\tmutex_unlock(&match->notify_lock);\\n\\n\\t/* Userspace requests to continue the syscall. */\\n\\tif (flags & SECCOMP_USER_NOTIF_FLAG_CONTINUE)\\n\\t\\treturn 0;\\n\\n\\tsyscall_set_return_value(current, current_pt_regs(),\\n\\t\\t\\t\\t err, ret);\\n\\treturn -1;\\n}\\n\\nstatic int __seccomp_filter(int this_syscall, const struct seccomp_data *sd,\\n\\t\\t\\t    const bool recheck_after_trace)\\n{\\n\\tu32 filter_ret, action;\\n\\tstruct seccomp_filter *match = NULL;\\n\\tint data;\\n\\tstruct seccomp_data sd_local;\\n\\n\\t/*\\n\\t * Make sure that any changes to mode from another thread have\\n\\t * been seen after SYSCALL_WORK_SECCOMP was seen.\\n\\t */\\n\\tsmp_rmb();\\n\\n\\tif (!sd) {\\n\\t\\tpopulate_seccomp_data(&sd_local);\\n\\t\\tsd = &sd_local;\\n\\t}\\n\\n\\tfilter_ret = seccomp_run_filters(sd, &match);\\n\\tdata = filter_ret & SECCOMP_RET_DATA;\\n\\taction = filter_ret & SECCOMP_RET_ACTION_FULL;\\n\\n\\tswitch (action) {\\n\\tcase SECCOMP_RET_ERRNO:\\n\\t\\t/* Set low-order bits as an errno, capped at MAX_ERRNO. */\\n\\t\\tif (data > MAX_ERRNO)\\n\\t\\t\\tdata = MAX_ERRNO;\\n\\t\\tsyscall_set_return_value(current, current_pt_regs(),\\n\\t\\t\\t\\t\\t -data, 0);\\n\\t\\tgoto skip;\\n\\n\\tcase SECCOMP_RET_TRAP:\\n\\t\\t/* Show the handler the original registers. */\\n\\t\\tsyscall_rollback(current, current_pt_regs());\\n\\t\\t/* Let the filter pass back 16 bits of data. */\\n\\t\\tforce_sig_seccomp(this_syscall, data, false);\\n\\t\\tgoto skip;\\n\\n\\tcase SECCOMP_RET_TRACE:\\n\\t\\t/* We\\'ve been put in this state by the ptracer already. */\\n\\t\\tif (recheck_after_trace)\\n\\t\\t\\treturn 0;\\n\\n\\t\\t/* ENOSYS these calls if there is no tracer attached. */\\n\\t\\tif (!ptrace_event_enabled(current, PTRACE_EVENT_SECCOMP)) {\\n\\t\\t\\tsyscall_set_return_value(current,\\n\\t\\t\\t\\t\\t\\t current_pt_regs(),\\n\\t\\t\\t\\t\\t\\t -ENOSYS, 0);\\n\\t\\t\\tgoto skip;\\n\\t\\t}\\n\\n\\t\\t/* Allow the BPF to provide the event message */\\n\\t\\tptrace_event(PTRACE_EVENT_SECCOMP, data);\\n\\t\\t/*\\n\\t\\t * The delivery of a fatal signal during event\\n\\t\\t * notification may silently skip tracer notification,\\n\\t\\t * which could leave us with a potentially unmodified\\n\\t\\t * syscall that the tracer would have liked to have\\n\\t\\t * changed. Since the process is about to die, we just\\n\\t\\t * force the syscall to be skipped and let the signal\\n\\t\\t * kill the process and correctly handle any tracer exit\\n\\t\\t * notifications.\\n\\t\\t */\\n\\t\\tif (fatal_signal_pending(current))\\n\\t\\t\\tgoto skip;\\n\\t\\t/* Check if the tracer forced the syscall to be skipped. */\\n\\t\\tthis_syscall = syscall_get_nr(current, current_pt_regs());\\n\\t\\tif (this_syscall < 0)\\n\\t\\t\\tgoto skip;\\n\\n\\t\\t/*\\n\\t\\t * Recheck the syscall, since it may have changed. This\\n\\t\\t * intentionally uses a NULL struct seccomp_data to force\\n\\t\\t * a reload of all registers. This does not goto skip since\\n\\t\\t * a skip would have already been reported.\\n\\t\\t */\\n\\t\\tif (__seccomp_filter(this_syscall, NULL, true))\\n\\t\\t\\treturn -1;\\n\\n\\t\\treturn 0;\\n\\n\\tcase SECCOMP_RET_USER_NOTIF:\\n\\t\\tif (seccomp_do_user_notification(this_syscall, match, sd))\\n\\t\\t\\tgoto skip;\\n\\n\\t\\treturn 0;\\n\\n\\tcase SECCOMP_RET_LOG:\\n\\t\\tseccomp_log(this_syscall, 0, action, true);\\n\\t\\treturn 0;\\n\\n\\tcase SECCOMP_RET_ALLOW:\\n\\t\\t/*\\n\\t\\t * Note that the \"match\" filter will always be NULL for\\n\\t\\t * this action since SECCOMP_RET_ALLOW is the starting\\n\\t\\t * state in seccomp_run_filters().\\n\\t\\t */\\n\\t\\treturn 0;\\n\\n\\tcase SECCOMP_RET_KILL_THREAD:\\n\\tcase SECCOMP_RET_KILL_PROCESS:\\n\\tdefault:\\n\\t\\tcurrent->seccomp.mode = SECCOMP_MODE_DEAD;\\n\\t\\tseccomp_log(this_syscall, SIGSYS, action, true);\\n\\t\\t/* Dump core only if this is the last remaining thread. */\\n\\t\\tif (action != SECCOMP_RET_KILL_THREAD ||\\n\\t\\t    (atomic_read(&current->signal->live) == 1)) {\\n\\t\\t\\t/* Show the original registers in the dump. */\\n\\t\\t\\tsyscall_rollback(current, current_pt_regs());\\n\\t\\t\\t/* Trigger a coredump with SIGSYS */\\n\\t\\t\\tforce_sig_seccomp(this_syscall, data, true);\\n\\t\\t} else {\\n\\t\\t\\tdo_exit(SIGSYS);\\n\\t\\t}\\n\\t\\treturn -1; /* skip the syscall go directly to signal handling */\\n\\t}\\n\\n\\tunreachable();\\n\\nskip:\\n\\tseccomp_log(this_syscall, 0, action, match ? match->log : false);\\n\\treturn -1;\\n}\\n#else\\nstatic int __seccomp_filter(int this_syscall, const struct seccomp_data *sd,\\n\\t\\t\\t    const bool recheck_after_trace)\\n{\\n\\tBUG();\\n\\n\\treturn -1;\\n}\\n#endif\\n\\nint __secure_computing(const struct seccomp_data *sd)\\n{\\n\\tint mode = current->seccomp.mode;\\n\\tint this_syscall;\\n\\n\\tif (IS_ENABLED(CONFIG_CHECKPOINT_RESTORE) &&\\n\\t    unlikely(current->ptrace & PT_SUSPEND_SECCOMP))\\n\\t\\treturn 0;\\n\\n\\tthis_syscall = sd ? sd->nr :\\n\\t\\tsyscall_get_nr(current, current_pt_regs());\\n\\n\\tswitch (mode) {\\n\\tcase SECCOMP_MODE_STRICT:\\n\\t\\t__secure_computing_strict(this_syscall);  /* may call do_exit */\\n\\t\\treturn 0;\\n\\tcase SECCOMP_MODE_FILTER:\\n\\t\\treturn __seccomp_filter(this_syscall, sd, false);\\n\\t/* Surviving SECCOMP_RET_KILL_* must be proactively impossible. */\\n\\tcase SECCOMP_MODE_DEAD:\\n\\t\\tWARN_ON_ONCE(1);\\n\\t\\tdo_exit(SIGKILL);\\n\\t\\treturn -1;\\n\\tdefault:\\n\\t\\tBUG();\\n\\t}\\n}\\n#endif /* CONFIG_HAVE_ARCH_SECCOMP_FILTER */\\n\\nlong prctl_get_seccomp(void)\\n{\\n\\treturn current->seccomp.mode;\\n}\\n\\n/**\\n * seccomp_set_mode_strict: internal function for setting strict seccomp\\n *\\n * Once current->seccomp.mode is non-zero, it may not be changed.\\n *\\n * Returns 0 on success or -EINVAL on failure.\\n */\\nstatic long seccomp_set_mode_strict(void)\\n{\\n\\tconst unsigned long seccomp_mode = SECCOMP_MODE_STRICT;\\n\\tlong ret = -EINVAL;\\n\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\n\\tif (!seccomp_may_assign_mode(seccomp_mode))\\n\\t\\tgoto out;\\n\\n#ifdef TIF_NOTSC\\n\\tdisable_TSC();\\n#endif\\n\\tseccomp_assign_mode(current, seccomp_mode, 0);\\n\\tret = 0;\\n\\nout:\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_SECCOMP_FILTER\\nstatic void seccomp_notify_free(struct seccomp_filter *filter)\\n{\\n\\tkfree(filter->notif);\\n\\tfilter->notif = NULL;\\n}\\n\\nstatic void seccomp_notify_detach(struct seccomp_filter *filter)\\n{\\n\\tstruct seccomp_knotif *knotif;\\n\\n\\tif (!filter)\\n\\t\\treturn;\\n\\n\\tmutex_lock(&filter->notify_lock);\\n\\n\\t/*\\n\\t * If this file is being closed because e.g. the task who owned it\\n\\t * died, let\\'s wake everyone up who was waiting on us.\\n\\t */\\n\\tlist_for_each_entry(knotif, &filter->notif->notifications, list) {\\n\\t\\tif (knotif->state == SECCOMP_NOTIFY_REPLIED)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tknotif->state = SECCOMP_NOTIFY_REPLIED;\\n\\t\\tknotif->error = -ENOSYS;\\n\\t\\tknotif->val = 0;\\n\\n\\t\\t/*\\n\\t\\t * We do not need to wake up any pending addfd messages, as\\n\\t\\t * the notifier will do that for us, as this just looks\\n\\t\\t * like a standard reply.\\n\\t\\t */\\n\\t\\tcomplete(&knotif->ready);\\n\\t}\\n\\n\\tseccomp_notify_free(filter);\\n\\tmutex_unlock(&filter->notify_lock);\\n}\\n\\nstatic int seccomp_notify_release(struct inode *inode, struct file *file)\\n{\\n\\tstruct seccomp_filter *filter = file->private_data;\\n\\n\\tseccomp_notify_detach(filter);\\n\\t__put_seccomp_filter(filter);\\n\\treturn 0;\\n}\\n\\n/* must be called with notif_lock held */\\nstatic inline struct seccomp_knotif *\\nfind_notification(struct seccomp_filter *filter, u64 id)\\n{\\n\\tstruct seccomp_knotif *cur;\\n\\n\\tlockdep_assert_held(&filter->notify_lock);\\n\\n\\tlist_for_each_entry(cur, &filter->notif->notifications, list) {\\n\\t\\tif (cur->id == id)\\n\\t\\t\\treturn cur;\\n\\t}\\n\\n\\treturn NULL;\\n}\\n\\nstatic int recv_wake_function(wait_queue_entry_t *wait, unsigned int mode, int sync,\\n\\t\\t\\t\\t  void *key)\\n{\\n\\t/* Avoid a wakeup if event not interesting for us. */\\n\\tif (key && !(key_to_poll(key) & (EPOLLIN | EPOLLERR | EPOLLHUP)))\\n\\t\\treturn 0;\\n\\treturn autoremove_wake_function(wait, mode, sync, key);\\n}\\n\\nstatic int recv_wait_event(struct seccomp_filter *filter)\\n{\\n\\tDEFINE_WAIT_FUNC(wait, recv_wake_function);\\n\\tint ret;\\n\\n\\tif (refcount_read(&filter->users) == 0)\\n\\t\\treturn 0;\\n\\n\\tif (atomic_dec_if_positive(&filter->notif->requests) >= 0)\\n\\t\\treturn 0;\\n\\n\\tfor (;;) {\\n\\t\\tret = prepare_to_wait_event(&filter->wqh, &wait, TASK_INTERRUPTIBLE);\\n\\n\\t\\tif (atomic_dec_if_positive(&filter->notif->requests) >= 0)\\n\\t\\t\\tbreak;\\n\\t\\tif (refcount_read(&filter->users) == 0)\\n\\t\\t\\tbreak;\\n\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\n\\t\\tschedule();\\n\\t}\\n\\tfinish_wait(&filter->wqh, &wait);\\n\\treturn 0;\\n}\\n\\nstatic long seccomp_notify_recv(struct seccomp_filter *filter,\\n\\t\\t\\t\\tvoid __user *buf)\\n{\\n\\tstruct seccomp_knotif *knotif = NULL, *cur;\\n\\tstruct seccomp_notif unotif;\\n\\tssize_t ret;\\n\\n\\t/* Verify that we\\'re not given garbage to keep struct extensible. */\\n\\tret = check_zeroed_user(buf, sizeof(unotif));\\n\\tif (ret < 0)\\n\\t\\treturn ret;\\n\\tif (!ret)\\n\\t\\treturn -EINVAL;\\n\\n\\tmemset(&unotif, 0, sizeof(unotif));\\n\\n\\tret = recv_wait_event(filter);\\n\\tif (ret < 0)\\n\\t\\treturn ret;\\n\\n\\tmutex_lock(&filter->notify_lock);\\n\\tlist_for_each_entry(cur, &filter->notif->notifications, list) {\\n\\t\\tif (cur->state == SECCOMP_NOTIFY_INIT) {\\n\\t\\t\\tknotif = cur;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * If we didn\\'t find a notification, it could be that the task was\\n\\t * interrupted by a fatal signal between the time we were woken and\\n\\t * when we were able to acquire the rw lock.\\n\\t */\\n\\tif (!knotif) {\\n\\t\\tret = -ENOENT;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tunotif.id = knotif->id;\\n\\tunotif.pid = task_pid_vnr(knotif->task);\\n\\tunotif.data = *(knotif->data);\\n\\n\\tknotif->state = SECCOMP_NOTIFY_SENT;\\n\\twake_up_poll(&filter->wqh, EPOLLOUT | EPOLLWRNORM);\\n\\tret = 0;\\nout:\\n\\tmutex_unlock(&filter->notify_lock);\\n\\n\\tif (ret == 0 && copy_to_user(buf, &unotif, sizeof(unotif))) {\\n\\t\\tret = -EFAULT;\\n\\n\\t\\t/*\\n\\t\\t * Userspace screwed up. To make sure that we keep this\\n\\t\\t * notification alive, let\\'s reset it back to INIT. It\\n\\t\\t * may have died when we released the lock, so we need to make\\n\\t\\t * sure it\\'s still around.\\n\\t\\t */\\n\\t\\tmutex_lock(&filter->notify_lock);\\n\\t\\tknotif = find_notification(filter, unotif.id);\\n\\t\\tif (knotif) {\\n\\t\\t\\t/* Reset the process to make sure it\\'s not stuck */\\n\\t\\t\\tif (should_sleep_killable(filter, knotif))\\n\\t\\t\\t\\tcomplete(&knotif->ready);\\n\\t\\t\\tknotif->state = SECCOMP_NOTIFY_INIT;\\n\\t\\t\\tatomic_inc(&filter->notif->requests);\\n\\t\\t\\twake_up_poll(&filter->wqh, EPOLLIN | EPOLLRDNORM);\\n\\t\\t}\\n\\t\\tmutex_unlock(&filter->notify_lock);\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic long seccomp_notify_send(struct seccomp_filter *filter,\\n\\t\\t\\t\\tvoid __user *buf)\\n{\\n\\tstruct seccomp_notif_resp resp = {};\\n\\tstruct seccomp_knotif *knotif;\\n\\tlong ret;\\n\\n\\tif (copy_from_user(&resp, buf, sizeof(resp)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (resp.flags & ~SECCOMP_USER_NOTIF_FLAG_CONTINUE)\\n\\t\\treturn -EINVAL;\\n\\n\\tif ((resp.flags & SECCOMP_USER_NOTIF_FLAG_CONTINUE) &&\\n\\t    (resp.error || resp.val))\\n\\t\\treturn -EINVAL;\\n\\n\\tret = mutex_lock_interruptible(&filter->notify_lock);\\n\\tif (ret < 0)\\n\\t\\treturn ret;\\n\\n\\tknotif = find_notification(filter, resp.id);\\n\\tif (!knotif) {\\n\\t\\tret = -ENOENT;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t/* Allow exactly one reply. */\\n\\tif (knotif->state != SECCOMP_NOTIFY_SENT) {\\n\\t\\tret = -EINPROGRESS;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tret = 0;\\n\\tknotif->state = SECCOMP_NOTIFY_REPLIED;\\n\\tknotif->error = resp.error;\\n\\tknotif->val = resp.val;\\n\\tknotif->flags = resp.flags;\\n\\tif (filter->notif->flags & SECCOMP_USER_NOTIF_FD_SYNC_WAKE_UP)\\n\\t\\tcomplete_on_current_cpu(&knotif->ready);\\n\\telse\\n\\t\\tcomplete(&knotif->ready);\\nout:\\n\\tmutex_unlock(&filter->notify_lock);\\n\\treturn ret;\\n}\\n\\nstatic long seccomp_notify_id_valid(struct seccomp_filter *filter,\\n\\t\\t\\t\\t    void __user *buf)\\n{\\n\\tstruct seccomp_knotif *knotif;\\n\\tu64 id;\\n\\tlong ret;\\n\\n\\tif (copy_from_user(&id, buf, sizeof(id)))\\n\\t\\treturn -EFAULT;\\n\\n\\tret = mutex_lock_interruptible(&filter->notify_lock);\\n\\tif (ret < 0)\\n\\t\\treturn ret;\\n\\n\\tknotif = find_notification(filter, id);\\n\\tif (knotif && knotif->state == SECCOMP_NOTIFY_SENT)\\n\\t\\tret = 0;\\n\\telse\\n\\t\\tret = -ENOENT;\\n\\n\\tmutex_unlock(&filter->notify_lock);\\n\\treturn ret;\\n}\\n\\nstatic long seccomp_notify_set_flags(struct seccomp_filter *filter,\\n\\t\\t\\t\\t    unsigned long flags)\\n{\\n\\tlong ret;\\n\\n\\tif (flags & ~SECCOMP_USER_NOTIF_FD_SYNC_WAKE_UP)\\n\\t\\treturn -EINVAL;\\n\\n\\tret = mutex_lock_interruptible(&filter->notify_lock);\\n\\tif (ret < 0)\\n\\t\\treturn ret;\\n\\tfilter->notif->flags = flags;\\n\\tmutex_unlock(&filter->notify_lock);\\n\\treturn 0;\\n}\\n\\nstatic long seccomp_notify_addfd(struct seccomp_filter *filter,\\n\\t\\t\\t\\t struct seccomp_notif_addfd __user *uaddfd,\\n\\t\\t\\t\\t unsigned int size)\\n{\\n\\tstruct seccomp_notif_addfd addfd;\\n\\tstruct seccomp_knotif *knotif;\\n\\tstruct seccomp_kaddfd kaddfd;\\n\\tint ret;\\n\\n\\tBUILD_BUG_ON(sizeof(addfd) < SECCOMP_NOTIFY_ADDFD_SIZE_VER0);\\n\\tBUILD_BUG_ON(sizeof(addfd) != SECCOMP_NOTIFY_ADDFD_SIZE_LATEST);\\n\\n\\tif (size < SECCOMP_NOTIFY_ADDFD_SIZE_VER0 || size >= PAGE_SIZE)\\n\\t\\treturn -EINVAL;\\n\\n\\tret = copy_struct_from_user(&addfd, sizeof(addfd), uaddfd, size);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (addfd.newfd_flags & ~O_CLOEXEC)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (addfd.flags & ~(SECCOMP_ADDFD_FLAG_SETFD | SECCOMP_ADDFD_FLAG_SEND))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (addfd.newfd && !(addfd.flags & SECCOMP_ADDFD_FLAG_SETFD))\\n\\t\\treturn -EINVAL;\\n\\n\\tkaddfd.file = fget(addfd.srcfd);\\n\\tif (!kaddfd.file)\\n\\t\\treturn -EBADF;\\n\\n\\tkaddfd.ioctl_flags = addfd.flags;\\n\\tkaddfd.flags = addfd.newfd_flags;\\n\\tkaddfd.setfd = addfd.flags & SECCOMP_ADDFD_FLAG_SETFD;\\n\\tkaddfd.fd = addfd.newfd;\\n\\tinit_completion(&kaddfd.completion);\\n\\n\\tret = mutex_lock_interruptible(&filter->notify_lock);\\n\\tif (ret < 0)\\n\\t\\tgoto out;\\n\\n\\tknotif = find_notification(filter, addfd.id);\\n\\tif (!knotif) {\\n\\t\\tret = -ENOENT;\\n\\t\\tgoto out_unlock;\\n\\t}\\n\\n\\t/*\\n\\t * We do not want to allow for FD injection to occur before the\\n\\t * notification has been picked up by a userspace handler, or after\\n\\t * the notification has been replied to.\\n\\t */\\n\\tif (knotif->state != SECCOMP_NOTIFY_SENT) {\\n\\t\\tret = -EINPROGRESS;\\n\\t\\tgoto out_unlock;\\n\\t}\\n\\n\\tif (addfd.flags & SECCOMP_ADDFD_FLAG_SEND) {\\n\\t\\t/*\\n\\t\\t * Disallow queuing an atomic addfd + send reply while there are\\n\\t\\t * some addfd requests still to process.\\n\\t\\t *\\n\\t\\t * There is no clear reason to support it and allows us to keep\\n\\t\\t * the loop on the other side straight-forward.\\n\\t\\t */\\n\\t\\tif (!list_empty(&knotif->addfd)) {\\n\\t\\t\\tret = -EBUSY;\\n\\t\\t\\tgoto out_unlock;\\n\\t\\t}\\n\\n\\t\\t/* Allow exactly only one reply */\\n\\t\\tknotif->state = SECCOMP_NOTIFY_REPLIED;\\n\\t}\\n\\n\\tlist_add(&kaddfd.list, &knotif->addfd);\\n\\tcomplete(&knotif->ready);\\n\\tmutex_unlock(&filter->notify_lock);\\n\\n\\t/* Now we wait for it to be processed or be interrupted */\\n\\tret = wait_for_completion_interruptible(&kaddfd.completion);\\n\\tif (ret == 0) {\\n\\t\\t/*\\n\\t\\t * We had a successful completion. The other side has already\\n\\t\\t * removed us from the addfd queue, and\\n\\t\\t * wait_for_completion_interruptible has a memory barrier upon\\n\\t\\t * success that lets us read this value directly without\\n\\t\\t * locking.\\n\\t\\t */\\n\\t\\tret = kaddfd.ret;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tmutex_lock(&filter->notify_lock);\\n\\t/*\\n\\t * Even though we were woken up by a signal and not a successful\\n\\t * completion, a completion may have happened in the mean time.\\n\\t *\\n\\t * We need to check again if the addfd request has been handled,\\n\\t * and if not, we will remove it from the queue.\\n\\t */\\n\\tif (list_empty(&kaddfd.list))\\n\\t\\tret = kaddfd.ret;\\n\\telse\\n\\t\\tlist_del(&kaddfd.list);\\n\\nout_unlock:\\n\\tmutex_unlock(&filter->notify_lock);\\nout:\\n\\tfput(kaddfd.file);\\n\\n\\treturn ret;\\n}\\n\\nstatic long seccomp_notify_ioctl(struct file *file, unsigned int cmd,\\n\\t\\t\\t\\t unsigned long arg)\\n{\\n\\tstruct seccomp_filter *filter = file->private_data;\\n\\tvoid __user *buf = (void __user *)arg;\\n\\n\\t/* Fixed-size ioctls */\\n\\tswitch (cmd) {\\n\\tcase SECCOMP_IOCTL_NOTIF_RECV:\\n\\t\\treturn seccomp_notify_recv(filter, buf);\\n\\tcase SECCOMP_IOCTL_NOTIF_SEND:\\n\\t\\treturn seccomp_notify_send(filter, buf);\\n\\tcase SECCOMP_IOCTL_NOTIF_ID_VALID_WRONG_DIR:\\n\\tcase SECCOMP_IOCTL_NOTIF_ID_VALID:\\n\\t\\treturn seccomp_notify_id_valid(filter, buf);\\n\\tcase SECCOMP_IOCTL_NOTIF_SET_FLAGS:\\n\\t\\treturn seccomp_notify_set_flags(filter, arg);\\n\\t}\\n\\n\\t/* Extensible Argument ioctls */\\n#define EA_IOCTL(cmd)\\t((cmd) & ~(IOC_INOUT | IOCSIZE_MASK))\\n\\tswitch (EA_IOCTL(cmd)) {\\n\\tcase EA_IOCTL(SECCOMP_IOCTL_NOTIF_ADDFD):\\n\\t\\treturn seccomp_notify_addfd(filter, buf, _IOC_SIZE(cmd));\\n\\tdefault:\\n\\t\\treturn -EINVAL;\\n\\t}\\n}\\n\\nstatic __poll_t seccomp_notify_poll(struct file *file,\\n\\t\\t\\t\\t    struct poll_table_struct *poll_tab)\\n{\\n\\tstruct seccomp_filter *filter = file->private_data;\\n\\t__poll_t ret = 0;\\n\\tstruct seccomp_knotif *cur;\\n\\n\\tpoll_wait(file, &filter->wqh, poll_tab);\\n\\n\\tif (mutex_lock_interruptible(&filter->notify_lock) < 0)\\n\\t\\treturn EPOLLERR;\\n\\n\\tlist_for_each_entry(cur, &filter->notif->notifications, list) {\\n\\t\\tif (cur->state == SECCOMP_NOTIFY_INIT)\\n\\t\\t\\tret |= EPOLLIN | EPOLLRDNORM;\\n\\t\\tif (cur->state == SECCOMP_NOTIFY_SENT)\\n\\t\\t\\tret |= EPOLLOUT | EPOLLWRNORM;\\n\\t\\tif ((ret & EPOLLIN) && (ret & EPOLLOUT))\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tmutex_unlock(&filter->notify_lock);\\n\\n\\tif (refcount_read(&filter->users) == 0)\\n\\t\\tret |= EPOLLHUP;\\n\\n\\treturn ret;\\n}\\n\\nstatic const struct file_operations seccomp_notify_ops = {\\n\\t.poll = seccomp_notify_poll,\\n\\t.release = seccomp_notify_release,\\n\\t.unlocked_ioctl = seccomp_notify_ioctl,\\n\\t.compat_ioctl = seccomp_notify_ioctl,\\n};\\n\\nstatic struct file *init_listener(struct seccomp_filter *filter)\\n{\\n\\tstruct file *ret;\\n\\n\\tret = ERR_PTR(-ENOMEM);\\n\\tfilter->notif = kzalloc(sizeof(*(filter->notif)), GFP_KERNEL);\\n\\tif (!filter->notif)\\n\\t\\tgoto out;\\n\\n\\tfilter->notif->next_id = get_random_u64();\\n\\tINIT_LIST_HEAD(&filter->notif->notifications);\\n\\n\\tret = anon_inode_getfile(\"seccomp notify\", &seccomp_notify_ops,\\n\\t\\t\\t\\t filter, O_RDWR);\\n\\tif (IS_ERR(ret))\\n\\t\\tgoto out_notif;\\n\\n\\t/* The file has a reference to it now */\\n\\t__get_seccomp_filter(filter);\\n\\nout_notif:\\n\\tif (IS_ERR(ret))\\n\\t\\tseccomp_notify_free(filter);\\nout:\\n\\treturn ret;\\n}\\n\\n/*\\n * Does @new_child have a listener while an ancestor also has a listener?\\n * If so, we\\'ll want to reject this filter.\\n * This only has to be tested for the current process, even in the TSYNC case,\\n * because TSYNC installs @child with the same parent on all threads.\\n * Note that @new_child is not hooked up to its parent at this point yet, so\\n * we use current->seccomp.filter.\\n */\\nstatic bool has_duplicate_listener(struct seccomp_filter *new_child)\\n{\\n\\tstruct seccomp_filter *cur;\\n\\n\\t/* must be protected against concurrent TSYNC */\\n\\tlockdep_assert_held(&current->sighand->siglock);\\n\\n\\tif (!new_child->notif)\\n\\t\\treturn false;\\n\\tfor (cur = current->seccomp.filter; cur; cur = cur->prev) {\\n\\t\\tif (cur->notif)\\n\\t\\t\\treturn true;\\n\\t}\\n\\n\\treturn false;\\n}\\n\\n/**\\n * seccomp_set_mode_filter: internal function for setting seccomp filter\\n * @flags:  flags to change filter behavior\\n * @filter: struct sock_fprog containing filter\\n *\\n * This function may be called repeatedly to install additional filters.\\n * Every filter successfully installed will be evaluated (in reverse order)\\n * for each system call the task makes.\\n *\\n * Once current->seccomp.mode is non-zero, it may not be changed.\\n *\\n * Returns 0 on success or -EINVAL on failure.\\n */\\nstatic long seccomp_set_mode_filter(unsigned int flags,\\n\\t\\t\\t\\t    const char __user *filter)\\n{\\n\\tconst unsigned long seccomp_mode = SECCOMP_MODE_FILTER;\\n\\tstruct seccomp_filter *prepared = NULL;\\n\\tlong ret = -EINVAL;\\n\\tint listener = -1;\\n\\tstruct file *listener_f = NULL;\\n\\n\\t/* Validate flags. */\\n\\tif (flags & ~SECCOMP_FILTER_FLAG_MASK)\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * In the successful case, NEW_LISTENER returns the new listener fd.\\n\\t * But in the failure case, TSYNC returns the thread that died. If you\\n\\t * combine these two flags, there\\'s no way to tell whether something\\n\\t * succeeded or failed. So, let\\'s disallow this combination if the user\\n\\t * has not explicitly requested no errors from TSYNC.\\n\\t */\\n\\tif ((flags & SECCOMP_FILTER_FLAG_TSYNC) &&\\n\\t    (flags & SECCOMP_FILTER_FLAG_NEW_LISTENER) &&\\n\\t    ((flags & SECCOMP_FILTER_FLAG_TSYNC_ESRCH) == 0))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * The SECCOMP_FILTER_FLAG_WAIT_KILLABLE_SENT flag doesn\\'t make sense\\n\\t * without the SECCOMP_FILTER_FLAG_NEW_LISTENER flag.\\n\\t */\\n\\tif ((flags & SECCOMP_FILTER_FLAG_WAIT_KILLABLE_RECV) &&\\n\\t    ((flags & SECCOMP_FILTER_FLAG_NEW_LISTENER) == 0))\\n\\t\\treturn -EINVAL;\\n\\n\\t/* Prepare the new filter before holding any locks. */\\n\\tprepared = seccomp_prepare_user_filter(filter);\\n\\tif (IS_ERR(prepared))\\n\\t\\treturn PTR_ERR(prepared);\\n\\n\\tif (flags & SECCOMP_FILTER_FLAG_NEW_LISTENER) {\\n\\t\\tlistener = get_unused_fd_flags(O_CLOEXEC);\\n\\t\\tif (listener < 0) {\\n\\t\\t\\tret = listener;\\n\\t\\t\\tgoto out_free;\\n\\t\\t}\\n\\n\\t\\tlistener_f = init_listener(prepared);\\n\\t\\tif (IS_ERR(listener_f)) {\\n\\t\\t\\tput_unused_fd(listener);\\n\\t\\t\\tret = PTR_ERR(listener_f);\\n\\t\\t\\tgoto out_free;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * Make sure we cannot change seccomp or nnp state via TSYNC\\n\\t * while another thread is in the middle of calling exec.\\n\\t */\\n\\tif (flags & SECCOMP_FILTER_FLAG_TSYNC &&\\n\\t    mutex_lock_killable(&current->signal->cred_guard_mutex))\\n\\t\\tgoto out_put_fd;\\n\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\n\\tif (!seccomp_may_assign_mode(seccomp_mode))\\n\\t\\tgoto out;\\n\\n\\tif (has_duplicate_listener(prepared)) {\\n\\t\\tret = -EBUSY;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tret = seccomp_attach_filter(flags, prepared);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\t/* Do not free the successfully attached filter. */\\n\\tprepared = NULL;\\n\\n\\tseccomp_assign_mode(current, seccomp_mode, flags);\\nout:\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\tif (flags & SECCOMP_FILTER_FLAG_TSYNC)\\n\\t\\tmutex_unlock(&current->signal->cred_guard_mutex);\\nout_put_fd:\\n\\tif (flags & SECCOMP_FILTER_FLAG_NEW_LISTENER) {\\n\\t\\tif (ret) {\\n\\t\\t\\tlistener_f->private_data = NULL;\\n\\t\\t\\tfput(listener_f);\\n\\t\\t\\tput_unused_fd(listener);\\n\\t\\t\\tseccomp_notify_detach(prepared);\\n\\t\\t} else {\\n\\t\\t\\tfd_install(listener, listener_f);\\n\\t\\t\\tret = listener;\\n\\t\\t}\\n\\t}\\nout_free:\\n\\tseccomp_filter_free(prepared);\\n\\treturn ret;\\n}\\n#else\\nstatic inline long seccomp_set_mode_filter(unsigned int flags,\\n\\t\\t\\t\\t\\t   const char __user *filter)\\n{\\n\\treturn -EINVAL;\\n}\\n#endif\\n\\nstatic long seccomp_get_action_avail(const char __user *uaction)\\n{\\n\\tu32 action;\\n\\n\\tif (copy_from_user(&action, uaction, sizeof(action)))\\n\\t\\treturn -EFAULT;\\n\\n\\tswitch (action) {\\n\\tcase SECCOMP_RET_KILL_PROCESS:\\n\\tcase SECCOMP_RET_KILL_THREAD:\\n\\tcase SECCOMP_RET_TRAP:\\n\\tcase SECCOMP_RET_ERRNO:\\n\\tcase SECCOMP_RET_USER_NOTIF:\\n\\tcase SECCOMP_RET_TRACE:\\n\\tcase SECCOMP_RET_LOG:\\n\\tcase SECCOMP_RET_ALLOW:\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn -EOPNOTSUPP;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic long seccomp_get_notif_sizes(void __user *usizes)\\n{\\n\\tstruct seccomp_notif_sizes sizes = {\\n\\t\\t.seccomp_notif = sizeof(struct seccomp_notif),\\n\\t\\t.seccomp_notif_resp = sizeof(struct seccomp_notif_resp),\\n\\t\\t.seccomp_data = sizeof(struct seccomp_data),\\n\\t};\\n\\n\\tif (copy_to_user(usizes, &sizes, sizeof(sizes)))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn 0;\\n}\\n\\n/* Common entry point for both prctl and syscall. */\\nstatic long do_seccomp(unsigned int op, unsigned int flags,\\n\\t\\t       void __user *uargs)\\n{\\n\\tswitch (op) {\\n\\tcase SECCOMP_SET_MODE_STRICT:\\n\\t\\tif (flags != 0 || uargs != NULL)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\treturn seccomp_set_mode_strict();\\n\\tcase SECCOMP_SET_MODE_FILTER:\\n\\t\\treturn seccomp_set_mode_filter(flags, uargs);\\n\\tcase SECCOMP_GET_ACTION_AVAIL:\\n\\t\\tif (flags != 0)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\treturn seccomp_get_action_avail(uargs);\\n\\tcase SECCOMP_GET_NOTIF_SIZES:\\n\\t\\tif (flags != 0)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\treturn seccomp_get_notif_sizes(uargs);\\n\\tdefault:\\n\\t\\treturn -EINVAL;\\n\\t}\\n}\\n\\nSYSCALL_DEFINE3(seccomp, unsigned int, op, unsigned int, flags,\\n\\t\\t\\t void __user *, uargs)\\n{\\n\\treturn do_seccomp(op, flags, uargs);\\n}\\n\\n/**\\n * prctl_set_seccomp: configures current->seccomp.mode\\n * @seccomp_mode: requested mode to use\\n * @filter: optional struct sock_fprog for use with SECCOMP_MODE_FILTER\\n *\\n * Returns 0 on success or -EINVAL on failure.\\n */\\nlong prctl_set_seccomp(unsigned long seccomp_mode, void __user *filter)\\n{\\n\\tunsigned int op;\\n\\tvoid __user *uargs;\\n\\n\\tswitch (seccomp_mode) {\\n\\tcase SECCOMP_MODE_STRICT:\\n\\t\\top = SECCOMP_SET_MODE_STRICT;\\n\\t\\t/*\\n\\t\\t * Setting strict mode through prctl always ignored filter,\\n\\t\\t * so make sure it is always NULL here to pass the internal\\n\\t\\t * check in do_seccomp().\\n\\t\\t */\\n\\t\\tuargs = NULL;\\n\\t\\tbreak;\\n\\tcase SECCOMP_MODE_FILTER:\\n\\t\\top = SECCOMP_SET_MODE_FILTER;\\n\\t\\tuargs = filter;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\t/* prctl interface doesn\\'t have flags, so they are always zero. */\\n\\treturn do_seccomp(op, 0, uargs);\\n}\\n\\n#if defined(CONFIG_SECCOMP_FILTER) && defined(CONFIG_CHECKPOINT_RESTORE)\\nstatic struct seccomp_filter *get_nth_filter(struct task_struct *task,\\n\\t\\t\\t\\t\\t     unsigned long filter_off)\\n{\\n\\tstruct seccomp_filter *orig, *filter;\\n\\tunsigned long count;\\n\\n\\t/*\\n\\t * Note: this is only correct because the caller should be the (ptrace)\\n\\t * tracer of the task, otherwise lock_task_sighand is needed.\\n\\t */\\n\\tspin_lock_irq(&task->sighand->siglock);\\n\\n\\tif (task->seccomp.mode != SECCOMP_MODE_FILTER) {\\n\\t\\tspin_unlock_irq(&task->sighand->siglock);\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\t}\\n\\n\\torig = task->seccomp.filter;\\n\\t__get_seccomp_filter(orig);\\n\\tspin_unlock_irq(&task->sighand->siglock);\\n\\n\\tcount = 0;\\n\\tfor (filter = orig; filter; filter = filter->prev)\\n\\t\\tcount++;\\n\\n\\tif (filter_off >= count) {\\n\\t\\tfilter = ERR_PTR(-ENOENT);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tcount -= filter_off;\\n\\tfor (filter = orig; filter && count > 1; filter = filter->prev)\\n\\t\\tcount--;\\n\\n\\tif (WARN_ON(count != 1 || !filter)) {\\n\\t\\tfilter = ERR_PTR(-ENOENT);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t__get_seccomp_filter(filter);\\n\\nout:\\n\\t__put_seccomp_filter(orig);\\n\\treturn filter;\\n}\\n\\nlong seccomp_get_filter(struct task_struct *task, unsigned long filter_off,\\n\\t\\t\\tvoid __user *data)\\n{\\n\\tstruct seccomp_filter *filter;\\n\\tstruct sock_fprog_kern *fprog;\\n\\tlong ret;\\n\\n\\tif (!capable(CAP_SYS_ADMIN) ||\\n\\t    current->seccomp.mode != SECCOMP_MODE_DISABLED) {\\n\\t\\treturn -EACCES;\\n\\t}\\n\\n\\tfilter = get_nth_filter(task, filter_off);\\n\\tif (IS_ERR(filter))\\n\\t\\treturn PTR_ERR(filter);\\n\\n\\tfprog = filter->prog->orig_prog;\\n\\tif (!fprog) {\\n\\t\\t/* This must be a new non-cBPF filter, since we save\\n\\t\\t * every cBPF filter\\'s orig_prog above when\\n\\t\\t * CONFIG_CHECKPOINT_RESTORE is enabled.\\n\\t\\t */\\n\\t\\tret = -EMEDIUMTYPE;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tret = fprog->len;\\n\\tif (!data)\\n\\t\\tgoto out;\\n\\n\\tif (copy_to_user(data, fprog->filter, bpf_classic_proglen(fprog)))\\n\\t\\tret = -EFAULT;\\n\\nout:\\n\\t__put_seccomp_filter(filter);\\n\\treturn ret;\\n}\\n\\nlong seccomp_get_metadata(struct task_struct *task,\\n\\t\\t\\t  unsigned long size, void __user *data)\\n{\\n\\tlong ret;\\n\\tstruct seccomp_filter *filter;\\n\\tstruct seccomp_metadata kmd = {};\\n\\n\\tif (!capable(CAP_SYS_ADMIN) ||\\n\\t    current->seccomp.mode != SECCOMP_MODE_DISABLED) {\\n\\t\\treturn -EACCES;\\n\\t}\\n\\n\\tsize = min_t(unsigned long, size, sizeof(kmd));\\n\\n\\tif (size < sizeof(kmd.filter_off))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (copy_from_user(&kmd.filter_off, data, sizeof(kmd.filter_off)))\\n\\t\\treturn -EFAULT;\\n\\n\\tfilter = get_nth_filter(task, kmd.filter_off);\\n\\tif (IS_ERR(filter))\\n\\t\\treturn PTR_ERR(filter);\\n\\n\\tif (filter->log)\\n\\t\\tkmd.flags |= SECCOMP_FILTER_FLAG_LOG;\\n\\n\\tret = size;\\n\\tif (copy_to_user(data, &kmd, size))\\n\\t\\tret = -EFAULT;\\n\\n\\t__put_seccomp_filter(filter);\\n\\treturn ret;\\n}\\n#endif\\n\\n#ifdef CONFIG_SYSCTL\\n\\n/* Human readable action names for friendly sysctl interaction */\\n#define SECCOMP_RET_KILL_PROCESS_NAME\\t\"kill_process\"\\n#define SECCOMP_RET_KILL_THREAD_NAME\\t\"kill_thread\"\\n#define SECCOMP_RET_TRAP_NAME\\t\\t\"trap\"\\n#define SECCOMP_RET_ERRNO_NAME\\t\\t\"errno\"\\n#define SECCOMP_RET_USER_NOTIF_NAME\\t\"user_notif\"\\n#define SECCOMP_RET_TRACE_NAME\\t\\t\"trace\"\\n#define SECCOMP_RET_LOG_NAME\\t\\t\"log\"\\n#define SECCOMP_RET_ALLOW_NAME\\t\\t\"allow\"\\n\\nstatic const char seccomp_actions_avail[] =\\n\\t\\t\\t\\tSECCOMP_RET_KILL_PROCESS_NAME\\t\" \"\\n\\t\\t\\t\\tSECCOMP_RET_KILL_THREAD_NAME\\t\" \"\\n\\t\\t\\t\\tSECCOMP_RET_TRAP_NAME\\t\\t\" \"\\n\\t\\t\\t\\tSECCOMP_RET_ERRNO_NAME\\t\\t\" \"\\n\\t\\t\\t\\tSECCOMP_RET_USER_NOTIF_NAME     \" \"\\n\\t\\t\\t\\tSECCOMP_RET_TRACE_NAME\\t\\t\" \"\\n\\t\\t\\t\\tSECCOMP_RET_LOG_NAME\\t\\t\" \"\\n\\t\\t\\t\\tSECCOMP_RET_ALLOW_NAME;\\n\\nstruct seccomp_log_name {\\n\\tu32\\t\\tlog;\\n\\tconst char\\t*name;\\n};\\n\\nstatic const struct seccomp_log_name seccomp_log_names[] = {\\n\\t{ SECCOMP_LOG_KILL_PROCESS, SECCOMP_RET_KILL_PROCESS_NAME },\\n\\t{ SECCOMP_LOG_KILL_THREAD, SECCOMP_RET_KILL_THREAD_NAME },\\n\\t{ SECCOMP_LOG_TRAP, SECCOMP_RET_TRAP_NAME },\\n\\t{ SECCOMP_LOG_ERRNO, SECCOMP_RET_ERRNO_NAME },\\n\\t{ SECCOMP_LOG_USER_NOTIF, SECCOMP_RET_USER_NOTIF_NAME },\\n\\t{ SECCOMP_LOG_TRACE, SECCOMP_RET_TRACE_NAME },\\n\\t{ SECCOMP_LOG_LOG, SECCOMP_RET_LOG_NAME },\\n\\t{ SECCOMP_LOG_ALLOW, SECCOMP_RET_ALLOW_NAME },\\n\\t{ }\\n};\\n\\nstatic bool seccomp_names_from_actions_logged(char *names, size_t size,\\n\\t\\t\\t\\t\\t      u32 actions_logged,\\n\\t\\t\\t\\t\\t      const char *sep)\\n{\\n\\tconst struct seccomp_log_name *cur;\\n\\tbool append_sep = false;\\n\\n\\tfor (cur = seccomp_log_names; cur->name && size; cur++) {\\n\\t\\tssize_t ret;\\n\\n\\t\\tif (!(actions_logged & cur->log))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (append_sep) {\\n\\t\\t\\tret = strscpy(names, sep, size);\\n\\t\\t\\tif (ret < 0)\\n\\t\\t\\t\\treturn false;\\n\\n\\t\\t\\tnames += ret;\\n\\t\\t\\tsize -= ret;\\n\\t\\t} else\\n\\t\\t\\tappend_sep = true;\\n\\n\\t\\tret = strscpy(names, cur->name, size);\\n\\t\\tif (ret < 0)\\n\\t\\t\\treturn false;\\n\\n\\t\\tnames += ret;\\n\\t\\tsize -= ret;\\n\\t}\\n\\n\\treturn true;\\n}\\n\\nstatic bool seccomp_action_logged_from_name(u32 *action_logged,\\n\\t\\t\\t\\t\\t    const char *name)\\n{\\n\\tconst struct seccomp_log_name *cur;\\n\\n\\tfor (cur = seccomp_log_names; cur->name; cur++) {\\n\\t\\tif (!strcmp(cur->name, name)) {\\n\\t\\t\\t*action_logged = cur->log;\\n\\t\\t\\treturn true;\\n\\t\\t}\\n\\t}\\n\\n\\treturn false;\\n}\\n\\nstatic bool seccomp_actions_logged_from_names(u32 *actions_logged, char *names)\\n{\\n\\tchar *name;\\n\\n\\t*actions_logged = 0;\\n\\twhile ((name = strsep(&names, \" \")) && *name) {\\n\\t\\tu32 action_logged = 0;\\n\\n\\t\\tif (!seccomp_action_logged_from_name(&action_logged, name))\\n\\t\\t\\treturn false;\\n\\n\\t\\t*actions_logged |= action_logged;\\n\\t}\\n\\n\\treturn true;\\n}\\n\\nstatic int read_actions_logged(const struct ctl_table *ro_table, void *buffer,\\n\\t\\t\\t       size_t *lenp, loff_t *ppos)\\n{\\n\\tchar names[sizeof(seccomp_actions_avail)];\\n\\tstruct ctl_table table;\\n\\n\\tmemset(names, 0, sizeof(names));\\n\\n\\tif (!seccomp_names_from_actions_logged(names, sizeof(names),\\n\\t\\t\\t\\t\\t       seccomp_actions_logged, \" \"))\\n\\t\\treturn -EINVAL;\\n\\n\\ttable = *ro_table;\\n\\ttable.data = names;\\n\\ttable.maxlen = sizeof(names);\\n\\treturn proc_dostring(&table, 0, buffer, lenp, ppos);\\n}\\n\\nstatic int write_actions_logged(const struct ctl_table *ro_table, void *buffer,\\n\\t\\t\\t\\tsize_t *lenp, loff_t *ppos, u32 *actions_logged)\\n{\\n\\tchar names[sizeof(seccomp_actions_avail)];\\n\\tstruct ctl_table table;\\n\\tint ret;\\n\\n\\tif (!capable(CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tmemset(names, 0, sizeof(names));\\n\\n\\ttable = *ro_table;\\n\\ttable.data = names;\\n\\ttable.maxlen = sizeof(names);\\n\\tret = proc_dostring(&table, 1, buffer, lenp, ppos);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (!seccomp_actions_logged_from_names(actions_logged, table.data))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (*actions_logged & SECCOMP_LOG_ALLOW)\\n\\t\\treturn -EINVAL;\\n\\n\\tseccomp_actions_logged = *actions_logged;\\n\\treturn 0;\\n}\\n\\nstatic void audit_actions_logged(u32 actions_logged, u32 old_actions_logged,\\n\\t\\t\\t\\t int ret)\\n{\\n\\tchar names[sizeof(seccomp_actions_avail)];\\n\\tchar old_names[sizeof(seccomp_actions_avail)];\\n\\tconst char *new = names;\\n\\tconst char *old = old_names;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\n\\tmemset(names, 0, sizeof(names));\\n\\tmemset(old_names, 0, sizeof(old_names));\\n\\n\\tif (ret)\\n\\t\\tnew = \"?\";\\n\\telse if (!actions_logged)\\n\\t\\tnew = \"(none)\";\\n\\telse if (!seccomp_names_from_actions_logged(names, sizeof(names),\\n\\t\\t\\t\\t\\t\\t    actions_logged, \",\"))\\n\\t\\tnew = \"?\";\\n\\n\\tif (!old_actions_logged)\\n\\t\\told = \"(none)\";\\n\\telse if (!seccomp_names_from_actions_logged(old_names,\\n\\t\\t\\t\\t\\t\\t    sizeof(old_names),\\n\\t\\t\\t\\t\\t\\t    old_actions_logged, \",\"))\\n\\t\\told = \"?\";\\n\\n\\treturn audit_seccomp_actions_logged(new, old, !ret);\\n}\\n\\nstatic int seccomp_actions_logged_handler(const struct ctl_table *ro_table, int write,\\n\\t\\t\\t\\t\\t  void *buffer, size_t *lenp,\\n\\t\\t\\t\\t\\t  loff_t *ppos)\\n{\\n\\tint ret;\\n\\n\\tif (write) {\\n\\t\\tu32 actions_logged = 0;\\n\\t\\tu32 old_actions_logged = seccomp_actions_logged;\\n\\n\\t\\tret = write_actions_logged(ro_table, buffer, lenp, ppos,\\n\\t\\t\\t\\t\\t   &actions_logged);\\n\\t\\taudit_actions_logged(actions_logged, old_actions_logged, ret);\\n\\t} else\\n\\t\\tret = read_actions_logged(ro_table, buffer, lenp, ppos);\\n\\n\\treturn ret;\\n}\\n\\nstatic struct ctl_table seccomp_sysctl_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"actions_avail\",\\n\\t\\t.data\\t\\t= (void *) &seccomp_actions_avail,\\n\\t\\t.maxlen\\t\\t= sizeof(seccomp_actions_avail),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"actions_logged\",\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= seccomp_actions_logged_handler,\\n\\t},\\n};\\n\\nstatic int __init seccomp_sysctl_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel/seccomp\", seccomp_sysctl_table);\\n\\treturn 0;\\n}\\n\\ndevice_initcall(seccomp_sysctl_init)\\n\\n#endif /* CONFIG_SYSCTL */\\n\\n#ifdef CONFIG_SECCOMP_CACHE_DEBUG\\n/* Currently CONFIG_SECCOMP_CACHE_DEBUG implies SECCOMP_ARCH_NATIVE */\\nstatic void proc_pid_seccomp_cache_arch(struct seq_file *m, const char *name,\\n\\t\\t\\t\\t\\tconst void *bitmap, size_t bitmap_size)\\n{\\n\\tint nr;\\n\\n\\tfor (nr = 0; nr < bitmap_size; nr++) {\\n\\t\\tbool cached = test_bit(nr, bitmap);\\n\\t\\tchar *status = cached ? \"ALLOW\" : \"FILTER\";\\n\\n\\t\\tseq_printf(m, \"%s %d %s\\\\n\", name, nr, status);\\n\\t}\\n}\\n\\nint proc_pid_seccomp_cache(struct seq_file *m, struct pid_namespace *ns,\\n\\t\\t\\t   struct pid *pid, struct task_struct *task)\\n{\\n\\tstruct seccomp_filter *f;\\n\\tunsigned long flags;\\n\\n\\t/*\\n\\t * We don\\'t want some sandboxed process to know what their seccomp\\n\\t * filters consist of.\\n\\t */\\n\\tif (!file_ns_capable(m->file, &init_user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EACCES;\\n\\n\\tif (!lock_task_sighand(task, &flags))\\n\\t\\treturn -ESRCH;\\n\\n\\tf = READ_ONCE(task->seccomp.filter);\\n\\tif (!f) {\\n\\t\\tunlock_task_sighand(task, &flags);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\t/* prevent filter from being freed while we are printing it */\\n\\t__get_seccomp_filter(f);\\n\\tunlock_task_sighand(task, &flags);\\n\\n\\tproc_pid_seccomp_cache_arch(m, SECCOMP_ARCH_NATIVE_NAME,\\n\\t\\t\\t\\t    f->cache.allow_native,\\n\\t\\t\\t\\t    SECCOMP_ARCH_NATIVE_NR);\\n\\n#ifdef SECCOMP_ARCH_COMPAT\\n\\tproc_pid_seccomp_cache_arch(m, SECCOMP_ARCH_COMPAT_NAME,\\n\\t\\t\\t\\t    f->cache.allow_compat,\\n\\t\\t\\t\\t    SECCOMP_ARCH_COMPAT_NR);\\n#endif /* SECCOMP_ARCH_COMPAT */\\n\\n\\t__put_seccomp_filter(f);\\n\\treturn 0;\\n}\\n#endif /* CONFIG_SECCOMP_CACHE_DEBUG */\\n\\n// SPDX-License-Identifier: GPL-2.0+\\n/*\\n * Restartable sequences system call\\n *\\n * Copyright (C) 2015, Google, Inc.,\\n * Paul Turner <pjt@google.com> and Andrew Hunter <ahh@google.com>\\n * Copyright (C) 2015-2018, EfficiOS Inc.,\\n * Mathieu Desnoyers <mathieu.desnoyers@efficios.com>\\n */\\n\\n#include <linux/sched.h>\\n#include <linux/uaccess.h>\\n#include <linux/syscalls.h>\\n#include <linux/rseq.h>\\n#include <linux/types.h>\\n#include <asm/ptrace.h>\\n\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/rseq.h>\\n\\n/* The original rseq structure size (including padding) is 32 bytes. */\\n#define ORIG_RSEQ_SIZE\\t\\t32\\n\\n#define RSEQ_CS_NO_RESTART_FLAGS (RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT | \\\\\\n\\t\\t\\t\\t  RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL | \\\\\\n\\t\\t\\t\\t  RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE)\\n\\n/*\\n *\\n * Restartable sequences are a lightweight interface that allows\\n * user-level code to be executed atomically relative to scheduler\\n * preemption and signal delivery. Typically used for implementing\\n * per-cpu operations.\\n *\\n * It allows user-space to perform update operations on per-cpu data\\n * without requiring heavy-weight atomic operations.\\n *\\n * Detailed algorithm of rseq user-space assembly sequences:\\n *\\n *                     init(rseq_cs)\\n *                     cpu = TLS->rseq::cpu_id_start\\n *   [1]               TLS->rseq::rseq_cs = rseq_cs\\n *   [start_ip]        ----------------------------\\n *   [2]               if (cpu != TLS->rseq::cpu_id)\\n *                             goto abort_ip;\\n *   [3]               <last_instruction_in_cs>\\n *   [post_commit_ip]  ----------------------------\\n *\\n *   The address of jump target abort_ip must be outside the critical\\n *   region, i.e.:\\n *\\n *     [abort_ip] < [start_ip]  || [abort_ip] >= [post_commit_ip]\\n *\\n *   Steps [2]-[3] (inclusive) need to be a sequence of instructions in\\n *   userspace that can handle being interrupted between any of those\\n *   instructions, and then resumed to the abort_ip.\\n *\\n *   1.  Userspace stores the address of the struct rseq_cs assembly\\n *       block descriptor into the rseq_cs field of the registered\\n *       struct rseq TLS area. This update is performed through a single\\n *       store within the inline assembly instruction sequence.\\n *       [start_ip]\\n *\\n *   2.  Userspace tests to check whether the current cpu_id field match\\n *       the cpu number loaded before start_ip, branching to abort_ip\\n *       in case of a mismatch.\\n *\\n *       If the sequence is preempted or interrupted by a signal\\n *       at or after start_ip and before post_commit_ip, then the kernel\\n *       clears TLS->__rseq_abi::rseq_cs, and sets the user-space return\\n *       ip to abort_ip before returning to user-space, so the preempted\\n *       execution resumes at abort_ip.\\n *\\n *   3.  Userspace critical section final instruction before\\n *       post_commit_ip is the commit. The critical section is\\n *       self-terminating.\\n *       [post_commit_ip]\\n *\\n *   4.  <success>\\n *\\n *   On failure at [2], or if interrupted by preempt or signal delivery\\n *   between [1] and [3]:\\n *\\n *       [abort_ip]\\n *   F1. <failure>\\n */\\n\\nstatic int rseq_update_cpu_node_id(struct task_struct *t)\\n{\\n\\tstruct rseq __user *rseq = t->rseq;\\n\\tu32 cpu_id = raw_smp_processor_id();\\n\\tu32 node_id = cpu_to_node(cpu_id);\\n\\tu32 mm_cid = task_mm_cid(t);\\n\\n\\tWARN_ON_ONCE((int) mm_cid < 0);\\n\\tif (!user_write_access_begin(rseq, t->rseq_len))\\n\\t\\tgoto efault;\\n\\tunsafe_put_user(cpu_id, &rseq->cpu_id_start, efault_end);\\n\\tunsafe_put_user(cpu_id, &rseq->cpu_id, efault_end);\\n\\tunsafe_put_user(node_id, &rseq->node_id, efault_end);\\n\\tunsafe_put_user(mm_cid, &rseq->mm_cid, efault_end);\\n\\t/*\\n\\t * Additional feature fields added after ORIG_RSEQ_SIZE\\n\\t * need to be conditionally updated only if\\n\\t * t->rseq_len != ORIG_RSEQ_SIZE.\\n\\t */\\n\\tuser_write_access_end();\\n\\ttrace_rseq_update(t);\\n\\treturn 0;\\n\\nefault_end:\\n\\tuser_write_access_end();\\nefault:\\n\\treturn -EFAULT;\\n}\\n\\nstatic int rseq_reset_rseq_cpu_node_id(struct task_struct *t)\\n{\\n\\tu32 cpu_id_start = 0, cpu_id = RSEQ_CPU_ID_UNINITIALIZED, node_id = 0,\\n\\t    mm_cid = 0;\\n\\n\\t/*\\n\\t * Reset cpu_id_start to its initial state (0).\\n\\t */\\n\\tif (put_user(cpu_id_start, &t->rseq->cpu_id_start))\\n\\t\\treturn -EFAULT;\\n\\t/*\\n\\t * Reset cpu_id to RSEQ_CPU_ID_UNINITIALIZED, so any user coming\\n\\t * in after unregistration can figure out that rseq needs to be\\n\\t * registered again.\\n\\t */\\n\\tif (put_user(cpu_id, &t->rseq->cpu_id))\\n\\t\\treturn -EFAULT;\\n\\t/*\\n\\t * Reset node_id to its initial state (0).\\n\\t */\\n\\tif (put_user(node_id, &t->rseq->node_id))\\n\\t\\treturn -EFAULT;\\n\\t/*\\n\\t * Reset mm_cid to its initial state (0).\\n\\t */\\n\\tif (put_user(mm_cid, &t->rseq->mm_cid))\\n\\t\\treturn -EFAULT;\\n\\t/*\\n\\t * Additional feature fields added after ORIG_RSEQ_SIZE\\n\\t * need to be conditionally reset only if\\n\\t * t->rseq_len != ORIG_RSEQ_SIZE.\\n\\t */\\n\\treturn 0;\\n}\\n\\nstatic int rseq_get_rseq_cs(struct task_struct *t, struct rseq_cs *rseq_cs)\\n{\\n\\tstruct rseq_cs __user *urseq_cs;\\n\\tu64 ptr;\\n\\tu32 __user *usig;\\n\\tu32 sig;\\n\\tint ret;\\n\\n#ifdef CONFIG_64BIT\\n\\tif (get_user(ptr, &t->rseq->rseq_cs))\\n\\t\\treturn -EFAULT;\\n#else\\n\\tif (copy_from_user(&ptr, &t->rseq->rseq_cs, sizeof(ptr)))\\n\\t\\treturn -EFAULT;\\n#endif\\n\\tif (!ptr) {\\n\\t\\tmemset(rseq_cs, 0, sizeof(*rseq_cs));\\n\\t\\treturn 0;\\n\\t}\\n\\tif (ptr >= TASK_SIZE)\\n\\t\\treturn -EINVAL;\\n\\turseq_cs = (struct rseq_cs __user *)(unsigned long)ptr;\\n\\tif (copy_from_user(rseq_cs, urseq_cs, sizeof(*rseq_cs)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (rseq_cs->start_ip >= TASK_SIZE ||\\n\\t    rseq_cs->start_ip + rseq_cs->post_commit_offset >= TASK_SIZE ||\\n\\t    rseq_cs->abort_ip >= TASK_SIZE ||\\n\\t    rseq_cs->version > 0)\\n\\t\\treturn -EINVAL;\\n\\t/* Check for overflow. */\\n\\tif (rseq_cs->start_ip + rseq_cs->post_commit_offset < rseq_cs->start_ip)\\n\\t\\treturn -EINVAL;\\n\\t/* Ensure that abort_ip is not in the critical section. */\\n\\tif (rseq_cs->abort_ip - rseq_cs->start_ip < rseq_cs->post_commit_offset)\\n\\t\\treturn -EINVAL;\\n\\n\\tusig = (u32 __user *)(unsigned long)(rseq_cs->abort_ip - sizeof(u32));\\n\\tret = get_user(sig, usig);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (current->rseq_sig != sig) {\\n\\t\\tprintk_ratelimited(KERN_WARNING\\n\\t\\t\\t\"Possible attack attempt. Unexpected rseq signature 0x%x, expecting 0x%x (pid=%d, addr=%p).\\\\n\",\\n\\t\\t\\tsig, current->rseq_sig, current->pid, usig);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic bool rseq_warn_flags(const char *str, u32 flags)\\n{\\n\\tu32 test_flags;\\n\\n\\tif (!flags)\\n\\t\\treturn false;\\n\\ttest_flags = flags & RSEQ_CS_NO_RESTART_FLAGS;\\n\\tif (test_flags)\\n\\t\\tpr_warn_once(\"Deprecated flags (%u) in %s ABI structure\", test_flags, str);\\n\\ttest_flags = flags & ~RSEQ_CS_NO_RESTART_FLAGS;\\n\\tif (test_flags)\\n\\t\\tpr_warn_once(\"Unknown flags (%u) in %s ABI structure\", test_flags, str);\\n\\treturn true;\\n}\\n\\nstatic int rseq_need_restart(struct task_struct *t, u32 cs_flags)\\n{\\n\\tu32 flags, event_mask;\\n\\tint ret;\\n\\n\\tif (rseq_warn_flags(\"rseq_cs\", cs_flags))\\n\\t\\treturn -EINVAL;\\n\\n\\t/* Get thread flags. */\\n\\tret = get_user(flags, &t->rseq->flags);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (rseq_warn_flags(\"rseq\", flags))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Load and clear event mask atomically with respect to\\n\\t * scheduler preemption.\\n\\t */\\n\\tpreempt_disable();\\n\\tevent_mask = t->rseq_event_mask;\\n\\tt->rseq_event_mask = 0;\\n\\tpreempt_enable();\\n\\n\\treturn !!event_mask;\\n}\\n\\nstatic int clear_rseq_cs(struct task_struct *t)\\n{\\n\\t/*\\n\\t * The rseq_cs field is set to NULL on preemption or signal\\n\\t * delivery on top of rseq assembly block, as well as on top\\n\\t * of code outside of the rseq assembly block. This performs\\n\\t * a lazy clear of the rseq_cs field.\\n\\t *\\n\\t * Set rseq_cs to NULL.\\n\\t */\\n#ifdef CONFIG_64BIT\\n\\treturn put_user(0UL, &t->rseq->rseq_cs);\\n#else\\n\\tif (clear_user(&t->rseq->rseq_cs, sizeof(t->rseq->rseq_cs)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n#endif\\n}\\n\\n/*\\n * Unsigned comparison will be true when ip >= start_ip, and when\\n * ip < start_ip + post_commit_offset.\\n */\\nstatic bool in_rseq_cs(unsigned long ip, struct rseq_cs *rseq_cs)\\n{\\n\\treturn ip - rseq_cs->start_ip < rseq_cs->post_commit_offset;\\n}\\n\\nstatic int rseq_ip_fixup(struct pt_regs *regs)\\n{\\n\\tunsigned long ip = instruction_pointer(regs);\\n\\tstruct task_struct *t = current;\\n\\tstruct rseq_cs rseq_cs;\\n\\tint ret;\\n\\n\\tret = rseq_get_rseq_cs(t, &rseq_cs);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/*\\n\\t * Handle potentially not being within a critical section.\\n\\t * If not nested over a rseq critical section, restart is useless.\\n\\t * Clear the rseq_cs pointer and return.\\n\\t */\\n\\tif (!in_rseq_cs(ip, &rseq_cs))\\n\\t\\treturn clear_rseq_cs(t);\\n\\tret = rseq_need_restart(t, rseq_cs.flags);\\n\\tif (ret <= 0)\\n\\t\\treturn ret;\\n\\tret = clear_rseq_cs(t);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\ttrace_rseq_ip_fixup(ip, rseq_cs.start_ip, rseq_cs.post_commit_offset,\\n\\t\\t\\t    rseq_cs.abort_ip);\\n\\tinstruction_pointer_set(regs, (unsigned long)rseq_cs.abort_ip);\\n\\treturn 0;\\n}\\n\\n/*\\n * This resume handler must always be executed between any of:\\n * - preemption,\\n * - signal delivery,\\n * and return to user-space.\\n *\\n * This is how we can ensure that the entire rseq critical section\\n * will issue the commit instruction only if executed atomically with\\n * respect to other threads scheduled on the same CPU, and with respect\\n * to signal handlers.\\n */\\nvoid __rseq_handle_notify_resume(struct ksignal *ksig, struct pt_regs *regs)\\n{\\n\\tstruct task_struct *t = current;\\n\\tint ret, sig;\\n\\n\\tif (unlikely(t->flags & PF_EXITING))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * regs is NULL if and only if the caller is in a syscall path.  Skip\\n\\t * fixup and leave rseq_cs as is so that rseq_sycall() will detect and\\n\\t * kill a misbehaving userspace on debug kernels.\\n\\t */\\n\\tif (regs) {\\n\\t\\tret = rseq_ip_fixup(regs);\\n\\t\\tif (unlikely(ret < 0))\\n\\t\\t\\tgoto error;\\n\\t}\\n\\tif (unlikely(rseq_update_cpu_node_id(t)))\\n\\t\\tgoto error;\\n\\treturn;\\n\\nerror:\\n\\tsig = ksig ? ksig->sig : 0;\\n\\tforce_sigsegv(sig);\\n}\\n\\n#ifdef CONFIG_DEBUG_RSEQ\\n\\n/*\\n * Terminate the process if a syscall is issued within a restartable\\n * sequence.\\n */\\nvoid rseq_syscall(struct pt_regs *regs)\\n{\\n\\tunsigned long ip = instruction_pointer(regs);\\n\\tstruct task_struct *t = current;\\n\\tstruct rseq_cs rseq_cs;\\n\\n\\tif (!t->rseq)\\n\\t\\treturn;\\n\\tif (rseq_get_rseq_cs(t, &rseq_cs) || in_rseq_cs(ip, &rseq_cs))\\n\\t\\tforce_sig(SIGSEGV);\\n}\\n\\n#endif\\n\\n/*\\n * sys_rseq - setup restartable sequences for caller thread.\\n */\\nSYSCALL_DEFINE4(rseq, struct rseq __user *, rseq, u32, rseq_len,\\n\\t\\tint, flags, u32, sig)\\n{\\n\\tint ret;\\n\\n\\tif (flags & RSEQ_FLAG_UNREGISTER) {\\n\\t\\tif (flags & ~RSEQ_FLAG_UNREGISTER)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t/* Unregister rseq for current thread. */\\n\\t\\tif (current->rseq != rseq || !current->rseq)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (rseq_len != current->rseq_len)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (current->rseq_sig != sig)\\n\\t\\t\\treturn -EPERM;\\n\\t\\tret = rseq_reset_rseq_cpu_node_id(current);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t\\tcurrent->rseq = NULL;\\n\\t\\tcurrent->rseq_sig = 0;\\n\\t\\tcurrent->rseq_len = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (unlikely(flags))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (current->rseq) {\\n\\t\\t/*\\n\\t\\t * If rseq is already registered, check whether\\n\\t\\t * the provided address differs from the prior\\n\\t\\t * one.\\n\\t\\t */\\n\\t\\tif (current->rseq != rseq || rseq_len != current->rseq_len)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (current->rseq_sig != sig)\\n\\t\\t\\treturn -EPERM;\\n\\t\\t/* Already registered. */\\n\\t\\treturn -EBUSY;\\n\\t}\\n\\n\\t/*\\n\\t * If there was no rseq previously registered, ensure the provided rseq\\n\\t * is properly aligned, as communcated to user-space through the ELF\\n\\t * auxiliary vector AT_RSEQ_ALIGN. If rseq_len is the original rseq\\n\\t * size, the required alignment is the original struct rseq alignment.\\n\\t *\\n\\t * In order to be valid, rseq_len is either the original rseq size, or\\n\\t * large enough to contain all supported fields, as communicated to\\n\\t * user-space through the ELF auxiliary vector AT_RSEQ_FEATURE_SIZE.\\n\\t */\\n\\tif (rseq_len < ORIG_RSEQ_SIZE ||\\n\\t    (rseq_len == ORIG_RSEQ_SIZE && !IS_ALIGNED((unsigned long)rseq, ORIG_RSEQ_SIZE)) ||\\n\\t    (rseq_len != ORIG_RSEQ_SIZE && (!IS_ALIGNED((unsigned long)rseq, __alignof__(*rseq)) ||\\n\\t\\t\\t\\t\\t    rseq_len < offsetof(struct rseq, end))))\\n\\t\\treturn -EINVAL;\\n\\tif (!access_ok(rseq, rseq_len))\\n\\t\\treturn -EFAULT;\\n\\tcurrent->rseq = rseq;\\n\\tcurrent->rseq_len = rseq_len;\\n\\tcurrent->rseq_sig = sig;\\n\\t/*\\n\\t * If rseq was previously inactive, and has just been\\n\\t * registered, ensure the cpu_id_start and cpu_id fields\\n\\t * are updated before returning to user-space.\\n\\t */\\n\\trseq_set_notify_resume(current);\\n\\n\\treturn 0;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  Copyright (C) 2006 IBM Corporation\\n *\\n *  Author: Serge Hallyn <serue@us.ibm.com>\\n *\\n *  Jun 2006 - namespaces support\\n *             OpenVZ, SWsoft Inc.\\n *             Pavel Emelianov <xemul@openvz.org>\\n */\\n\\n#include <linux/slab.h>\\n#include <linux/export.h>\\n#include <linux/nsproxy.h>\\n#include <linux/init_task.h>\\n#include <linux/mnt_namespace.h>\\n#include <linux/utsname.h>\\n#include <linux/pid_namespace.h>\\n#include <net/net_namespace.h>\\n#include <linux/ipc_namespace.h>\\n#include <linux/time_namespace.h>\\n#include <linux/fs_struct.h>\\n#include <linux/proc_fs.h>\\n#include <linux/proc_ns.h>\\n#include <linux/file.h>\\n#include <linux/syscalls.h>\\n#include <linux/cgroup.h>\\n#include <linux/perf_event.h>\\n\\nstatic struct kmem_cache *nsproxy_cachep;\\n\\nstruct nsproxy init_nsproxy = {\\n\\t.count\\t\\t\\t= REFCOUNT_INIT(1),\\n\\t.uts_ns\\t\\t\\t= &init_uts_ns,\\n#if defined(CONFIG_POSIX_MQUEUE) || defined(CONFIG_SYSVIPC)\\n\\t.ipc_ns\\t\\t\\t= &init_ipc_ns,\\n#endif\\n\\t.mnt_ns\\t\\t\\t= NULL,\\n\\t.pid_ns_for_children\\t= &init_pid_ns,\\n#ifdef CONFIG_NET\\n\\t.net_ns\\t\\t\\t= &init_net,\\n#endif\\n#ifdef CONFIG_CGROUPS\\n\\t.cgroup_ns\\t\\t= &init_cgroup_ns,\\n#endif\\n#ifdef CONFIG_TIME_NS\\n\\t.time_ns\\t\\t= &init_time_ns,\\n\\t.time_ns_for_children\\t= &init_time_ns,\\n#endif\\n};\\n\\nstatic inline struct nsproxy *create_nsproxy(void)\\n{\\n\\tstruct nsproxy *nsproxy;\\n\\n\\tnsproxy = kmem_cache_alloc(nsproxy_cachep, GFP_KERNEL);\\n\\tif (nsproxy)\\n\\t\\trefcount_set(&nsproxy->count, 1);\\n\\treturn nsproxy;\\n}\\n\\n/*\\n * Create new nsproxy and all of its the associated namespaces.\\n * Return the newly created nsproxy.  Do not attach this to the task,\\n * leave it to the caller to do proper locking and attach it to task.\\n */\\nstatic struct nsproxy *create_new_namespaces(unsigned long flags,\\n\\tstruct task_struct *tsk, struct user_namespace *user_ns,\\n\\tstruct fs_struct *new_fs)\\n{\\n\\tstruct nsproxy *new_nsp;\\n\\tint err;\\n\\n\\tnew_nsp = create_nsproxy();\\n\\tif (!new_nsp)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tnew_nsp->mnt_ns = copy_mnt_ns(flags, tsk->nsproxy->mnt_ns, user_ns, new_fs);\\n\\tif (IS_ERR(new_nsp->mnt_ns)) {\\n\\t\\terr = PTR_ERR(new_nsp->mnt_ns);\\n\\t\\tgoto out_ns;\\n\\t}\\n\\n\\tnew_nsp->uts_ns = copy_utsname(flags, user_ns, tsk->nsproxy->uts_ns);\\n\\tif (IS_ERR(new_nsp->uts_ns)) {\\n\\t\\terr = PTR_ERR(new_nsp->uts_ns);\\n\\t\\tgoto out_uts;\\n\\t}\\n\\n\\tnew_nsp->ipc_ns = copy_ipcs(flags, user_ns, tsk->nsproxy->ipc_ns);\\n\\tif (IS_ERR(new_nsp->ipc_ns)) {\\n\\t\\terr = PTR_ERR(new_nsp->ipc_ns);\\n\\t\\tgoto out_ipc;\\n\\t}\\n\\n\\tnew_nsp->pid_ns_for_children =\\n\\t\\tcopy_pid_ns(flags, user_ns, tsk->nsproxy->pid_ns_for_children);\\n\\tif (IS_ERR(new_nsp->pid_ns_for_children)) {\\n\\t\\terr = PTR_ERR(new_nsp->pid_ns_for_children);\\n\\t\\tgoto out_pid;\\n\\t}\\n\\n\\tnew_nsp->cgroup_ns = copy_cgroup_ns(flags, user_ns,\\n\\t\\t\\t\\t\\t    tsk->nsproxy->cgroup_ns);\\n\\tif (IS_ERR(new_nsp->cgroup_ns)) {\\n\\t\\terr = PTR_ERR(new_nsp->cgroup_ns);\\n\\t\\tgoto out_cgroup;\\n\\t}\\n\\n\\tnew_nsp->net_ns = copy_net_ns(flags, user_ns, tsk->nsproxy->net_ns);\\n\\tif (IS_ERR(new_nsp->net_ns)) {\\n\\t\\terr = PTR_ERR(new_nsp->net_ns);\\n\\t\\tgoto out_net;\\n\\t}\\n\\n\\tnew_nsp->time_ns_for_children = copy_time_ns(flags, user_ns,\\n\\t\\t\\t\\t\\ttsk->nsproxy->time_ns_for_children);\\n\\tif (IS_ERR(new_nsp->time_ns_for_children)) {\\n\\t\\terr = PTR_ERR(new_nsp->time_ns_for_children);\\n\\t\\tgoto out_time;\\n\\t}\\n\\tnew_nsp->time_ns = get_time_ns(tsk->nsproxy->time_ns);\\n\\n\\treturn new_nsp;\\n\\nout_time:\\n\\tput_net(new_nsp->net_ns);\\nout_net:\\n\\tput_cgroup_ns(new_nsp->cgroup_ns);\\nout_cgroup:\\n\\tif (new_nsp->pid_ns_for_children)\\n\\t\\tput_pid_ns(new_nsp->pid_ns_for_children);\\nout_pid:\\n\\tif (new_nsp->ipc_ns)\\n\\t\\tput_ipc_ns(new_nsp->ipc_ns);\\nout_ipc:\\n\\tif (new_nsp->uts_ns)\\n\\t\\tput_uts_ns(new_nsp->uts_ns);\\nout_uts:\\n\\tif (new_nsp->mnt_ns)\\n\\t\\tput_mnt_ns(new_nsp->mnt_ns);\\nout_ns:\\n\\tkmem_cache_free(nsproxy_cachep, new_nsp);\\n\\treturn ERR_PTR(err);\\n}\\n\\n/*\\n * called from clone.  This now handles copy for nsproxy and all\\n * namespaces therein.\\n */\\nint copy_namespaces(unsigned long flags, struct task_struct *tsk)\\n{\\n\\tstruct nsproxy *old_ns = tsk->nsproxy;\\n\\tstruct user_namespace *user_ns = task_cred_xxx(tsk, user_ns);\\n\\tstruct nsproxy *new_ns;\\n\\n\\tif (likely(!(flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |\\n\\t\\t\\t      CLONE_NEWPID | CLONE_NEWNET |\\n\\t\\t\\t      CLONE_NEWCGROUP | CLONE_NEWTIME)))) {\\n\\t\\tif ((flags & CLONE_VM) ||\\n\\t\\t    likely(old_ns->time_ns_for_children == old_ns->time_ns)) {\\n\\t\\t\\tget_nsproxy(old_ns);\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\t} else if (!ns_capable(user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\t/*\\n\\t * CLONE_NEWIPC must detach from the undolist: after switching\\n\\t * to a new ipc namespace, the semaphore arrays from the old\\n\\t * namespace are unreachable.  In clone parlance, CLONE_SYSVSEM\\n\\t * means share undolist with parent, so we must forbid using\\n\\t * it along with CLONE_NEWIPC.\\n\\t */\\n\\tif ((flags & (CLONE_NEWIPC | CLONE_SYSVSEM)) ==\\n\\t\\t(CLONE_NEWIPC | CLONE_SYSVSEM))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew_ns = create_new_namespaces(flags, tsk, user_ns, tsk->fs);\\n\\tif (IS_ERR(new_ns))\\n\\t\\treturn  PTR_ERR(new_ns);\\n\\n\\tif ((flags & CLONE_VM) == 0)\\n\\t\\ttimens_on_fork(new_ns, tsk);\\n\\n\\ttsk->nsproxy = new_ns;\\n\\treturn 0;\\n}\\n\\nvoid free_nsproxy(struct nsproxy *ns)\\n{\\n\\tif (ns->mnt_ns)\\n\\t\\tput_mnt_ns(ns->mnt_ns);\\n\\tif (ns->uts_ns)\\n\\t\\tput_uts_ns(ns->uts_ns);\\n\\tif (ns->ipc_ns)\\n\\t\\tput_ipc_ns(ns->ipc_ns);\\n\\tif (ns->pid_ns_for_children)\\n\\t\\tput_pid_ns(ns->pid_ns_for_children);\\n\\tif (ns->time_ns)\\n\\t\\tput_time_ns(ns->time_ns);\\n\\tif (ns->time_ns_for_children)\\n\\t\\tput_time_ns(ns->time_ns_for_children);\\n\\tput_cgroup_ns(ns->cgroup_ns);\\n\\tput_net(ns->net_ns);\\n\\tkmem_cache_free(nsproxy_cachep, ns);\\n}\\n\\n/*\\n * Called from unshare. Unshare all the namespaces part of nsproxy.\\n * On success, returns the new nsproxy.\\n */\\nint unshare_nsproxy_namespaces(unsigned long unshare_flags,\\n\\tstruct nsproxy **new_nsp, struct cred *new_cred, struct fs_struct *new_fs)\\n{\\n\\tstruct user_namespace *user_ns;\\n\\tint err = 0;\\n\\n\\tif (!(unshare_flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |\\n\\t\\t\\t       CLONE_NEWNET | CLONE_NEWPID | CLONE_NEWCGROUP |\\n\\t\\t\\t       CLONE_NEWTIME)))\\n\\t\\treturn 0;\\n\\n\\tuser_ns = new_cred ? new_cred->user_ns : current_user_ns();\\n\\tif (!ns_capable(user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\t*new_nsp = create_new_namespaces(unshare_flags, current, user_ns,\\n\\t\\t\\t\\t\\t new_fs ? new_fs : current->fs);\\n\\tif (IS_ERR(*new_nsp)) {\\n\\t\\terr = PTR_ERR(*new_nsp);\\n\\t\\tgoto out;\\n\\t}\\n\\nout:\\n\\treturn err;\\n}\\n\\nvoid switch_task_namespaces(struct task_struct *p, struct nsproxy *new)\\n{\\n\\tstruct nsproxy *ns;\\n\\n\\tmight_sleep();\\n\\n\\ttask_lock(p);\\n\\tns = p->nsproxy;\\n\\tp->nsproxy = new;\\n\\ttask_unlock(p);\\n\\n\\tif (ns)\\n\\t\\tput_nsproxy(ns);\\n}\\n\\nvoid exit_task_namespaces(struct task_struct *p)\\n{\\n\\tswitch_task_namespaces(p, NULL);\\n}\\n\\nint exec_task_namespaces(void)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\tstruct nsproxy *new;\\n\\n\\tif (tsk->nsproxy->time_ns_for_children == tsk->nsproxy->time_ns)\\n\\t\\treturn 0;\\n\\n\\tnew = create_new_namespaces(0, tsk, current_user_ns(), tsk->fs);\\n\\tif (IS_ERR(new))\\n\\t\\treturn PTR_ERR(new);\\n\\n\\ttimens_on_fork(new, tsk);\\n\\tswitch_task_namespaces(tsk, new);\\n\\treturn 0;\\n}\\n\\nstatic int check_setns_flags(unsigned long flags)\\n{\\n\\tif (!flags || (flags & ~(CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |\\n\\t\\t\\t\\t CLONE_NEWNET | CLONE_NEWTIME | CLONE_NEWUSER |\\n\\t\\t\\t\\t CLONE_NEWPID | CLONE_NEWCGROUP)))\\n\\t\\treturn -EINVAL;\\n\\n#ifndef CONFIG_USER_NS\\n\\tif (flags & CLONE_NEWUSER)\\n\\t\\treturn -EINVAL;\\n#endif\\n#ifndef CONFIG_PID_NS\\n\\tif (flags & CLONE_NEWPID)\\n\\t\\treturn -EINVAL;\\n#endif\\n#ifndef CONFIG_UTS_NS\\n\\tif (flags & CLONE_NEWUTS)\\n\\t\\treturn -EINVAL;\\n#endif\\n#ifndef CONFIG_IPC_NS\\n\\tif (flags & CLONE_NEWIPC)\\n\\t\\treturn -EINVAL;\\n#endif\\n#ifndef CONFIG_CGROUPS\\n\\tif (flags & CLONE_NEWCGROUP)\\n\\t\\treturn -EINVAL;\\n#endif\\n#ifndef CONFIG_NET_NS\\n\\tif (flags & CLONE_NEWNET)\\n\\t\\treturn -EINVAL;\\n#endif\\n#ifndef CONFIG_TIME_NS\\n\\tif (flags & CLONE_NEWTIME)\\n\\t\\treturn -EINVAL;\\n#endif\\n\\n\\treturn 0;\\n}\\n\\nstatic void put_nsset(struct nsset *nsset)\\n{\\n\\tunsigned flags = nsset->flags;\\n\\n\\tif (flags & CLONE_NEWUSER)\\n\\t\\tput_cred(nsset_cred(nsset));\\n\\t/*\\n\\t * We only created a temporary copy if we attached to more than just\\n\\t * the mount namespace.\\n\\t */\\n\\tif (nsset->fs && (flags & CLONE_NEWNS) && (flags & ~CLONE_NEWNS))\\n\\t\\tfree_fs_struct(nsset->fs);\\n\\tif (nsset->nsproxy)\\n\\t\\tfree_nsproxy(nsset->nsproxy);\\n}\\n\\nstatic int prepare_nsset(unsigned flags, struct nsset *nsset)\\n{\\n\\tstruct task_struct *me = current;\\n\\n\\tnsset->nsproxy = create_new_namespaces(0, me, current_user_ns(), me->fs);\\n\\tif (IS_ERR(nsset->nsproxy))\\n\\t\\treturn PTR_ERR(nsset->nsproxy);\\n\\n\\tif (flags & CLONE_NEWUSER)\\n\\t\\tnsset->cred = prepare_creds();\\n\\telse\\n\\t\\tnsset->cred = current_cred();\\n\\tif (!nsset->cred)\\n\\t\\tgoto out;\\n\\n\\t/* Only create a temporary copy of fs_struct if we really need to. */\\n\\tif (flags == CLONE_NEWNS) {\\n\\t\\tnsset->fs = me->fs;\\n\\t} else if (flags & CLONE_NEWNS) {\\n\\t\\tnsset->fs = copy_fs_struct(me->fs);\\n\\t\\tif (!nsset->fs)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\tnsset->flags = flags;\\n\\treturn 0;\\n\\nout:\\n\\tput_nsset(nsset);\\n\\treturn -ENOMEM;\\n}\\n\\nstatic inline int validate_ns(struct nsset *nsset, struct ns_common *ns)\\n{\\n\\treturn ns->ops->install(nsset, ns);\\n}\\n\\n/*\\n * This is the inverse operation to unshare().\\n * Ordering is equivalent to the standard ordering used everywhere else\\n * during unshare and process creation. The switch to the new set of\\n * namespaces occurs at the point of no return after installation of\\n * all requested namespaces was successful in commit_nsset().\\n */\\nstatic int validate_nsset(struct nsset *nsset, struct pid *pid)\\n{\\n\\tint ret = 0;\\n\\tunsigned flags = nsset->flags;\\n\\tstruct user_namespace *user_ns = NULL;\\n\\tstruct pid_namespace *pid_ns = NULL;\\n\\tstruct nsproxy *nsp;\\n\\tstruct task_struct *tsk;\\n\\n\\t/* Take a \"snapshot\" of the target task\\'s namespaces. */\\n\\trcu_read_lock();\\n\\ttsk = pid_task(pid, PIDTYPE_PID);\\n\\tif (!tsk) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn -ESRCH;\\n\\t}\\n\\n\\tif (!ptrace_may_access(tsk, PTRACE_MODE_READ_REALCREDS)) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn -EPERM;\\n\\t}\\n\\n\\ttask_lock(tsk);\\n\\tnsp = tsk->nsproxy;\\n\\tif (nsp)\\n\\t\\tget_nsproxy(nsp);\\n\\ttask_unlock(tsk);\\n\\tif (!nsp) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn -ESRCH;\\n\\t}\\n\\n#ifdef CONFIG_PID_NS\\n\\tif (flags & CLONE_NEWPID) {\\n\\t\\tpid_ns = task_active_pid_ns(tsk);\\n\\t\\tif (unlikely(!pid_ns)) {\\n\\t\\t\\trcu_read_unlock();\\n\\t\\t\\tret = -ESRCH;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tget_pid_ns(pid_ns);\\n\\t}\\n#endif\\n\\n#ifdef CONFIG_USER_NS\\n\\tif (flags & CLONE_NEWUSER)\\n\\t\\tuser_ns = get_user_ns(__task_cred(tsk)->user_ns);\\n#endif\\n\\trcu_read_unlock();\\n\\n\\t/*\\n\\t * Install requested namespaces. The caller will have\\n\\t * verified earlier that the requested namespaces are\\n\\t * supported on this kernel. We don\\'t report errors here\\n\\t * if a namespace is requested that isn\\'t supported.\\n\\t */\\n#ifdef CONFIG_USER_NS\\n\\tif (flags & CLONE_NEWUSER) {\\n\\t\\tret = validate_ns(nsset, &user_ns->ns);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n#endif\\n\\n\\tif (flags & CLONE_NEWNS) {\\n\\t\\tret = validate_ns(nsset, from_mnt_ns(nsp->mnt_ns));\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n#ifdef CONFIG_UTS_NS\\n\\tif (flags & CLONE_NEWUTS) {\\n\\t\\tret = validate_ns(nsset, &nsp->uts_ns->ns);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n#endif\\n\\n#ifdef CONFIG_IPC_NS\\n\\tif (flags & CLONE_NEWIPC) {\\n\\t\\tret = validate_ns(nsset, &nsp->ipc_ns->ns);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n#endif\\n\\n#ifdef CONFIG_PID_NS\\n\\tif (flags & CLONE_NEWPID) {\\n\\t\\tret = validate_ns(nsset, &pid_ns->ns);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n#endif\\n\\n#ifdef CONFIG_CGROUPS\\n\\tif (flags & CLONE_NEWCGROUP) {\\n\\t\\tret = validate_ns(nsset, &nsp->cgroup_ns->ns);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n#endif\\n\\n#ifdef CONFIG_NET_NS\\n\\tif (flags & CLONE_NEWNET) {\\n\\t\\tret = validate_ns(nsset, &nsp->net_ns->ns);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n#endif\\n\\n#ifdef CONFIG_TIME_NS\\n\\tif (flags & CLONE_NEWTIME) {\\n\\t\\tret = validate_ns(nsset, &nsp->time_ns->ns);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n#endif\\n\\nout:\\n\\tif (pid_ns)\\n\\t\\tput_pid_ns(pid_ns);\\n\\tif (nsp)\\n\\t\\tput_nsproxy(nsp);\\n\\tput_user_ns(user_ns);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * This is the point of no return. There are just a few namespaces\\n * that do some actual work here and it\\'s sufficiently minimal that\\n * a separate ns_common operation seems unnecessary for now.\\n * Unshare is doing the same thing. If we\\'ll end up needing to do\\n * more in a given namespace or a helper here is ultimately not\\n * exported anymore a simple commit handler for each namespace\\n * should be added to ns_common.\\n */\\nstatic void commit_nsset(struct nsset *nsset)\\n{\\n\\tunsigned flags = nsset->flags;\\n\\tstruct task_struct *me = current;\\n\\n#ifdef CONFIG_USER_NS\\n\\tif (flags & CLONE_NEWUSER) {\\n\\t\\t/* transfer ownership */\\n\\t\\tcommit_creds(nsset_cred(nsset));\\n\\t\\tnsset->cred = NULL;\\n\\t}\\n#endif\\n\\n\\t/* We only need to commit if we have used a temporary fs_struct. */\\n\\tif ((flags & CLONE_NEWNS) && (flags & ~CLONE_NEWNS)) {\\n\\t\\tset_fs_root(me->fs, &nsset->fs->root);\\n\\t\\tset_fs_pwd(me->fs, &nsset->fs->pwd);\\n\\t}\\n\\n#ifdef CONFIG_IPC_NS\\n\\tif (flags & CLONE_NEWIPC)\\n\\t\\texit_sem(me);\\n#endif\\n\\n#ifdef CONFIG_TIME_NS\\n\\tif (flags & CLONE_NEWTIME)\\n\\t\\ttimens_commit(me, nsset->nsproxy->time_ns);\\n#endif\\n\\n\\t/* transfer ownership */\\n\\tswitch_task_namespaces(me, nsset->nsproxy);\\n\\tnsset->nsproxy = NULL;\\n}\\n\\nSYSCALL_DEFINE2(setns, int, fd, int, flags)\\n{\\n\\tCLASS(fd, f)(fd);\\n\\tstruct ns_common *ns = NULL;\\n\\tstruct nsset nsset = {};\\n\\tint err = 0;\\n\\n\\tif (fd_empty(f))\\n\\t\\treturn -EBADF;\\n\\n\\tif (proc_ns_file(fd_file(f))) {\\n\\t\\tns = get_proc_ns(file_inode(fd_file(f)));\\n\\t\\tif (flags && (ns->ops->type != flags))\\n\\t\\t\\terr = -EINVAL;\\n\\t\\tflags = ns->ops->type;\\n\\t} else if (!IS_ERR(pidfd_pid(fd_file(f)))) {\\n\\t\\terr = check_setns_flags(flags);\\n\\t} else {\\n\\t\\terr = -EINVAL;\\n\\t}\\n\\tif (err)\\n\\t\\tgoto out;\\n\\n\\terr = prepare_nsset(flags, &nsset);\\n\\tif (err)\\n\\t\\tgoto out;\\n\\n\\tif (proc_ns_file(fd_file(f)))\\n\\t\\terr = validate_ns(&nsset, ns);\\n\\telse\\n\\t\\terr = validate_nsset(&nsset, pidfd_pid(fd_file(f)));\\n\\tif (!err) {\\n\\t\\tcommit_nsset(&nsset);\\n\\t\\tperf_event_namespaces(current);\\n\\t}\\n\\tput_nsset(&nsset);\\nout:\\n\\treturn err;\\n}\\n\\nint __init nsproxy_cache_init(void)\\n{\\n\\tnsproxy_cachep = KMEM_CACHE(nsproxy, SLAB_PANIC|SLAB_ACCOUNT);\\n\\treturn 0;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/*\\n * kernel/stop_machine.c\\n *\\n * Copyright (C) 2008, 2005\\tIBM Corporation.\\n * Copyright (C) 2008, 2005\\tRusty Russell rusty@rustcorp.com.au\\n * Copyright (C) 2010\\t\\tSUSE Linux Products GmbH\\n * Copyright (C) 2010\\t\\tTejun Heo <tj@kernel.org>\\n */\\n#include <linux/compiler.h>\\n#include <linux/completion.h>\\n#include <linux/cpu.h>\\n#include <linux/init.h>\\n#include <linux/kthread.h>\\n#include <linux/export.h>\\n#include <linux/percpu.h>\\n#include <linux/sched.h>\\n#include <linux/stop_machine.h>\\n#include <linux/interrupt.h>\\n#include <linux/kallsyms.h>\\n#include <linux/smpboot.h>\\n#include <linux/atomic.h>\\n#include <linux/nmi.h>\\n#include <linux/sched/wake_q.h>\\n\\n/*\\n * Structure to determine completion condition and record errors.  May\\n * be shared by works on different cpus.\\n */\\nstruct cpu_stop_done {\\n\\tatomic_t\\t\\tnr_todo;\\t/* nr left to execute */\\n\\tint\\t\\t\\tret;\\t\\t/* collected return value */\\n\\tstruct completion\\tcompletion;\\t/* fired if nr_todo reaches 0 */\\n};\\n\\n/* the actual stopper, one per every possible cpu, enabled on online cpus */\\nstruct cpu_stopper {\\n\\tstruct task_struct\\t*thread;\\n\\n\\traw_spinlock_t\\t\\tlock;\\n\\tbool\\t\\t\\tenabled;\\t/* is this stopper enabled? */\\n\\tstruct list_head\\tworks;\\t\\t/* list of pending works */\\n\\n\\tstruct cpu_stop_work\\tstop_work;\\t/* for stop_cpus */\\n\\tunsigned long\\t\\tcaller;\\n\\tcpu_stop_fn_t\\t\\tfn;\\n};\\n\\nstatic DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);\\nstatic bool stop_machine_initialized = false;\\n\\nvoid print_stop_info(const char *log_lvl, struct task_struct *task)\\n{\\n\\t/*\\n\\t * If @task is a stopper task, it cannot migrate and task_cpu() is\\n\\t * stable.\\n\\t */\\n\\tstruct cpu_stopper *stopper = per_cpu_ptr(&cpu_stopper, task_cpu(task));\\n\\n\\tif (task != stopper->thread)\\n\\t\\treturn;\\n\\n\\tprintk(\"%sStopper: %pS <- %pS\\\\n\", log_lvl, stopper->fn, (void *)stopper->caller);\\n}\\n\\n/* static data for stop_cpus */\\nstatic DEFINE_MUTEX(stop_cpus_mutex);\\nstatic bool stop_cpus_in_progress;\\n\\nstatic void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)\\n{\\n\\tmemset(done, 0, sizeof(*done));\\n\\tatomic_set(&done->nr_todo, nr_todo);\\n\\tinit_completion(&done->completion);\\n}\\n\\n/* signal completion unless @done is NULL */\\nstatic void cpu_stop_signal_done(struct cpu_stop_done *done)\\n{\\n\\tif (atomic_dec_and_test(&done->nr_todo))\\n\\t\\tcomplete(&done->completion);\\n}\\n\\nstatic void __cpu_stop_queue_work(struct cpu_stopper *stopper,\\n\\t\\t\\t\\t\\tstruct cpu_stop_work *work,\\n\\t\\t\\t\\t\\tstruct wake_q_head *wakeq)\\n{\\n\\tlist_add_tail(&work->list, &stopper->works);\\n\\twake_q_add(wakeq, stopper->thread);\\n}\\n\\n/* queue @work to @stopper.  if offline, @work is completed immediately */\\nstatic bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\tDEFINE_WAKE_Q(wakeq);\\n\\tunsigned long flags;\\n\\tbool enabled;\\n\\n\\tpreempt_disable();\\n\\traw_spin_lock_irqsave(&stopper->lock, flags);\\n\\tenabled = stopper->enabled;\\n\\tif (enabled)\\n\\t\\t__cpu_stop_queue_work(stopper, work, &wakeq);\\n\\telse if (work->done)\\n\\t\\tcpu_stop_signal_done(work->done);\\n\\traw_spin_unlock_irqrestore(&stopper->lock, flags);\\n\\n\\twake_up_q(&wakeq);\\n\\tpreempt_enable();\\n\\n\\treturn enabled;\\n}\\n\\n/**\\n * stop_one_cpu - stop a cpu\\n * @cpu: cpu to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n *\\n * Execute @fn(@arg) on @cpu.  @fn is run in a process context with\\n * the highest priority preempting any task on the cpu and\\n * monopolizing it.  This function returns after the execution is\\n * complete.\\n *\\n * This function doesn\\'t guarantee @cpu stays online till @fn\\n * completes.  If @cpu goes down in the middle, execution may happen\\n * partially or fully on different cpus.  @fn should either be ready\\n * for that or the caller should ensure that @cpu stays online until\\n * this function completes.\\n *\\n * CONTEXT:\\n * Might sleep.\\n *\\n * RETURNS:\\n * -ENOENT if @fn(@arg) was not executed because @cpu was offline;\\n * otherwise, the return value of @fn.\\n */\\nint stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)\\n{\\n\\tstruct cpu_stop_done done;\\n\\tstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done, .caller = _RET_IP_ };\\n\\n\\tcpu_stop_init_done(&done, 1);\\n\\tif (!cpu_stop_queue_work(cpu, &work))\\n\\t\\treturn -ENOENT;\\n\\t/*\\n\\t * In case @cpu == smp_proccessor_id() we can avoid a sleep+wakeup\\n\\t * cycle by doing a preemption:\\n\\t */\\n\\tcond_resched();\\n\\twait_for_completion(&done.completion);\\n\\treturn done.ret;\\n}\\n\\n/* This controls the threads on each CPU. */\\nenum multi_stop_state {\\n\\t/* Dummy starting state for thread. */\\n\\tMULTI_STOP_NONE,\\n\\t/* Awaiting everyone to be scheduled. */\\n\\tMULTI_STOP_PREPARE,\\n\\t/* Disable interrupts. */\\n\\tMULTI_STOP_DISABLE_IRQ,\\n\\t/* Run the function */\\n\\tMULTI_STOP_RUN,\\n\\t/* Exit */\\n\\tMULTI_STOP_EXIT,\\n};\\n\\nstruct multi_stop_data {\\n\\tcpu_stop_fn_t\\t\\tfn;\\n\\tvoid\\t\\t\\t*data;\\n\\t/* Like num_online_cpus(), but hotplug cpu uses us, so we need this. */\\n\\tunsigned int\\t\\tnum_threads;\\n\\tconst struct cpumask\\t*active_cpus;\\n\\n\\tenum multi_stop_state\\tstate;\\n\\tatomic_t\\t\\tthread_ack;\\n};\\n\\nstatic void set_state(struct multi_stop_data *msdata,\\n\\t\\t      enum multi_stop_state newstate)\\n{\\n\\t/* Reset ack counter. */\\n\\tatomic_set(&msdata->thread_ack, msdata->num_threads);\\n\\tsmp_wmb();\\n\\tWRITE_ONCE(msdata->state, newstate);\\n}\\n\\n/* Last one to ack a state moves to the next state. */\\nstatic void ack_state(struct multi_stop_data *msdata)\\n{\\n\\tif (atomic_dec_and_test(&msdata->thread_ack))\\n\\t\\tset_state(msdata, msdata->state + 1);\\n}\\n\\nnotrace void __weak stop_machine_yield(const struct cpumask *cpumask)\\n{\\n\\tcpu_relax();\\n}\\n\\n/* This is the cpu_stop function which stops the CPU. */\\nstatic int multi_cpu_stop(void *data)\\n{\\n\\tstruct multi_stop_data *msdata = data;\\n\\tenum multi_stop_state newstate, curstate = MULTI_STOP_NONE;\\n\\tint cpu = smp_processor_id(), err = 0;\\n\\tconst struct cpumask *cpumask;\\n\\tunsigned long flags;\\n\\tbool is_active;\\n\\n\\t/*\\n\\t * When called from stop_machine_from_inactive_cpu(), irq might\\n\\t * already be disabled.  Save the state and restore it on exit.\\n\\t */\\n\\tlocal_save_flags(flags);\\n\\n\\tif (!msdata->active_cpus) {\\n\\t\\tcpumask = cpu_online_mask;\\n\\t\\tis_active = cpu == cpumask_first(cpumask);\\n\\t} else {\\n\\t\\tcpumask = msdata->active_cpus;\\n\\t\\tis_active = cpumask_test_cpu(cpu, cpumask);\\n\\t}\\n\\n\\t/* Simple state machine */\\n\\tdo {\\n\\t\\t/* Chill out and ensure we re-read multi_stop_state. */\\n\\t\\tstop_machine_yield(cpumask);\\n\\t\\tnewstate = READ_ONCE(msdata->state);\\n\\t\\tif (newstate != curstate) {\\n\\t\\t\\tcurstate = newstate;\\n\\t\\t\\tswitch (curstate) {\\n\\t\\t\\tcase MULTI_STOP_DISABLE_IRQ:\\n\\t\\t\\t\\tlocal_irq_disable();\\n\\t\\t\\t\\thard_irq_disable();\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase MULTI_STOP_RUN:\\n\\t\\t\\t\\tif (is_active)\\n\\t\\t\\t\\t\\terr = msdata->fn(msdata->data);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tdefault:\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tack_state(msdata);\\n\\t\\t} else if (curstate > MULTI_STOP_PREPARE) {\\n\\t\\t\\t/*\\n\\t\\t\\t * At this stage all other CPUs we depend on must spin\\n\\t\\t\\t * in the same loop. Any reason for hard-lockup should\\n\\t\\t\\t * be detected and reported on their side.\\n\\t\\t\\t */\\n\\t\\t\\ttouch_nmi_watchdog();\\n\\t\\t}\\n\\t\\trcu_momentary_eqs();\\n\\t} while (curstate != MULTI_STOP_EXIT);\\n\\n\\tlocal_irq_restore(flags);\\n\\treturn err;\\n}\\n\\nstatic int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,\\n\\t\\t\\t\\t    int cpu2, struct cpu_stop_work *work2)\\n{\\n\\tstruct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);\\n\\tstruct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);\\n\\tDEFINE_WAKE_Q(wakeq);\\n\\tint err;\\n\\nretry:\\n\\t/*\\n\\t * The waking up of stopper threads has to happen in the same\\n\\t * scheduling context as the queueing.  Otherwise, there is a\\n\\t * possibility of one of the above stoppers being woken up by another\\n\\t * CPU, and preempting us. This will cause us to not wake up the other\\n\\t * stopper forever.\\n\\t */\\n\\tpreempt_disable();\\n\\traw_spin_lock_irq(&stopper1->lock);\\n\\traw_spin_lock_nested(&stopper2->lock, SINGLE_DEPTH_NESTING);\\n\\n\\tif (!stopper1->enabled || !stopper2->enabled) {\\n\\t\\terr = -ENOENT;\\n\\t\\tgoto unlock;\\n\\t}\\n\\n\\t/*\\n\\t * Ensure that if we race with __stop_cpus() the stoppers won\\'t get\\n\\t * queued up in reverse order leading to system deadlock.\\n\\t *\\n\\t * We can\\'t miss stop_cpus_in_progress if queue_stop_cpus_work() has\\n\\t * queued a work on cpu1 but not on cpu2, we hold both locks.\\n\\t *\\n\\t * It can be falsely true but it is safe to spin until it is cleared,\\n\\t * queue_stop_cpus_work() does everything under preempt_disable().\\n\\t */\\n\\tif (unlikely(stop_cpus_in_progress)) {\\n\\t\\terr = -EDEADLK;\\n\\t\\tgoto unlock;\\n\\t}\\n\\n\\terr = 0;\\n\\t__cpu_stop_queue_work(stopper1, work1, &wakeq);\\n\\t__cpu_stop_queue_work(stopper2, work2, &wakeq);\\n\\nunlock:\\n\\traw_spin_unlock(&stopper2->lock);\\n\\traw_spin_unlock_irq(&stopper1->lock);\\n\\n\\tif (unlikely(err == -EDEADLK)) {\\n\\t\\tpreempt_enable();\\n\\n\\t\\twhile (stop_cpus_in_progress)\\n\\t\\t\\tcpu_relax();\\n\\n\\t\\tgoto retry;\\n\\t}\\n\\n\\twake_up_q(&wakeq);\\n\\tpreempt_enable();\\n\\n\\treturn err;\\n}\\n/**\\n * stop_two_cpus - stops two cpus\\n * @cpu1: the cpu to stop\\n * @cpu2: the other cpu to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n *\\n * Stops both the current and specified CPU and runs @fn on one of them.\\n *\\n * returns when both are completed.\\n */\\nint stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)\\n{\\n\\tstruct cpu_stop_done done;\\n\\tstruct cpu_stop_work work1, work2;\\n\\tstruct multi_stop_data msdata;\\n\\n\\tmsdata = (struct multi_stop_data){\\n\\t\\t.fn = fn,\\n\\t\\t.data = arg,\\n\\t\\t.num_threads = 2,\\n\\t\\t.active_cpus = cpumask_of(cpu1),\\n\\t};\\n\\n\\twork1 = work2 = (struct cpu_stop_work){\\n\\t\\t.fn = multi_cpu_stop,\\n\\t\\t.arg = &msdata,\\n\\t\\t.done = &done,\\n\\t\\t.caller = _RET_IP_,\\n\\t};\\n\\n\\tcpu_stop_init_done(&done, 2);\\n\\tset_state(&msdata, MULTI_STOP_PREPARE);\\n\\n\\tif (cpu1 > cpu2)\\n\\t\\tswap(cpu1, cpu2);\\n\\tif (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))\\n\\t\\treturn -ENOENT;\\n\\n\\twait_for_completion(&done.completion);\\n\\treturn done.ret;\\n}\\n\\n/**\\n * stop_one_cpu_nowait - stop a cpu but don\\'t wait for completion\\n * @cpu: cpu to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n * @work_buf: pointer to cpu_stop_work structure\\n *\\n * Similar to stop_one_cpu() but doesn\\'t wait for completion.  The\\n * caller is responsible for ensuring @work_buf is currently unused\\n * and will remain untouched until stopper starts executing @fn.\\n *\\n * CONTEXT:\\n * Don\\'t care.\\n *\\n * RETURNS:\\n * true if cpu_stop_work was queued successfully and @fn will be called,\\n * false otherwise.\\n */\\nbool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,\\n\\t\\t\\tstruct cpu_stop_work *work_buf)\\n{\\n\\t*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, };\\n\\treturn cpu_stop_queue_work(cpu, work_buf);\\n}\\n\\nstatic bool queue_stop_cpus_work(const struct cpumask *cpumask,\\n\\t\\t\\t\\t cpu_stop_fn_t fn, void *arg,\\n\\t\\t\\t\\t struct cpu_stop_done *done)\\n{\\n\\tstruct cpu_stop_work *work;\\n\\tunsigned int cpu;\\n\\tbool queued = false;\\n\\n\\t/*\\n\\t * Disable preemption while queueing to avoid getting\\n\\t * preempted by a stopper which might wait for other stoppers\\n\\t * to enter @fn which can lead to deadlock.\\n\\t */\\n\\tpreempt_disable();\\n\\tstop_cpus_in_progress = true;\\n\\tbarrier();\\n\\tfor_each_cpu(cpu, cpumask) {\\n\\t\\twork = &per_cpu(cpu_stopper.stop_work, cpu);\\n\\t\\twork->fn = fn;\\n\\t\\twork->arg = arg;\\n\\t\\twork->done = done;\\n\\t\\twork->caller = _RET_IP_;\\n\\t\\tif (cpu_stop_queue_work(cpu, work))\\n\\t\\t\\tqueued = true;\\n\\t}\\n\\tbarrier();\\n\\tstop_cpus_in_progress = false;\\n\\tpreempt_enable();\\n\\n\\treturn queued;\\n}\\n\\nstatic int __stop_cpus(const struct cpumask *cpumask,\\n\\t\\t       cpu_stop_fn_t fn, void *arg)\\n{\\n\\tstruct cpu_stop_done done;\\n\\n\\tcpu_stop_init_done(&done, cpumask_weight(cpumask));\\n\\tif (!queue_stop_cpus_work(cpumask, fn, arg, &done))\\n\\t\\treturn -ENOENT;\\n\\twait_for_completion(&done.completion);\\n\\treturn done.ret;\\n}\\n\\n/**\\n * stop_cpus - stop multiple cpus\\n * @cpumask: cpus to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n *\\n * Execute @fn(@arg) on online cpus in @cpumask.  On each target cpu,\\n * @fn is run in a process context with the highest priority\\n * preempting any task on the cpu and monopolizing it.  This function\\n * returns after all executions are complete.\\n *\\n * This function doesn\\'t guarantee the cpus in @cpumask stay online\\n * till @fn completes.  If some cpus go down in the middle, execution\\n * on the cpu may happen partially or fully on different cpus.  @fn\\n * should either be ready for that or the caller should ensure that\\n * the cpus stay online until this function completes.\\n *\\n * All stop_cpus() calls are serialized making it safe for @fn to wait\\n * for all cpus to start executing it.\\n *\\n * CONTEXT:\\n * Might sleep.\\n *\\n * RETURNS:\\n * -ENOENT if @fn(@arg) was not executed at all because all cpus in\\n * @cpumask were offline; otherwise, 0 if all executions of @fn\\n * returned 0, any non zero return value if any returned non zero.\\n */\\nstatic int stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\\n{\\n\\tint ret;\\n\\n\\t/* static works are used, process one request at a time */\\n\\tmutex_lock(&stop_cpus_mutex);\\n\\tret = __stop_cpus(cpumask, fn, arg);\\n\\tmutex_unlock(&stop_cpus_mutex);\\n\\treturn ret;\\n}\\n\\nstatic int cpu_stop_should_run(unsigned int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\tunsigned long flags;\\n\\tint run;\\n\\n\\traw_spin_lock_irqsave(&stopper->lock, flags);\\n\\trun = !list_empty(&stopper->works);\\n\\traw_spin_unlock_irqrestore(&stopper->lock, flags);\\n\\treturn run;\\n}\\n\\nstatic void cpu_stopper_thread(unsigned int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\tstruct cpu_stop_work *work;\\n\\nrepeat:\\n\\twork = NULL;\\n\\traw_spin_lock_irq(&stopper->lock);\\n\\tif (!list_empty(&stopper->works)) {\\n\\t\\twork = list_first_entry(&stopper->works,\\n\\t\\t\\t\\t\\tstruct cpu_stop_work, list);\\n\\t\\tlist_del_init(&work->list);\\n\\t}\\n\\traw_spin_unlock_irq(&stopper->lock);\\n\\n\\tif (work) {\\n\\t\\tcpu_stop_fn_t fn = work->fn;\\n\\t\\tvoid *arg = work->arg;\\n\\t\\tstruct cpu_stop_done *done = work->done;\\n\\t\\tint ret;\\n\\n\\t\\t/* cpu stop callbacks must not sleep, make in_atomic() == T */\\n\\t\\tstopper->caller = work->caller;\\n\\t\\tstopper->fn = fn;\\n\\t\\tpreempt_count_inc();\\n\\t\\tret = fn(arg);\\n\\t\\tif (done) {\\n\\t\\t\\tif (ret)\\n\\t\\t\\t\\tdone->ret = ret;\\n\\t\\t\\tcpu_stop_signal_done(done);\\n\\t\\t}\\n\\t\\tpreempt_count_dec();\\n\\t\\tstopper->fn = NULL;\\n\\t\\tstopper->caller = 0;\\n\\t\\tWARN_ONCE(preempt_count(),\\n\\t\\t\\t  \"cpu_stop: %ps(%p) leaked preempt count\\\\n\", fn, arg);\\n\\t\\tgoto repeat;\\n\\t}\\n}\\n\\nvoid stop_machine_park(int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\t/*\\n\\t * Lockless. cpu_stopper_thread() will take stopper->lock and flush\\n\\t * the pending works before it parks, until then it is fine to queue\\n\\t * the new works.\\n\\t */\\n\\tstopper->enabled = false;\\n\\tkthread_park(stopper->thread);\\n}\\n\\nstatic void cpu_stop_create(unsigned int cpu)\\n{\\n\\tsched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));\\n}\\n\\nstatic void cpu_stop_park(unsigned int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\n\\tWARN_ON(!list_empty(&stopper->works));\\n}\\n\\nvoid stop_machine_unpark(int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\n\\tstopper->enabled = true;\\n\\tkthread_unpark(stopper->thread);\\n}\\n\\nstatic struct smp_hotplug_thread cpu_stop_threads = {\\n\\t.store\\t\\t\\t= &cpu_stopper.thread,\\n\\t.thread_should_run\\t= cpu_stop_should_run,\\n\\t.thread_fn\\t\\t= cpu_stopper_thread,\\n\\t.thread_comm\\t\\t= \"migration/%u\",\\n\\t.create\\t\\t\\t= cpu_stop_create,\\n\\t.park\\t\\t\\t= cpu_stop_park,\\n\\t.selfparking\\t\\t= true,\\n};\\n\\nstatic int __init cpu_stop_init(void)\\n{\\n\\tunsigned int cpu;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\n\\t\\traw_spin_lock_init(&stopper->lock);\\n\\t\\tINIT_LIST_HEAD(&stopper->works);\\n\\t}\\n\\n\\tBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));\\n\\tstop_machine_unpark(raw_smp_processor_id());\\n\\tstop_machine_initialized = true;\\n\\treturn 0;\\n}\\nearly_initcall(cpu_stop_init);\\n\\nint stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,\\n\\t\\t\\t    const struct cpumask *cpus)\\n{\\n\\tstruct multi_stop_data msdata = {\\n\\t\\t.fn = fn,\\n\\t\\t.data = data,\\n\\t\\t.num_threads = num_online_cpus(),\\n\\t\\t.active_cpus = cpus,\\n\\t};\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tif (!stop_machine_initialized) {\\n\\t\\t/*\\n\\t\\t * Handle the case where stop_machine() is called\\n\\t\\t * early in boot before stop_machine() has been\\n\\t\\t * initialized.\\n\\t\\t */\\n\\t\\tunsigned long flags;\\n\\t\\tint ret;\\n\\n\\t\\tWARN_ON_ONCE(msdata.num_threads != 1);\\n\\n\\t\\tlocal_irq_save(flags);\\n\\t\\thard_irq_disable();\\n\\t\\tret = (*fn)(data);\\n\\t\\tlocal_irq_restore(flags);\\n\\n\\t\\treturn ret;\\n\\t}\\n\\n\\t/* Set the initial state and stop all online cpus. */\\n\\tset_state(&msdata, MULTI_STOP_PREPARE);\\n\\treturn stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);\\n}\\n\\nint stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)\\n{\\n\\tint ret;\\n\\n\\t/* No CPUs can come up or down during this. */\\n\\tcpus_read_lock();\\n\\tret = stop_machine_cpuslocked(fn, data, cpus);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(stop_machine);\\n\\n#ifdef CONFIG_SCHED_SMT\\nint stop_core_cpuslocked(unsigned int cpu, cpu_stop_fn_t fn, void *data)\\n{\\n\\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);\\n\\n\\tstruct multi_stop_data msdata = {\\n\\t\\t.fn = fn,\\n\\t\\t.data = data,\\n\\t\\t.num_threads = cpumask_weight(smt_mask),\\n\\t\\t.active_cpus = smt_mask,\\n\\t};\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\t/* Set the initial state and stop all online cpus. */\\n\\tset_state(&msdata, MULTI_STOP_PREPARE);\\n\\treturn stop_cpus(smt_mask, multi_cpu_stop, &msdata);\\n}\\nEXPORT_SYMBOL_GPL(stop_core_cpuslocked);\\n#endif\\n\\n/**\\n * stop_machine_from_inactive_cpu - stop_machine() from inactive CPU\\n * @fn: the function to run\\n * @data: the data ptr for the @fn()\\n * @cpus: the cpus to run the @fn() on (NULL = any online cpu)\\n *\\n * This is identical to stop_machine() but can be called from a CPU which\\n * is not active.  The local CPU is in the process of hotplug (so no other\\n * CPU hotplug can start) and not marked active and doesn\\'t have enough\\n * context to sleep.\\n *\\n * This function provides stop_machine() functionality for such state by\\n * using busy-wait for synchronization and executing @fn directly for local\\n * CPU.\\n *\\n * CONTEXT:\\n * Local CPU is inactive.  Temporarily stops all active CPUs.\\n *\\n * RETURNS:\\n * 0 if all executions of @fn returned 0, any non zero return value if any\\n * returned non zero.\\n */\\nint stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,\\n\\t\\t\\t\\t  const struct cpumask *cpus)\\n{\\n\\tstruct multi_stop_data msdata = { .fn = fn, .data = data,\\n\\t\\t\\t\\t\\t    .active_cpus = cpus };\\n\\tstruct cpu_stop_done done;\\n\\tint ret;\\n\\n\\t/* Local CPU must be inactive and CPU hotplug in progress. */\\n\\tBUG_ON(cpu_active(raw_smp_processor_id()));\\n\\tmsdata.num_threads = num_active_cpus() + 1;\\t/* +1 for local */\\n\\n\\t/* No proper task established and can\\'t sleep - busy wait for lock. */\\n\\twhile (!mutex_trylock(&stop_cpus_mutex))\\n\\t\\tcpu_relax();\\n\\n\\t/* Schedule work on other CPUs and execute directly for local CPU */\\n\\tset_state(&msdata, MULTI_STOP_PREPARE);\\n\\tcpu_stop_init_done(&done, num_active_cpus());\\n\\tqueue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,\\n\\t\\t\\t     &done);\\n\\tret = multi_cpu_stop(&msdata);\\n\\n\\t/* Busy wait for completion. */\\n\\twhile (!completion_done(&done.completion))\\n\\t\\tcpu_relax();\\n\\n\\tmutex_unlock(&stop_cpus_mutex);\\n\\treturn ret ?: done.ret;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * sysctl.c: General linux system control interface\\n *\\n * Begun 24 March 1995, Stephen Tweedie\\n * Added /proc support, Dec 1995\\n * Added bdflush entry and intvec min/max checking, 2/23/96, Tom Dyas.\\n * Added hooks for /proc/sys/net (minor, minor patch), 96/4/1, Mike Shaver.\\n * Added kernel/java-{interpreter,appletviewer}, 96/5/10, Mike Shaver.\\n * Dynamic registration fixes, Stephen Tweedie.\\n * Added kswapd-interval, ctrl-alt-del, printk stuff, 1/8/97, Chris Horn.\\n * Made sysctl support optional via CONFIG_SYSCTL, 1/10/97, Chris\\n *  Horn.\\n * Added proc_doulongvec_ms_jiffies_minmax, 09/08/99, Carlos H. Bauer.\\n * Added proc_doulongvec_minmax, 09/08/99, Carlos H. Bauer.\\n * Changed linked lists to use list.h instead of lists.h, 02/24/00, Bill\\n *  Wendling.\\n * The list_for_each() macro wasn\\'t appropriate for the sysctl loop.\\n *  Removed it and replaced it with older style, 03/23/00, Bill Wendling\\n */\\n\\n#include <linux/module.h>\\n#include <linux/mm.h>\\n#include <linux/swap.h>\\n#include <linux/slab.h>\\n#include <linux/sysctl.h>\\n#include <linux/bitmap.h>\\n#include <linux/signal.h>\\n#include <linux/panic.h>\\n#include <linux/printk.h>\\n#include <linux/proc_fs.h>\\n#include <linux/security.h>\\n#include <linux/ctype.h>\\n#include <linux/kmemleak.h>\\n#include <linux/filter.h>\\n#include <linux/fs.h>\\n#include <linux/init.h>\\n#include <linux/kernel.h>\\n#include <linux/kobject.h>\\n#include <linux/net.h>\\n#include <linux/sysrq.h>\\n#include <linux/highuid.h>\\n#include <linux/writeback.h>\\n#include <linux/ratelimit.h>\\n#include <linux/hugetlb.h>\\n#include <linux/initrd.h>\\n#include <linux/key.h>\\n#include <linux/times.h>\\n#include <linux/limits.h>\\n#include <linux/dcache.h>\\n#include <linux/syscalls.h>\\n#include <linux/vmstat.h>\\n#include <linux/nfs_fs.h>\\n#include <linux/acpi.h>\\n#include <linux/reboot.h>\\n#include <linux/ftrace.h>\\n#include <linux/perf_event.h>\\n#include <linux/oom.h>\\n#include <linux/kmod.h>\\n#include <linux/capability.h>\\n#include <linux/binfmts.h>\\n#include <linux/sched/sysctl.h>\\n#include <linux/mount.h>\\n#include <linux/userfaultfd_k.h>\\n#include <linux/pid.h>\\n\\n#include \"../lib/kstrtox.h\"\\n\\n#include <linux/uaccess.h>\\n#include <asm/processor.h>\\n\\n#ifdef CONFIG_X86\\n#include <asm/nmi.h>\\n#include <asm/stacktrace.h>\\n#include <asm/io.h>\\n#endif\\n#ifdef CONFIG_SPARC\\n#include <asm/setup.h>\\n#endif\\n#ifdef CONFIG_RT_MUTEXES\\n#include <linux/rtmutex.h>\\n#endif\\n\\n/* shared constants to be used in various sysctls */\\nconst int sysctl_vals[] = { 0, 1, 2, 3, 4, 100, 200, 1000, 3000, INT_MAX, 65535, -1 };\\nEXPORT_SYMBOL(sysctl_vals);\\n\\nconst unsigned long sysctl_long_vals[] = { 0, 1, LONG_MAX };\\nEXPORT_SYMBOL_GPL(sysctl_long_vals);\\n\\n#if defined(CONFIG_SYSCTL)\\n\\n/* Constants used for minimum and maximum */\\n\\n#ifdef CONFIG_PERF_EVENTS\\nstatic const int six_hundred_forty_kb = 640 * 1024;\\n#endif\\n\\n\\nstatic const int ngroups_max = NGROUPS_MAX;\\nstatic const int cap_last_cap = CAP_LAST_CAP;\\n\\n#ifdef CONFIG_PROC_SYSCTL\\n\\n/**\\n * enum sysctl_writes_mode - supported sysctl write modes\\n *\\n * @SYSCTL_WRITES_LEGACY: each write syscall must fully contain the sysctl value\\n *\\tto be written, and multiple writes on the same sysctl file descriptor\\n *\\twill rewrite the sysctl value, regardless of file position. No warning\\n *\\tis issued when the initial position is not 0.\\n * @SYSCTL_WRITES_WARN: same as above but warn when the initial file position is\\n *\\tnot 0.\\n * @SYSCTL_WRITES_STRICT: writes to numeric sysctl entries must always be at\\n *\\tfile position 0 and the value must be fully contained in the buffer\\n *\\tsent to the write syscall. If dealing with strings respect the file\\n *\\tposition, but restrict this to the max length of the buffer, anything\\n *\\tpassed the max length will be ignored. Multiple writes will append\\n *\\tto the buffer.\\n *\\n * These write modes control how current file position affects the behavior of\\n * updating sysctl values through the proc interface on each write.\\n */\\nenum sysctl_writes_mode {\\n\\tSYSCTL_WRITES_LEGACY\\t\\t= -1,\\n\\tSYSCTL_WRITES_WARN\\t\\t= 0,\\n\\tSYSCTL_WRITES_STRICT\\t\\t= 1,\\n};\\n\\nstatic enum sysctl_writes_mode sysctl_writes_strict = SYSCTL_WRITES_STRICT;\\n#endif /* CONFIG_PROC_SYSCTL */\\n\\n#if defined(HAVE_ARCH_PICK_MMAP_LAYOUT) || \\\\\\n    defined(CONFIG_ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT)\\nint sysctl_legacy_va_layout;\\n#endif\\n\\n#endif /* CONFIG_SYSCTL */\\n\\n/*\\n * /proc/sys support\\n */\\n\\n#ifdef CONFIG_PROC_SYSCTL\\n\\nstatic int _proc_do_string(char *data, int maxlen, int write,\\n\\t\\tchar *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tsize_t len;\\n\\tchar c, *p;\\n\\n\\tif (!data || !maxlen || !*lenp) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (write) {\\n\\t\\tif (sysctl_writes_strict == SYSCTL_WRITES_STRICT) {\\n\\t\\t\\t/* Only continue writes not past the end of buffer. */\\n\\t\\t\\tlen = strlen(data);\\n\\t\\t\\tif (len > maxlen - 1)\\n\\t\\t\\t\\tlen = maxlen - 1;\\n\\n\\t\\t\\tif (*ppos > len)\\n\\t\\t\\t\\treturn 0;\\n\\t\\t\\tlen = *ppos;\\n\\t\\t} else {\\n\\t\\t\\t/* Start writing from beginning of buffer. */\\n\\t\\t\\tlen = 0;\\n\\t\\t}\\n\\n\\t\\t*ppos += *lenp;\\n\\t\\tp = buffer;\\n\\t\\twhile ((p - buffer) < *lenp && len < maxlen - 1) {\\n\\t\\t\\tc = *(p++);\\n\\t\\t\\tif (c == 0 || c == \\'\\\\n\\')\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tdata[len++] = c;\\n\\t\\t}\\n\\t\\tdata[len] = 0;\\n\\t} else {\\n\\t\\tlen = strlen(data);\\n\\t\\tif (len > maxlen)\\n\\t\\t\\tlen = maxlen;\\n\\n\\t\\tif (*ppos > len) {\\n\\t\\t\\t*lenp = 0;\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\n\\t\\tdata += *ppos;\\n\\t\\tlen  -= *ppos;\\n\\n\\t\\tif (len > *lenp)\\n\\t\\t\\tlen = *lenp;\\n\\t\\tif (len)\\n\\t\\t\\tmemcpy(buffer, data, len);\\n\\t\\tif (len < *lenp) {\\n\\t\\t\\tbuffer[len] = \\'\\\\n\\';\\n\\t\\t\\tlen++;\\n\\t\\t}\\n\\t\\t*lenp = len;\\n\\t\\t*ppos += len;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic void warn_sysctl_write(const struct ctl_table *table)\\n{\\n\\tpr_warn_once(\"%s wrote to %s when file position was not 0!\\\\n\"\\n\\t\\t\"This will not be supported in the future. To silence this\\\\n\"\\n\\t\\t\"warning, set kernel.sysctl_writes_strict = -1\\\\n\",\\n\\t\\tcurrent->comm, table->procname);\\n}\\n\\n/**\\n * proc_first_pos_non_zero_ignore - check if first position is allowed\\n * @ppos: file position\\n * @table: the sysctl table\\n *\\n * Returns true if the first position is non-zero and the sysctl_writes_strict\\n * mode indicates this is not allowed for numeric input types. String proc\\n * handlers can ignore the return value.\\n */\\nstatic bool proc_first_pos_non_zero_ignore(loff_t *ppos,\\n\\t\\t\\t\\t\\t   const struct ctl_table *table)\\n{\\n\\tif (!*ppos)\\n\\t\\treturn false;\\n\\n\\tswitch (sysctl_writes_strict) {\\n\\tcase SYSCTL_WRITES_STRICT:\\n\\t\\treturn true;\\n\\tcase SYSCTL_WRITES_WARN:\\n\\t\\twarn_sysctl_write(table);\\n\\t\\treturn false;\\n\\tdefault:\\n\\t\\treturn false;\\n\\t}\\n}\\n\\n/**\\n * proc_dostring - read a string sysctl\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes a string from/to the user buffer. If the kernel\\n * buffer provided is not large enough to hold the string, the\\n * string is truncated. The copied string is %NULL-terminated.\\n * If the string is being read by the user process, it is copied\\n * and a newline \\'\\\\n\\' is added. It is truncated if the buffer is\\n * not large enough.\\n *\\n * Returns 0 on success.\\n */\\nint proc_dostring(const struct ctl_table *table, int write,\\n\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tif (write)\\n\\t\\tproc_first_pos_non_zero_ignore(ppos, table);\\n\\n\\treturn _proc_do_string(table->data, table->maxlen, write, buffer, lenp,\\n\\t\\t\\tppos);\\n}\\n\\nstatic void proc_skip_spaces(char **buf, size_t *size)\\n{\\n\\twhile (*size) {\\n\\t\\tif (!isspace(**buf))\\n\\t\\t\\tbreak;\\n\\t\\t(*size)--;\\n\\t\\t(*buf)++;\\n\\t}\\n}\\n\\nstatic void proc_skip_char(char **buf, size_t *size, const char v)\\n{\\n\\twhile (*size) {\\n\\t\\tif (**buf != v)\\n\\t\\t\\tbreak;\\n\\t\\t(*size)--;\\n\\t\\t(*buf)++;\\n\\t}\\n}\\n\\n/**\\n * strtoul_lenient - parse an ASCII formatted integer from a buffer and only\\n *                   fail on overflow\\n *\\n * @cp: kernel buffer containing the string to parse\\n * @endp: pointer to store the trailing characters\\n * @base: the base to use\\n * @res: where the parsed integer will be stored\\n *\\n * In case of success 0 is returned and @res will contain the parsed integer,\\n * @endp will hold any trailing characters.\\n * This function will fail the parse on overflow. If there wasn\\'t an overflow\\n * the function will defer the decision what characters count as invalid to the\\n * caller.\\n */\\nstatic int strtoul_lenient(const char *cp, char **endp, unsigned int base,\\n\\t\\t\\t   unsigned long *res)\\n{\\n\\tunsigned long long result;\\n\\tunsigned int rv;\\n\\n\\tcp = _parse_integer_fixup_radix(cp, &base);\\n\\trv = _parse_integer(cp, base, &result);\\n\\tif ((rv & KSTRTOX_OVERFLOW) || (result != (unsigned long)result))\\n\\t\\treturn -ERANGE;\\n\\n\\tcp += rv;\\n\\n\\tif (endp)\\n\\t\\t*endp = (char *)cp;\\n\\n\\t*res = (unsigned long)result;\\n\\treturn 0;\\n}\\n\\n#define TMPBUFLEN 22\\n/**\\n * proc_get_long - reads an ASCII formatted integer from a user buffer\\n *\\n * @buf: a kernel buffer\\n * @size: size of the kernel buffer\\n * @val: this is where the number will be stored\\n * @neg: set to %TRUE if number is negative\\n * @perm_tr: a vector which contains the allowed trailers\\n * @perm_tr_len: size of the perm_tr vector\\n * @tr: pointer to store the trailer character\\n *\\n * In case of success %0 is returned and @buf and @size are updated with\\n * the amount of bytes read. If @tr is non-NULL and a trailing\\n * character exists (size is non-zero after returning from this\\n * function), @tr is updated with the trailing character.\\n */\\nstatic int proc_get_long(char **buf, size_t *size,\\n\\t\\t\\t  unsigned long *val, bool *neg,\\n\\t\\t\\t  const char *perm_tr, unsigned perm_tr_len, char *tr)\\n{\\n\\tchar *p, tmp[TMPBUFLEN];\\n\\tssize_t len = *size;\\n\\n\\tif (len <= 0)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (len > TMPBUFLEN - 1)\\n\\t\\tlen = TMPBUFLEN - 1;\\n\\n\\tmemcpy(tmp, *buf, len);\\n\\n\\ttmp[len] = 0;\\n\\tp = tmp;\\n\\tif (*p == \\'-\\' && *size > 1) {\\n\\t\\t*neg = true;\\n\\t\\tp++;\\n\\t} else\\n\\t\\t*neg = false;\\n\\tif (!isdigit(*p))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (strtoul_lenient(p, &p, 0, val))\\n\\t\\treturn -EINVAL;\\n\\n\\tlen = p - tmp;\\n\\n\\t/* We don\\'t know if the next char is whitespace thus we may accept\\n\\t * invalid integers (e.g. 1234...a) or two integers instead of one\\n\\t * (e.g. 123...1). So lets not allow such large numbers. */\\n\\tif (len == TMPBUFLEN - 1)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (len < *size && perm_tr_len && !memchr(perm_tr, *p, perm_tr_len))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (tr && (len < *size))\\n\\t\\t*tr = *p;\\n\\n\\t*buf += len;\\n\\t*size -= len;\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * proc_put_long - converts an integer to a decimal ASCII formatted string\\n *\\n * @buf: the user buffer\\n * @size: the size of the user buffer\\n * @val: the integer to be converted\\n * @neg: sign of the number, %TRUE for negative\\n *\\n * In case of success @buf and @size are updated with the amount of bytes\\n * written.\\n */\\nstatic void proc_put_long(void **buf, size_t *size, unsigned long val, bool neg)\\n{\\n\\tint len;\\n\\tchar tmp[TMPBUFLEN], *p = tmp;\\n\\n\\tsprintf(p, \"%s%lu\", neg ? \"-\" : \"\", val);\\n\\tlen = strlen(tmp);\\n\\tif (len > *size)\\n\\t\\tlen = *size;\\n\\tmemcpy(*buf, tmp, len);\\n\\t*size -= len;\\n\\t*buf += len;\\n}\\n#undef TMPBUFLEN\\n\\nstatic void proc_put_char(void **buf, size_t *size, char c)\\n{\\n\\tif (*size) {\\n\\t\\tchar **buffer = (char **)buf;\\n\\t\\t**buffer = c;\\n\\n\\t\\t(*size)--;\\n\\t\\t(*buffer)++;\\n\\t\\t*buf = *buffer;\\n\\t}\\n}\\n\\nstatic int do_proc_dointvec_conv(bool *negp, unsigned long *lvalp,\\n\\t\\t\\t\\t int *valp,\\n\\t\\t\\t\\t int write, void *data)\\n{\\n\\tif (write) {\\n\\t\\tif (*negp) {\\n\\t\\t\\tif (*lvalp > (unsigned long) INT_MAX + 1)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tWRITE_ONCE(*valp, -*lvalp);\\n\\t\\t} else {\\n\\t\\t\\tif (*lvalp > (unsigned long) INT_MAX)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tWRITE_ONCE(*valp, *lvalp);\\n\\t\\t}\\n\\t} else {\\n\\t\\tint val = READ_ONCE(*valp);\\n\\t\\tif (val < 0) {\\n\\t\\t\\t*negp = true;\\n\\t\\t\\t*lvalp = -(unsigned long)val;\\n\\t\\t} else {\\n\\t\\t\\t*negp = false;\\n\\t\\t\\t*lvalp = (unsigned long)val;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int do_proc_douintvec_conv(unsigned long *lvalp,\\n\\t\\t\\t\\t  unsigned int *valp,\\n\\t\\t\\t\\t  int write, void *data)\\n{\\n\\tif (write) {\\n\\t\\tif (*lvalp > UINT_MAX)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tWRITE_ONCE(*valp, *lvalp);\\n\\t} else {\\n\\t\\tunsigned int val = READ_ONCE(*valp);\\n\\t\\t*lvalp = (unsigned long)val;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic const char proc_wspace_sep[] = { \\' \\', \\'\\\\t\\', \\'\\\\n\\' };\\n\\nstatic int __do_proc_dointvec(void *tbl_data, const struct ctl_table *table,\\n\\t\\t  int write, void *buffer,\\n\\t\\t  size_t *lenp, loff_t *ppos,\\n\\t\\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\\n\\t\\t\\t      int write, void *data),\\n\\t\\t  void *data)\\n{\\n\\tint *i, vleft, first = 1, err = 0;\\n\\tsize_t left;\\n\\tchar *p;\\n\\n\\tif (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\ti = (int *) tbl_data;\\n\\tvleft = table->maxlen / sizeof(*i);\\n\\tleft = *lenp;\\n\\n\\tif (!conv)\\n\\t\\tconv = do_proc_dointvec_conv;\\n\\n\\tif (write) {\\n\\t\\tif (proc_first_pos_non_zero_ignore(ppos, table))\\n\\t\\t\\tgoto out;\\n\\n\\t\\tif (left > PAGE_SIZE - 1)\\n\\t\\t\\tleft = PAGE_SIZE - 1;\\n\\t\\tp = buffer;\\n\\t}\\n\\n\\tfor (; left && vleft--; i++, first=0) {\\n\\t\\tunsigned long lval;\\n\\t\\tbool neg;\\n\\n\\t\\tif (write) {\\n\\t\\t\\tproc_skip_spaces(&p, &left);\\n\\n\\t\\t\\tif (!left)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\terr = proc_get_long(&p, &left, &lval, &neg,\\n\\t\\t\\t\\t\\t     proc_wspace_sep,\\n\\t\\t\\t\\t\\t     sizeof(proc_wspace_sep), NULL);\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tif (conv(&neg, &lval, i, 1, data)) {\\n\\t\\t\\t\\terr = -EINVAL;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\tif (conv(&neg, &lval, i, 0, data)) {\\n\\t\\t\\t\\terr = -EINVAL;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tif (!first)\\n\\t\\t\\t\\tproc_put_char(&buffer, &left, \\'\\\\t\\');\\n\\t\\t\\tproc_put_long(&buffer, &left, lval, neg);\\n\\t\\t}\\n\\t}\\n\\n\\tif (!write && !first && left && !err)\\n\\t\\tproc_put_char(&buffer, &left, \\'\\\\n\\');\\n\\tif (write && !err && left)\\n\\t\\tproc_skip_spaces(&p, &left);\\n\\tif (write && first)\\n\\t\\treturn err ? : -EINVAL;\\n\\t*lenp -= left;\\nout:\\n\\t*ppos += *lenp;\\n\\treturn err;\\n}\\n\\nstatic int do_proc_dointvec(const struct ctl_table *table, int write,\\n\\t\\t  void *buffer, size_t *lenp, loff_t *ppos,\\n\\t\\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\\n\\t\\t\\t      int write, void *data),\\n\\t\\t  void *data)\\n{\\n\\treturn __do_proc_dointvec(table->data, table, write,\\n\\t\\t\\tbuffer, lenp, ppos, conv, data);\\n}\\n\\nstatic int do_proc_douintvec_w(unsigned int *tbl_data,\\n\\t\\t\\t       const struct ctl_table *table,\\n\\t\\t\\t       void *buffer,\\n\\t\\t\\t       size_t *lenp, loff_t *ppos,\\n\\t\\t\\t       int (*conv)(unsigned long *lvalp,\\n\\t\\t\\t\\t\\t   unsigned int *valp,\\n\\t\\t\\t\\t\\t   int write, void *data),\\n\\t\\t\\t       void *data)\\n{\\n\\tunsigned long lval;\\n\\tint err = 0;\\n\\tsize_t left;\\n\\tbool neg;\\n\\tchar *p = buffer;\\n\\n\\tleft = *lenp;\\n\\n\\tif (proc_first_pos_non_zero_ignore(ppos, table))\\n\\t\\tgoto bail_early;\\n\\n\\tif (left > PAGE_SIZE - 1)\\n\\t\\tleft = PAGE_SIZE - 1;\\n\\n\\tproc_skip_spaces(&p, &left);\\n\\tif (!left) {\\n\\t\\terr = -EINVAL;\\n\\t\\tgoto out_free;\\n\\t}\\n\\n\\terr = proc_get_long(&p, &left, &lval, &neg,\\n\\t\\t\\t     proc_wspace_sep,\\n\\t\\t\\t     sizeof(proc_wspace_sep), NULL);\\n\\tif (err || neg) {\\n\\t\\terr = -EINVAL;\\n\\t\\tgoto out_free;\\n\\t}\\n\\n\\tif (conv(&lval, tbl_data, 1, data)) {\\n\\t\\terr = -EINVAL;\\n\\t\\tgoto out_free;\\n\\t}\\n\\n\\tif (!err && left)\\n\\t\\tproc_skip_spaces(&p, &left);\\n\\nout_free:\\n\\tif (err)\\n\\t\\treturn -EINVAL;\\n\\n\\treturn 0;\\n\\n\\t/* This is in keeping with old __do_proc_dointvec() */\\nbail_early:\\n\\t*ppos += *lenp;\\n\\treturn err;\\n}\\n\\nstatic int do_proc_douintvec_r(unsigned int *tbl_data, void *buffer,\\n\\t\\t\\t       size_t *lenp, loff_t *ppos,\\n\\t\\t\\t       int (*conv)(unsigned long *lvalp,\\n\\t\\t\\t\\t\\t   unsigned int *valp,\\n\\t\\t\\t\\t\\t   int write, void *data),\\n\\t\\t\\t       void *data)\\n{\\n\\tunsigned long lval;\\n\\tint err = 0;\\n\\tsize_t left;\\n\\n\\tleft = *lenp;\\n\\n\\tif (conv(&lval, tbl_data, 0, data)) {\\n\\t\\terr = -EINVAL;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tproc_put_long(&buffer, &left, lval, false);\\n\\tif (!left)\\n\\t\\tgoto out;\\n\\n\\tproc_put_char(&buffer, &left, \\'\\\\n\\');\\n\\nout:\\n\\t*lenp -= left;\\n\\t*ppos += *lenp;\\n\\n\\treturn err;\\n}\\n\\nstatic int __do_proc_douintvec(void *tbl_data, const struct ctl_table *table,\\n\\t\\t\\t       int write, void *buffer,\\n\\t\\t\\t       size_t *lenp, loff_t *ppos,\\n\\t\\t\\t       int (*conv)(unsigned long *lvalp,\\n\\t\\t\\t\\t\\t   unsigned int *valp,\\n\\t\\t\\t\\t\\t   int write, void *data),\\n\\t\\t\\t       void *data)\\n{\\n\\tunsigned int *i, vleft;\\n\\n\\tif (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\ti = (unsigned int *) tbl_data;\\n\\tvleft = table->maxlen / sizeof(*i);\\n\\n\\t/*\\n\\t * Arrays are not supported, keep this simple. *Do not* add\\n\\t * support for them.\\n\\t */\\n\\tif (vleft != 1) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tif (!conv)\\n\\t\\tconv = do_proc_douintvec_conv;\\n\\n\\tif (write)\\n\\t\\treturn do_proc_douintvec_w(i, table, buffer, lenp, ppos,\\n\\t\\t\\t\\t\\t   conv, data);\\n\\treturn do_proc_douintvec_r(i, buffer, lenp, ppos, conv, data);\\n}\\n\\nint do_proc_douintvec(const struct ctl_table *table, int write,\\n\\t\\t      void *buffer, size_t *lenp, loff_t *ppos,\\n\\t\\t      int (*conv)(unsigned long *lvalp,\\n\\t\\t\\t\\t  unsigned int *valp,\\n\\t\\t\\t\\t  int write, void *data),\\n\\t\\t      void *data)\\n{\\n\\treturn __do_proc_douintvec(table->data, table, write,\\n\\t\\t\\t\\t   buffer, lenp, ppos, conv, data);\\n}\\n\\n/**\\n * proc_dobool - read/write a bool\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes one integer value from/to the user buffer,\\n * treated as an ASCII string.\\n *\\n * table->data must point to a bool variable and table->maxlen must\\n * be sizeof(bool).\\n *\\n * Returns 0 on success.\\n */\\nint proc_dobool(const struct ctl_table *table, int write, void *buffer,\\n\\t\\tsize_t *lenp, loff_t *ppos)\\n{\\n\\tstruct ctl_table tmp;\\n\\tbool *data = table->data;\\n\\tint res, val;\\n\\n\\t/* Do not support arrays yet. */\\n\\tif (table->maxlen != sizeof(bool))\\n\\t\\treturn -EINVAL;\\n\\n\\ttmp = *table;\\n\\ttmp.maxlen = sizeof(val);\\n\\ttmp.data = &val;\\n\\n\\tval = READ_ONCE(*data);\\n\\tres = proc_dointvec(&tmp, write, buffer, lenp, ppos);\\n\\tif (res)\\n\\t\\treturn res;\\n\\tif (write)\\n\\t\\tWRITE_ONCE(*data, val);\\n\\treturn 0;\\n}\\n\\n/**\\n * proc_dointvec - read a vector of integers\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\\n * values from/to the user buffer, treated as an ASCII string.\\n *\\n * Returns 0 on success.\\n */\\nint proc_dointvec(const struct ctl_table *table, int write, void *buffer,\\n\\t\\t  size_t *lenp, loff_t *ppos)\\n{\\n\\treturn do_proc_dointvec(table, write, buffer, lenp, ppos, NULL, NULL);\\n}\\n\\n/**\\n * proc_douintvec - read a vector of unsigned integers\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) unsigned integer\\n * values from/to the user buffer, treated as an ASCII string.\\n *\\n * Returns 0 on success.\\n */\\nint proc_douintvec(const struct ctl_table *table, int write, void *buffer,\\n\\t\\tsize_t *lenp, loff_t *ppos)\\n{\\n\\treturn do_proc_douintvec(table, write, buffer, lenp, ppos,\\n\\t\\t\\t\\t do_proc_douintvec_conv, NULL);\\n}\\n\\n/*\\n * Taint values can only be increased\\n * This means we can safely use a temporary.\\n */\\nstatic int proc_taint(const struct ctl_table *table, int write,\\n\\t\\t\\t       void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct ctl_table t;\\n\\tunsigned long tmptaint = get_taint();\\n\\tint err;\\n\\n\\tif (write && !capable(CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tt = *table;\\n\\tt.data = &tmptaint;\\n\\terr = proc_doulongvec_minmax(&t, write, buffer, lenp, ppos);\\n\\tif (err < 0)\\n\\t\\treturn err;\\n\\n\\tif (write) {\\n\\t\\tint i;\\n\\n\\t\\t/*\\n\\t\\t * If we are relying on panic_on_taint not producing\\n\\t\\t * false positives due to userspace input, bail out\\n\\t\\t * before setting the requested taint flags.\\n\\t\\t */\\n\\t\\tif (panic_on_taint_nousertaint && (tmptaint & panic_on_taint))\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\t/*\\n\\t\\t * Poor man\\'s atomic or. Not worth adding a primitive\\n\\t\\t * to everyone\\'s atomic.h for this\\n\\t\\t */\\n\\t\\tfor (i = 0; i < TAINT_FLAGS_COUNT; i++)\\n\\t\\t\\tif ((1UL << i) & tmptaint)\\n\\t\\t\\t\\tadd_taint(i, LOCKDEP_STILL_OK);\\n\\t}\\n\\n\\treturn err;\\n}\\n\\n/**\\n * struct do_proc_dointvec_minmax_conv_param - proc_dointvec_minmax() range checking structure\\n * @min: pointer to minimum allowable value\\n * @max: pointer to maximum allowable value\\n *\\n * The do_proc_dointvec_minmax_conv_param structure provides the\\n * minimum and maximum values for doing range checking for those sysctl\\n * parameters that use the proc_dointvec_minmax() handler.\\n */\\nstruct do_proc_dointvec_minmax_conv_param {\\n\\tint *min;\\n\\tint *max;\\n};\\n\\nstatic int do_proc_dointvec_minmax_conv(bool *negp, unsigned long *lvalp,\\n\\t\\t\\t\\t\\tint *valp,\\n\\t\\t\\t\\t\\tint write, void *data)\\n{\\n\\tint tmp, ret;\\n\\tstruct do_proc_dointvec_minmax_conv_param *param = data;\\n\\t/*\\n\\t * If writing, first do so via a temporary local int so we can\\n\\t * bounds-check it before touching *valp.\\n\\t */\\n\\tint *ip = write ? &tmp : valp;\\n\\n\\tret = do_proc_dointvec_conv(negp, lvalp, ip, write, data);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (write) {\\n\\t\\tif ((param->min && *param->min > tmp) ||\\n\\t\\t    (param->max && *param->max < tmp))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tWRITE_ONCE(*valp, tmp);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * proc_dointvec_minmax - read a vector of integers with min/max values\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\\n * values from/to the user buffer, treated as an ASCII string.\\n *\\n * This routine will ensure the values are within the range specified by\\n * table->extra1 (min) and table->extra2 (max).\\n *\\n * Returns 0 on success or -EINVAL on write when the range check fails.\\n */\\nint proc_dointvec_minmax(const struct ctl_table *table, int write,\\n\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct do_proc_dointvec_minmax_conv_param param = {\\n\\t\\t.min = (int *) table->extra1,\\n\\t\\t.max = (int *) table->extra2,\\n\\t};\\n\\treturn do_proc_dointvec(table, write, buffer, lenp, ppos,\\n\\t\\t\\t\\tdo_proc_dointvec_minmax_conv, &param);\\n}\\n\\n/**\\n * struct do_proc_douintvec_minmax_conv_param - proc_douintvec_minmax() range checking structure\\n * @min: pointer to minimum allowable value\\n * @max: pointer to maximum allowable value\\n *\\n * The do_proc_douintvec_minmax_conv_param structure provides the\\n * minimum and maximum values for doing range checking for those sysctl\\n * parameters that use the proc_douintvec_minmax() handler.\\n */\\nstruct do_proc_douintvec_minmax_conv_param {\\n\\tunsigned int *min;\\n\\tunsigned int *max;\\n};\\n\\nstatic int do_proc_douintvec_minmax_conv(unsigned long *lvalp,\\n\\t\\t\\t\\t\\t unsigned int *valp,\\n\\t\\t\\t\\t\\t int write, void *data)\\n{\\n\\tint ret;\\n\\tunsigned int tmp;\\n\\tstruct do_proc_douintvec_minmax_conv_param *param = data;\\n\\t/* write via temporary local uint for bounds-checking */\\n\\tunsigned int *up = write ? &tmp : valp;\\n\\n\\tret = do_proc_douintvec_conv(lvalp, up, write, data);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (write) {\\n\\t\\tif ((param->min && *param->min > tmp) ||\\n\\t\\t    (param->max && *param->max < tmp))\\n\\t\\t\\treturn -ERANGE;\\n\\n\\t\\tWRITE_ONCE(*valp, tmp);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * proc_douintvec_minmax - read a vector of unsigned ints with min/max values\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) unsigned integer\\n * values from/to the user buffer, treated as an ASCII string. Negative\\n * strings are not allowed.\\n *\\n * This routine will ensure the values are within the range specified by\\n * table->extra1 (min) and table->extra2 (max). There is a final sanity\\n * check for UINT_MAX to avoid having to support wrap around uses from\\n * userspace.\\n *\\n * Returns 0 on success or -ERANGE on write when the range check fails.\\n */\\nint proc_douintvec_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct do_proc_douintvec_minmax_conv_param param = {\\n\\t\\t.min = (unsigned int *) table->extra1,\\n\\t\\t.max = (unsigned int *) table->extra2,\\n\\t};\\n\\treturn do_proc_douintvec(table, write, buffer, lenp, ppos,\\n\\t\\t\\t\\t do_proc_douintvec_minmax_conv, &param);\\n}\\n\\n/**\\n * proc_dou8vec_minmax - read a vector of unsigned chars with min/max values\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(u8) unsigned chars\\n * values from/to the user buffer, treated as an ASCII string. Negative\\n * strings are not allowed.\\n *\\n * This routine will ensure the values are within the range specified by\\n * table->extra1 (min) and table->extra2 (max).\\n *\\n * Returns 0 on success or an error on write when the range check fails.\\n */\\nint proc_dou8vec_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct ctl_table tmp;\\n\\tunsigned int min = 0, max = 255U, val;\\n\\tu8 *data = table->data;\\n\\tstruct do_proc_douintvec_minmax_conv_param param = {\\n\\t\\t.min = &min,\\n\\t\\t.max = &max,\\n\\t};\\n\\tint res;\\n\\n\\t/* Do not support arrays yet. */\\n\\tif (table->maxlen != sizeof(u8))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (table->extra1)\\n\\t\\tmin = *(unsigned int *) table->extra1;\\n\\tif (table->extra2)\\n\\t\\tmax = *(unsigned int *) table->extra2;\\n\\n\\ttmp = *table;\\n\\n\\ttmp.maxlen = sizeof(val);\\n\\ttmp.data = &val;\\n\\tval = READ_ONCE(*data);\\n\\tres = do_proc_douintvec(&tmp, write, buffer, lenp, ppos,\\n\\t\\t\\t\\tdo_proc_douintvec_minmax_conv, &param);\\n\\tif (res)\\n\\t\\treturn res;\\n\\tif (write)\\n\\t\\tWRITE_ONCE(*data, val);\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(proc_dou8vec_minmax);\\n\\n#ifdef CONFIG_MAGIC_SYSRQ\\nstatic int sysrq_sysctl_handler(const struct ctl_table *table, int write,\\n\\t\\t\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tint tmp, ret;\\n\\n\\ttmp = sysrq_mask();\\n\\n\\tret = __do_proc_dointvec(&tmp, table, write, buffer,\\n\\t\\t\\t       lenp, ppos, NULL, NULL);\\n\\tif (ret || !write)\\n\\t\\treturn ret;\\n\\n\\tif (write)\\n\\t\\tsysrq_toggle_support(tmp);\\n\\n\\treturn 0;\\n}\\n#endif\\n\\nstatic int __do_proc_doulongvec_minmax(void *data,\\n\\t\\tconst struct ctl_table *table, int write,\\n\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos,\\n\\t\\tunsigned long convmul, unsigned long convdiv)\\n{\\n\\tunsigned long *i, *min, *max;\\n\\tint vleft, first = 1, err = 0;\\n\\tsize_t left;\\n\\tchar *p;\\n\\n\\tif (!data || !table->maxlen || !*lenp || (*ppos && !write)) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\ti = data;\\n\\tmin = table->extra1;\\n\\tmax = table->extra2;\\n\\tvleft = table->maxlen / sizeof(unsigned long);\\n\\tleft = *lenp;\\n\\n\\tif (write) {\\n\\t\\tif (proc_first_pos_non_zero_ignore(ppos, table))\\n\\t\\t\\tgoto out;\\n\\n\\t\\tif (left > PAGE_SIZE - 1)\\n\\t\\t\\tleft = PAGE_SIZE - 1;\\n\\t\\tp = buffer;\\n\\t}\\n\\n\\tfor (; left && vleft--; i++, first = 0) {\\n\\t\\tunsigned long val;\\n\\n\\t\\tif (write) {\\n\\t\\t\\tbool neg;\\n\\n\\t\\t\\tproc_skip_spaces(&p, &left);\\n\\t\\t\\tif (!left)\\n\\t\\t\\t\\tbreak;\\n\\n\\t\\t\\terr = proc_get_long(&p, &left, &val, &neg,\\n\\t\\t\\t\\t\\t     proc_wspace_sep,\\n\\t\\t\\t\\t\\t     sizeof(proc_wspace_sep), NULL);\\n\\t\\t\\tif (err || neg) {\\n\\t\\t\\t\\terr = -EINVAL;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\tval = convmul * val / convdiv;\\n\\t\\t\\tif ((min && val < *min) || (max && val > *max)) {\\n\\t\\t\\t\\terr = -EINVAL;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tWRITE_ONCE(*i, val);\\n\\t\\t} else {\\n\\t\\t\\tval = convdiv * READ_ONCE(*i) / convmul;\\n\\t\\t\\tif (!first)\\n\\t\\t\\t\\tproc_put_char(&buffer, &left, \\'\\\\t\\');\\n\\t\\t\\tproc_put_long(&buffer, &left, val, false);\\n\\t\\t}\\n\\t}\\n\\n\\tif (!write && !first && left && !err)\\n\\t\\tproc_put_char(&buffer, &left, \\'\\\\n\\');\\n\\tif (write && !err)\\n\\t\\tproc_skip_spaces(&p, &left);\\n\\tif (write && first)\\n\\t\\treturn err ? : -EINVAL;\\n\\t*lenp -= left;\\nout:\\n\\t*ppos += *lenp;\\n\\treturn err;\\n}\\n\\nstatic int do_proc_doulongvec_minmax(const struct ctl_table *table, int write,\\n\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos, unsigned long convmul,\\n\\t\\tunsigned long convdiv)\\n{\\n\\treturn __do_proc_doulongvec_minmax(table->data, table, write,\\n\\t\\t\\tbuffer, lenp, ppos, convmul, convdiv);\\n}\\n\\n/**\\n * proc_doulongvec_minmax - read a vector of long integers with min/max values\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long\\n * values from/to the user buffer, treated as an ASCII string.\\n *\\n * This routine will ensure the values are within the range specified by\\n * table->extra1 (min) and table->extra2 (max).\\n *\\n * Returns 0 on success.\\n */\\nint proc_doulongvec_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\t   void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n    return do_proc_doulongvec_minmax(table, write, buffer, lenp, ppos, 1l, 1l);\\n}\\n\\n/**\\n * proc_doulongvec_ms_jiffies_minmax - read a vector of millisecond values with min/max values\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long\\n * values from/to the user buffer, treated as an ASCII string. The values\\n * are treated as milliseconds, and converted to jiffies when they are stored.\\n *\\n * This routine will ensure the values are within the range specified by\\n * table->extra1 (min) and table->extra2 (max).\\n *\\n * Returns 0 on success.\\n */\\nint proc_doulongvec_ms_jiffies_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\t\\t      void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n    return do_proc_doulongvec_minmax(table, write, buffer,\\n\\t\\t\\t\\t     lenp, ppos, HZ, 1000l);\\n}\\n\\n\\nstatic int do_proc_dointvec_jiffies_conv(bool *negp, unsigned long *lvalp,\\n\\t\\t\\t\\t\\t int *valp,\\n\\t\\t\\t\\t\\t int write, void *data)\\n{\\n\\tif (write) {\\n\\t\\tif (*lvalp > INT_MAX / HZ)\\n\\t\\t\\treturn 1;\\n\\t\\tif (*negp)\\n\\t\\t\\tWRITE_ONCE(*valp, -*lvalp * HZ);\\n\\t\\telse\\n\\t\\t\\tWRITE_ONCE(*valp, *lvalp * HZ);\\n\\t} else {\\n\\t\\tint val = READ_ONCE(*valp);\\n\\t\\tunsigned long lval;\\n\\t\\tif (val < 0) {\\n\\t\\t\\t*negp = true;\\n\\t\\t\\tlval = -(unsigned long)val;\\n\\t\\t} else {\\n\\t\\t\\t*negp = false;\\n\\t\\t\\tlval = (unsigned long)val;\\n\\t\\t}\\n\\t\\t*lvalp = lval / HZ;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int do_proc_dointvec_userhz_jiffies_conv(bool *negp, unsigned long *lvalp,\\n\\t\\t\\t\\t\\t\\tint *valp,\\n\\t\\t\\t\\t\\t\\tint write, void *data)\\n{\\n\\tif (write) {\\n\\t\\tif (USER_HZ < HZ && *lvalp > (LONG_MAX / HZ) * USER_HZ)\\n\\t\\t\\treturn 1;\\n\\t\\t*valp = clock_t_to_jiffies(*negp ? -*lvalp : *lvalp);\\n\\t} else {\\n\\t\\tint val = *valp;\\n\\t\\tunsigned long lval;\\n\\t\\tif (val < 0) {\\n\\t\\t\\t*negp = true;\\n\\t\\t\\tlval = -(unsigned long)val;\\n\\t\\t} else {\\n\\t\\t\\t*negp = false;\\n\\t\\t\\tlval = (unsigned long)val;\\n\\t\\t}\\n\\t\\t*lvalp = jiffies_to_clock_t(lval);\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int do_proc_dointvec_ms_jiffies_conv(bool *negp, unsigned long *lvalp,\\n\\t\\t\\t\\t\\t    int *valp,\\n\\t\\t\\t\\t\\t    int write, void *data)\\n{\\n\\tif (write) {\\n\\t\\tunsigned long jif = msecs_to_jiffies(*negp ? -*lvalp : *lvalp);\\n\\n\\t\\tif (jif > INT_MAX)\\n\\t\\t\\treturn 1;\\n\\t\\tWRITE_ONCE(*valp, (int)jif);\\n\\t} else {\\n\\t\\tint val = READ_ONCE(*valp);\\n\\t\\tunsigned long lval;\\n\\t\\tif (val < 0) {\\n\\t\\t\\t*negp = true;\\n\\t\\t\\tlval = -(unsigned long)val;\\n\\t\\t} else {\\n\\t\\t\\t*negp = false;\\n\\t\\t\\tlval = (unsigned long)val;\\n\\t\\t}\\n\\t\\t*lvalp = jiffies_to_msecs(lval);\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int do_proc_dointvec_ms_jiffies_minmax_conv(bool *negp, unsigned long *lvalp,\\n\\t\\t\\t\\t\\t\\tint *valp, int write, void *data)\\n{\\n\\tint tmp, ret;\\n\\tstruct do_proc_dointvec_minmax_conv_param *param = data;\\n\\t/*\\n\\t * If writing, first do so via a temporary local int so we can\\n\\t * bounds-check it before touching *valp.\\n\\t */\\n\\tint *ip = write ? &tmp : valp;\\n\\n\\tret = do_proc_dointvec_ms_jiffies_conv(negp, lvalp, ip, write, data);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (write) {\\n\\t\\tif ((param->min && *param->min > tmp) ||\\n\\t\\t\\t\\t(param->max && *param->max < tmp))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t*valp = tmp;\\n\\t}\\n\\treturn 0;\\n}\\n\\n/**\\n * proc_dointvec_jiffies - read a vector of integers as seconds\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\\n * values from/to the user buffer, treated as an ASCII string.\\n * The values read are assumed to be in seconds, and are converted into\\n * jiffies.\\n *\\n * Returns 0 on success.\\n */\\nint proc_dointvec_jiffies(const struct ctl_table *table, int write,\\n\\t\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n    return do_proc_dointvec(table,write,buffer,lenp,ppos,\\n\\t\\t    \\t    do_proc_dointvec_jiffies_conv,NULL);\\n}\\n\\nint proc_dointvec_ms_jiffies_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct do_proc_dointvec_minmax_conv_param param = {\\n\\t\\t.min = (int *) table->extra1,\\n\\t\\t.max = (int *) table->extra2,\\n\\t};\\n\\treturn do_proc_dointvec(table, write, buffer, lenp, ppos,\\n\\t\\t\\tdo_proc_dointvec_ms_jiffies_minmax_conv, &param);\\n}\\n\\n/**\\n * proc_dointvec_userhz_jiffies - read a vector of integers as 1/USER_HZ seconds\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: pointer to the file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\\n * values from/to the user buffer, treated as an ASCII string.\\n * The values read are assumed to be in 1/USER_HZ seconds, and\\n * are converted into jiffies.\\n *\\n * Returns 0 on success.\\n */\\nint proc_dointvec_userhz_jiffies(const struct ctl_table *table, int write,\\n\\t\\t\\t\\t void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn do_proc_dointvec(table, write, buffer, lenp, ppos,\\n\\t\\t\\t\\tdo_proc_dointvec_userhz_jiffies_conv, NULL);\\n}\\n\\n/**\\n * proc_dointvec_ms_jiffies - read a vector of integers as 1 milliseconds\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: the current position in the file\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\\n * values from/to the user buffer, treated as an ASCII string.\\n * The values read are assumed to be in 1/1000 seconds, and\\n * are converted into jiffies.\\n *\\n * Returns 0 on success.\\n */\\nint proc_dointvec_ms_jiffies(const struct ctl_table *table, int write, void *buffer,\\n\\t\\tsize_t *lenp, loff_t *ppos)\\n{\\n\\treturn do_proc_dointvec(table, write, buffer, lenp, ppos,\\n\\t\\t\\t\\tdo_proc_dointvec_ms_jiffies_conv, NULL);\\n}\\n\\nstatic int proc_do_cad_pid(const struct ctl_table *table, int write, void *buffer,\\n\\t\\tsize_t *lenp, loff_t *ppos)\\n{\\n\\tstruct pid *new_pid;\\n\\tpid_t tmp;\\n\\tint r;\\n\\n\\ttmp = pid_vnr(cad_pid);\\n\\n\\tr = __do_proc_dointvec(&tmp, table, write, buffer,\\n\\t\\t\\t       lenp, ppos, NULL, NULL);\\n\\tif (r || !write)\\n\\t\\treturn r;\\n\\n\\tnew_pid = find_get_pid(tmp);\\n\\tif (!new_pid)\\n\\t\\treturn -ESRCH;\\n\\n\\tput_pid(xchg(&cad_pid, new_pid));\\n\\treturn 0;\\n}\\n\\n/**\\n * proc_do_large_bitmap - read/write from/to a large bitmap\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * The bitmap is stored at table->data and the bitmap length (in bits)\\n * in table->maxlen.\\n *\\n * We use a range comma separated format (e.g. 1,3-4,10-10) so that\\n * large bitmaps may be represented in a compact manner. Writing into\\n * the file will clear the bitmap then update it with the given input.\\n *\\n * Returns 0 on success.\\n */\\nint proc_do_large_bitmap(const struct ctl_table *table, int write,\\n\\t\\t\\t void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tint err = 0;\\n\\tsize_t left = *lenp;\\n\\tunsigned long bitmap_len = table->maxlen;\\n\\tunsigned long *bitmap = *(unsigned long **) table->data;\\n\\tunsigned long *tmp_bitmap = NULL;\\n\\tchar tr_a[] = { \\'-\\', \\',\\', \\'\\\\n\\' }, tr_b[] = { \\',\\', \\'\\\\n\\', 0 }, c;\\n\\n\\tif (!bitmap || !bitmap_len || !left || (*ppos && !write)) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (write) {\\n\\t\\tchar *p = buffer;\\n\\t\\tsize_t skipped = 0;\\n\\n\\t\\tif (left > PAGE_SIZE - 1) {\\n\\t\\t\\tleft = PAGE_SIZE - 1;\\n\\t\\t\\t/* How much of the buffer we\\'ll skip this pass */\\n\\t\\t\\tskipped = *lenp - left;\\n\\t\\t}\\n\\n\\t\\ttmp_bitmap = bitmap_zalloc(bitmap_len, GFP_KERNEL);\\n\\t\\tif (!tmp_bitmap)\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\tproc_skip_char(&p, &left, \\'\\\\n\\');\\n\\t\\twhile (!err && left) {\\n\\t\\t\\tunsigned long val_a, val_b;\\n\\t\\t\\tbool neg;\\n\\t\\t\\tsize_t saved_left;\\n\\n\\t\\t\\t/* In case we stop parsing mid-number, we can reset */\\n\\t\\t\\tsaved_left = left;\\n\\t\\t\\terr = proc_get_long(&p, &left, &val_a, &neg, tr_a,\\n\\t\\t\\t\\t\\t     sizeof(tr_a), &c);\\n\\t\\t\\t/*\\n\\t\\t\\t * If we consumed the entirety of a truncated buffer or\\n\\t\\t\\t * only one char is left (may be a \"-\"), then stop here,\\n\\t\\t\\t * reset, & come back for more.\\n\\t\\t\\t */\\n\\t\\t\\tif ((left <= 1) && skipped) {\\n\\t\\t\\t\\tleft = saved_left;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tif (val_a >= bitmap_len || neg) {\\n\\t\\t\\t\\terr = -EINVAL;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\tval_b = val_a;\\n\\t\\t\\tif (left) {\\n\\t\\t\\t\\tp++;\\n\\t\\t\\t\\tleft--;\\n\\t\\t\\t}\\n\\n\\t\\t\\tif (c == \\'-\\') {\\n\\t\\t\\t\\terr = proc_get_long(&p, &left, &val_b,\\n\\t\\t\\t\\t\\t\\t     &neg, tr_b, sizeof(tr_b),\\n\\t\\t\\t\\t\\t\\t     &c);\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * If we consumed all of a truncated buffer or\\n\\t\\t\\t\\t * then stop here, reset, & come back for more.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\tif (!left && skipped) {\\n\\t\\t\\t\\t\\tleft = saved_left;\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\n\\t\\t\\t\\tif (err)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tif (val_b >= bitmap_len || neg ||\\n\\t\\t\\t\\t    val_a > val_b) {\\n\\t\\t\\t\\t\\terr = -EINVAL;\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tif (left) {\\n\\t\\t\\t\\t\\tp++;\\n\\t\\t\\t\\t\\tleft--;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\n\\t\\t\\tbitmap_set(tmp_bitmap, val_a, val_b - val_a + 1);\\n\\t\\t\\tproc_skip_char(&p, &left, \\'\\\\n\\');\\n\\t\\t}\\n\\t\\tleft += skipped;\\n\\t} else {\\n\\t\\tunsigned long bit_a, bit_b = 0;\\n\\t\\tbool first = 1;\\n\\n\\t\\twhile (left) {\\n\\t\\t\\tbit_a = find_next_bit(bitmap, bitmap_len, bit_b);\\n\\t\\t\\tif (bit_a >= bitmap_len)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tbit_b = find_next_zero_bit(bitmap, bitmap_len,\\n\\t\\t\\t\\t\\t\\t   bit_a + 1) - 1;\\n\\n\\t\\t\\tif (!first)\\n\\t\\t\\t\\tproc_put_char(&buffer, &left, \\',\\');\\n\\t\\t\\tproc_put_long(&buffer, &left, bit_a, false);\\n\\t\\t\\tif (bit_a != bit_b) {\\n\\t\\t\\t\\tproc_put_char(&buffer, &left, \\'-\\');\\n\\t\\t\\t\\tproc_put_long(&buffer, &left, bit_b, false);\\n\\t\\t\\t}\\n\\n\\t\\t\\tfirst = 0; bit_b++;\\n\\t\\t}\\n\\t\\tproc_put_char(&buffer, &left, \\'\\\\n\\');\\n\\t}\\n\\n\\tif (!err) {\\n\\t\\tif (write) {\\n\\t\\t\\tif (*ppos)\\n\\t\\t\\t\\tbitmap_or(bitmap, bitmap, tmp_bitmap, bitmap_len);\\n\\t\\t\\telse\\n\\t\\t\\t\\tbitmap_copy(bitmap, tmp_bitmap, bitmap_len);\\n\\t\\t}\\n\\t\\t*lenp -= left;\\n\\t\\t*ppos += *lenp;\\n\\t}\\n\\n\\tbitmap_free(tmp_bitmap);\\n\\treturn err;\\n}\\n\\n#else /* CONFIG_PROC_SYSCTL */\\n\\nint proc_dostring(const struct ctl_table *table, int write,\\n\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_dobool(const struct ctl_table *table, int write,\\n\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_dointvec(const struct ctl_table *table, int write,\\n\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_douintvec(const struct ctl_table *table, int write,\\n\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_dointvec_minmax(const struct ctl_table *table, int write,\\n\\t\\t    void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_douintvec_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_dou8vec_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_dointvec_jiffies(const struct ctl_table *table, int write,\\n\\t\\t    void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_dointvec_ms_jiffies_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\t\\t    void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_dointvec_userhz_jiffies(const struct ctl_table *table, int write,\\n\\t\\t    void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_dointvec_ms_jiffies(const struct ctl_table *table, int write,\\n\\t\\t\\t     void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_doulongvec_minmax(const struct ctl_table *table, int write,\\n\\t\\t    void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_doulongvec_ms_jiffies_minmax(const struct ctl_table *table, int write,\\n\\t\\t\\t\\t      void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\nint proc_do_large_bitmap(const struct ctl_table *table, int write,\\n\\t\\t\\t void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\n#endif /* CONFIG_PROC_SYSCTL */\\n\\n#if defined(CONFIG_SYSCTL)\\nint proc_do_static_key(const struct ctl_table *table, int write,\\n\\t\\t       void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct static_key *key = (struct static_key *)table->data;\\n\\tstatic DEFINE_MUTEX(static_key_mutex);\\n\\tint val, ret;\\n\\tstruct ctl_table tmp = {\\n\\t\\t.data   = &val,\\n\\t\\t.maxlen = sizeof(val),\\n\\t\\t.mode   = table->mode,\\n\\t\\t.extra1 = SYSCTL_ZERO,\\n\\t\\t.extra2 = SYSCTL_ONE,\\n\\t};\\n\\n\\tif (write && !capable(CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tmutex_lock(&static_key_mutex);\\n\\tval = static_key_enabled(key);\\n\\tret = proc_dointvec_minmax(&tmp, write, buffer, lenp, ppos);\\n\\tif (write && !ret) {\\n\\t\\tif (val)\\n\\t\\t\\tstatic_key_enable(key);\\n\\t\\telse\\n\\t\\t\\tstatic_key_disable(key);\\n\\t}\\n\\tmutex_unlock(&static_key_mutex);\\n\\treturn ret;\\n}\\n\\nstatic struct ctl_table kern_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"panic\",\\n\\t\\t.data\\t\\t= &panic_timeout,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#ifdef CONFIG_PROC_SYSCTL\\n\\t{\\n\\t\\t.procname\\t= \"tainted\",\\n\\t\\t.maxlen \\t= sizeof(long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_taint,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"sysctl_writes_strict\",\\n\\t\\t.data\\t\\t= &sysctl_writes_strict,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_NEG_ONE,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"print-fatal-signals\",\\n\\t\\t.data\\t\\t= &print_fatal_signals,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#ifdef CONFIG_SPARC\\n\\t{\\n\\t\\t.procname\\t= \"reboot-cmd\",\\n\\t\\t.data\\t\\t= reboot_command,\\n\\t\\t.maxlen\\t\\t= 256,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"stop-a\",\\n\\t\\t.data\\t\\t= &stop_a_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"scons-poweroff\",\\n\\t\\t.data\\t\\t= &scons_pwroff,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_SPARC64\\n\\t{\\n\\t\\t.procname\\t= \"tsb-ratio\",\\n\\t\\t.data\\t\\t= &sysctl_tsb_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_PARISC\\n\\t{\\n\\t\\t.procname\\t= \"soft-power\",\\n\\t\\t.data\\t\\t= &pwrsw_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_ALLOW\\n\\t{\\n\\t\\t.procname\\t= \"unaligned-trap\",\\n\\t\\t.data\\t\\t= &unaligned_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_STACK_TRACER\\n\\t{\\n\\t\\t.procname\\t= \"stack_tracer_enabled\",\\n\\t\\t.data\\t\\t= &stack_tracer_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= stack_trace_sysctl,\\n\\t},\\n#endif\\n#ifdef CONFIG_TRACING\\n\\t{\\n\\t\\t.procname\\t= \"ftrace_dump_on_oops\",\\n\\t\\t.data\\t\\t= &ftrace_dump_on_oops,\\n\\t\\t.maxlen\\t\\t= MAX_TRACER_SIZE,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"traceoff_on_warning\",\\n\\t\\t.data\\t\\t= &__disable_trace_on_warning,\\n\\t\\t.maxlen\\t\\t= sizeof(__disable_trace_on_warning),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"tracepoint_printk\",\\n\\t\\t.data\\t\\t= &tracepoint_printk,\\n\\t\\t.maxlen\\t\\t= sizeof(tracepoint_printk),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= tracepoint_printk_sysctl,\\n\\t},\\n#endif\\n#ifdef CONFIG_MODULES\\n\\t{\\n\\t\\t.procname\\t= \"modprobe\",\\n\\t\\t.data\\t\\t= &modprobe_path,\\n\\t\\t.maxlen\\t\\t= KMOD_PATH_LEN,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"modules_disabled\",\\n\\t\\t.data\\t\\t= &modules_disabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t/* only handle a transition from default \"0\" to \"1\" */\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ONE,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#endif\\n#ifdef CONFIG_UEVENT_HELPER\\n\\t{\\n\\t\\t.procname\\t= \"hotplug\",\\n\\t\\t.data\\t\\t= &uevent_helper,\\n\\t\\t.maxlen\\t\\t= UEVENT_HELPER_PATH_LEN,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n#endif\\n#ifdef CONFIG_MAGIC_SYSRQ\\n\\t{\\n\\t\\t.procname\\t= \"sysrq\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysrq_sysctl_handler,\\n\\t},\\n#endif\\n#ifdef CONFIG_PROC_SYSCTL\\n\\t{\\n\\t\\t.procname\\t= \"cad_pid\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_do_cad_pid,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"threads-max\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_max_threads,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overflowuid\",\\n\\t\\t.data\\t\\t= &overflowuid,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_MAXOLDUID,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overflowgid\",\\n\\t\\t.data\\t\\t= &overflowgid,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_MAXOLDUID,\\n\\t},\\n#ifdef CONFIG_S390\\n\\t{\\n\\t\\t.procname\\t= \"userprocess_debug\",\\n\\t\\t.data\\t\\t= &show_unhandled_signals,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"pid_max\",\\n\\t\\t.data\\t\\t= &pid_max,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &pid_max_min,\\n\\t\\t.extra2\\t\\t= &pid_max_max,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_oops\",\\n\\t\\t.data\\t\\t= &panic_on_oops,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"panic_print\",\\n\\t\\t.data\\t\\t= &panic_print,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"ngroups_max\",\\n\\t\\t.data\\t\\t= (void *)&ngroups_max,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"cap_last_cap\",\\n\\t\\t.data\\t\\t= (void *)&cap_last_cap,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_X86)\\n\\t{\\n\\t\\t.procname       = \"unknown_nmi_panic\",\\n\\t\\t.data           = &unknown_nmi_panic,\\n\\t\\t.maxlen         = sizeof (int),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = proc_dointvec,\\n\\t},\\n#endif\\n\\n#if (defined(CONFIG_X86_32) || defined(CONFIG_PARISC)) && \\\\\\n\\tdefined(CONFIG_DEBUG_STACKOVERFLOW)\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_stackoverflow\",\\n\\t\\t.data\\t\\t= &sysctl_panic_on_stackoverflow,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#if defined(CONFIG_X86)\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_unrecovered_nmi\",\\n\\t\\t.data\\t\\t= &panic_on_unrecovered_nmi,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_io_nmi\",\\n\\t\\t.data\\t\\t= &panic_on_io_nmi,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"bootloader_type\",\\n\\t\\t.data\\t\\t= &bootloader_type,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"bootloader_version\",\\n\\t\\t.data\\t\\t= &bootloader_version,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"io_delay_type\",\\n\\t\\t.data\\t\\t= &io_delay_type,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#if defined(CONFIG_MMU)\\n\\t{\\n\\t\\t.procname\\t= \"randomize_va_space\",\\n\\t\\t.data\\t\\t= &randomize_va_space,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#if defined(CONFIG_S390) && defined(CONFIG_SMP)\\n\\t{\\n\\t\\t.procname\\t= \"spin_retry\",\\n\\t\\t.data\\t\\t= &spin_retry,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#if\\tdefined(CONFIG_ACPI_SLEEP) && defined(CONFIG_X86)\\n\\t{\\n\\t\\t.procname\\t= \"acpi_video_flags\",\\n\\t\\t.data\\t\\t= &acpi_realmode_flags,\\n\\t\\t.maxlen\\t\\t= sizeof (unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n#endif\\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_NO_WARN\\n\\t{\\n\\t\\t.procname\\t= \"ignore-unaligned-usertrap\",\\n\\t\\t.data\\t\\t= &no_unaligned_warning,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_RT_MUTEXES\\n\\t{\\n\\t\\t.procname\\t= \"max_lock_depth\",\\n\\t\\t.data\\t\\t= &max_lock_depth,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_PERF_EVENTS\\n\\t/*\\n\\t * User-space scripts rely on the existence of this file\\n\\t * as a feature check for perf_events being enabled.\\n\\t *\\n\\t * So it\\'s an ABI, do not remove!\\n\\t */\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_paranoid\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_paranoid,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_paranoid),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_mlock_kb\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_mlock,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_mlock),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_max_sample_rate\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_sample_rate,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_sample_rate),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= perf_event_max_sample_rate_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ONE,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_cpu_time_max_percent\",\\n\\t\\t.data\\t\\t= &sysctl_perf_cpu_time_max_percent,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_cpu_time_max_percent),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= perf_cpu_time_max_percent_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE_HUNDRED,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_max_stack\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_max_stack,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_max_stack),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= perf_event_max_stack_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= (void *)&six_hundred_forty_kb,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_max_contexts_per_stack\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_max_contexts_per_stack,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_max_contexts_per_stack),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= perf_event_max_stack_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE_THOUSAND,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_warn\",\\n\\t\\t.data\\t\\t= &panic_on_warn,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#ifdef CONFIG_TREE_RCU\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_rcu_stall\",\\n\\t\\t.data\\t\\t= &sysctl_panic_on_rcu_stall,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_panic_on_rcu_stall),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"max_rcu_stall_to_panic\",\\n\\t\\t.data\\t\\t= &sysctl_max_rcu_stall_to_panic,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_max_rcu_stall_to_panic),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ONE,\\n\\t\\t.extra2\\t\\t= SYSCTL_INT_MAX,\\n\\t},\\n#endif\\n};\\n\\nstatic struct ctl_table vm_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"overcommit_memory\",\\n\\t\\t.data\\t\\t= &sysctl_overcommit_memory,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_overcommit_memory),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= overcommit_policy_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_TWO,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overcommit_ratio\",\\n\\t\\t.data\\t\\t= &sysctl_overcommit_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_overcommit_ratio),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= overcommit_ratio_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overcommit_kbytes\",\\n\\t\\t.data\\t\\t= &sysctl_overcommit_kbytes,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_overcommit_kbytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= overcommit_kbytes_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"page-cluster\",\\n\\t\\t.data\\t\\t= &page_cluster,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= (void *)&page_cluster_max,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dirtytime_expire_seconds\",\\n\\t\\t.data\\t\\t= &dirtytime_expire_interval,\\n\\t\\t.maxlen\\t\\t= sizeof(dirtytime_expire_interval),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= dirtytime_interval_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"swappiness\",\\n\\t\\t.data\\t\\t= &vm_swappiness,\\n\\t\\t.maxlen\\t\\t= sizeof(vm_swappiness),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_TWO_HUNDRED,\\n\\t},\\n#ifdef CONFIG_NUMA\\n\\t{\\n\\t\\t.procname\\t= \"numa_stat\",\\n\\t\\t.data\\t\\t= &sysctl_vm_numa_stat,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_vm_numa_stat_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"drop_caches\",\\n\\t\\t.data\\t\\t= &sysctl_drop_caches,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0200,\\n\\t\\t.proc_handler\\t= drop_caches_sysctl_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ONE,\\n\\t\\t.extra2\\t\\t= SYSCTL_FOUR,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"page_lock_unfairness\",\\n\\t\\t.data\\t\\t= &sysctl_page_lock_unfairness,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_page_lock_unfairness),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n#ifdef CONFIG_MMU\\n\\t{\\n\\t\\t.procname\\t= \"max_map_count\",\\n\\t\\t.data\\t\\t= &sysctl_max_map_count,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_max_map_count),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n#else\\n\\t{\\n\\t\\t.procname\\t= \"nr_trim_pages\",\\n\\t\\t.data\\t\\t= &sysctl_nr_trim_pages,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_nr_trim_pages),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"vfs_cache_pressure\",\\n\\t\\t.data\\t\\t= &sysctl_vfs_cache_pressure,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_vfs_cache_pressure),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n#if defined(HAVE_ARCH_PICK_MMAP_LAYOUT) || \\\\\\n    defined(CONFIG_ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT)\\n\\t{\\n\\t\\t.procname\\t= \"legacy_va_layout\",\\n\\t\\t.data\\t\\t= &sysctl_legacy_va_layout,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_legacy_va_layout),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n#endif\\n#ifdef CONFIG_NUMA\\n\\t{\\n\\t\\t.procname\\t= \"zone_reclaim_mode\",\\n\\t\\t.data\\t\\t= &node_reclaim_mode,\\n\\t\\t.maxlen\\t\\t= sizeof(node_reclaim_mode),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n#endif\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname\\t= \"stat_interval\",\\n\\t\\t.data\\t\\t= &sysctl_stat_interval,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_stat_interval),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_jiffies,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"stat_refresh\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= 0,\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= vmstat_refresh,\\n\\t},\\n#endif\\n#ifdef CONFIG_MMU\\n\\t{\\n\\t\\t.procname\\t= \"mmap_min_addr\",\\n\\t\\t.data\\t\\t= &dac_mmap_min_addr,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= mmap_min_addr_handler,\\n\\t},\\n#endif\\n#if (defined(CONFIG_X86_32) && !defined(CONFIG_UML))|| \\\\\\n   (defined(CONFIG_SUPERH) && defined(CONFIG_VSYSCALL))\\n\\t{\\n\\t\\t.procname\\t= \"vdso_enabled\",\\n#ifdef CONFIG_X86_32\\n\\t\\t.data\\t\\t= &vdso32_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(vdso32_enabled),\\n#else\\n\\t\\t.data\\t\\t= &vdso_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(vdso_enabled),\\n#endif\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"user_reserve_kbytes\",\\n\\t\\t.data\\t\\t= &sysctl_user_reserve_kbytes,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_user_reserve_kbytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"admin_reserve_kbytes\",\\n\\t\\t.data\\t\\t= &sysctl_admin_reserve_kbytes,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_admin_reserve_kbytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\\n\\t{\\n\\t\\t.procname\\t= \"mmap_rnd_bits\",\\n\\t\\t.data\\t\\t= &mmap_rnd_bits,\\n\\t\\t.maxlen\\t\\t= sizeof(mmap_rnd_bits),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= (void *)&mmap_rnd_bits_min,\\n\\t\\t.extra2\\t\\t= (void *)&mmap_rnd_bits_max,\\n\\t},\\n#endif\\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\\n\\t{\\n\\t\\t.procname\\t= \"mmap_rnd_compat_bits\",\\n\\t\\t.data\\t\\t= &mmap_rnd_compat_bits,\\n\\t\\t.maxlen\\t\\t= sizeof(mmap_rnd_compat_bits),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= (void *)&mmap_rnd_compat_bits_min,\\n\\t\\t.extra2\\t\\t= (void *)&mmap_rnd_compat_bits_max,\\n\\t},\\n#endif\\n};\\n\\nint __init sysctl_init_bases(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", kern_table);\\n\\tregister_sysctl_init(\"vm\", vm_table);\\n\\n\\treturn 0;\\n}\\n#endif /* CONFIG_SYSCTL */\\n/*\\n * No sense putting this after each symbol definition, twice,\\n * exception granted :-)\\n */\\nEXPORT_SYMBOL(proc_dobool);\\nEXPORT_SYMBOL(proc_dointvec);\\nEXPORT_SYMBOL(proc_douintvec);\\nEXPORT_SYMBOL(proc_dointvec_jiffies);\\nEXPORT_SYMBOL(proc_dointvec_minmax);\\nEXPORT_SYMBOL_GPL(proc_douintvec_minmax);\\nEXPORT_SYMBOL(proc_dointvec_userhz_jiffies);\\nEXPORT_SYMBOL(proc_dointvec_ms_jiffies);\\nEXPORT_SYMBOL(proc_dostring);\\nEXPORT_SYMBOL(proc_doulongvec_minmax);\\nEXPORT_SYMBOL(proc_doulongvec_ms_jiffies_minmax);\\nEXPORT_SYMBOL(proc_do_large_bitmap);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * latencytop.c: Latency display infrastructure\\n *\\n * (C) Copyright 2008 Intel Corporation\\n * Author: Arjan van de Ven <arjan@linux.intel.com>\\n */\\n\\n/*\\n * CONFIG_LATENCYTOP enables a kernel latency tracking infrastructure that is\\n * used by the \"latencytop\" userspace tool. The latency that is tracked is not\\n * the \\'traditional\\' interrupt latency (which is primarily caused by something\\n * else consuming CPU), but instead, it is the latency an application encounters\\n * because the kernel sleeps on its behalf for various reasons.\\n *\\n * This code tracks 2 levels of statistics:\\n * 1) System level latency\\n * 2) Per process latency\\n *\\n * The latency is stored in fixed sized data structures in an accumulated form;\\n * if the \"same\" latency cause is hit twice, this will be tracked as one entry\\n * in the data structure. Both the count, total accumulated latency and maximum\\n * latency are tracked in this data structure. When the fixed size structure is\\n * full, no new causes are tracked until the buffer is flushed by writing to\\n * the /proc file; the userspace tool does this on a regular basis.\\n *\\n * A latency cause is identified by a stringified backtrace at the point that\\n * the scheduler gets invoked. The userland tool will use this string to\\n * identify the cause of the latency in human readable form.\\n *\\n * The information is exported via /proc/latency_stats and /proc/<pid>/latency.\\n * These files look like this:\\n *\\n * Latency Top version : v0.1\\n * 70 59433 4897 i915_irq_wait drm_ioctl vfs_ioctl do_vfs_ioctl sys_ioctl\\n * |    |    |    |\\n * |    |    |    +----> the stringified backtrace\\n * |    |    +---------> The maximum latency for this entry in microseconds\\n * |    +--------------> The accumulated latency for this entry (microseconds)\\n * +-------------------> The number of times this entry is hit\\n *\\n * (note: the average latency is the accumulated latency divided by the number\\n * of times)\\n */\\n\\n#include <linux/kallsyms.h>\\n#include <linux/seq_file.h>\\n#include <linux/notifier.h>\\n#include <linux/spinlock.h>\\n#include <linux/proc_fs.h>\\n#include <linux/latencytop.h>\\n#include <linux/export.h>\\n#include <linux/sched.h>\\n#include <linux/sched/debug.h>\\n#include <linux/sched/stat.h>\\n#include <linux/list.h>\\n#include <linux/stacktrace.h>\\n#include <linux/sysctl.h>\\n\\nstatic DEFINE_RAW_SPINLOCK(latency_lock);\\n\\n#define MAXLR 128\\nstatic struct latency_record latency_record[MAXLR];\\n\\nint latencytop_enabled;\\n\\n#ifdef CONFIG_SYSCTL\\nstatic int sysctl_latencytop(const struct ctl_table *table, int write, void *buffer,\\n\\t\\tsize_t *lenp, loff_t *ppos)\\n{\\n\\tint err;\\n\\n\\terr = proc_dointvec(table, write, buffer, lenp, ppos);\\n\\tif (latencytop_enabled)\\n\\t\\tforce_schedstat_enabled();\\n\\n\\treturn err;\\n}\\n\\nstatic struct ctl_table latencytop_sysctl[] = {\\n\\t{\\n\\t\\t.procname   = \"latencytop\",\\n\\t\\t.data       = &latencytop_enabled,\\n\\t\\t.maxlen     = sizeof(int),\\n\\t\\t.mode       = 0644,\\n\\t\\t.proc_handler   = sysctl_latencytop,\\n\\t},\\n};\\n#endif\\n\\nvoid clear_tsk_latency_tracing(struct task_struct *p)\\n{\\n\\tunsigned long flags;\\n\\n\\traw_spin_lock_irqsave(&latency_lock, flags);\\n\\tmemset(&p->latency_record, 0, sizeof(p->latency_record));\\n\\tp->latency_record_count = 0;\\n\\traw_spin_unlock_irqrestore(&latency_lock, flags);\\n}\\n\\nstatic void clear_global_latency_tracing(void)\\n{\\n\\tunsigned long flags;\\n\\n\\traw_spin_lock_irqsave(&latency_lock, flags);\\n\\tmemset(&latency_record, 0, sizeof(latency_record));\\n\\traw_spin_unlock_irqrestore(&latency_lock, flags);\\n}\\n\\nstatic void __sched\\naccount_global_scheduler_latency(struct task_struct *tsk,\\n\\t\\t\\t\\t struct latency_record *lat)\\n{\\n\\tint firstnonnull = MAXLR;\\n\\tint i;\\n\\n\\t/* skip kernel threads for now */\\n\\tif (!tsk->mm)\\n\\t\\treturn;\\n\\n\\tfor (i = 0; i < MAXLR; i++) {\\n\\t\\tint q, same = 1;\\n\\n\\t\\t/* Nothing stored: */\\n\\t\\tif (!latency_record[i].backtrace[0]) {\\n\\t\\t\\tif (firstnonnull > i)\\n\\t\\t\\t\\tfirstnonnull = i;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tfor (q = 0; q < LT_BACKTRACEDEPTH; q++) {\\n\\t\\t\\tunsigned long record = lat->backtrace[q];\\n\\n\\t\\t\\tif (latency_record[i].backtrace[q] != record) {\\n\\t\\t\\t\\tsame = 0;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/* 0 entry marks end of backtrace: */\\n\\t\\t\\tif (!record)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tif (same) {\\n\\t\\t\\tlatency_record[i].count++;\\n\\t\\t\\tlatency_record[i].time += lat->time;\\n\\t\\t\\tif (lat->time > latency_record[i].max)\\n\\t\\t\\t\\tlatency_record[i].max = lat->time;\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t}\\n\\n\\ti = firstnonnull;\\n\\tif (i >= MAXLR)\\n\\t\\treturn;\\n\\n\\t/* Allocted a new one: */\\n\\tmemcpy(&latency_record[i], lat, sizeof(struct latency_record));\\n}\\n\\n/**\\n * __account_scheduler_latency - record an occurred latency\\n * @tsk - the task struct of the task hitting the latency\\n * @usecs - the duration of the latency in microseconds\\n * @inter - 1 if the sleep was interruptible, 0 if uninterruptible\\n *\\n * This function is the main entry point for recording latency entries\\n * as called by the scheduler.\\n *\\n * This function has a few special cases to deal with normal \\'non-latency\\'\\n * sleeps: specifically, interruptible sleep longer than 5 msec is skipped\\n * since this usually is caused by waiting for events via select() and co.\\n *\\n * Negative latencies (caused by time going backwards) are also explicitly\\n * skipped.\\n */\\nvoid __sched\\n__account_scheduler_latency(struct task_struct *tsk, int usecs, int inter)\\n{\\n\\tunsigned long flags;\\n\\tint i, q;\\n\\tstruct latency_record lat;\\n\\n\\t/* Long interruptible waits are generally user requested... */\\n\\tif (inter && usecs > 5000)\\n\\t\\treturn;\\n\\n\\t/* Negative sleeps are time going backwards */\\n\\t/* Zero-time sleeps are non-interesting */\\n\\tif (usecs <= 0)\\n\\t\\treturn;\\n\\n\\tmemset(&lat, 0, sizeof(lat));\\n\\tlat.count = 1;\\n\\tlat.time = usecs;\\n\\tlat.max = usecs;\\n\\n\\tstack_trace_save_tsk(tsk, lat.backtrace, LT_BACKTRACEDEPTH, 0);\\n\\n\\traw_spin_lock_irqsave(&latency_lock, flags);\\n\\n\\taccount_global_scheduler_latency(tsk, &lat);\\n\\n\\tfor (i = 0; i < tsk->latency_record_count; i++) {\\n\\t\\tstruct latency_record *mylat;\\n\\t\\tint same = 1;\\n\\n\\t\\tmylat = &tsk->latency_record[i];\\n\\t\\tfor (q = 0; q < LT_BACKTRACEDEPTH; q++) {\\n\\t\\t\\tunsigned long record = lat.backtrace[q];\\n\\n\\t\\t\\tif (mylat->backtrace[q] != record) {\\n\\t\\t\\t\\tsame = 0;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/* 0 entry is end of backtrace */\\n\\t\\t\\tif (!record)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tif (same) {\\n\\t\\t\\tmylat->count++;\\n\\t\\t\\tmylat->time += lat.time;\\n\\t\\t\\tif (lat.time > mylat->max)\\n\\t\\t\\t\\tmylat->max = lat.time;\\n\\t\\t\\tgoto out_unlock;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * short term hack; if we\\'re > 32 we stop; future we recycle:\\n\\t */\\n\\tif (tsk->latency_record_count >= LT_SAVECOUNT)\\n\\t\\tgoto out_unlock;\\n\\n\\t/* Allocated a new one: */\\n\\ti = tsk->latency_record_count++;\\n\\tmemcpy(&tsk->latency_record[i], &lat, sizeof(struct latency_record));\\n\\nout_unlock:\\n\\traw_spin_unlock_irqrestore(&latency_lock, flags);\\n}\\n\\nstatic int lstats_show(struct seq_file *m, void *v)\\n{\\n\\tint i;\\n\\n\\tseq_puts(m, \"Latency Top version : v0.1\\\\n\");\\n\\n\\tfor (i = 0; i < MAXLR; i++) {\\n\\t\\tstruct latency_record *lr = &latency_record[i];\\n\\n\\t\\tif (lr->backtrace[0]) {\\n\\t\\t\\tint q;\\n\\t\\t\\tseq_printf(m, \"%i %lu %lu\",\\n\\t\\t\\t\\t   lr->count, lr->time, lr->max);\\n\\t\\t\\tfor (q = 0; q < LT_BACKTRACEDEPTH; q++) {\\n\\t\\t\\t\\tunsigned long bt = lr->backtrace[q];\\n\\n\\t\\t\\t\\tif (!bt)\\n\\t\\t\\t\\t\\tbreak;\\n\\n\\t\\t\\t\\tseq_printf(m, \" %ps\", (void *)bt);\\n\\t\\t\\t}\\n\\t\\t\\tseq_puts(m, \"\\\\n\");\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic ssize_t\\nlstats_write(struct file *file, const char __user *buf, size_t count,\\n\\t     loff_t *offs)\\n{\\n\\tclear_global_latency_tracing();\\n\\n\\treturn count;\\n}\\n\\nstatic int lstats_open(struct inode *inode, struct file *filp)\\n{\\n\\treturn single_open(filp, lstats_show, NULL);\\n}\\n\\nstatic const struct proc_ops lstats_proc_ops = {\\n\\t.proc_open\\t= lstats_open,\\n\\t.proc_read\\t= seq_read,\\n\\t.proc_write\\t= lstats_write,\\n\\t.proc_lseek\\t= seq_lseek,\\n\\t.proc_release\\t= single_release,\\n};\\n\\nstatic int __init init_lstats_procfs(void)\\n{\\n\\tproc_create(\"latency_stats\", 0644, NULL, &lstats_proc_ops);\\n#ifdef CONFIG_SYSCTL\\n\\tregister_sysctl_init(\"kernel\", latencytop_sysctl);\\n#endif\\n\\treturn 0;\\n}\\ndevice_initcall(init_lstats_procfs);\\n\\n// SPDX-License-Identifier: GPL-2.0+\\n/*\\n * Module signature checker\\n *\\n * Copyright (C) 2012 Red Hat, Inc. All Rights Reserved.\\n * Written by David Howells (dhowells@redhat.com)\\n */\\n\\n#include <linux/errno.h>\\n#include <linux/printk.h>\\n#include <linux/module_signature.h>\\n#include <asm/byteorder.h>\\n\\n/**\\n * mod_check_sig - check that the given signature is sane\\n *\\n * @ms:\\t\\tSignature to check.\\n * @file_len:\\tSize of the file to which @ms is appended.\\n * @name:\\tWhat is being checked. Used for error messages.\\n */\\nint mod_check_sig(const struct module_signature *ms, size_t file_len,\\n\\t\\t  const char *name)\\n{\\n\\tif (be32_to_cpu(ms->sig_len) >= file_len - sizeof(*ms))\\n\\t\\treturn -EBADMSG;\\n\\n\\tif (ms->id_type != PKEY_ID_PKCS7) {\\n\\t\\tpr_err(\"%s: not signed with expected PKCS#7 message\\\\n\",\\n\\t\\t       name);\\n\\t\\treturn -ENOPKG;\\n\\t}\\n\\n\\tif (ms->algo != 0 ||\\n\\t    ms->hash != 0 ||\\n\\t    ms->signer_len != 0 ||\\n\\t    ms->key_id_len != 0 ||\\n\\t    ms->__pad[0] != 0 ||\\n\\t    ms->__pad[1] != 0 ||\\n\\t    ms->__pad[2] != 0) {\\n\\t\\tpr_err(\"%s: PKCS#7 signature info has unexpected non-zero params\\\\n\",\\n\\t\\t       name);\\n\\t\\treturn -EBADMSG;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  linux/kernel/panic.c\\n *\\n *  Copyright (C) 1991, 1992  Linus Torvalds\\n */\\n\\n/*\\n * This function is used through-out the kernel (including mm and fs)\\n * to indicate a major problem.\\n */\\n#include <linux/debug_locks.h>\\n#include <linux/sched/debug.h>\\n#include <linux/interrupt.h>\\n#include <linux/kgdb.h>\\n#include <linux/kmsg_dump.h>\\n#include <linux/kallsyms.h>\\n#include <linux/notifier.h>\\n#include <linux/vt_kern.h>\\n#include <linux/module.h>\\n#include <linux/random.h>\\n#include <linux/ftrace.h>\\n#include <linux/reboot.h>\\n#include <linux/delay.h>\\n#include <linux/kexec.h>\\n#include <linux/panic_notifier.h>\\n#include <linux/sched.h>\\n#include <linux/string_helpers.h>\\n#include <linux/sysrq.h>\\n#include <linux/init.h>\\n#include <linux/nmi.h>\\n#include <linux/console.h>\\n#include <linux/bug.h>\\n#include <linux/ratelimit.h>\\n#include <linux/debugfs.h>\\n#include <linux/sysfs.h>\\n#include <linux/context_tracking.h>\\n#include <linux/seq_buf.h>\\n#include <trace/events/error_report.h>\\n#include <asm/sections.h>\\n\\n#define PANIC_TIMER_STEP 100\\n#define PANIC_BLINK_SPD 18\\n\\n#ifdef CONFIG_SMP\\n/*\\n * Should we dump all CPUs backtraces in an oops event?\\n * Defaults to 0, can be changed via sysctl.\\n */\\nstatic unsigned int __read_mostly sysctl_oops_all_cpu_backtrace;\\n#else\\n#define sysctl_oops_all_cpu_backtrace 0\\n#endif /* CONFIG_SMP */\\n\\nint panic_on_oops = CONFIG_PANIC_ON_OOPS_VALUE;\\nstatic unsigned long tainted_mask =\\n\\tIS_ENABLED(CONFIG_RANDSTRUCT) ? (1 << TAINT_RANDSTRUCT) : 0;\\nstatic int pause_on_oops;\\nstatic int pause_on_oops_flag;\\nstatic DEFINE_SPINLOCK(pause_on_oops_lock);\\nbool crash_kexec_post_notifiers;\\nint panic_on_warn __read_mostly;\\nunsigned long panic_on_taint;\\nbool panic_on_taint_nousertaint = false;\\nstatic unsigned int warn_limit __read_mostly;\\n\\nbool panic_triggering_all_cpu_backtrace;\\n\\nint panic_timeout = CONFIG_PANIC_TIMEOUT;\\nEXPORT_SYMBOL_GPL(panic_timeout);\\n\\n#define PANIC_PRINT_TASK_INFO\\t\\t0x00000001\\n#define PANIC_PRINT_MEM_INFO\\t\\t0x00000002\\n#define PANIC_PRINT_TIMER_INFO\\t\\t0x00000004\\n#define PANIC_PRINT_LOCK_INFO\\t\\t0x00000008\\n#define PANIC_PRINT_FTRACE_INFO\\t\\t0x00000010\\n#define PANIC_PRINT_ALL_PRINTK_MSG\\t0x00000020\\n#define PANIC_PRINT_ALL_CPU_BT\\t\\t0x00000040\\n#define PANIC_PRINT_BLOCKED_TASKS\\t0x00000080\\nunsigned long panic_print;\\n\\nATOMIC_NOTIFIER_HEAD(panic_notifier_list);\\n\\nEXPORT_SYMBOL(panic_notifier_list);\\n\\n#ifdef CONFIG_SYSCTL\\nstatic struct ctl_table kern_panic_table[] = {\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname       = \"oops_all_cpu_backtrace\",\\n\\t\\t.data           = &sysctl_oops_all_cpu_backtrace,\\n\\t\\t.maxlen         = sizeof(int),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = proc_dointvec_minmax,\\n\\t\\t.extra1         = SYSCTL_ZERO,\\n\\t\\t.extra2         = SYSCTL_ONE,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname       = \"warn_limit\",\\n\\t\\t.data           = &warn_limit,\\n\\t\\t.maxlen         = sizeof(warn_limit),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = proc_douintvec,\\n\\t},\\n};\\n\\nstatic __init int kernel_panic_sysctls_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", kern_panic_table);\\n\\treturn 0;\\n}\\nlate_initcall(kernel_panic_sysctls_init);\\n#endif\\n\\nstatic atomic_t warn_count = ATOMIC_INIT(0);\\n\\n#ifdef CONFIG_SYSFS\\nstatic ssize_t warn_count_show(struct kobject *kobj, struct kobj_attribute *attr,\\n\\t\\t\\t       char *page)\\n{\\n\\treturn sysfs_emit(page, \"%d\\\\n\", atomic_read(&warn_count));\\n}\\n\\nstatic struct kobj_attribute warn_count_attr = __ATTR_RO(warn_count);\\n\\nstatic __init int kernel_panic_sysfs_init(void)\\n{\\n\\tsysfs_add_file_to_group(kernel_kobj, &warn_count_attr.attr, NULL);\\n\\treturn 0;\\n}\\nlate_initcall(kernel_panic_sysfs_init);\\n#endif\\n\\nstatic long no_blink(int state)\\n{\\n\\treturn 0;\\n}\\n\\n/* Returns how long it waited in ms */\\nlong (*panic_blink)(int state);\\nEXPORT_SYMBOL(panic_blink);\\n\\n/*\\n * Stop ourself in panic -- architecture code may override this\\n */\\nvoid __weak __noreturn panic_smp_self_stop(void)\\n{\\n\\twhile (1)\\n\\t\\tcpu_relax();\\n}\\n\\n/*\\n * Stop ourselves in NMI context if another CPU has already panicked. Arch code\\n * may override this to prepare for crash dumping, e.g. save regs info.\\n */\\nvoid __weak __noreturn nmi_panic_self_stop(struct pt_regs *regs)\\n{\\n\\tpanic_smp_self_stop();\\n}\\n\\n/*\\n * Stop other CPUs in panic.  Architecture dependent code may override this\\n * with more suitable version.  For example, if the architecture supports\\n * crash dump, it should save registers of each stopped CPU and disable\\n * per-CPU features such as virtualization extensions.\\n */\\nvoid __weak crash_smp_send_stop(void)\\n{\\n\\tstatic int cpus_stopped;\\n\\n\\t/*\\n\\t * This function can be called twice in panic path, but obviously\\n\\t * we execute this only once.\\n\\t */\\n\\tif (cpus_stopped)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Note smp_send_stop is the usual smp shutdown function, which\\n\\t * unfortunately means it may not be hardened to work in a panic\\n\\t * situation.\\n\\t */\\n\\tsmp_send_stop();\\n\\tcpus_stopped = 1;\\n}\\n\\natomic_t panic_cpu = ATOMIC_INIT(PANIC_CPU_INVALID);\\n\\n/*\\n * A variant of panic() called from NMI context. We return if we\\'ve already\\n * panicked on this CPU. If another CPU already panicked, loop in\\n * nmi_panic_self_stop() which can provide architecture dependent code such\\n * as saving register state for crash dump.\\n */\\nvoid nmi_panic(struct pt_regs *regs, const char *msg)\\n{\\n\\tint old_cpu, this_cpu;\\n\\n\\told_cpu = PANIC_CPU_INVALID;\\n\\tthis_cpu = raw_smp_processor_id();\\n\\n\\t/* atomic_try_cmpxchg updates old_cpu on failure */\\n\\tif (atomic_try_cmpxchg(&panic_cpu, &old_cpu, this_cpu))\\n\\t\\tpanic(\"%s\", msg);\\n\\telse if (old_cpu != this_cpu)\\n\\t\\tnmi_panic_self_stop(regs);\\n}\\nEXPORT_SYMBOL(nmi_panic);\\n\\nstatic void panic_print_sys_info(bool console_flush)\\n{\\n\\tif (console_flush) {\\n\\t\\tif (panic_print & PANIC_PRINT_ALL_PRINTK_MSG)\\n\\t\\t\\tconsole_flush_on_panic(CONSOLE_REPLAY_ALL);\\n\\t\\treturn;\\n\\t}\\n\\n\\tif (panic_print & PANIC_PRINT_TASK_INFO)\\n\\t\\tshow_state();\\n\\n\\tif (panic_print & PANIC_PRINT_MEM_INFO)\\n\\t\\tshow_mem();\\n\\n\\tif (panic_print & PANIC_PRINT_TIMER_INFO)\\n\\t\\tsysrq_timer_list_show();\\n\\n\\tif (panic_print & PANIC_PRINT_LOCK_INFO)\\n\\t\\tdebug_show_all_locks();\\n\\n\\tif (panic_print & PANIC_PRINT_FTRACE_INFO)\\n\\t\\tftrace_dump(DUMP_ALL);\\n\\n\\tif (panic_print & PANIC_PRINT_BLOCKED_TASKS)\\n\\t\\tshow_state_filter(TASK_UNINTERRUPTIBLE);\\n}\\n\\nvoid check_panic_on_warn(const char *origin)\\n{\\n\\tunsigned int limit;\\n\\n\\tif (panic_on_warn)\\n\\t\\tpanic(\"%s: panic_on_warn set ...\\\\n\", origin);\\n\\n\\tlimit = READ_ONCE(warn_limit);\\n\\tif (atomic_inc_return(&warn_count) >= limit && limit)\\n\\t\\tpanic(\"%s: system warned too often (kernel.warn_limit is %d)\",\\n\\t\\t      origin, limit);\\n}\\n\\n/*\\n * Helper that triggers the NMI backtrace (if set in panic_print)\\n * and then performs the secondary CPUs shutdown - we cannot have\\n * the NMI backtrace after the CPUs are off!\\n */\\nstatic void panic_other_cpus_shutdown(bool crash_kexec)\\n{\\n\\tif (panic_print & PANIC_PRINT_ALL_CPU_BT) {\\n\\t\\t/* Temporary allow non-panic CPUs to write their backtraces. */\\n\\t\\tpanic_triggering_all_cpu_backtrace = true;\\n\\t\\ttrigger_all_cpu_backtrace();\\n\\t\\tpanic_triggering_all_cpu_backtrace = false;\\n\\t}\\n\\n\\t/*\\n\\t * Note that smp_send_stop() is the usual SMP shutdown function,\\n\\t * which unfortunately may not be hardened to work in a panic\\n\\t * situation. If we want to do crash dump after notifier calls\\n\\t * and kmsg_dump, we will need architecture dependent extra\\n\\t * bits in addition to stopping other CPUs, hence we rely on\\n\\t * crash_smp_send_stop() for that.\\n\\t */\\n\\tif (!crash_kexec)\\n\\t\\tsmp_send_stop();\\n\\telse\\n\\t\\tcrash_smp_send_stop();\\n}\\n\\n/**\\n *\\tpanic - halt the system\\n *\\t@fmt: The text string to print\\n *\\n *\\tDisplay a message, then perform cleanups.\\n *\\n *\\tThis function never returns.\\n */\\nvoid panic(const char *fmt, ...)\\n{\\n\\tstatic char buf[1024];\\n\\tva_list args;\\n\\tlong i, i_next = 0, len;\\n\\tint state = 0;\\n\\tint old_cpu, this_cpu;\\n\\tbool _crash_kexec_post_notifiers = crash_kexec_post_notifiers;\\n\\n\\tif (panic_on_warn) {\\n\\t\\t/*\\n\\t\\t * This thread may hit another WARN() in the panic path.\\n\\t\\t * Resetting this prevents additional WARN() from panicking the\\n\\t\\t * system on this thread.  Other threads are blocked by the\\n\\t\\t * panic_mutex in panic().\\n\\t\\t */\\n\\t\\tpanic_on_warn = 0;\\n\\t}\\n\\n\\t/*\\n\\t * Disable local interrupts. This will prevent panic_smp_self_stop\\n\\t * from deadlocking the first cpu that invokes the panic, since\\n\\t * there is nothing to prevent an interrupt handler (that runs\\n\\t * after setting panic_cpu) from invoking panic() again.\\n\\t */\\n\\tlocal_irq_disable();\\n\\tpreempt_disable_notrace();\\n\\n\\t/*\\n\\t * It\\'s possible to come here directly from a panic-assertion and\\n\\t * not have preempt disabled. Some functions called from here want\\n\\t * preempt to be disabled. No point enabling it later though...\\n\\t *\\n\\t * Only one CPU is allowed to execute the panic code from here. For\\n\\t * multiple parallel invocations of panic, all other CPUs either\\n\\t * stop themself or will wait until they are stopped by the 1st CPU\\n\\t * with smp_send_stop().\\n\\t *\\n\\t * cmpxchg success means this is the 1st CPU which comes here,\\n\\t * so go ahead.\\n\\t * `old_cpu == this_cpu\\' means we came from nmi_panic() which sets\\n\\t * panic_cpu to this CPU.  In this case, this is also the 1st CPU.\\n\\t */\\n\\told_cpu = PANIC_CPU_INVALID;\\n\\tthis_cpu = raw_smp_processor_id();\\n\\n\\t/* atomic_try_cmpxchg updates old_cpu on failure */\\n\\tif (atomic_try_cmpxchg(&panic_cpu, &old_cpu, this_cpu)) {\\n\\t\\t/* go ahead */\\n\\t} else if (old_cpu != this_cpu)\\n\\t\\tpanic_smp_self_stop();\\n\\n\\tconsole_verbose();\\n\\tbust_spinlocks(1);\\n\\tva_start(args, fmt);\\n\\tlen = vscnprintf(buf, sizeof(buf), fmt, args);\\n\\tva_end(args);\\n\\n\\tif (len && buf[len - 1] == \\'\\\\n\\')\\n\\t\\tbuf[len - 1] = \\'\\\\0\\';\\n\\n\\tpr_emerg(\"Kernel panic - not syncing: %s\\\\n\", buf);\\n#ifdef CONFIG_DEBUG_BUGVERBOSE\\n\\t/*\\n\\t * Avoid nested stack-dumping if a panic occurs during oops processing\\n\\t */\\n\\tif (!test_taint(TAINT_DIE) && oops_in_progress <= 1)\\n\\t\\tdump_stack();\\n#endif\\n\\n\\t/*\\n\\t * If kgdb is enabled, give it a chance to run before we stop all\\n\\t * the other CPUs or else we won\\'t be able to debug processes left\\n\\t * running on them.\\n\\t */\\n\\tkgdb_panic(buf);\\n\\n\\t/*\\n\\t * If we have crashed and we have a crash kernel loaded let it handle\\n\\t * everything else.\\n\\t * If we want to run this after calling panic_notifiers, pass\\n\\t * the \"crash_kexec_post_notifiers\" option to the kernel.\\n\\t *\\n\\t * Bypass the panic_cpu check and call __crash_kexec directly.\\n\\t */\\n\\tif (!_crash_kexec_post_notifiers)\\n\\t\\t__crash_kexec(NULL);\\n\\n\\tpanic_other_cpus_shutdown(_crash_kexec_post_notifiers);\\n\\n\\tprintk_legacy_allow_panic_sync();\\n\\n\\t/*\\n\\t * Run any panic handlers, including those that might need to\\n\\t * add information to the kmsg dump output.\\n\\t */\\n\\tatomic_notifier_call_chain(&panic_notifier_list, 0, buf);\\n\\n\\tpanic_print_sys_info(false);\\n\\n\\tkmsg_dump_desc(KMSG_DUMP_PANIC, buf);\\n\\n\\t/*\\n\\t * If you doubt kdump always works fine in any situation,\\n\\t * \"crash_kexec_post_notifiers\" offers you a chance to run\\n\\t * panic_notifiers and dumping kmsg before kdump.\\n\\t * Note: since some panic_notifiers can make crashed kernel\\n\\t * more unstable, it can increase risks of the kdump failure too.\\n\\t *\\n\\t * Bypass the panic_cpu check and call __crash_kexec directly.\\n\\t */\\n\\tif (_crash_kexec_post_notifiers)\\n\\t\\t__crash_kexec(NULL);\\n\\n\\tconsole_unblank();\\n\\n\\t/*\\n\\t * We may have ended up stopping the CPU holding the lock (in\\n\\t * smp_send_stop()) while still having some valuable data in the console\\n\\t * buffer.  Try to acquire the lock then release it regardless of the\\n\\t * result.  The release will also print the buffers out.  Locks debug\\n\\t * should be disabled to avoid reporting bad unlock balance when\\n\\t * panic() is not being callled from OOPS.\\n\\t */\\n\\tdebug_locks_off();\\n\\tconsole_flush_on_panic(CONSOLE_FLUSH_PENDING);\\n\\n\\tpanic_print_sys_info(true);\\n\\n\\tif (!panic_blink)\\n\\t\\tpanic_blink = no_blink;\\n\\n\\tif (panic_timeout > 0) {\\n\\t\\t/*\\n\\t\\t * Delay timeout seconds before rebooting the machine.\\n\\t\\t * We can\\'t use the \"normal\" timers since we just panicked.\\n\\t\\t */\\n\\t\\tpr_emerg(\"Rebooting in %d seconds..\\\\n\", panic_timeout);\\n\\n\\t\\tfor (i = 0; i < panic_timeout * 1000; i += PANIC_TIMER_STEP) {\\n\\t\\t\\ttouch_nmi_watchdog();\\n\\t\\t\\tif (i >= i_next) {\\n\\t\\t\\t\\ti += panic_blink(state ^= 1);\\n\\t\\t\\t\\ti_next = i + 3600 / PANIC_BLINK_SPD;\\n\\t\\t\\t}\\n\\t\\t\\tmdelay(PANIC_TIMER_STEP);\\n\\t\\t}\\n\\t}\\n\\tif (panic_timeout != 0) {\\n\\t\\t/*\\n\\t\\t * This will not be a clean reboot, with everything\\n\\t\\t * shutting down.  But if there is a chance of\\n\\t\\t * rebooting the system it will be rebooted.\\n\\t\\t */\\n\\t\\tif (panic_reboot_mode != REBOOT_UNDEFINED)\\n\\t\\t\\treboot_mode = panic_reboot_mode;\\n\\t\\temergency_restart();\\n\\t}\\n#ifdef __sparc__\\n\\t{\\n\\t\\textern int stop_a_enabled;\\n\\t\\t/* Make sure the user can actually press Stop-A (L1-A) */\\n\\t\\tstop_a_enabled = 1;\\n\\t\\tpr_emerg(\"Press Stop-A (L1-A) from sun keyboard or send break\\\\n\"\\n\\t\\t\\t \"twice on console to return to the boot prom\\\\n\");\\n\\t}\\n#endif\\n#if defined(CONFIG_S390)\\n\\tdisabled_wait();\\n#endif\\n\\tpr_emerg(\"---[ end Kernel panic - not syncing: %s ]---\\\\n\", buf);\\n\\n\\t/* Do not scroll important messages printed above */\\n\\tsuppress_printk = 1;\\n\\n\\t/*\\n\\t * The final messages may not have been printed if in a context that\\n\\t * defers printing (such as NMI) and irq_work is not available.\\n\\t * Explicitly flush the kernel log buffer one last time.\\n\\t */\\n\\tconsole_flush_on_panic(CONSOLE_FLUSH_PENDING);\\n\\tnbcon_atomic_flush_unsafe();\\n\\n\\tlocal_irq_enable();\\n\\tfor (i = 0; ; i += PANIC_TIMER_STEP) {\\n\\t\\ttouch_softlockup_watchdog();\\n\\t\\tif (i >= i_next) {\\n\\t\\t\\ti += panic_blink(state ^= 1);\\n\\t\\t\\ti_next = i + 3600 / PANIC_BLINK_SPD;\\n\\t\\t}\\n\\t\\tmdelay(PANIC_TIMER_STEP);\\n\\t}\\n}\\n\\nEXPORT_SYMBOL(panic);\\n\\n#define TAINT_FLAG(taint, _c_true, _c_false, _module)\\t\\t\\t\\\\\\n\\t[ TAINT_##taint ] = {\\t\\t\\t\\t\\t\\t\\\\\\n\\t\\t.c_true = _c_true, .c_false = _c_false,\\t\\t\\t\\\\\\n\\t\\t.module = _module,\\t\\t\\t\\t\\t\\\\\\n\\t\\t.desc = #taint,\\t\\t\\t\\t\\t\\t\\\\\\n\\t}\\n\\n/*\\n * TAINT_FORCED_RMMOD could be a per-module flag but the module\\n * is being removed anyway.\\n */\\nconst struct taint_flag taint_flags[TAINT_FLAGS_COUNT] = {\\n\\tTAINT_FLAG(PROPRIETARY_MODULE,\\t\\t\\'P\\', \\'G\\', true),\\n\\tTAINT_FLAG(FORCED_MODULE,\\t\\t\\'F\\', \\' \\', true),\\n\\tTAINT_FLAG(CPU_OUT_OF_SPEC,\\t\\t\\'S\\', \\' \\', false),\\n\\tTAINT_FLAG(FORCED_RMMOD,\\t\\t\\'R\\', \\' \\', false),\\n\\tTAINT_FLAG(MACHINE_CHECK,\\t\\t\\'M\\', \\' \\', false),\\n\\tTAINT_FLAG(BAD_PAGE,\\t\\t\\t\\'B\\', \\' \\', false),\\n\\tTAINT_FLAG(USER,\\t\\t\\t\\'U\\', \\' \\', false),\\n\\tTAINT_FLAG(DIE,\\t\\t\\t\\t\\'D\\', \\' \\', false),\\n\\tTAINT_FLAG(OVERRIDDEN_ACPI_TABLE,\\t\\'A\\', \\' \\', false),\\n\\tTAINT_FLAG(WARN,\\t\\t\\t\\'W\\', \\' \\', false),\\n\\tTAINT_FLAG(CRAP,\\t\\t\\t\\'C\\', \\' \\', true),\\n\\tTAINT_FLAG(FIRMWARE_WORKAROUND,\\t\\t\\'I\\', \\' \\', false),\\n\\tTAINT_FLAG(OOT_MODULE,\\t\\t\\t\\'O\\', \\' \\', true),\\n\\tTAINT_FLAG(UNSIGNED_MODULE,\\t\\t\\'E\\', \\' \\', true),\\n\\tTAINT_FLAG(SOFTLOCKUP,\\t\\t\\t\\'L\\', \\' \\', false),\\n\\tTAINT_FLAG(LIVEPATCH,\\t\\t\\t\\'K\\', \\' \\', true),\\n\\tTAINT_FLAG(AUX,\\t\\t\\t\\t\\'X\\', \\' \\', true),\\n\\tTAINT_FLAG(RANDSTRUCT,\\t\\t\\t\\'T\\', \\' \\', true),\\n\\tTAINT_FLAG(TEST,\\t\\t\\t\\'N\\', \\' \\', true),\\n};\\n\\n#undef TAINT_FLAG\\n\\nstatic void print_tainted_seq(struct seq_buf *s, bool verbose)\\n{\\n\\tconst char *sep = \"\";\\n\\tint i;\\n\\n\\tif (!tainted_mask) {\\n\\t\\tseq_buf_puts(s, \"Not tainted\");\\n\\t\\treturn;\\n\\t}\\n\\n\\tseq_buf_printf(s, \"Tainted: \");\\n\\tfor (i = 0; i < TAINT_FLAGS_COUNT; i++) {\\n\\t\\tconst struct taint_flag *t = &taint_flags[i];\\n\\t\\tbool is_set = test_bit(i, &tainted_mask);\\n\\t\\tchar c = is_set ? t->c_true : t->c_false;\\n\\n\\t\\tif (verbose) {\\n\\t\\t\\tif (is_set) {\\n\\t\\t\\t\\tseq_buf_printf(s, \"%s[%c]=%s\", sep, c, t->desc);\\n\\t\\t\\t\\tsep = \", \";\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\tseq_buf_putc(s, c);\\n\\t\\t}\\n\\t}\\n}\\n\\nstatic const char *_print_tainted(bool verbose)\\n{\\n\\t/* FIXME: what should the size be? */\\n\\tstatic char buf[sizeof(taint_flags)];\\n\\tstruct seq_buf s;\\n\\n\\tBUILD_BUG_ON(ARRAY_SIZE(taint_flags) != TAINT_FLAGS_COUNT);\\n\\n\\tseq_buf_init(&s, buf, sizeof(buf));\\n\\n\\tprint_tainted_seq(&s, verbose);\\n\\n\\treturn seq_buf_str(&s);\\n}\\n\\n/**\\n * print_tainted - return a string to represent the kernel taint state.\\n *\\n * For individual taint flag meanings, see Documentation/admin-guide/sysctl/kernel.rst\\n *\\n * The string is overwritten by the next call to print_tainted(),\\n * but is always NULL terminated.\\n */\\nconst char *print_tainted(void)\\n{\\n\\treturn _print_tainted(false);\\n}\\n\\n/**\\n * print_tainted_verbose - A more verbose version of print_tainted()\\n */\\nconst char *print_tainted_verbose(void)\\n{\\n\\treturn _print_tainted(true);\\n}\\n\\nint test_taint(unsigned flag)\\n{\\n\\treturn test_bit(flag, &tainted_mask);\\n}\\nEXPORT_SYMBOL(test_taint);\\n\\nunsigned long get_taint(void)\\n{\\n\\treturn tainted_mask;\\n}\\n\\n/**\\n * add_taint: add a taint flag if not already set.\\n * @flag: one of the TAINT_* constants.\\n * @lockdep_ok: whether lock debugging is still OK.\\n *\\n * If something bad has gone wrong, you\\'ll want @lockdebug_ok = false, but for\\n * some notewortht-but-not-corrupting cases, it can be set to true.\\n */\\nvoid add_taint(unsigned flag, enum lockdep_ok lockdep_ok)\\n{\\n\\tif (lockdep_ok == LOCKDEP_NOW_UNRELIABLE && __debug_locks_off())\\n\\t\\tpr_warn(\"Disabling lock debugging due to kernel taint\\\\n\");\\n\\n\\tset_bit(flag, &tainted_mask);\\n\\n\\tif (tainted_mask & panic_on_taint) {\\n\\t\\tpanic_on_taint = 0;\\n\\t\\tpanic(\"panic_on_taint set ...\");\\n\\t}\\n}\\nEXPORT_SYMBOL(add_taint);\\n\\nstatic void spin_msec(int msecs)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < msecs; i++) {\\n\\t\\ttouch_nmi_watchdog();\\n\\t\\tmdelay(1);\\n\\t}\\n}\\n\\n/*\\n * It just happens that oops_enter() and oops_exit() are identically\\n * implemented...\\n */\\nstatic void do_oops_enter_exit(void)\\n{\\n\\tunsigned long flags;\\n\\tstatic int spin_counter;\\n\\n\\tif (!pause_on_oops)\\n\\t\\treturn;\\n\\n\\tspin_lock_irqsave(&pause_on_oops_lock, flags);\\n\\tif (pause_on_oops_flag == 0) {\\n\\t\\t/* This CPU may now print the oops message */\\n\\t\\tpause_on_oops_flag = 1;\\n\\t} else {\\n\\t\\t/* We need to stall this CPU */\\n\\t\\tif (!spin_counter) {\\n\\t\\t\\t/* This CPU gets to do the counting */\\n\\t\\t\\tspin_counter = pause_on_oops;\\n\\t\\t\\tdo {\\n\\t\\t\\t\\tspin_unlock(&pause_on_oops_lock);\\n\\t\\t\\t\\tspin_msec(MSEC_PER_SEC);\\n\\t\\t\\t\\tspin_lock(&pause_on_oops_lock);\\n\\t\\t\\t} while (--spin_counter);\\n\\t\\t\\tpause_on_oops_flag = 0;\\n\\t\\t} else {\\n\\t\\t\\t/* This CPU waits for a different one */\\n\\t\\t\\twhile (spin_counter) {\\n\\t\\t\\t\\tspin_unlock(&pause_on_oops_lock);\\n\\t\\t\\t\\tspin_msec(1);\\n\\t\\t\\t\\tspin_lock(&pause_on_oops_lock);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\tspin_unlock_irqrestore(&pause_on_oops_lock, flags);\\n}\\n\\n/*\\n * Return true if the calling CPU is allowed to print oops-related info.\\n * This is a bit racy..\\n */\\nbool oops_may_print(void)\\n{\\n\\treturn pause_on_oops_flag == 0;\\n}\\n\\n/*\\n * Called when the architecture enters its oops handler, before it prints\\n * anything.  If this is the first CPU to oops, and it\\'s oopsing the first\\n * time then let it proceed.\\n *\\n * This is all enabled by the pause_on_oops kernel boot option.  We do all\\n * this to ensure that oopses don\\'t scroll off the screen.  It has the\\n * side-effect of preventing later-oopsing CPUs from mucking up the display,\\n * too.\\n *\\n * It turns out that the CPU which is allowed to print ends up pausing for\\n * the right duration, whereas all the other CPUs pause for twice as long:\\n * once in oops_enter(), once in oops_exit().\\n */\\nvoid oops_enter(void)\\n{\\n\\tnbcon_cpu_emergency_enter();\\n\\ttracing_off();\\n\\t/* can\\'t trust the integrity of the kernel anymore: */\\n\\tdebug_locks_off();\\n\\tdo_oops_enter_exit();\\n\\n\\tif (sysctl_oops_all_cpu_backtrace)\\n\\t\\ttrigger_all_cpu_backtrace();\\n}\\n\\nstatic void print_oops_end_marker(void)\\n{\\n\\tpr_warn(\"---[ end trace %016llx ]---\\\\n\", 0ULL);\\n}\\n\\n/*\\n * Called when the architecture exits its oops handler, after printing\\n * everything.\\n */\\nvoid oops_exit(void)\\n{\\n\\tdo_oops_enter_exit();\\n\\tprint_oops_end_marker();\\n\\tnbcon_cpu_emergency_exit();\\n\\tkmsg_dump(KMSG_DUMP_OOPS);\\n}\\n\\nstruct warn_args {\\n\\tconst char *fmt;\\n\\tva_list args;\\n};\\n\\nvoid __warn(const char *file, int line, void *caller, unsigned taint,\\n\\t    struct pt_regs *regs, struct warn_args *args)\\n{\\n\\tnbcon_cpu_emergency_enter();\\n\\n\\tdisable_trace_on_warning();\\n\\n\\tif (file)\\n\\t\\tpr_warn(\"WARNING: CPU: %d PID: %d at %s:%d %pS\\\\n\",\\n\\t\\t\\traw_smp_processor_id(), current->pid, file, line,\\n\\t\\t\\tcaller);\\n\\telse\\n\\t\\tpr_warn(\"WARNING: CPU: %d PID: %d at %pS\\\\n\",\\n\\t\\t\\traw_smp_processor_id(), current->pid, caller);\\n\\n#pragma GCC diagnostic push\\n#ifndef __clang__\\n#pragma GCC diagnostic ignored \"-Wsuggest-attribute=format\"\\n#endif\\n\\tif (args)\\n\\t\\tvprintk(args->fmt, args->args);\\n#pragma GCC diagnostic pop\\n\\n\\tprint_modules();\\n\\n\\tif (regs)\\n\\t\\tshow_regs(regs);\\n\\n\\tcheck_panic_on_warn(\"kernel\");\\n\\n\\tif (!regs)\\n\\t\\tdump_stack();\\n\\n\\tprint_irqtrace_events(current);\\n\\n\\tprint_oops_end_marker();\\n\\ttrace_error_report_end(ERROR_DETECTOR_WARN, (unsigned long)caller);\\n\\n\\t/* Just a warning, don\\'t kill lockdep. */\\n\\tadd_taint(taint, LOCKDEP_STILL_OK);\\n\\n\\tnbcon_cpu_emergency_exit();\\n}\\n\\n#ifdef CONFIG_BUG\\n#ifndef __WARN_FLAGS\\nvoid warn_slowpath_fmt(const char *file, int line, unsigned taint,\\n\\t\\t       const char *fmt, ...)\\n{\\n\\tbool rcu = warn_rcu_enter();\\n\\tstruct warn_args args;\\n\\n\\tpr_warn(CUT_HERE);\\n\\n\\tif (!fmt) {\\n\\t\\t__warn(file, line, __builtin_return_address(0), taint,\\n\\t\\t       NULL, NULL);\\n\\t\\twarn_rcu_exit(rcu);\\n\\t\\treturn;\\n\\t}\\n\\n\\targs.fmt = fmt;\\n\\tva_start(args.args, fmt);\\n\\t__warn(file, line, __builtin_return_address(0), taint, NULL, &args);\\n\\tva_end(args.args);\\n\\twarn_rcu_exit(rcu);\\n}\\nEXPORT_SYMBOL(warn_slowpath_fmt);\\n#else\\nvoid __warn_printk(const char *fmt, ...)\\n{\\n\\tbool rcu = warn_rcu_enter();\\n\\tva_list args;\\n\\n\\tpr_warn(CUT_HERE);\\n\\n\\tva_start(args, fmt);\\n\\tvprintk(fmt, args);\\n\\tva_end(args);\\n\\twarn_rcu_exit(rcu);\\n}\\nEXPORT_SYMBOL(__warn_printk);\\n#endif\\n\\n/* Support resetting WARN*_ONCE state */\\n\\nstatic int clear_warn_once_set(void *data, u64 val)\\n{\\n\\tgeneric_bug_clear_once();\\n\\tmemset(__start_once, 0, __end_once - __start_once);\\n\\treturn 0;\\n}\\n\\nDEFINE_DEBUGFS_ATTRIBUTE(clear_warn_once_fops, NULL, clear_warn_once_set,\\n\\t\\t\\t \"%lld\\\\n\");\\n\\nstatic __init int register_warn_debugfs(void)\\n{\\n\\t/* Don\\'t care about failure */\\n\\tdebugfs_create_file_unsafe(\"clear_warn_once\", 0200, NULL, NULL,\\n\\t\\t\\t\\t   &clear_warn_once_fops);\\n\\treturn 0;\\n}\\n\\ndevice_initcall(register_warn_debugfs);\\n#endif\\n\\n#ifdef CONFIG_STACKPROTECTOR\\n\\n/*\\n * Called when gcc\\'s -fstack-protector feature is used, and\\n * gcc detects corruption of the on-stack canary value\\n */\\n__visible noinstr void __stack_chk_fail(void)\\n{\\n\\tinstrumentation_begin();\\n\\tpanic(\"stack-protector: Kernel stack is corrupted in: %pB\",\\n\\t\\t__builtin_return_address(0));\\n\\tinstrumentation_end();\\n}\\nEXPORT_SYMBOL(__stack_chk_fail);\\n\\n#endif\\n\\ncore_param(panic, panic_timeout, int, 0644);\\ncore_param(panic_print, panic_print, ulong, 0644);\\ncore_param(pause_on_oops, pause_on_oops, int, 0644);\\ncore_param(panic_on_warn, panic_on_warn, int, 0644);\\ncore_param(crash_kexec_post_notifiers, crash_kexec_post_notifiers, bool, 0644);\\n\\nstatic int __init oops_setup(char *s)\\n{\\n\\tif (!s)\\n\\t\\treturn -EINVAL;\\n\\tif (!strcmp(s, \"panic\"))\\n\\t\\tpanic_on_oops = 1;\\n\\treturn 0;\\n}\\nearly_param(\"oops\", oops_setup);\\n\\nstatic int __init panic_on_taint_setup(char *s)\\n{\\n\\tchar *taint_str;\\n\\n\\tif (!s)\\n\\t\\treturn -EINVAL;\\n\\n\\ttaint_str = strsep(&s, \",\");\\n\\tif (kstrtoul(taint_str, 16, &panic_on_taint))\\n\\t\\treturn -EINVAL;\\n\\n\\t/* make sure panic_on_taint doesn\\'t hold out-of-range TAINT flags */\\n\\tpanic_on_taint &= TAINT_FLAGS_MAX;\\n\\n\\tif (!panic_on_taint)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (s && !strcmp(s, \"nousertaint\"))\\n\\t\\tpanic_on_taint_nousertaint = true;\\n\\n\\tpr_info(\"panic_on_taint: bitmask=0x%lx nousertaint_mode=%s\\\\n\",\\n\\t\\tpanic_on_taint, str_enabled_disabled(panic_on_taint_nousertaint));\\n\\n\\treturn 0;\\n}\\nearly_param(\"panic_on_taint\", panic_on_taint_setup);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n#include <linux/export.h>\\n#include <linux/slab.h>\\n#include <linux/regset.h>\\n\\nstatic int __regset_get(struct task_struct *target,\\n\\t\\t\\tconst struct user_regset *regset,\\n\\t\\t\\tunsigned int size,\\n\\t\\t\\tvoid **data)\\n{\\n\\tvoid *p = *data, *to_free = NULL;\\n\\tint res;\\n\\n\\tif (!regset->regset_get)\\n\\t\\treturn -EOPNOTSUPP;\\n\\tif (size > regset->n * regset->size)\\n\\t\\tsize = regset->n * regset->size;\\n\\tif (!p) {\\n\\t\\tto_free = p = kvzalloc(size, GFP_KERNEL);\\n\\t\\tif (!p)\\n\\t\\t\\treturn -ENOMEM;\\n\\t}\\n\\tres = regset->regset_get(target, regset,\\n\\t\\t\\t   (struct membuf){.p = p, .left = size});\\n\\tif (res < 0) {\\n\\t\\tkvfree(to_free);\\n\\t\\treturn res;\\n\\t}\\n\\t*data = p;\\n\\treturn size - res;\\n}\\n\\nint regset_get(struct task_struct *target,\\n\\t       const struct user_regset *regset,\\n\\t       unsigned int size,\\n\\t       void *data)\\n{\\n\\treturn __regset_get(target, regset, size, &data);\\n}\\nEXPORT_SYMBOL(regset_get);\\n\\nint regset_get_alloc(struct task_struct *target,\\n\\t\\t     const struct user_regset *regset,\\n\\t\\t     unsigned int size,\\n\\t\\t     void **data)\\n{\\n\\t*data = NULL;\\n\\treturn __regset_get(target, regset, size, data);\\n}\\nEXPORT_SYMBOL(regset_get_alloc);\\n\\n/**\\n * copy_regset_to_user - fetch a thread\\'s user_regset data into user memory\\n * @target:\\tthread to be examined\\n * @view:\\t&struct user_regset_view describing user thread machine state\\n * @setno:\\tindex in @view->regsets\\n * @offset:\\toffset into the regset data, in bytes\\n * @size:\\tamount of data to copy, in bytes\\n * @data:\\tuser-mode pointer to copy into\\n */\\nint copy_regset_to_user(struct task_struct *target,\\n\\t\\t\\tconst struct user_regset_view *view,\\n\\t\\t\\tunsigned int setno,\\n\\t\\t\\tunsigned int offset, unsigned int size,\\n\\t\\t\\tvoid __user *data)\\n{\\n\\tconst struct user_regset *regset = &view->regsets[setno];\\n\\tvoid *buf;\\n\\tint ret;\\n\\n\\tret = regset_get_alloc(target, regset, size, &buf);\\n\\tif (ret > 0)\\n\\t\\tret = copy_to_user(data, buf, ret) ? -EFAULT : 0;\\n\\tkvfree(buf);\\n\\treturn ret;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * kernel/stacktrace.c\\n *\\n * Stack trace management functions\\n *\\n *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>\\n */\\n#include <linux/sched/task_stack.h>\\n#include <linux/sched/debug.h>\\n#include <linux/sched.h>\\n#include <linux/kernel.h>\\n#include <linux/export.h>\\n#include <linux/kallsyms.h>\\n#include <linux/stacktrace.h>\\n#include <linux/interrupt.h>\\n\\n/**\\n * stack_trace_print - Print the entries in the stack trace\\n * @entries:\\tPointer to storage array\\n * @nr_entries:\\tNumber of entries in the storage array\\n * @spaces:\\tNumber of leading spaces to print\\n */\\nvoid stack_trace_print(const unsigned long *entries, unsigned int nr_entries,\\n\\t\\t       int spaces)\\n{\\n\\tunsigned int i;\\n\\n\\tif (WARN_ON(!entries))\\n\\t\\treturn;\\n\\n\\tfor (i = 0; i < nr_entries; i++)\\n\\t\\tprintk(\"%*c%pS\\\\n\", 1 + spaces, \\' \\', (void *)entries[i]);\\n}\\nEXPORT_SYMBOL_GPL(stack_trace_print);\\n\\n/**\\n * stack_trace_snprint - Print the entries in the stack trace into a buffer\\n * @buf:\\tPointer to the print buffer\\n * @size:\\tSize of the print buffer\\n * @entries:\\tPointer to storage array\\n * @nr_entries:\\tNumber of entries in the storage array\\n * @spaces:\\tNumber of leading spaces to print\\n *\\n * Return: Number of bytes printed.\\n */\\nint stack_trace_snprint(char *buf, size_t size, const unsigned long *entries,\\n\\t\\t\\tunsigned int nr_entries, int spaces)\\n{\\n\\tunsigned int generated, i, total = 0;\\n\\n\\tif (WARN_ON(!entries))\\n\\t\\treturn 0;\\n\\n\\tfor (i = 0; i < nr_entries && size; i++) {\\n\\t\\tgenerated = snprintf(buf, size, \"%*c%pS\\\\n\", 1 + spaces, \\' \\',\\n\\t\\t\\t\\t     (void *)entries[i]);\\n\\n\\t\\ttotal += generated;\\n\\t\\tif (generated >= size) {\\n\\t\\t\\tbuf += size;\\n\\t\\t\\tsize = 0;\\n\\t\\t} else {\\n\\t\\t\\tbuf += generated;\\n\\t\\t\\tsize -= generated;\\n\\t\\t}\\n\\t}\\n\\n\\treturn total;\\n}\\nEXPORT_SYMBOL_GPL(stack_trace_snprint);\\n\\n#ifdef CONFIG_ARCH_STACKWALK\\n\\nstruct stacktrace_cookie {\\n\\tunsigned long\\t*store;\\n\\tunsigned int\\tsize;\\n\\tunsigned int\\tskip;\\n\\tunsigned int\\tlen;\\n};\\n\\nstatic bool stack_trace_consume_entry(void *cookie, unsigned long addr)\\n{\\n\\tstruct stacktrace_cookie *c = cookie;\\n\\n\\tif (c->len >= c->size)\\n\\t\\treturn false;\\n\\n\\tif (c->skip > 0) {\\n\\t\\tc->skip--;\\n\\t\\treturn true;\\n\\t}\\n\\tc->store[c->len++] = addr;\\n\\treturn c->len < c->size;\\n}\\n\\nstatic bool stack_trace_consume_entry_nosched(void *cookie, unsigned long addr)\\n{\\n\\tif (in_sched_functions(addr))\\n\\t\\treturn true;\\n\\treturn stack_trace_consume_entry(cookie, addr);\\n}\\n\\n/**\\n * stack_trace_save - Save a stack trace into a storage array\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n * @skipnr:\\tNumber of entries to skip at the start of the stack trace\\n *\\n * Return: Number of trace entries stored.\\n */\\nunsigned int stack_trace_save(unsigned long *store, unsigned int size,\\n\\t\\t\\t      unsigned int skipnr)\\n{\\n\\tstack_trace_consume_fn consume_entry = stack_trace_consume_entry;\\n\\tstruct stacktrace_cookie c = {\\n\\t\\t.store\\t= store,\\n\\t\\t.size\\t= size,\\n\\t\\t.skip\\t= skipnr + 1,\\n\\t};\\n\\n\\tarch_stack_walk(consume_entry, &c, current, NULL);\\n\\treturn c.len;\\n}\\nEXPORT_SYMBOL_GPL(stack_trace_save);\\n\\n/**\\n * stack_trace_save_tsk - Save a task stack trace into a storage array\\n * @tsk:\\tThe task to examine\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n * @skipnr:\\tNumber of entries to skip at the start of the stack trace\\n *\\n * Return: Number of trace entries stored.\\n */\\nunsigned int stack_trace_save_tsk(struct task_struct *tsk, unsigned long *store,\\n\\t\\t\\t\\t  unsigned int size, unsigned int skipnr)\\n{\\n\\tstack_trace_consume_fn consume_entry = stack_trace_consume_entry_nosched;\\n\\tstruct stacktrace_cookie c = {\\n\\t\\t.store\\t= store,\\n\\t\\t.size\\t= size,\\n\\t\\t/* skip this function if they are tracing us */\\n\\t\\t.skip\\t= skipnr + (current == tsk),\\n\\t};\\n\\n\\tif (!try_get_task_stack(tsk))\\n\\t\\treturn 0;\\n\\n\\tarch_stack_walk(consume_entry, &c, tsk, NULL);\\n\\tput_task_stack(tsk);\\n\\treturn c.len;\\n}\\nEXPORT_SYMBOL_GPL(stack_trace_save_tsk);\\n\\n/**\\n * stack_trace_save_regs - Save a stack trace based on pt_regs into a storage array\\n * @regs:\\tPointer to pt_regs to examine\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n * @skipnr:\\tNumber of entries to skip at the start of the stack trace\\n *\\n * Return: Number of trace entries stored.\\n */\\nunsigned int stack_trace_save_regs(struct pt_regs *regs, unsigned long *store,\\n\\t\\t\\t\\t   unsigned int size, unsigned int skipnr)\\n{\\n\\tstack_trace_consume_fn consume_entry = stack_trace_consume_entry;\\n\\tstruct stacktrace_cookie c = {\\n\\t\\t.store\\t= store,\\n\\t\\t.size\\t= size,\\n\\t\\t.skip\\t= skipnr,\\n\\t};\\n\\n\\tarch_stack_walk(consume_entry, &c, current, regs);\\n\\treturn c.len;\\n}\\n\\n#ifdef CONFIG_HAVE_RELIABLE_STACKTRACE\\n/**\\n * stack_trace_save_tsk_reliable - Save task stack with verification\\n * @tsk:\\tPointer to the task to examine\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n *\\n * Return:\\tAn error if it detects any unreliable features of the\\n *\\t\\tstack. Otherwise it guarantees that the stack trace is\\n *\\t\\treliable and returns the number of entries stored.\\n *\\n * If the task is not \\'current\\', the caller *must* ensure the task is inactive.\\n */\\nint stack_trace_save_tsk_reliable(struct task_struct *tsk, unsigned long *store,\\n\\t\\t\\t\\t  unsigned int size)\\n{\\n\\tstack_trace_consume_fn consume_entry = stack_trace_consume_entry;\\n\\tstruct stacktrace_cookie c = {\\n\\t\\t.store\\t= store,\\n\\t\\t.size\\t= size,\\n\\t};\\n\\tint ret;\\n\\n\\t/*\\n\\t * If the task doesn\\'t have a stack (e.g., a zombie), the stack is\\n\\t * \"reliably\" empty.\\n\\t */\\n\\tif (!try_get_task_stack(tsk))\\n\\t\\treturn 0;\\n\\n\\tret = arch_stack_walk_reliable(consume_entry, &c, tsk);\\n\\tput_task_stack(tsk);\\n\\treturn ret ? ret : c.len;\\n}\\n#endif\\n\\n#ifdef CONFIG_USER_STACKTRACE_SUPPORT\\n/**\\n * stack_trace_save_user - Save a user space stack trace into a storage array\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n *\\n * Return: Number of trace entries stored.\\n */\\nunsigned int stack_trace_save_user(unsigned long *store, unsigned int size)\\n{\\n\\tstack_trace_consume_fn consume_entry = stack_trace_consume_entry;\\n\\tstruct stacktrace_cookie c = {\\n\\t\\t.store\\t= store,\\n\\t\\t.size\\t= size,\\n\\t};\\n\\n\\t/* Trace user stack if not a kernel thread */\\n\\tif (current->flags & PF_KTHREAD)\\n\\t\\treturn 0;\\n\\n\\tarch_stack_walk_user(consume_entry, &c, task_pt_regs(current));\\n\\n\\treturn c.len;\\n}\\n#endif\\n\\n#else /* CONFIG_ARCH_STACKWALK */\\n\\n/*\\n * Architectures that do not implement save_stack_trace_*()\\n * get these weak aliases and once-per-bootup warnings\\n * (whenever this facility is utilized - for example by procfs):\\n */\\n__weak void\\nsave_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)\\n{\\n\\tWARN_ONCE(1, KERN_INFO \"save_stack_trace_tsk() not implemented yet.\\\\n\");\\n}\\n\\n__weak void\\nsave_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)\\n{\\n\\tWARN_ONCE(1, KERN_INFO \"save_stack_trace_regs() not implemented yet.\\\\n\");\\n}\\n\\n/**\\n * stack_trace_save - Save a stack trace into a storage array\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n * @skipnr:\\tNumber of entries to skip at the start of the stack trace\\n *\\n * Return: Number of trace entries stored\\n */\\nunsigned int stack_trace_save(unsigned long *store, unsigned int size,\\n\\t\\t\\t      unsigned int skipnr)\\n{\\n\\tstruct stack_trace trace = {\\n\\t\\t.entries\\t= store,\\n\\t\\t.max_entries\\t= size,\\n\\t\\t.skip\\t\\t= skipnr + 1,\\n\\t};\\n\\n\\tsave_stack_trace(&trace);\\n\\treturn trace.nr_entries;\\n}\\nEXPORT_SYMBOL_GPL(stack_trace_save);\\n\\n/**\\n * stack_trace_save_tsk - Save a task stack trace into a storage array\\n * @task:\\tThe task to examine\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n * @skipnr:\\tNumber of entries to skip at the start of the stack trace\\n *\\n * Return: Number of trace entries stored\\n */\\nunsigned int stack_trace_save_tsk(struct task_struct *task,\\n\\t\\t\\t\\t  unsigned long *store, unsigned int size,\\n\\t\\t\\t\\t  unsigned int skipnr)\\n{\\n\\tstruct stack_trace trace = {\\n\\t\\t.entries\\t= store,\\n\\t\\t.max_entries\\t= size,\\n\\t\\t/* skip this function if they are tracing us */\\n\\t\\t.skip\\t= skipnr + (current == task),\\n\\t};\\n\\n\\tsave_stack_trace_tsk(task, &trace);\\n\\treturn trace.nr_entries;\\n}\\nEXPORT_SYMBOL_GPL(stack_trace_save_tsk);\\n\\n/**\\n * stack_trace_save_regs - Save a stack trace based on pt_regs into a storage array\\n * @regs:\\tPointer to pt_regs to examine\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n * @skipnr:\\tNumber of entries to skip at the start of the stack trace\\n *\\n * Return: Number of trace entries stored\\n */\\nunsigned int stack_trace_save_regs(struct pt_regs *regs, unsigned long *store,\\n\\t\\t\\t\\t   unsigned int size, unsigned int skipnr)\\n{\\n\\tstruct stack_trace trace = {\\n\\t\\t.entries\\t= store,\\n\\t\\t.max_entries\\t= size,\\n\\t\\t.skip\\t\\t= skipnr,\\n\\t};\\n\\n\\tsave_stack_trace_regs(regs, &trace);\\n\\treturn trace.nr_entries;\\n}\\n\\n#ifdef CONFIG_HAVE_RELIABLE_STACKTRACE\\n/**\\n * stack_trace_save_tsk_reliable - Save task stack with verification\\n * @tsk:\\tPointer to the task to examine\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n *\\n * Return:\\tAn error if it detects any unreliable features of the\\n *\\t\\tstack. Otherwise it guarantees that the stack trace is\\n *\\t\\treliable and returns the number of entries stored.\\n *\\n * If the task is not \\'current\\', the caller *must* ensure the task is inactive.\\n */\\nint stack_trace_save_tsk_reliable(struct task_struct *tsk, unsigned long *store,\\n\\t\\t\\t\\t  unsigned int size)\\n{\\n\\tstruct stack_trace trace = {\\n\\t\\t.entries\\t= store,\\n\\t\\t.max_entries\\t= size,\\n\\t};\\n\\tint ret = save_stack_trace_tsk_reliable(tsk, &trace);\\n\\n\\treturn ret ? ret : trace.nr_entries;\\n}\\n#endif\\n\\n#ifdef CONFIG_USER_STACKTRACE_SUPPORT\\n/**\\n * stack_trace_save_user - Save a user space stack trace into a storage array\\n * @store:\\tPointer to storage array\\n * @size:\\tSize of the storage array\\n *\\n * Return: Number of trace entries stored\\n */\\nunsigned int stack_trace_save_user(unsigned long *store, unsigned int size)\\n{\\n\\tstruct stack_trace trace = {\\n\\t\\t.entries\\t= store,\\n\\t\\t.max_entries\\t= size,\\n\\t};\\n\\n\\tsave_stack_trace_user(&trace);\\n\\treturn trace.nr_entries;\\n}\\n#endif /* CONFIG_USER_STACKTRACE_SUPPORT */\\n\\n#endif /* !CONFIG_ARCH_STACKWALK */\\n\\nstatic inline bool in_irqentry_text(unsigned long ptr)\\n{\\n\\treturn (ptr >= (unsigned long)&__irqentry_text_start &&\\n\\t\\tptr < (unsigned long)&__irqentry_text_end) ||\\n\\t\\t(ptr >= (unsigned long)&__softirqentry_text_start &&\\n\\t\\t ptr < (unsigned long)&__softirqentry_text_end);\\n}\\n\\n/**\\n * filter_irq_stacks - Find first IRQ stack entry in trace\\n * @entries:\\tPointer to stack trace array\\n * @nr_entries:\\tNumber of entries in the storage array\\n *\\n * Return: Number of trace entries until IRQ stack starts.\\n */\\nunsigned int filter_irq_stacks(unsigned long *entries, unsigned int nr_entries)\\n{\\n\\tunsigned int i;\\n\\n\\tfor (i = 0; i < nr_entries; i++) {\\n\\t\\tif (in_irqentry_text(entries[i])) {\\n\\t\\t\\t/* Include the irqentry function into the stack. */\\n\\t\\t\\treturn i + 1;\\n\\t\\t}\\n\\t}\\n\\treturn nr_entries;\\n}\\nEXPORT_SYMBOL_GPL(filter_irq_stacks);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * This code fills the used part of the kernel stack with a poison value\\n * before returning to userspace. It\\'s part of the STACKLEAK feature\\n * ported from grsecurity/PaX.\\n *\\n * Author: Alexander Popov <alex.popov@linux.com>\\n *\\n * STACKLEAK reduces the information which kernel stack leak bugs can\\n * reveal and blocks some uninitialized stack variable attacks.\\n */\\n\\n#include <linux/stackleak.h>\\n#include <linux/kprobes.h>\\n\\n#ifdef CONFIG_STACKLEAK_RUNTIME_DISABLE\\n#include <linux/jump_label.h>\\n#include <linux/sysctl.h>\\n#include <linux/init.h>\\n\\nstatic DEFINE_STATIC_KEY_FALSE(stack_erasing_bypass);\\n\\n#ifdef CONFIG_SYSCTL\\nstatic int stack_erasing_sysctl(const struct ctl_table *table, int write,\\n\\t\\t\\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tint ret = 0;\\n\\tint state = !static_branch_unlikely(&stack_erasing_bypass);\\n\\tint prev_state = state;\\n\\tstruct ctl_table table_copy = *table;\\n\\n\\ttable_copy.data = &state;\\n\\tret = proc_dointvec_minmax(&table_copy, write, buffer, lenp, ppos);\\n\\tstate = !!state;\\n\\tif (ret || !write || state == prev_state)\\n\\t\\treturn ret;\\n\\n\\tif (state)\\n\\t\\tstatic_branch_disable(&stack_erasing_bypass);\\n\\telse\\n\\t\\tstatic_branch_enable(&stack_erasing_bypass);\\n\\n\\tpr_warn(\"stackleak: kernel stack erasing is %s\\\\n\",\\n\\t\\t\\t\\t\\tstate ? \"enabled\" : \"disabled\");\\n\\treturn ret;\\n}\\nstatic struct ctl_table stackleak_sysctls[] = {\\n\\t{\\n\\t\\t.procname\\t= \"stack_erasing\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= stack_erasing_sysctl,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n};\\n\\nstatic int __init stackleak_sysctls_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", stackleak_sysctls);\\n\\treturn 0;\\n}\\nlate_initcall(stackleak_sysctls_init);\\n#endif /* CONFIG_SYSCTL */\\n\\n#define skip_erasing()\\tstatic_branch_unlikely(&stack_erasing_bypass)\\n#else\\n#define skip_erasing()\\tfalse\\n#endif /* CONFIG_STACKLEAK_RUNTIME_DISABLE */\\n\\n#ifndef __stackleak_poison\\nstatic __always_inline void __stackleak_poison(unsigned long erase_low,\\n\\t\\t\\t\\t\\t       unsigned long erase_high,\\n\\t\\t\\t\\t\\t       unsigned long poison)\\n{\\n\\twhile (erase_low < erase_high) {\\n\\t\\t*(unsigned long *)erase_low = poison;\\n\\t\\terase_low += sizeof(unsigned long);\\n\\t}\\n}\\n#endif\\n\\nstatic __always_inline void __stackleak_erase(bool on_task_stack)\\n{\\n\\tconst unsigned long task_stack_low = stackleak_task_low_bound(current);\\n\\tconst unsigned long task_stack_high = stackleak_task_high_bound(current);\\n\\tunsigned long erase_low, erase_high;\\n\\n\\terase_low = stackleak_find_top_of_poison(task_stack_low,\\n\\t\\t\\t\\t\\t\\t current->lowest_stack);\\n\\n#ifdef CONFIG_STACKLEAK_METRICS\\n\\tcurrent->prev_lowest_stack = erase_low;\\n#endif\\n\\n\\t/*\\n\\t * Write poison to the task\\'s stack between \\'erase_low\\' and\\n\\t * \\'erase_high\\'.\\n\\t *\\n\\t * If we\\'re running on a different stack (e.g. an entry trampoline\\n\\t * stack) we can erase everything below the pt_regs at the top of the\\n\\t * task stack.\\n\\t *\\n\\t * If we\\'re running on the task stack itself, we must not clobber any\\n\\t * stack used by this function and its caller. We assume that this\\n\\t * function has a fixed-size stack frame, and the current stack pointer\\n\\t * doesn\\'t change while we write poison.\\n\\t */\\n\\tif (on_task_stack)\\n\\t\\terase_high = current_stack_pointer;\\n\\telse\\n\\t\\terase_high = task_stack_high;\\n\\n\\t__stackleak_poison(erase_low, erase_high, STACKLEAK_POISON);\\n\\n\\t/* Reset the \\'lowest_stack\\' value for the next syscall */\\n\\tcurrent->lowest_stack = task_stack_high;\\n}\\n\\n/*\\n * Erase and poison the portion of the task stack used since the last erase.\\n * Can be called from the task stack or an entry stack when the task stack is\\n * no longer in use.\\n */\\nasmlinkage void noinstr stackleak_erase(void)\\n{\\n\\tif (skip_erasing())\\n\\t\\treturn;\\n\\n\\t__stackleak_erase(on_thread_stack());\\n}\\n\\n/*\\n * Erase and poison the portion of the task stack used since the last erase.\\n * Can only be called from the task stack.\\n */\\nasmlinkage void noinstr stackleak_erase_on_task_stack(void)\\n{\\n\\tif (skip_erasing())\\n\\t\\treturn;\\n\\n\\t__stackleak_erase(true);\\n}\\n\\n/*\\n * Erase and poison the portion of the task stack used since the last erase.\\n * Can only be called from a stack other than the task stack.\\n */\\nasmlinkage void noinstr stackleak_erase_off_task_stack(void)\\n{\\n\\tif (skip_erasing())\\n\\t\\treturn;\\n\\n\\t__stackleak_erase(false);\\n}\\n\\nvoid __used __no_caller_saved_registers noinstr stackleak_track_stack(void)\\n{\\n\\tunsigned long sp = current_stack_pointer;\\n\\n\\t/*\\n\\t * Having CONFIG_STACKLEAK_TRACK_MIN_SIZE larger than\\n\\t * STACKLEAK_SEARCH_DEPTH makes the poison search in\\n\\t * stackleak_erase() unreliable. Let\\'s prevent that.\\n\\t */\\n\\tBUILD_BUG_ON(CONFIG_STACKLEAK_TRACK_MIN_SIZE > STACKLEAK_SEARCH_DEPTH);\\n\\n\\t/* \\'lowest_stack\\' should be aligned on the register width boundary */\\n\\tsp = ALIGN(sp, sizeof(unsigned long));\\n\\tif (sp < current->lowest_stack &&\\n\\t    sp >= stackleak_task_low_bound(current)) {\\n\\t\\tcurrent->lowest_stack = sp;\\n\\t}\\n}\\nEXPORT_SYMBOL(stackleak_track_stack);\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/*\\n * tsacct.c - System accounting over taskstats interface\\n *\\n * Copyright (C) Jay Lan,\\t<jlan@sgi.com>\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/sched/signal.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/tsacct_kern.h>\\n#include <linux/acct.h>\\n#include <linux/jiffies.h>\\n#include <linux/mm.h>\\n\\n/*\\n * fill in basic accounting fields\\n */\\nvoid bacct_add_tsk(struct user_namespace *user_ns,\\n\\t\\t   struct pid_namespace *pid_ns,\\n\\t\\t   struct taskstats *stats, struct task_struct *tsk)\\n{\\n\\tconst struct cred *tcred;\\n\\tu64 utime, stime, utimescaled, stimescaled;\\n\\tu64 now_ns, delta;\\n\\ttime64_t btime;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* calculate task elapsed time in nsec */\\n\\tnow_ns = ktime_get_ns();\\n\\t/* store whole group time first */\\n\\tdelta = now_ns - tsk->group_leader->start_time;\\n\\t/* Convert to micro seconds */\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tstats->ac_tgetime = delta;\\n\\tdelta = now_ns - tsk->start_time;\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tstats->ac_etime = delta;\\n\\t/* Convert to seconds for btime (note y2106 limit) */\\n\\tbtime = ktime_get_real_seconds() - div_u64(delta, USEC_PER_SEC);\\n\\tstats->ac_btime = clamp_t(time64_t, btime, 0, U32_MAX);\\n\\tstats->ac_btime64 = btime;\\n\\n\\tif (tsk->flags & PF_EXITING)\\n\\t\\tstats->ac_exitcode = tsk->exit_code;\\n\\tif (thread_group_leader(tsk) && (tsk->flags & PF_FORKNOEXEC))\\n\\t\\tstats->ac_flag |= AFORK;\\n\\tif (tsk->flags & PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (tsk->flags & PF_DUMPCORE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif (tsk->flags & PF_SIGNALED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tstats->ac_nice\\t = task_nice(tsk);\\n\\tstats->ac_sched\\t = tsk->policy;\\n\\tstats->ac_pid\\t = task_pid_nr_ns(tsk, pid_ns);\\n\\tstats->ac_tgid   = task_tgid_nr_ns(tsk, pid_ns);\\n\\trcu_read_lock();\\n\\ttcred = __task_cred(tsk);\\n\\tstats->ac_uid\\t = from_kuid_munged(user_ns, tcred->uid);\\n\\tstats->ac_gid\\t = from_kgid_munged(user_ns, tcred->gid);\\n\\tstats->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_dereference(tsk->real_parent), pid_ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\tstats->ac_utime = div_u64(utime, NSEC_PER_USEC);\\n\\tstats->ac_stime = div_u64(stime, NSEC_PER_USEC);\\n\\n\\ttask_cputime_scaled(tsk, &utimescaled, &stimescaled);\\n\\tstats->ac_utimescaled = div_u64(utimescaled, NSEC_PER_USEC);\\n\\tstats->ac_stimescaled = div_u64(stimescaled, NSEC_PER_USEC);\\n\\n\\tstats->ac_minflt = tsk->min_flt;\\n\\tstats->ac_majflt = tsk->maj_flt;\\n\\n\\tstrscpy_pad(stats->ac_comm, tsk->comm);\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#define KB 1024\\n#define MB (1024*KB)\\n#define KB_MASK (~(KB-1))\\n/*\\n * fill in extended accounting fields\\n */\\nvoid xacct_add_tsk(struct taskstats *stats, struct task_struct *p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* convert pages-nsec/1024 to Mbyte-usec, see __acct_update_integrals */\\n\\tstats->coremem = p->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->coremem, 1000 * KB);\\n\\tstats->virtmem = p->acct_vm_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->virtmem, 1000 * KB);\\n\\tmm = get_task_mm(p);\\n\\tif (mm) {\\n\\t\\t/* adjust to KB unit */\\n\\t\\tstats->hiwater_rss   = get_mm_hiwater_rss(mm) * PAGE_SIZE / KB;\\n\\t\\tstats->hiwater_vm    = get_mm_hiwater_vm(mm)  * PAGE_SIZE / KB;\\n\\t\\tmmput(mm);\\n\\t}\\n\\tstats->read_char\\t= p->ioac.rchar & KB_MASK;\\n\\tstats->write_char\\t= p->ioac.wchar & KB_MASK;\\n\\tstats->read_syscalls\\t= p->ioac.syscr & KB_MASK;\\n\\tstats->write_syscalls\\t= p->ioac.syscw & KB_MASK;\\n#ifdef CONFIG_TASK_IO_ACCOUNTING\\n\\tstats->read_bytes\\t= p->ioac.read_bytes & KB_MASK;\\n\\tstats->write_bytes\\t= p->ioac.write_bytes & KB_MASK;\\n\\tstats->cancelled_write_bytes = p->ioac.cancelled_write_bytes & KB_MASK;\\n#else\\n\\tstats->read_bytes\\t= 0;\\n\\tstats->write_bytes\\t= 0;\\n\\tstats->cancelled_write_bytes = 0;\\n#endif\\n}\\n#undef KB\\n#undef MB\\n\\nstatic void __acct_update_integrals(struct task_struct *tsk,\\n\\t\\t\\t\\t    u64 utime, u64 stime)\\n{\\n\\tu64 time, delta;\\n\\n\\tif (!likely(tsk->mm))\\n\\t\\treturn;\\n\\n\\ttime = stime + utime;\\n\\tdelta = time - tsk->acct_timexpd;\\n\\n\\tif (delta < TICK_NSEC)\\n\\t\\treturn;\\n\\n\\ttsk->acct_timexpd = time;\\n\\t/*\\n\\t * Divide by 1024 to avoid overflow, and to avoid division.\\n\\t * The final unit reported to userspace is Mbyte-usecs,\\n\\t * the rest of the math is done in xacct_add_tsk.\\n\\t */\\n\\ttsk->acct_rss_mem1 += delta * get_mm_rss(tsk->mm) >> 10;\\n\\ttsk->acct_vm_mem1 += delta * READ_ONCE(tsk->mm->total_vm) >> 10;\\n}\\n\\n/**\\n * acct_update_integrals - update mm integral fields in task_struct\\n * @tsk: task_struct for accounting\\n */\\nvoid acct_update_integrals(struct task_struct *tsk)\\n{\\n\\tu64 utime, stime;\\n\\tunsigned long flags;\\n\\n\\tlocal_irq_save(flags);\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\t__acct_update_integrals(tsk, utime, stime);\\n\\tlocal_irq_restore(flags);\\n}\\n\\n/**\\n * acct_account_cputime - update mm integral after cputime update\\n * @tsk: task_struct for accounting\\n */\\nvoid acct_account_cputime(struct task_struct *tsk)\\n{\\n\\t__acct_update_integrals(tsk, tsk->utime, tsk->stime);\\n}\\n\\n/**\\n * acct_clear_integrals - clear the mm integral fields in task_struct\\n * @tsk: task_struct whose accounting fields are cleared\\n */\\nvoid acct_clear_integrals(struct task_struct *tsk)\\n{\\n\\ttsk->acct_timexpd = 0;\\n\\ttsk->acct_rss_mem1 = 0;\\n\\ttsk->acct_vm_mem1 = 0;\\n}\\n#endif\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *\\tlinux/kernel/resource.c\\n *\\n * Copyright (C) 1999\\tLinus Torvalds\\n * Copyright (C) 1999\\tMartin Mares <mj@ucw.cz>\\n *\\n * Arbitrary resource management.\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/export.h>\\n#include <linux/errno.h>\\n#include <linux/ioport.h>\\n#include <linux/init.h>\\n#include <linux/slab.h>\\n#include <linux/spinlock.h>\\n#include <linux/fs.h>\\n#include <linux/proc_fs.h>\\n#include <linux/pseudo_fs.h>\\n#include <linux/sched.h>\\n#include <linux/seq_file.h>\\n#include <linux/device.h>\\n#include <linux/pfn.h>\\n#include <linux/mm.h>\\n#include <linux/mount.h>\\n#include <linux/resource_ext.h>\\n#include <uapi/linux/magic.h>\\n#include <linux/string.h>\\n#include <linux/vmalloc.h>\\n#include <asm/io.h>\\n\\n\\nstruct resource ioport_resource = {\\n\\t.name\\t= \"PCI IO\",\\n\\t.start\\t= 0,\\n\\t.end\\t= IO_SPACE_LIMIT,\\n\\t.flags\\t= IORESOURCE_IO,\\n};\\nEXPORT_SYMBOL(ioport_resource);\\n\\nstruct resource iomem_resource = {\\n\\t.name\\t= \"PCI mem\",\\n\\t.start\\t= 0,\\n\\t.end\\t= -1,\\n\\t.flags\\t= IORESOURCE_MEM,\\n};\\nEXPORT_SYMBOL(iomem_resource);\\n\\nstatic DEFINE_RWLOCK(resource_lock);\\n\\n/*\\n * Return the next node of @p in pre-order tree traversal.  If\\n * @skip_children is true, skip the descendant nodes of @p in\\n * traversal.  If @p is a descendant of @subtree_root, only traverse\\n * the subtree under @subtree_root.\\n */\\nstatic struct resource *next_resource(struct resource *p, bool skip_children,\\n\\t\\t\\t\\t      struct resource *subtree_root)\\n{\\n\\tif (!skip_children && p->child)\\n\\t\\treturn p->child;\\n\\twhile (!p->sibling && p->parent) {\\n\\t\\tp = p->parent;\\n\\t\\tif (p == subtree_root)\\n\\t\\t\\treturn NULL;\\n\\t}\\n\\treturn p->sibling;\\n}\\n\\n/*\\n * Traverse the resource subtree under @_root in pre-order, excluding\\n * @_root itself.\\n *\\n * NOTE: \\'__p\\' is introduced to avoid shadowing \\'_p\\' outside of loop.\\n * And it is referenced to avoid unused variable warning.\\n */\\n#define for_each_resource(_root, _p, _skip_children) \\\\\\n\\tfor (typeof(_root) __root = (_root), __p = _p = __root->child;\\t\\\\\\n\\t     __p && _p; _p = next_resource(_p, _skip_children, __root))\\n\\n#ifdef CONFIG_PROC_FS\\n\\nenum { MAX_IORES_LEVEL = 5 };\\n\\nstatic void *r_start(struct seq_file *m, loff_t *pos)\\n\\t__acquires(resource_lock)\\n{\\n\\tstruct resource *root = pde_data(file_inode(m->file));\\n\\tstruct resource *p;\\n\\tloff_t l = *pos;\\n\\n\\tread_lock(&resource_lock);\\n\\tfor_each_resource(root, p, false) {\\n\\t\\tif (l-- == 0)\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\treturn p;\\n}\\n\\nstatic void *r_next(struct seq_file *m, void *v, loff_t *pos)\\n{\\n\\tstruct resource *p = v;\\n\\n\\t(*pos)++;\\n\\n\\treturn (void *)next_resource(p, false, NULL);\\n}\\n\\nstatic void r_stop(struct seq_file *m, void *v)\\n\\t__releases(resource_lock)\\n{\\n\\tread_unlock(&resource_lock);\\n}\\n\\nstatic int r_show(struct seq_file *m, void *v)\\n{\\n\\tstruct resource *root = pde_data(file_inode(m->file));\\n\\tstruct resource *r = v, *p;\\n\\tunsigned long long start, end;\\n\\tint width = root->end < 0x10000 ? 4 : 8;\\n\\tint depth;\\n\\n\\tfor (depth = 0, p = r; depth < MAX_IORES_LEVEL; depth++, p = p->parent)\\n\\t\\tif (p->parent == root)\\n\\t\\t\\tbreak;\\n\\n\\tif (file_ns_capable(m->file, &init_user_ns, CAP_SYS_ADMIN)) {\\n\\t\\tstart = r->start;\\n\\t\\tend = r->end;\\n\\t} else {\\n\\t\\tstart = end = 0;\\n\\t}\\n\\n\\tseq_printf(m, \"%*s%0*llx-%0*llx : %s\\\\n\",\\n\\t\\t\\tdepth * 2, \"\",\\n\\t\\t\\twidth, start,\\n\\t\\t\\twidth, end,\\n\\t\\t\\tr->name ? r->name : \"<BAD>\");\\n\\treturn 0;\\n}\\n\\nstatic const struct seq_operations resource_op = {\\n\\t.start\\t= r_start,\\n\\t.next\\t= r_next,\\n\\t.stop\\t= r_stop,\\n\\t.show\\t= r_show,\\n};\\n\\nstatic int __init ioresources_init(void)\\n{\\n\\tproc_create_seq_data(\"ioports\", 0, NULL, &resource_op,\\n\\t\\t\\t&ioport_resource);\\n\\tproc_create_seq_data(\"iomem\", 0, NULL, &resource_op, &iomem_resource);\\n\\treturn 0;\\n}\\n__initcall(ioresources_init);\\n\\n#endif /* CONFIG_PROC_FS */\\n\\nstatic void free_resource(struct resource *res)\\n{\\n\\t/**\\n\\t * If the resource was allocated using memblock early during boot\\n\\t * we\\'ll leak it here: we can only return full pages back to the\\n\\t * buddy and trying to be smart and reusing them eventually in\\n\\t * alloc_resource() overcomplicates resource handling.\\n\\t */\\n\\tif (res && PageSlab(virt_to_head_page(res)))\\n\\t\\tkfree(res);\\n}\\n\\nstatic struct resource *alloc_resource(gfp_t flags)\\n{\\n\\treturn kzalloc(sizeof(struct resource), flags);\\n}\\n\\n/* Return the conflict entry if you can\\'t request it */\\nstatic struct resource * __request_resource(struct resource *root, struct resource *new)\\n{\\n\\tresource_size_t start = new->start;\\n\\tresource_size_t end = new->end;\\n\\tstruct resource *tmp, **p;\\n\\n\\tif (end < start)\\n\\t\\treturn root;\\n\\tif (start < root->start)\\n\\t\\treturn root;\\n\\tif (end > root->end)\\n\\t\\treturn root;\\n\\tp = &root->child;\\n\\tfor (;;) {\\n\\t\\ttmp = *p;\\n\\t\\tif (!tmp || tmp->start > end) {\\n\\t\\t\\tnew->sibling = tmp;\\n\\t\\t\\t*p = new;\\n\\t\\t\\tnew->parent = root;\\n\\t\\t\\treturn NULL;\\n\\t\\t}\\n\\t\\tp = &tmp->sibling;\\n\\t\\tif (tmp->end < start)\\n\\t\\t\\tcontinue;\\n\\t\\treturn tmp;\\n\\t}\\n}\\n\\nstatic int __release_resource(struct resource *old, bool release_child)\\n{\\n\\tstruct resource *tmp, **p, *chd;\\n\\n\\tp = &old->parent->child;\\n\\tfor (;;) {\\n\\t\\ttmp = *p;\\n\\t\\tif (!tmp)\\n\\t\\t\\tbreak;\\n\\t\\tif (tmp == old) {\\n\\t\\t\\tif (release_child || !(tmp->child)) {\\n\\t\\t\\t\\t*p = tmp->sibling;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tfor (chd = tmp->child;; chd = chd->sibling) {\\n\\t\\t\\t\\t\\tchd->parent = tmp->parent;\\n\\t\\t\\t\\t\\tif (!(chd->sibling))\\n\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t*p = tmp->child;\\n\\t\\t\\t\\tchd->sibling = tmp->sibling;\\n\\t\\t\\t}\\n\\t\\t\\told->parent = NULL;\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\t\\tp = &tmp->sibling;\\n\\t}\\n\\treturn -EINVAL;\\n}\\n\\nstatic void __release_child_resources(struct resource *r)\\n{\\n\\tstruct resource *tmp, *p;\\n\\tresource_size_t size;\\n\\n\\tp = r->child;\\n\\tr->child = NULL;\\n\\twhile (p) {\\n\\t\\ttmp = p;\\n\\t\\tp = p->sibling;\\n\\n\\t\\ttmp->parent = NULL;\\n\\t\\ttmp->sibling = NULL;\\n\\t\\t__release_child_resources(tmp);\\n\\n\\t\\tprintk(KERN_DEBUG \"release child resource %pR\\\\n\", tmp);\\n\\t\\t/* need to restore size, and keep flags */\\n\\t\\tsize = resource_size(tmp);\\n\\t\\ttmp->start = 0;\\n\\t\\ttmp->end = size - 1;\\n\\t}\\n}\\n\\nvoid release_child_resources(struct resource *r)\\n{\\n\\twrite_lock(&resource_lock);\\n\\t__release_child_resources(r);\\n\\twrite_unlock(&resource_lock);\\n}\\n\\n/**\\n * request_resource_conflict - request and reserve an I/O or memory resource\\n * @root: root resource descriptor\\n * @new: resource descriptor desired by caller\\n *\\n * Returns 0 for success, conflict resource on error.\\n */\\nstruct resource *request_resource_conflict(struct resource *root, struct resource *new)\\n{\\n\\tstruct resource *conflict;\\n\\n\\twrite_lock(&resource_lock);\\n\\tconflict = __request_resource(root, new);\\n\\twrite_unlock(&resource_lock);\\n\\treturn conflict;\\n}\\n\\n/**\\n * request_resource - request and reserve an I/O or memory resource\\n * @root: root resource descriptor\\n * @new: resource descriptor desired by caller\\n *\\n * Returns 0 for success, negative error code on error.\\n */\\nint request_resource(struct resource *root, struct resource *new)\\n{\\n\\tstruct resource *conflict;\\n\\n\\tconflict = request_resource_conflict(root, new);\\n\\treturn conflict ? -EBUSY : 0;\\n}\\n\\nEXPORT_SYMBOL(request_resource);\\n\\n/**\\n * release_resource - release a previously reserved resource\\n * @old: resource pointer\\n */\\nint release_resource(struct resource *old)\\n{\\n\\tint retval;\\n\\n\\twrite_lock(&resource_lock);\\n\\tretval = __release_resource(old, true);\\n\\twrite_unlock(&resource_lock);\\n\\treturn retval;\\n}\\n\\nEXPORT_SYMBOL(release_resource);\\n\\nstatic bool is_type_match(struct resource *p, unsigned long flags, unsigned long desc)\\n{\\n\\treturn (p->flags & flags) == flags && (desc == IORES_DESC_NONE || desc == p->desc);\\n}\\n\\n/**\\n * find_next_iomem_res - Finds the lowest iomem resource that covers part of\\n *\\t\\t\\t [@start..@end].\\n *\\n * If a resource is found, returns 0 and @*res is overwritten with the part\\n * of the resource that\\'s within [@start..@end]; if none is found, returns\\n * -ENODEV.  Returns -EINVAL for invalid parameters.\\n *\\n * @start:\\tstart address of the resource searched for\\n * @end:\\tend address of same resource\\n * @flags:\\tflags which the resource must have\\n * @desc:\\tdescriptor the resource must have\\n * @res:\\treturn ptr, if resource found\\n *\\n * The caller must specify @start, @end, @flags, and @desc\\n * (which may be IORES_DESC_NONE).\\n */\\nstatic int find_next_iomem_res(resource_size_t start, resource_size_t end,\\n\\t\\t\\t       unsigned long flags, unsigned long desc,\\n\\t\\t\\t       struct resource *res)\\n{\\n\\tstruct resource *p;\\n\\n\\tif (!res)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (start >= end)\\n\\t\\treturn -EINVAL;\\n\\n\\tread_lock(&resource_lock);\\n\\n\\tfor_each_resource(&iomem_resource, p, false) {\\n\\t\\t/* If we passed the resource we are looking for, stop */\\n\\t\\tif (p->start > end) {\\n\\t\\t\\tp = NULL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\t/* Skip until we find a range that matches what we look for */\\n\\t\\tif (p->end < start)\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* Found a match, break */\\n\\t\\tif (is_type_match(p, flags, desc))\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tif (p) {\\n\\t\\t/* copy data */\\n\\t\\t*res = (struct resource) {\\n\\t\\t\\t.start = max(start, p->start),\\n\\t\\t\\t.end = min(end, p->end),\\n\\t\\t\\t.flags = p->flags,\\n\\t\\t\\t.desc = p->desc,\\n\\t\\t\\t.parent = p->parent,\\n\\t\\t};\\n\\t}\\n\\n\\tread_unlock(&resource_lock);\\n\\treturn p ? 0 : -ENODEV;\\n}\\n\\nstatic int __walk_iomem_res_desc(resource_size_t start, resource_size_t end,\\n\\t\\t\\t\\t unsigned long flags, unsigned long desc,\\n\\t\\t\\t\\t void *arg,\\n\\t\\t\\t\\t int (*func)(struct resource *, void *))\\n{\\n\\tstruct resource res;\\n\\tint ret = -EINVAL;\\n\\n\\twhile (start < end &&\\n\\t       !find_next_iomem_res(start, end, flags, desc, &res)) {\\n\\t\\tret = (*func)(&res, arg);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\n\\t\\tstart = res.end + 1;\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\n/**\\n * walk_iomem_res_desc - Walks through iomem resources and calls func()\\n *\\t\\t\\t with matching resource ranges.\\n * *\\n * @desc: I/O resource descriptor. Use IORES_DESC_NONE to skip @desc check.\\n * @flags: I/O resource flags\\n * @start: start addr\\n * @end: end addr\\n * @arg: function argument for the callback @func\\n * @func: callback function that is called for each qualifying resource area\\n *\\n * All the memory ranges which overlap start,end and also match flags and\\n * desc are valid candidates.\\n *\\n * NOTE: For a new descriptor search, define a new IORES_DESC in\\n * <linux/ioport.h> and set it in \\'desc\\' of a target resource entry.\\n */\\nint walk_iomem_res_desc(unsigned long desc, unsigned long flags, u64 start,\\n\\t\\tu64 end, void *arg, int (*func)(struct resource *, void *))\\n{\\n\\treturn __walk_iomem_res_desc(start, end, flags, desc, arg, func);\\n}\\nEXPORT_SYMBOL_GPL(walk_iomem_res_desc);\\n\\n/*\\n * This function calls the @func callback against all memory ranges of type\\n * System RAM which are marked as IORESOURCE_SYSTEM_RAM and IORESOUCE_BUSY.\\n * Now, this function is only for System RAM, it deals with full ranges and\\n * not PFNs. If resources are not PFN-aligned, dealing with PFNs can truncate\\n * ranges.\\n */\\nint walk_system_ram_res(u64 start, u64 end, void *arg,\\n\\t\\t\\tint (*func)(struct resource *, void *))\\n{\\n\\tunsigned long flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\\n\\n\\treturn __walk_iomem_res_desc(start, end, flags, IORES_DESC_NONE, arg,\\n\\t\\t\\t\\t     func);\\n}\\n\\n/*\\n * This function, being a variant of walk_system_ram_res(), calls the @func\\n * callback against all memory ranges of type System RAM which are marked as\\n * IORESOURCE_SYSTEM_RAM and IORESOUCE_BUSY in reversed order, i.e., from\\n * higher to lower.\\n */\\nint walk_system_ram_res_rev(u64 start, u64 end, void *arg,\\n\\t\\t\\t\\tint (*func)(struct resource *, void *))\\n{\\n\\tstruct resource res, *rams;\\n\\tint rams_size = 16, i;\\n\\tunsigned long flags;\\n\\tint ret = -1;\\n\\n\\t/* create a list */\\n\\trams = kvcalloc(rams_size, sizeof(struct resource), GFP_KERNEL);\\n\\tif (!rams)\\n\\t\\treturn ret;\\n\\n\\tflags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\\n\\ti = 0;\\n\\twhile ((start < end) &&\\n\\t\\t(!find_next_iomem_res(start, end, flags, IORES_DESC_NONE, &res))) {\\n\\t\\tif (i >= rams_size) {\\n\\t\\t\\t/* re-alloc */\\n\\t\\t\\tstruct resource *rams_new;\\n\\n\\t\\t\\trams_new = kvrealloc(rams, (rams_size + 16) * sizeof(struct resource),\\n\\t\\t\\t\\t\\t     GFP_KERNEL);\\n\\t\\t\\tif (!rams_new)\\n\\t\\t\\t\\tgoto out;\\n\\n\\t\\t\\trams = rams_new;\\n\\t\\t\\trams_size += 16;\\n\\t\\t}\\n\\n\\t\\trams[i++] = res;\\n\\t\\tstart = res.end + 1;\\n\\t}\\n\\n\\t/* go reverse */\\n\\tfor (i--; i >= 0; i--) {\\n\\t\\tret = (*func)(&rams[i], arg);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t}\\n\\nout:\\n\\tkvfree(rams);\\n\\treturn ret;\\n}\\n\\n/*\\n * This function calls the @func callback against all memory ranges, which\\n * are ranges marked as IORESOURCE_MEM and IORESOUCE_BUSY.\\n */\\nint walk_mem_res(u64 start, u64 end, void *arg,\\n\\t\\t int (*func)(struct resource *, void *))\\n{\\n\\tunsigned long flags = IORESOURCE_MEM | IORESOURCE_BUSY;\\n\\n\\treturn __walk_iomem_res_desc(start, end, flags, IORES_DESC_NONE, arg,\\n\\t\\t\\t\\t     func);\\n}\\n\\n/*\\n * This function calls the @func callback against all memory ranges of type\\n * System RAM which are marked as IORESOURCE_SYSTEM_RAM and IORESOUCE_BUSY.\\n * It is to be used only for System RAM.\\n */\\nint walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,\\n\\t\\t\\t  void *arg, int (*func)(unsigned long, unsigned long, void *))\\n{\\n\\tresource_size_t start, end;\\n\\tunsigned long flags;\\n\\tstruct resource res;\\n\\tunsigned long pfn, end_pfn;\\n\\tint ret = -EINVAL;\\n\\n\\tstart = (u64) start_pfn << PAGE_SHIFT;\\n\\tend = ((u64)(start_pfn + nr_pages) << PAGE_SHIFT) - 1;\\n\\tflags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\\n\\twhile (start < end &&\\n\\t       !find_next_iomem_res(start, end, flags, IORES_DESC_NONE, &res)) {\\n\\t\\tpfn = PFN_UP(res.start);\\n\\t\\tend_pfn = PFN_DOWN(res.end + 1);\\n\\t\\tif (end_pfn > pfn)\\n\\t\\t\\tret = (*func)(pfn, end_pfn - pfn, arg);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t\\tstart = res.end + 1;\\n\\t}\\n\\treturn ret;\\n}\\n\\nstatic int __is_ram(unsigned long pfn, unsigned long nr_pages, void *arg)\\n{\\n\\treturn 1;\\n}\\n\\n/*\\n * This generic page_is_ram() returns true if specified address is\\n * registered as System RAM in iomem_resource list.\\n */\\nint __weak page_is_ram(unsigned long pfn)\\n{\\n\\treturn walk_system_ram_range(pfn, 1, NULL, __is_ram) == 1;\\n}\\nEXPORT_SYMBOL_GPL(page_is_ram);\\n\\nstatic int __region_intersects(struct resource *parent, resource_size_t start,\\n\\t\\t\\t       size_t size, unsigned long flags,\\n\\t\\t\\t       unsigned long desc)\\n{\\n\\tint type = 0; int other = 0;\\n\\tstruct resource *p, *dp;\\n\\tstruct resource res, o;\\n\\tbool covered;\\n\\n\\tres.start = start;\\n\\tres.end = start + size - 1;\\n\\n\\tfor (p = parent->child; p ; p = p->sibling) {\\n\\t\\tif (!resource_intersection(p, &res, &o))\\n\\t\\t\\tcontinue;\\n\\t\\tif (is_type_match(p, flags, desc)) {\\n\\t\\t\\ttype++;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\t/*\\n\\t\\t * Continue to search in descendant resources as if the\\n\\t\\t * matched descendant resources cover some ranges of \\'p\\'.\\n\\t\\t *\\n\\t\\t * |------------- \"CXL Window 0\" ------------|\\n\\t\\t * |-- \"System RAM\" --|\\n\\t\\t *\\n\\t\\t * will behave similar as the following fake resource\\n\\t\\t * tree when searching \"System RAM\".\\n\\t\\t *\\n\\t\\t * |-- \"System RAM\" --||-- \"CXL Window 0a\" --|\\n\\t\\t */\\n\\t\\tcovered = false;\\n\\t\\tfor_each_resource(p, dp, false) {\\n\\t\\t\\tif (!resource_overlaps(dp, &res))\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\tif (is_type_match(dp, flags, desc)) {\\n\\t\\t\\t\\ttype++;\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * Range from \\'o.start\\' to \\'dp->start\\'\\n\\t\\t\\t\\t * isn\\'t covered by matched resource.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\tif (dp->start > o.start)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tif (dp->end >= o.end) {\\n\\t\\t\\t\\t\\tcovered = true;\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t/* Remove covered range */\\n\\t\\t\\t\\to.start = max(o.start, dp->end + 1);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif (!covered)\\n\\t\\t\\tother++;\\n\\t}\\n\\n\\tif (type == 0)\\n\\t\\treturn REGION_DISJOINT;\\n\\n\\tif (other == 0)\\n\\t\\treturn REGION_INTERSECTS;\\n\\n\\treturn REGION_MIXED;\\n}\\n\\n/**\\n * region_intersects() - determine intersection of region with known resources\\n * @start: region start address\\n * @size: size of region\\n * @flags: flags of resource (in iomem_resource)\\n * @desc: descriptor of resource (in iomem_resource) or IORES_DESC_NONE\\n *\\n * Check if the specified region partially overlaps or fully eclipses a\\n * resource identified by @flags and @desc (optional with IORES_DESC_NONE).\\n * Return REGION_DISJOINT if the region does not overlap @flags/@desc,\\n * return REGION_MIXED if the region overlaps @flags/@desc and another\\n * resource, and return REGION_INTERSECTS if the region overlaps @flags/@desc\\n * and no other defined resource. Note that REGION_INTERSECTS is also\\n * returned in the case when the specified region overlaps RAM and undefined\\n * memory holes.\\n *\\n * region_intersect() is used by memory remapping functions to ensure\\n * the user is not remapping RAM and is a vast speed up over walking\\n * through the resource table page by page.\\n */\\nint region_intersects(resource_size_t start, size_t size, unsigned long flags,\\n\\t\\t      unsigned long desc)\\n{\\n\\tint ret;\\n\\n\\tread_lock(&resource_lock);\\n\\tret = __region_intersects(&iomem_resource, start, size, flags, desc);\\n\\tread_unlock(&resource_lock);\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(region_intersects);\\n\\nvoid __weak arch_remove_reservations(struct resource *avail)\\n{\\n}\\n\\nstatic void resource_clip(struct resource *res, resource_size_t min,\\n\\t\\t\\t  resource_size_t max)\\n{\\n\\tif (res->start < min)\\n\\t\\tres->start = min;\\n\\tif (res->end > max)\\n\\t\\tres->end = max;\\n}\\n\\n/*\\n * Find empty space in the resource tree with the given range and\\n * alignment constraints\\n */\\nstatic int __find_resource_space(struct resource *root, struct resource *old,\\n\\t\\t\\t\\t struct resource *new, resource_size_t size,\\n\\t\\t\\t\\t struct resource_constraint *constraint)\\n{\\n\\tstruct resource *this = root->child;\\n\\tstruct resource tmp = *new, avail, alloc;\\n\\tresource_alignf alignf = constraint->alignf;\\n\\n\\ttmp.start = root->start;\\n\\t/*\\n\\t * Skip past an allocated resource that starts at 0, since the assignment\\n\\t * of this->start - 1 to tmp->end below would cause an underflow.\\n\\t */\\n\\tif (this && this->start == root->start) {\\n\\t\\ttmp.start = (this == old) ? old->start : this->end + 1;\\n\\t\\tthis = this->sibling;\\n\\t}\\n\\tfor(;;) {\\n\\t\\tif (this)\\n\\t\\t\\ttmp.end = (this == old) ?  this->end : this->start - 1;\\n\\t\\telse\\n\\t\\t\\ttmp.end = root->end;\\n\\n\\t\\tif (tmp.end < tmp.start)\\n\\t\\t\\tgoto next;\\n\\n\\t\\tresource_clip(&tmp, constraint->min, constraint->max);\\n\\t\\tarch_remove_reservations(&tmp);\\n\\n\\t\\t/* Check for overflow after ALIGN() */\\n\\t\\tavail.start = ALIGN(tmp.start, constraint->align);\\n\\t\\tavail.end = tmp.end;\\n\\t\\tavail.flags = new->flags & ~IORESOURCE_UNSET;\\n\\t\\tif (avail.start >= tmp.start) {\\n\\t\\t\\talloc.flags = avail.flags;\\n\\t\\t\\tif (alignf) {\\n\\t\\t\\t\\talloc.start = alignf(constraint->alignf_data,\\n\\t\\t\\t\\t\\t\\t     &avail, size, constraint->align);\\n\\t\\t\\t} else {\\n\\t\\t\\t\\talloc.start = avail.start;\\n\\t\\t\\t}\\n\\t\\t\\talloc.end = alloc.start + size - 1;\\n\\t\\t\\tif (alloc.start <= alloc.end &&\\n\\t\\t\\t    resource_contains(&avail, &alloc)) {\\n\\t\\t\\t\\tnew->start = alloc.start;\\n\\t\\t\\t\\tnew->end = alloc.end;\\n\\t\\t\\t\\treturn 0;\\n\\t\\t\\t}\\n\\t\\t}\\n\\nnext:\\t\\tif (!this || this->end == root->end)\\n\\t\\t\\tbreak;\\n\\n\\t\\tif (this != old)\\n\\t\\t\\ttmp.start = this->end + 1;\\n\\t\\tthis = this->sibling;\\n\\t}\\n\\treturn -EBUSY;\\n}\\n\\n/**\\n * find_resource_space - Find empty space in the resource tree\\n * @root:\\tRoot resource descriptor\\n * @new:\\tResource descriptor awaiting an empty resource space\\n * @size:\\tThe minimum size of the empty space\\n * @constraint:\\tThe range and alignment constraints to be met\\n *\\n * Finds an empty space under @root in the resource tree satisfying range and\\n * alignment @constraints.\\n *\\n * Return:\\n * * %0\\t\\t- if successful, @new members start, end, and flags are altered.\\n * * %-EBUSY\\t- if no empty space was found.\\n */\\nint find_resource_space(struct resource *root, struct resource *new,\\n\\t\\t\\tresource_size_t size,\\n\\t\\t\\tstruct resource_constraint *constraint)\\n{\\n\\treturn  __find_resource_space(root, NULL, new, size, constraint);\\n}\\nEXPORT_SYMBOL_GPL(find_resource_space);\\n\\n/**\\n * reallocate_resource - allocate a slot in the resource tree given range & alignment.\\n *\\tThe resource will be relocated if the new size cannot be reallocated in the\\n *\\tcurrent location.\\n *\\n * @root: root resource descriptor\\n * @old:  resource descriptor desired by caller\\n * @newsize: new size of the resource descriptor\\n * @constraint: the memory range and alignment constraints to be met.\\n */\\nstatic int reallocate_resource(struct resource *root, struct resource *old,\\n\\t\\t\\t       resource_size_t newsize,\\n\\t\\t\\t       struct resource_constraint *constraint)\\n{\\n\\tint err=0;\\n\\tstruct resource new = *old;\\n\\tstruct resource *conflict;\\n\\n\\twrite_lock(&resource_lock);\\n\\n\\tif ((err = __find_resource_space(root, old, &new, newsize, constraint)))\\n\\t\\tgoto out;\\n\\n\\tif (resource_contains(&new, old)) {\\n\\t\\told->start = new.start;\\n\\t\\told->end = new.end;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (old->child) {\\n\\t\\terr = -EBUSY;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (resource_contains(old, &new)) {\\n\\t\\told->start = new.start;\\n\\t\\told->end = new.end;\\n\\t} else {\\n\\t\\t__release_resource(old, true);\\n\\t\\t*old = new;\\n\\t\\tconflict = __request_resource(root, old);\\n\\t\\tBUG_ON(conflict);\\n\\t}\\nout:\\n\\twrite_unlock(&resource_lock);\\n\\treturn err;\\n}\\n\\n\\n/**\\n * allocate_resource - allocate empty slot in the resource tree given range & alignment.\\n * \\tThe resource will be reallocated with a new size if it was already allocated\\n * @root: root resource descriptor\\n * @new: resource descriptor desired by caller\\n * @size: requested resource region size\\n * @min: minimum boundary to allocate\\n * @max: maximum boundary to allocate\\n * @align: alignment requested, in bytes\\n * @alignf: alignment function, optional, called if not NULL\\n * @alignf_data: arbitrary data to pass to the @alignf function\\n */\\nint allocate_resource(struct resource *root, struct resource *new,\\n\\t\\t      resource_size_t size, resource_size_t min,\\n\\t\\t      resource_size_t max, resource_size_t align,\\n\\t\\t      resource_alignf alignf,\\n\\t\\t      void *alignf_data)\\n{\\n\\tint err;\\n\\tstruct resource_constraint constraint;\\n\\n\\tconstraint.min = min;\\n\\tconstraint.max = max;\\n\\tconstraint.align = align;\\n\\tconstraint.alignf = alignf;\\n\\tconstraint.alignf_data = alignf_data;\\n\\n\\tif ( new->parent ) {\\n\\t\\t/* resource is already allocated, try reallocating with\\n\\t\\t   the new constraints */\\n\\t\\treturn reallocate_resource(root, new, size, &constraint);\\n\\t}\\n\\n\\twrite_lock(&resource_lock);\\n\\terr = find_resource_space(root, new, size, &constraint);\\n\\tif (err >= 0 && __request_resource(root, new))\\n\\t\\terr = -EBUSY;\\n\\twrite_unlock(&resource_lock);\\n\\treturn err;\\n}\\n\\nEXPORT_SYMBOL(allocate_resource);\\n\\n/**\\n * lookup_resource - find an existing resource by a resource start address\\n * @root: root resource descriptor\\n * @start: resource start address\\n *\\n * Returns a pointer to the resource if found, NULL otherwise\\n */\\nstruct resource *lookup_resource(struct resource *root, resource_size_t start)\\n{\\n\\tstruct resource *res;\\n\\n\\tread_lock(&resource_lock);\\n\\tfor (res = root->child; res; res = res->sibling) {\\n\\t\\tif (res->start == start)\\n\\t\\t\\tbreak;\\n\\t}\\n\\tread_unlock(&resource_lock);\\n\\n\\treturn res;\\n}\\n\\n/*\\n * Insert a resource into the resource tree. If successful, return NULL,\\n * otherwise return the conflicting resource (compare to __request_resource())\\n */\\nstatic struct resource * __insert_resource(struct resource *parent, struct resource *new)\\n{\\n\\tstruct resource *first, *next;\\n\\n\\tfor (;; parent = first) {\\n\\t\\tfirst = __request_resource(parent, new);\\n\\t\\tif (!first)\\n\\t\\t\\treturn first;\\n\\n\\t\\tif (first == parent)\\n\\t\\t\\treturn first;\\n\\t\\tif (WARN_ON(first == new))\\t/* duplicated insertion */\\n\\t\\t\\treturn first;\\n\\n\\t\\tif ((first->start > new->start) || (first->end < new->end))\\n\\t\\t\\tbreak;\\n\\t\\tif ((first->start == new->start) && (first->end == new->end))\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tfor (next = first; ; next = next->sibling) {\\n\\t\\t/* Partial overlap? Bad, and unfixable */\\n\\t\\tif (next->start < new->start || next->end > new->end)\\n\\t\\t\\treturn next;\\n\\t\\tif (!next->sibling)\\n\\t\\t\\tbreak;\\n\\t\\tif (next->sibling->start > new->end)\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tnew->parent = parent;\\n\\tnew->sibling = next->sibling;\\n\\tnew->child = first;\\n\\n\\tnext->sibling = NULL;\\n\\tfor (next = first; next; next = next->sibling)\\n\\t\\tnext->parent = new;\\n\\n\\tif (parent->child == first) {\\n\\t\\tparent->child = new;\\n\\t} else {\\n\\t\\tnext = parent->child;\\n\\t\\twhile (next->sibling != first)\\n\\t\\t\\tnext = next->sibling;\\n\\t\\tnext->sibling = new;\\n\\t}\\n\\treturn NULL;\\n}\\n\\n/**\\n * insert_resource_conflict - Inserts resource in the resource tree\\n * @parent: parent of the new resource\\n * @new: new resource to insert\\n *\\n * Returns 0 on success, conflict resource if the resource can\\'t be inserted.\\n *\\n * This function is equivalent to request_resource_conflict when no conflict\\n * happens. If a conflict happens, and the conflicting resources\\n * entirely fit within the range of the new resource, then the new\\n * resource is inserted and the conflicting resources become children of\\n * the new resource.\\n *\\n * This function is intended for producers of resources, such as FW modules\\n * and bus drivers.\\n */\\nstruct resource *insert_resource_conflict(struct resource *parent, struct resource *new)\\n{\\n\\tstruct resource *conflict;\\n\\n\\twrite_lock(&resource_lock);\\n\\tconflict = __insert_resource(parent, new);\\n\\twrite_unlock(&resource_lock);\\n\\treturn conflict;\\n}\\n\\n/**\\n * insert_resource - Inserts a resource in the resource tree\\n * @parent: parent of the new resource\\n * @new: new resource to insert\\n *\\n * Returns 0 on success, -EBUSY if the resource can\\'t be inserted.\\n *\\n * This function is intended for producers of resources, such as FW modules\\n * and bus drivers.\\n */\\nint insert_resource(struct resource *parent, struct resource *new)\\n{\\n\\tstruct resource *conflict;\\n\\n\\tconflict = insert_resource_conflict(parent, new);\\n\\treturn conflict ? -EBUSY : 0;\\n}\\nEXPORT_SYMBOL_GPL(insert_resource);\\n\\n/**\\n * insert_resource_expand_to_fit - Insert a resource into the resource tree\\n * @root: root resource descriptor\\n * @new: new resource to insert\\n *\\n * Insert a resource into the resource tree, possibly expanding it in order\\n * to make it encompass any conflicting resources.\\n */\\nvoid insert_resource_expand_to_fit(struct resource *root, struct resource *new)\\n{\\n\\tif (new->parent)\\n\\t\\treturn;\\n\\n\\twrite_lock(&resource_lock);\\n\\tfor (;;) {\\n\\t\\tstruct resource *conflict;\\n\\n\\t\\tconflict = __insert_resource(root, new);\\n\\t\\tif (!conflict)\\n\\t\\t\\tbreak;\\n\\t\\tif (conflict == root)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* Ok, expand resource to cover the conflict, then try again .. */\\n\\t\\tif (conflict->start < new->start)\\n\\t\\t\\tnew->start = conflict->start;\\n\\t\\tif (conflict->end > new->end)\\n\\t\\t\\tnew->end = conflict->end;\\n\\n\\t\\tpr_info(\"Expanded resource %s due to conflict with %s\\\\n\", new->name, conflict->name);\\n\\t}\\n\\twrite_unlock(&resource_lock);\\n}\\n/*\\n * Not for general consumption, only early boot memory map parsing, PCI\\n * resource discovery, and late discovery of CXL resources are expected\\n * to use this interface. The former are built-in and only the latter,\\n * CXL, is a module.\\n */\\nEXPORT_SYMBOL_NS_GPL(insert_resource_expand_to_fit, \"CXL\");\\n\\n/**\\n * remove_resource - Remove a resource in the resource tree\\n * @old: resource to remove\\n *\\n * Returns 0 on success, -EINVAL if the resource is not valid.\\n *\\n * This function removes a resource previously inserted by insert_resource()\\n * or insert_resource_conflict(), and moves the children (if any) up to\\n * where they were before.  insert_resource() and insert_resource_conflict()\\n * insert a new resource, and move any conflicting resources down to the\\n * children of the new resource.\\n *\\n * insert_resource(), insert_resource_conflict() and remove_resource() are\\n * intended for producers of resources, such as FW modules and bus drivers.\\n */\\nint remove_resource(struct resource *old)\\n{\\n\\tint retval;\\n\\n\\twrite_lock(&resource_lock);\\n\\tretval = __release_resource(old, false);\\n\\twrite_unlock(&resource_lock);\\n\\treturn retval;\\n}\\nEXPORT_SYMBOL_GPL(remove_resource);\\n\\nstatic int __adjust_resource(struct resource *res, resource_size_t start,\\n\\t\\t\\t\\tresource_size_t size)\\n{\\n\\tstruct resource *tmp, *parent = res->parent;\\n\\tresource_size_t end = start + size - 1;\\n\\tint result = -EBUSY;\\n\\n\\tif (!parent)\\n\\t\\tgoto skip;\\n\\n\\tif ((start < parent->start) || (end > parent->end))\\n\\t\\tgoto out;\\n\\n\\tif (res->sibling && (res->sibling->start <= end))\\n\\t\\tgoto out;\\n\\n\\ttmp = parent->child;\\n\\tif (tmp != res) {\\n\\t\\twhile (tmp->sibling != res)\\n\\t\\t\\ttmp = tmp->sibling;\\n\\t\\tif (start <= tmp->end)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\nskip:\\n\\tfor (tmp = res->child; tmp; tmp = tmp->sibling)\\n\\t\\tif ((tmp->start < start) || (tmp->end > end))\\n\\t\\t\\tgoto out;\\n\\n\\tres->start = start;\\n\\tres->end = end;\\n\\tresult = 0;\\n\\n out:\\n\\treturn result;\\n}\\n\\n/**\\n * adjust_resource - modify a resource\\'s start and size\\n * @res: resource to modify\\n * @start: new start value\\n * @size: new size\\n *\\n * Given an existing resource, change its start and size to match the\\n * arguments.  Returns 0 on success, -EBUSY if it can\\'t fit.\\n * Existing children of the resource are assumed to be immutable.\\n */\\nint adjust_resource(struct resource *res, resource_size_t start,\\n\\t\\t    resource_size_t size)\\n{\\n\\tint result;\\n\\n\\twrite_lock(&resource_lock);\\n\\tresult = __adjust_resource(res, start, size);\\n\\twrite_unlock(&resource_lock);\\n\\treturn result;\\n}\\nEXPORT_SYMBOL(adjust_resource);\\n\\nstatic void __init\\n__reserve_region_with_split(struct resource *root, resource_size_t start,\\n\\t\\t\\t    resource_size_t end, const char *name)\\n{\\n\\tstruct resource *parent = root;\\n\\tstruct resource *conflict;\\n\\tstruct resource *res = alloc_resource(GFP_ATOMIC);\\n\\tstruct resource *next_res = NULL;\\n\\tint type = resource_type(root);\\n\\n\\tif (!res)\\n\\t\\treturn;\\n\\n\\tres->name = name;\\n\\tres->start = start;\\n\\tres->end = end;\\n\\tres->flags = type | IORESOURCE_BUSY;\\n\\tres->desc = IORES_DESC_NONE;\\n\\n\\twhile (1) {\\n\\n\\t\\tconflict = __request_resource(parent, res);\\n\\t\\tif (!conflict) {\\n\\t\\t\\tif (!next_res)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tres = next_res;\\n\\t\\t\\tnext_res = NULL;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\t/* conflict covered whole area */\\n\\t\\tif (conflict->start <= res->start &&\\n\\t\\t\\t\\tconflict->end >= res->end) {\\n\\t\\t\\tfree_resource(res);\\n\\t\\t\\tWARN_ON(next_res);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\t/* failed, split and try again */\\n\\t\\tif (conflict->start > res->start) {\\n\\t\\t\\tend = res->end;\\n\\t\\t\\tres->end = conflict->start - 1;\\n\\t\\t\\tif (conflict->end < end) {\\n\\t\\t\\t\\tnext_res = alloc_resource(GFP_ATOMIC);\\n\\t\\t\\t\\tif (!next_res) {\\n\\t\\t\\t\\t\\tfree_resource(res);\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tnext_res->name = name;\\n\\t\\t\\t\\tnext_res->start = conflict->end + 1;\\n\\t\\t\\t\\tnext_res->end = end;\\n\\t\\t\\t\\tnext_res->flags = type | IORESOURCE_BUSY;\\n\\t\\t\\t\\tnext_res->desc = IORES_DESC_NONE;\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\tres->start = conflict->end + 1;\\n\\t\\t}\\n\\t}\\n\\n}\\n\\nvoid __init\\nreserve_region_with_split(struct resource *root, resource_size_t start,\\n\\t\\t\\t  resource_size_t end, const char *name)\\n{\\n\\tint abort = 0;\\n\\n\\twrite_lock(&resource_lock);\\n\\tif (root->start > start || root->end < end) {\\n\\t\\tpr_err(\"requested range [0x%llx-0x%llx] not in root %pr\\\\n\",\\n\\t\\t       (unsigned long long)start, (unsigned long long)end,\\n\\t\\t       root);\\n\\t\\tif (start > root->end || end < root->start)\\n\\t\\t\\tabort = 1;\\n\\t\\telse {\\n\\t\\t\\tif (end > root->end)\\n\\t\\t\\t\\tend = root->end;\\n\\t\\t\\tif (start < root->start)\\n\\t\\t\\t\\tstart = root->start;\\n\\t\\t\\tpr_err(\"fixing request to [0x%llx-0x%llx]\\\\n\",\\n\\t\\t\\t       (unsigned long long)start,\\n\\t\\t\\t       (unsigned long long)end);\\n\\t\\t}\\n\\t\\tdump_stack();\\n\\t}\\n\\tif (!abort)\\n\\t\\t__reserve_region_with_split(root, start, end, name);\\n\\twrite_unlock(&resource_lock);\\n}\\n\\n/**\\n * resource_alignment - calculate resource\\'s alignment\\n * @res: resource pointer\\n *\\n * Returns alignment on success, 0 (invalid alignment) on failure.\\n */\\nresource_size_t resource_alignment(struct resource *res)\\n{\\n\\tswitch (res->flags & (IORESOURCE_SIZEALIGN | IORESOURCE_STARTALIGN)) {\\n\\tcase IORESOURCE_SIZEALIGN:\\n\\t\\treturn resource_size(res);\\n\\tcase IORESOURCE_STARTALIGN:\\n\\t\\treturn res->start;\\n\\tdefault:\\n\\t\\treturn 0;\\n\\t}\\n}\\n\\n/*\\n * This is compatibility stuff for IO resources.\\n *\\n * Note how this, unlike the above, knows about\\n * the IO flag meanings (busy etc).\\n *\\n * request_region creates a new busy region.\\n *\\n * release_region releases a matching busy region.\\n */\\n\\nstatic DECLARE_WAIT_QUEUE_HEAD(muxed_resource_wait);\\n\\nstatic struct inode *iomem_inode;\\n\\n#ifdef CONFIG_IO_STRICT_DEVMEM\\nstatic void revoke_iomem(struct resource *res)\\n{\\n\\t/* pairs with smp_store_release() in iomem_init_inode() */\\n\\tstruct inode *inode = smp_load_acquire(&iomem_inode);\\n\\n\\t/*\\n\\t * Check that the initialization has completed. Losing the race\\n\\t * is ok because it means drivers are claiming resources before\\n\\t * the fs_initcall level of init and prevent iomem_get_mapping users\\n\\t * from establishing mappings.\\n\\t */\\n\\tif (!inode)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * The expectation is that the driver has successfully marked\\n\\t * the resource busy by this point, so devmem_is_allowed()\\n\\t * should start returning false, however for performance this\\n\\t * does not iterate the entire resource range.\\n\\t */\\n\\tif (devmem_is_allowed(PHYS_PFN(res->start)) &&\\n\\t    devmem_is_allowed(PHYS_PFN(res->end))) {\\n\\t\\t/*\\n\\t\\t * *cringe* iomem=relaxed says \"go ahead, what\\'s the\\n\\t\\t * worst that can happen?\"\\n\\t\\t */\\n\\t\\treturn;\\n\\t}\\n\\n\\tunmap_mapping_range(inode->i_mapping, res->start, resource_size(res), 1);\\n}\\n#else\\nstatic void revoke_iomem(struct resource *res) {}\\n#endif\\n\\nstruct address_space *iomem_get_mapping(void)\\n{\\n\\t/*\\n\\t * This function is only called from file open paths, hence guaranteed\\n\\t * that fs_initcalls have completed and no need to check for NULL. But\\n\\t * since revoke_iomem can be called before the initcall we still need\\n\\t * the barrier to appease checkers.\\n\\t */\\n\\treturn smp_load_acquire(&iomem_inode)->i_mapping;\\n}\\n\\nstatic int __request_region_locked(struct resource *res, struct resource *parent,\\n\\t\\t\\t\\t   resource_size_t start, resource_size_t n,\\n\\t\\t\\t\\t   const char *name, int flags)\\n{\\n\\tDECLARE_WAITQUEUE(wait, current);\\n\\n\\tres->name = name;\\n\\tres->start = start;\\n\\tres->end = start + n - 1;\\n\\n\\tfor (;;) {\\n\\t\\tstruct resource *conflict;\\n\\n\\t\\tres->flags = resource_type(parent) | resource_ext_type(parent);\\n\\t\\tres->flags |= IORESOURCE_BUSY | flags;\\n\\t\\tres->desc = parent->desc;\\n\\n\\t\\tconflict = __request_resource(parent, res);\\n\\t\\tif (!conflict)\\n\\t\\t\\tbreak;\\n\\t\\t/*\\n\\t\\t * mm/hmm.c reserves physical addresses which then\\n\\t\\t * become unavailable to other users.  Conflicts are\\n\\t\\t * not expected.  Warn to aid debugging if encountered.\\n\\t\\t */\\n\\t\\tif (conflict->desc == IORES_DESC_DEVICE_PRIVATE_MEMORY) {\\n\\t\\t\\tpr_warn(\"Unaddressable device %s %pR conflicts with %pR\",\\n\\t\\t\\t\\tconflict->name, conflict, res);\\n\\t\\t}\\n\\t\\tif (conflict != parent) {\\n\\t\\t\\tif (!(conflict->flags & IORESOURCE_BUSY)) {\\n\\t\\t\\t\\tparent = conflict;\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif (conflict->flags & flags & IORESOURCE_MUXED) {\\n\\t\\t\\tadd_wait_queue(&muxed_resource_wait, &wait);\\n\\t\\t\\twrite_unlock(&resource_lock);\\n\\t\\t\\tset_current_state(TASK_UNINTERRUPTIBLE);\\n\\t\\t\\tschedule();\\n\\t\\t\\tremove_wait_queue(&muxed_resource_wait, &wait);\\n\\t\\t\\twrite_lock(&resource_lock);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\t/* Uhhuh, that didn\\'t work out.. */\\n\\t\\treturn -EBUSY;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * __request_region - create a new busy resource region\\n * @parent: parent resource descriptor\\n * @start: resource start address\\n * @n: resource region size\\n * @name: reserving caller\\'s ID string\\n * @flags: IO resource flags\\n */\\nstruct resource *__request_region(struct resource *parent,\\n\\t\\t\\t\\t  resource_size_t start, resource_size_t n,\\n\\t\\t\\t\\t  const char *name, int flags)\\n{\\n\\tstruct resource *res = alloc_resource(GFP_KERNEL);\\n\\tint ret;\\n\\n\\tif (!res)\\n\\t\\treturn NULL;\\n\\n\\twrite_lock(&resource_lock);\\n\\tret = __request_region_locked(res, parent, start, n, name, flags);\\n\\twrite_unlock(&resource_lock);\\n\\n\\tif (ret) {\\n\\t\\tfree_resource(res);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tif (parent == &iomem_resource)\\n\\t\\trevoke_iomem(res);\\n\\n\\treturn res;\\n}\\nEXPORT_SYMBOL(__request_region);\\n\\n/**\\n * __release_region - release a previously reserved resource region\\n * @parent: parent resource descriptor\\n * @start: resource start address\\n * @n: resource region size\\n *\\n * The described resource region must match a currently busy region.\\n */\\nvoid __release_region(struct resource *parent, resource_size_t start,\\n\\t\\t      resource_size_t n)\\n{\\n\\tstruct resource **p;\\n\\tresource_size_t end;\\n\\n\\tp = &parent->child;\\n\\tend = start + n - 1;\\n\\n\\twrite_lock(&resource_lock);\\n\\n\\tfor (;;) {\\n\\t\\tstruct resource *res = *p;\\n\\n\\t\\tif (!res)\\n\\t\\t\\tbreak;\\n\\t\\tif (res->start <= start && res->end >= end) {\\n\\t\\t\\tif (!(res->flags & IORESOURCE_BUSY)) {\\n\\t\\t\\t\\tp = &res->child;\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\t\\t\\tif (res->start != start || res->end != end)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t*p = res->sibling;\\n\\t\\t\\twrite_unlock(&resource_lock);\\n\\t\\t\\tif (res->flags & IORESOURCE_MUXED)\\n\\t\\t\\t\\twake_up(&muxed_resource_wait);\\n\\t\\t\\tfree_resource(res);\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tp = &res->sibling;\\n\\t}\\n\\n\\twrite_unlock(&resource_lock);\\n\\n\\tpr_warn(\"Trying to free nonexistent resource <%pa-%pa>\\\\n\", &start, &end);\\n}\\nEXPORT_SYMBOL(__release_region);\\n\\n#ifdef CONFIG_MEMORY_HOTREMOVE\\n/**\\n * release_mem_region_adjustable - release a previously reserved memory region\\n * @start: resource start address\\n * @size: resource region size\\n *\\n * This interface is intended for memory hot-delete.  The requested region\\n * is released from a currently busy memory resource.  The requested region\\n * must either match exactly or fit into a single busy resource entry.  In\\n * the latter case, the remaining resource is adjusted accordingly.\\n * Existing children of the busy memory resource must be immutable in the\\n * request.\\n *\\n * Note:\\n * - Additional release conditions, such as overlapping region, can be\\n *   supported after they are confirmed as valid cases.\\n * - When a busy memory resource gets split into two entries, the code\\n *   assumes that all children remain in the lower address entry for\\n *   simplicity.  Enhance this logic when necessary.\\n */\\nvoid release_mem_region_adjustable(resource_size_t start, resource_size_t size)\\n{\\n\\tstruct resource *parent = &iomem_resource;\\n\\tstruct resource *new_res = NULL;\\n\\tbool alloc_nofail = false;\\n\\tstruct resource **p;\\n\\tstruct resource *res;\\n\\tresource_size_t end;\\n\\n\\tend = start + size - 1;\\n\\tif (WARN_ON_ONCE((start < parent->start) || (end > parent->end)))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * We free up quite a lot of memory on memory hotunplug (esp., memap),\\n\\t * just before releasing the region. This is highly unlikely to\\n\\t * fail - let\\'s play save and make it never fail as the caller cannot\\n\\t * perform any error handling (e.g., trying to re-add memory will fail\\n\\t * similarly).\\n\\t */\\nretry:\\n\\tnew_res = alloc_resource(GFP_KERNEL | (alloc_nofail ? __GFP_NOFAIL : 0));\\n\\n\\tp = &parent->child;\\n\\twrite_lock(&resource_lock);\\n\\n\\twhile ((res = *p)) {\\n\\t\\tif (res->start >= end)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* look for the next resource if it does not fit into */\\n\\t\\tif (res->start > start || res->end < end) {\\n\\t\\t\\tp = &res->sibling;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tif (!(res->flags & IORESOURCE_MEM))\\n\\t\\t\\tbreak;\\n\\n\\t\\tif (!(res->flags & IORESOURCE_BUSY)) {\\n\\t\\t\\tp = &res->child;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\t/* found the target resource; let\\'s adjust accordingly */\\n\\t\\tif (res->start == start && res->end == end) {\\n\\t\\t\\t/* free the whole entry */\\n\\t\\t\\t*p = res->sibling;\\n\\t\\t\\tfree_resource(res);\\n\\t\\t} else if (res->start == start && res->end != end) {\\n\\t\\t\\t/* adjust the start */\\n\\t\\t\\tWARN_ON_ONCE(__adjust_resource(res, end + 1,\\n\\t\\t\\t\\t\\t\\t       res->end - end));\\n\\t\\t} else if (res->start != start && res->end == end) {\\n\\t\\t\\t/* adjust the end */\\n\\t\\t\\tWARN_ON_ONCE(__adjust_resource(res, res->start,\\n\\t\\t\\t\\t\\t\\t       start - res->start));\\n\\t\\t} else {\\n\\t\\t\\t/* split into two entries - we need a new resource */\\n\\t\\t\\tif (!new_res) {\\n\\t\\t\\t\\tnew_res = alloc_resource(GFP_ATOMIC);\\n\\t\\t\\t\\tif (!new_res) {\\n\\t\\t\\t\\t\\talloc_nofail = true;\\n\\t\\t\\t\\t\\twrite_unlock(&resource_lock);\\n\\t\\t\\t\\t\\tgoto retry;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\tnew_res->name = res->name;\\n\\t\\t\\tnew_res->start = end + 1;\\n\\t\\t\\tnew_res->end = res->end;\\n\\t\\t\\tnew_res->flags = res->flags;\\n\\t\\t\\tnew_res->desc = res->desc;\\n\\t\\t\\tnew_res->parent = res->parent;\\n\\t\\t\\tnew_res->sibling = res->sibling;\\n\\t\\t\\tnew_res->child = NULL;\\n\\n\\t\\t\\tif (WARN_ON_ONCE(__adjust_resource(res, res->start,\\n\\t\\t\\t\\t\\t\\t\\t   start - res->start)))\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tres->sibling = new_res;\\n\\t\\t\\tnew_res = NULL;\\n\\t\\t}\\n\\n\\t\\tbreak;\\n\\t}\\n\\n\\twrite_unlock(&resource_lock);\\n\\tfree_resource(new_res);\\n}\\n#endif\\t/* CONFIG_MEMORY_HOTREMOVE */\\n\\n#ifdef CONFIG_MEMORY_HOTPLUG\\nstatic bool system_ram_resources_mergeable(struct resource *r1,\\n\\t\\t\\t\\t\\t   struct resource *r2)\\n{\\n\\t/* We assume either r1 or r2 is IORESOURCE_SYSRAM_MERGEABLE. */\\n\\treturn r1->flags == r2->flags && r1->end + 1 == r2->start &&\\n\\t       r1->name == r2->name && r1->desc == r2->desc &&\\n\\t       !r1->child && !r2->child;\\n}\\n\\n/**\\n * merge_system_ram_resource - mark the System RAM resource mergeable and try to\\n *\\tmerge it with adjacent, mergeable resources\\n * @res: resource descriptor\\n *\\n * This interface is intended for memory hotplug, whereby lots of contiguous\\n * system ram resources are added (e.g., via add_memory*()) by a driver, and\\n * the actual resource boundaries are not of interest (e.g., it might be\\n * relevant for DIMMs). Only resources that are marked mergeable, that have the\\n * same parent, and that don\\'t have any children are considered. All mergeable\\n * resources must be immutable during the request.\\n *\\n * Note:\\n * - The caller has to make sure that no pointers to resources that are\\n *   marked mergeable are used anymore after this call - the resource might\\n *   be freed and the pointer might be stale!\\n * - release_mem_region_adjustable() will split on demand on memory hotunplug\\n */\\nvoid merge_system_ram_resource(struct resource *res)\\n{\\n\\tconst unsigned long flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\\n\\tstruct resource *cur;\\n\\n\\tif (WARN_ON_ONCE((res->flags & flags) != flags))\\n\\t\\treturn;\\n\\n\\twrite_lock(&resource_lock);\\n\\tres->flags |= IORESOURCE_SYSRAM_MERGEABLE;\\n\\n\\t/* Try to merge with next item in the list. */\\n\\tcur = res->sibling;\\n\\tif (cur && system_ram_resources_mergeable(res, cur)) {\\n\\t\\tres->end = cur->end;\\n\\t\\tres->sibling = cur->sibling;\\n\\t\\tfree_resource(cur);\\n\\t}\\n\\n\\t/* Try to merge with previous item in the list. */\\n\\tcur = res->parent->child;\\n\\twhile (cur && cur->sibling != res)\\n\\t\\tcur = cur->sibling;\\n\\tif (cur && system_ram_resources_mergeable(cur, res)) {\\n\\t\\tcur->end = res->end;\\n\\t\\tcur->sibling = res->sibling;\\n\\t\\tfree_resource(res);\\n\\t}\\n\\twrite_unlock(&resource_lock);\\n}\\n#endif\\t/* CONFIG_MEMORY_HOTPLUG */\\n\\n/*\\n * Managed region resource\\n */\\nstatic void devm_resource_release(struct device *dev, void *ptr)\\n{\\n\\tstruct resource **r = ptr;\\n\\n\\trelease_resource(*r);\\n}\\n\\n/**\\n * devm_request_resource() - request and reserve an I/O or memory resource\\n * @dev: device for which to request the resource\\n * @root: root of the resource tree from which to request the resource\\n * @new: descriptor of the resource to request\\n *\\n * This is a device-managed version of request_resource(). There is usually\\n * no need to release resources requested by this function explicitly since\\n * that will be taken care of when the device is unbound from its driver.\\n * If for some reason the resource needs to be released explicitly, because\\n * of ordering issues for example, drivers must call devm_release_resource()\\n * rather than the regular release_resource().\\n *\\n * When a conflict is detected between any existing resources and the newly\\n * requested resource, an error message will be printed.\\n *\\n * Returns 0 on success or a negative error code on failure.\\n */\\nint devm_request_resource(struct device *dev, struct resource *root,\\n\\t\\t\\t  struct resource *new)\\n{\\n\\tstruct resource *conflict, **ptr;\\n\\n\\tptr = devres_alloc(devm_resource_release, sizeof(*ptr), GFP_KERNEL);\\n\\tif (!ptr)\\n\\t\\treturn -ENOMEM;\\n\\n\\t*ptr = new;\\n\\n\\tconflict = request_resource_conflict(root, new);\\n\\tif (conflict) {\\n\\t\\tdev_err(dev, \"resource collision: %pR conflicts with %s %pR\\\\n\",\\n\\t\\t\\tnew, conflict->name, conflict);\\n\\t\\tdevres_free(ptr);\\n\\t\\treturn -EBUSY;\\n\\t}\\n\\n\\tdevres_add(dev, ptr);\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(devm_request_resource);\\n\\nstatic int devm_resource_match(struct device *dev, void *res, void *data)\\n{\\n\\tstruct resource **ptr = res;\\n\\n\\treturn *ptr == data;\\n}\\n\\n/**\\n * devm_release_resource() - release a previously requested resource\\n * @dev: device for which to release the resource\\n * @new: descriptor of the resource to release\\n *\\n * Releases a resource previously requested using devm_request_resource().\\n */\\nvoid devm_release_resource(struct device *dev, struct resource *new)\\n{\\n\\tWARN_ON(devres_release(dev, devm_resource_release, devm_resource_match,\\n\\t\\t\\t       new));\\n}\\nEXPORT_SYMBOL(devm_release_resource);\\n\\nstruct region_devres {\\n\\tstruct resource *parent;\\n\\tresource_size_t start;\\n\\tresource_size_t n;\\n};\\n\\nstatic void devm_region_release(struct device *dev, void *res)\\n{\\n\\tstruct region_devres *this = res;\\n\\n\\t__release_region(this->parent, this->start, this->n);\\n}\\n\\nstatic int devm_region_match(struct device *dev, void *res, void *match_data)\\n{\\n\\tstruct region_devres *this = res, *match = match_data;\\n\\n\\treturn this->parent == match->parent &&\\n\\t\\tthis->start == match->start && this->n == match->n;\\n}\\n\\nstruct resource *\\n__devm_request_region(struct device *dev, struct resource *parent,\\n\\t\\t      resource_size_t start, resource_size_t n, const char *name)\\n{\\n\\tstruct region_devres *dr = NULL;\\n\\tstruct resource *res;\\n\\n\\tdr = devres_alloc(devm_region_release, sizeof(struct region_devres),\\n\\t\\t\\t  GFP_KERNEL);\\n\\tif (!dr)\\n\\t\\treturn NULL;\\n\\n\\tdr->parent = parent;\\n\\tdr->start = start;\\n\\tdr->n = n;\\n\\n\\tres = __request_region(parent, start, n, name, 0);\\n\\tif (res)\\n\\t\\tdevres_add(dev, dr);\\n\\telse\\n\\t\\tdevres_free(dr);\\n\\n\\treturn res;\\n}\\nEXPORT_SYMBOL(__devm_request_region);\\n\\nvoid __devm_release_region(struct device *dev, struct resource *parent,\\n\\t\\t\\t   resource_size_t start, resource_size_t n)\\n{\\n\\tstruct region_devres match_data = { parent, start, n };\\n\\n\\t__release_region(parent, start, n);\\n\\tWARN_ON(devres_destroy(dev, devm_region_release, devm_region_match,\\n\\t\\t\\t       &match_data));\\n}\\nEXPORT_SYMBOL(__devm_release_region);\\n\\n/*\\n * Reserve I/O ports or memory based on \"reserve=\" kernel parameter.\\n */\\n#define MAXRESERVE 4\\nstatic int __init reserve_setup(char *str)\\n{\\n\\tstatic int reserved;\\n\\tstatic struct resource reserve[MAXRESERVE];\\n\\n\\tfor (;;) {\\n\\t\\tunsigned int io_start, io_num;\\n\\t\\tint x = reserved;\\n\\t\\tstruct resource *parent;\\n\\n\\t\\tif (get_option(&str, &io_start) != 2)\\n\\t\\t\\tbreak;\\n\\t\\tif (get_option(&str, &io_num) == 0)\\n\\t\\t\\tbreak;\\n\\t\\tif (x < MAXRESERVE) {\\n\\t\\t\\tstruct resource *res = reserve + x;\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * If the region starts below 0x10000, we assume it\\'s\\n\\t\\t\\t * I/O port space; otherwise assume it\\'s memory.\\n\\t\\t\\t */\\n\\t\\t\\tif (io_start < 0x10000) {\\n\\t\\t\\t\\tres->flags = IORESOURCE_IO;\\n\\t\\t\\t\\tparent = &ioport_resource;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tres->flags = IORESOURCE_MEM;\\n\\t\\t\\t\\tparent = &iomem_resource;\\n\\t\\t\\t}\\n\\t\\t\\tres->name = \"reserved\";\\n\\t\\t\\tres->start = io_start;\\n\\t\\t\\tres->end = io_start + io_num - 1;\\n\\t\\t\\tres->flags |= IORESOURCE_BUSY;\\n\\t\\t\\tres->desc = IORES_DESC_NONE;\\n\\t\\t\\tres->child = NULL;\\n\\t\\t\\tif (request_resource(parent, res) == 0)\\n\\t\\t\\t\\treserved = x+1;\\n\\t\\t}\\n\\t}\\n\\treturn 1;\\n}\\n__setup(\"reserve=\", reserve_setup);\\n\\n/*\\n * Check if the requested addr and size spans more than any slot in the\\n * iomem resource tree.\\n */\\nint iomem_map_sanity_check(resource_size_t addr, unsigned long size)\\n{\\n\\tresource_size_t end = addr + size - 1;\\n\\tstruct resource *p;\\n\\tint err = 0;\\n\\n\\tread_lock(&resource_lock);\\n\\tfor_each_resource(&iomem_resource, p, false) {\\n\\t\\t/*\\n\\t\\t * We can probably skip the resources without\\n\\t\\t * IORESOURCE_IO attribute?\\n\\t\\t */\\n\\t\\tif (p->start > end)\\n\\t\\t\\tcontinue;\\n\\t\\tif (p->end < addr)\\n\\t\\t\\tcontinue;\\n\\t\\tif (PFN_DOWN(p->start) <= PFN_DOWN(addr) &&\\n\\t\\t    PFN_DOWN(p->end) >= PFN_DOWN(end))\\n\\t\\t\\tcontinue;\\n\\t\\t/*\\n\\t\\t * if a resource is \"BUSY\", it\\'s not a hardware resource\\n\\t\\t * but a driver mapping of such a resource; we don\\'t want\\n\\t\\t * to warn for those; some drivers legitimately map only\\n\\t\\t * partial hardware resources. (example: vesafb)\\n\\t\\t */\\n\\t\\tif (p->flags & IORESOURCE_BUSY)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tpr_warn(\"resource sanity check: requesting [mem %pa-%pa], which spans more than %s %pR\\\\n\",\\n\\t\\t\\t&addr, &end, p->name, p);\\n\\t\\terr = -1;\\n\\t\\tbreak;\\n\\t}\\n\\tread_unlock(&resource_lock);\\n\\n\\treturn err;\\n}\\n\\n#ifdef CONFIG_STRICT_DEVMEM\\nstatic int strict_iomem_checks = 1;\\n#else\\nstatic int strict_iomem_checks;\\n#endif\\n\\n/*\\n * Check if an address is exclusive to the kernel and must not be mapped to\\n * user space, for example, via /dev/mem.\\n *\\n * Returns true if exclusive to the kernel, otherwise returns false.\\n */\\nbool resource_is_exclusive(struct resource *root, u64 addr, resource_size_t size)\\n{\\n\\tconst unsigned int exclusive_system_ram = IORESOURCE_SYSTEM_RAM |\\n\\t\\t\\t\\t\\t\\t  IORESOURCE_EXCLUSIVE;\\n\\tbool skip_children = false, err = false;\\n\\tstruct resource *p;\\n\\n\\tread_lock(&resource_lock);\\n\\tfor_each_resource(root, p, skip_children) {\\n\\t\\tif (p->start >= addr + size)\\n\\t\\t\\tbreak;\\n\\t\\tif (p->end < addr) {\\n\\t\\t\\tskip_children = true;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tskip_children = false;\\n\\n\\t\\t/*\\n\\t\\t * IORESOURCE_SYSTEM_RAM resources are exclusive if\\n\\t\\t * IORESOURCE_EXCLUSIVE is set, even if they\\n\\t\\t * are not busy and even if \"iomem=relaxed\" is set. The\\n\\t\\t * responsible driver dynamically adds/removes system RAM within\\n\\t\\t * such an area and uncontrolled access is dangerous.\\n\\t\\t */\\n\\t\\tif ((p->flags & exclusive_system_ram) == exclusive_system_ram) {\\n\\t\\t\\terr = true;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * A resource is exclusive if IORESOURCE_EXCLUSIVE is set\\n\\t\\t * or CONFIG_IO_STRICT_DEVMEM is enabled and the\\n\\t\\t * resource is busy.\\n\\t\\t */\\n\\t\\tif (!strict_iomem_checks || !(p->flags & IORESOURCE_BUSY))\\n\\t\\t\\tcontinue;\\n\\t\\tif (IS_ENABLED(CONFIG_IO_STRICT_DEVMEM)\\n\\t\\t\\t\\t|| p->flags & IORESOURCE_EXCLUSIVE) {\\n\\t\\t\\terr = true;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\tread_unlock(&resource_lock);\\n\\n\\treturn err;\\n}\\n\\nbool iomem_is_exclusive(u64 addr)\\n{\\n\\treturn resource_is_exclusive(&iomem_resource, addr & PAGE_MASK,\\n\\t\\t\\t\\t     PAGE_SIZE);\\n}\\n\\nstruct resource_entry *resource_list_create_entry(struct resource *res,\\n\\t\\t\\t\\t\\t\\t  size_t extra_size)\\n{\\n\\tstruct resource_entry *entry;\\n\\n\\tentry = kzalloc(sizeof(*entry) + extra_size, GFP_KERNEL);\\n\\tif (entry) {\\n\\t\\tINIT_LIST_HEAD(&entry->node);\\n\\t\\tentry->res = res ? res : &entry->__res;\\n\\t}\\n\\n\\treturn entry;\\n}\\nEXPORT_SYMBOL(resource_list_create_entry);\\n\\nvoid resource_list_free(struct list_head *head)\\n{\\n\\tstruct resource_entry *entry, *tmp;\\n\\n\\tlist_for_each_entry_safe(entry, tmp, head, node)\\n\\t\\tresource_list_destroy_entry(entry);\\n}\\nEXPORT_SYMBOL(resource_list_free);\\n\\n#ifdef CONFIG_GET_FREE_REGION\\n#define GFR_DESCENDING\\t\\t(1UL << 0)\\n#define GFR_REQUEST_REGION\\t(1UL << 1)\\n#ifdef PA_SECTION_SHIFT\\n#define GFR_DEFAULT_ALIGN\\t(1UL << PA_SECTION_SHIFT)\\n#else\\n#define GFR_DEFAULT_ALIGN\\tPAGE_SIZE\\n#endif\\n\\nstatic resource_size_t gfr_start(struct resource *base, resource_size_t size,\\n\\t\\t\\t\\t resource_size_t align, unsigned long flags)\\n{\\n\\tif (flags & GFR_DESCENDING) {\\n\\t\\tresource_size_t end;\\n\\n\\t\\tend = min_t(resource_size_t, base->end, DIRECT_MAP_PHYSMEM_END);\\n\\t\\treturn end - size + 1;\\n\\t}\\n\\n\\treturn ALIGN(max(base->start, align), align);\\n}\\n\\nstatic bool gfr_continue(struct resource *base, resource_size_t addr,\\n\\t\\t\\t resource_size_t size, unsigned long flags)\\n{\\n\\tif (flags & GFR_DESCENDING)\\n\\t\\treturn addr > size && addr >= base->start;\\n\\t/*\\n\\t * In the ascend case be careful that the last increment by\\n\\t * @size did not wrap 0.\\n\\t */\\n\\treturn addr > addr - size &&\\n\\t       addr <= min_t(resource_size_t, base->end, DIRECT_MAP_PHYSMEM_END);\\n}\\n\\nstatic resource_size_t gfr_next(resource_size_t addr, resource_size_t size,\\n\\t\\t\\t\\tunsigned long flags)\\n{\\n\\tif (flags & GFR_DESCENDING)\\n\\t\\treturn addr - size;\\n\\treturn addr + size;\\n}\\n\\nstatic void remove_free_mem_region(void *_res)\\n{\\n\\tstruct resource *res = _res;\\n\\n\\tif (res->parent)\\n\\t\\tremove_resource(res);\\n\\tfree_resource(res);\\n}\\n\\nstatic struct resource *\\nget_free_mem_region(struct device *dev, struct resource *base,\\n\\t\\t    resource_size_t size, const unsigned long align,\\n\\t\\t    const char *name, const unsigned long desc,\\n\\t\\t    const unsigned long flags)\\n{\\n\\tresource_size_t addr;\\n\\tstruct resource *res;\\n\\tstruct region_devres *dr = NULL;\\n\\n\\tsize = ALIGN(size, align);\\n\\n\\tres = alloc_resource(GFP_KERNEL);\\n\\tif (!res)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tif (dev && (flags & GFR_REQUEST_REGION)) {\\n\\t\\tdr = devres_alloc(devm_region_release,\\n\\t\\t\\t\\tsizeof(struct region_devres), GFP_KERNEL);\\n\\t\\tif (!dr) {\\n\\t\\t\\tfree_resource(res);\\n\\t\\t\\treturn ERR_PTR(-ENOMEM);\\n\\t\\t}\\n\\t} else if (dev) {\\n\\t\\tif (devm_add_action_or_reset(dev, remove_free_mem_region, res))\\n\\t\\t\\treturn ERR_PTR(-ENOMEM);\\n\\t}\\n\\n\\twrite_lock(&resource_lock);\\n\\tfor (addr = gfr_start(base, size, align, flags);\\n\\t     gfr_continue(base, addr, align, flags);\\n\\t     addr = gfr_next(addr, align, flags)) {\\n\\t\\tif (__region_intersects(base, addr, size, 0, IORES_DESC_NONE) !=\\n\\t\\t    REGION_DISJOINT)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (flags & GFR_REQUEST_REGION) {\\n\\t\\t\\tif (__request_region_locked(res, &iomem_resource, addr,\\n\\t\\t\\t\\t\\t\\t    size, name, 0))\\n\\t\\t\\t\\tbreak;\\n\\n\\t\\t\\tif (dev) {\\n\\t\\t\\t\\tdr->parent = &iomem_resource;\\n\\t\\t\\t\\tdr->start = addr;\\n\\t\\t\\t\\tdr->n = size;\\n\\t\\t\\t\\tdevres_add(dev, dr);\\n\\t\\t\\t}\\n\\n\\t\\t\\tres->desc = desc;\\n\\t\\t\\twrite_unlock(&resource_lock);\\n\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * A driver is claiming this region so revoke any\\n\\t\\t\\t * mappings.\\n\\t\\t\\t */\\n\\t\\t\\trevoke_iomem(res);\\n\\t\\t} else {\\n\\t\\t\\tres->start = addr;\\n\\t\\t\\tres->end = addr + size - 1;\\n\\t\\t\\tres->name = name;\\n\\t\\t\\tres->desc = desc;\\n\\t\\t\\tres->flags = IORESOURCE_MEM;\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * Only succeed if the resource hosts an exclusive\\n\\t\\t\\t * range after the insert\\n\\t\\t\\t */\\n\\t\\t\\tif (__insert_resource(base, res) || res->child)\\n\\t\\t\\t\\tbreak;\\n\\n\\t\\t\\twrite_unlock(&resource_lock);\\n\\t\\t}\\n\\n\\t\\treturn res;\\n\\t}\\n\\twrite_unlock(&resource_lock);\\n\\n\\tif (flags & GFR_REQUEST_REGION) {\\n\\t\\tfree_resource(res);\\n\\t\\tdevres_free(dr);\\n\\t} else if (dev)\\n\\t\\tdevm_release_action(dev, remove_free_mem_region, res);\\n\\n\\treturn ERR_PTR(-ERANGE);\\n}\\n\\n/**\\n * devm_request_free_mem_region - find free region for device private memory\\n *\\n * @dev: device struct to bind the resource to\\n * @size: size in bytes of the device memory to add\\n * @base: resource tree to look in\\n *\\n * This function tries to find an empty range of physical address big enough to\\n * contain the new resource, so that it can later be hotplugged as ZONE_DEVICE\\n * memory, which in turn allocates struct pages.\\n */\\nstruct resource *devm_request_free_mem_region(struct device *dev,\\n\\t\\tstruct resource *base, unsigned long size)\\n{\\n\\tunsigned long flags = GFR_DESCENDING | GFR_REQUEST_REGION;\\n\\n\\treturn get_free_mem_region(dev, base, size, GFR_DEFAULT_ALIGN,\\n\\t\\t\\t\\t   dev_name(dev),\\n\\t\\t\\t\\t   IORES_DESC_DEVICE_PRIVATE_MEMORY, flags);\\n}\\nEXPORT_SYMBOL_GPL(devm_request_free_mem_region);\\n\\nstruct resource *request_free_mem_region(struct resource *base,\\n\\t\\tunsigned long size, const char *name)\\n{\\n\\tunsigned long flags = GFR_DESCENDING | GFR_REQUEST_REGION;\\n\\n\\treturn get_free_mem_region(NULL, base, size, GFR_DEFAULT_ALIGN, name,\\n\\t\\t\\t\\t   IORES_DESC_DEVICE_PRIVATE_MEMORY, flags);\\n}\\nEXPORT_SYMBOL_GPL(request_free_mem_region);\\n\\n/**\\n * alloc_free_mem_region - find a free region relative to @base\\n * @base: resource that will parent the new resource\\n * @size: size in bytes of memory to allocate from @base\\n * @align: alignment requirements for the allocation\\n * @name: resource name\\n *\\n * Buses like CXL, that can dynamically instantiate new memory regions,\\n * need a method to allocate physical address space for those regions.\\n * Allocate and insert a new resource to cover a free, unclaimed by a\\n * descendant of @base, range in the span of @base.\\n */\\nstruct resource *alloc_free_mem_region(struct resource *base,\\n\\t\\t\\t\\t       unsigned long size, unsigned long align,\\n\\t\\t\\t\\t       const char *name)\\n{\\n\\t/* Default of ascending direction and insert resource */\\n\\tunsigned long flags = 0;\\n\\n\\treturn get_free_mem_region(NULL, base, size, align, name,\\n\\t\\t\\t\\t   IORES_DESC_NONE, flags);\\n}\\nEXPORT_SYMBOL_GPL(alloc_free_mem_region);\\n#endif /* CONFIG_GET_FREE_REGION */\\n\\nstatic int __init strict_iomem(char *str)\\n{\\n\\tif (strstr(str, \"relaxed\"))\\n\\t\\tstrict_iomem_checks = 0;\\n\\tif (strstr(str, \"strict\"))\\n\\t\\tstrict_iomem_checks = 1;\\n\\treturn 1;\\n}\\n\\nstatic int iomem_fs_init_fs_context(struct fs_context *fc)\\n{\\n\\treturn init_pseudo(fc, DEVMEM_MAGIC) ? 0 : -ENOMEM;\\n}\\n\\nstatic struct file_system_type iomem_fs_type = {\\n\\t.name\\t\\t= \"iomem\",\\n\\t.owner\\t\\t= THIS_MODULE,\\n\\t.init_fs_context = iomem_fs_init_fs_context,\\n\\t.kill_sb\\t= kill_anon_super,\\n};\\n\\nstatic int __init iomem_init_inode(void)\\n{\\n\\tstatic struct vfsmount *iomem_vfs_mount;\\n\\tstatic int iomem_fs_cnt;\\n\\tstruct inode *inode;\\n\\tint rc;\\n\\n\\trc = simple_pin_fs(&iomem_fs_type, &iomem_vfs_mount, &iomem_fs_cnt);\\n\\tif (rc < 0) {\\n\\t\\tpr_err(\"Cannot mount iomem pseudo filesystem: %d\\\\n\", rc);\\n\\t\\treturn rc;\\n\\t}\\n\\n\\tinode = alloc_anon_inode(iomem_vfs_mount->mnt_sb);\\n\\tif (IS_ERR(inode)) {\\n\\t\\trc = PTR_ERR(inode);\\n\\t\\tpr_err(\"Cannot allocate inode for iomem: %d\\\\n\", rc);\\n\\t\\tsimple_release_fs(&iomem_vfs_mount, &iomem_fs_cnt);\\n\\t\\treturn rc;\\n\\t}\\n\\n\\t/*\\n\\t * Publish iomem revocation inode initialized.\\n\\t * Pairs with smp_load_acquire() in revoke_iomem().\\n\\t */\\n\\tsmp_store_release(&iomem_inode, inode);\\n\\n\\treturn 0;\\n}\\n\\nfs_initcall(iomem_init_inode);\\n\\n__setup(\"iomem=\", strict_iomem);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * kernel/ksysfs.c - sysfs attributes in /sys/kernel, which\\n * \\t\\t     are not related to any other subsystem\\n *\\n * Copyright (C) 2004 Kay Sievers <kay.sievers@vrfy.org>\\n */\\n\\n#include <asm/byteorder.h>\\n#include <linux/kobject.h>\\n#include <linux/string.h>\\n#include <linux/sysfs.h>\\n#include <linux/export.h>\\n#include <linux/init.h>\\n#include <linux/kexec.h>\\n#include <linux/profile.h>\\n#include <linux/stat.h>\\n#include <linux/sched.h>\\n#include <linux/capability.h>\\n#include <linux/compiler.h>\\n\\n#include <linux/rcupdate.h>\\t/* rcu_expedited and rcu_normal */\\n\\n#if defined(__LITTLE_ENDIAN)\\n#define CPU_BYTEORDER_STRING\\t\"little\"\\n#elif defined(__BIG_ENDIAN)\\n#define CPU_BYTEORDER_STRING\\t\"big\"\\n#else\\n#error Unknown byteorder\\n#endif\\n\\n#define KERNEL_ATTR_RO(_name) \\\\\\nstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\\n\\n#define KERNEL_ATTR_RW(_name) \\\\\\nstatic struct kobj_attribute _name##_attr = __ATTR_RW(_name)\\n\\n/* current uevent sequence number */\\nstatic ssize_t uevent_seqnum_show(struct kobject *kobj,\\n\\t\\t\\t\\t  struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%llu\\\\n\", (u64)atomic64_read(&uevent_seqnum));\\n}\\nKERNEL_ATTR_RO(uevent_seqnum);\\n\\n/* cpu byteorder */\\nstatic ssize_t cpu_byteorder_show(struct kobject *kobj,\\n\\t\\t\\t\\t  struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%s\\\\n\", CPU_BYTEORDER_STRING);\\n}\\nKERNEL_ATTR_RO(cpu_byteorder);\\n\\n/* address bits */\\nstatic ssize_t address_bits_show(struct kobject *kobj,\\n\\t\\t\\t\\t struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%zu\\\\n\", sizeof(void *) * 8 /* CHAR_BIT */);\\n}\\nKERNEL_ATTR_RO(address_bits);\\n\\n#ifdef CONFIG_UEVENT_HELPER\\n/* uevent helper program, used during early boot */\\nstatic ssize_t uevent_helper_show(struct kobject *kobj,\\n\\t\\t\\t\\t  struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%s\\\\n\", uevent_helper);\\n}\\nstatic ssize_t uevent_helper_store(struct kobject *kobj,\\n\\t\\t\\t\\t   struct kobj_attribute *attr,\\n\\t\\t\\t\\t   const char *buf, size_t count)\\n{\\n\\tif (count+1 > UEVENT_HELPER_PATH_LEN)\\n\\t\\treturn -ENOENT;\\n\\tmemcpy(uevent_helper, buf, count);\\n\\tuevent_helper[count] = \\'\\\\0\\';\\n\\tif (count && uevent_helper[count-1] == \\'\\\\n\\')\\n\\t\\tuevent_helper[count-1] = \\'\\\\0\\';\\n\\treturn count;\\n}\\nKERNEL_ATTR_RW(uevent_helper);\\n#endif\\n\\n#ifdef CONFIG_PROFILING\\nstatic ssize_t profiling_show(struct kobject *kobj,\\n\\t\\t\\t\\t  struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", prof_on);\\n}\\nstatic ssize_t profiling_store(struct kobject *kobj,\\n\\t\\t\\t\\t   struct kobj_attribute *attr,\\n\\t\\t\\t\\t   const char *buf, size_t count)\\n{\\n\\tint ret;\\n\\tstatic DEFINE_MUTEX(lock);\\n\\n\\t/*\\n\\t * We need serialization, for profile_setup() initializes prof_on\\n\\t * value and profile_init() must not reallocate prof_buffer after\\n\\t * once allocated.\\n\\t */\\n\\tguard(mutex)(&lock);\\n\\tif (prof_on)\\n\\t\\treturn -EEXIST;\\n\\t/*\\n\\t * This eventually calls into get_option() which\\n\\t * has a ton of callers and is not const.  It is\\n\\t * easiest to cast it away here.\\n\\t */\\n\\tprofile_setup((char *)buf);\\n\\tret = profile_init();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\tret = create_proc_profile();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\treturn count;\\n}\\nKERNEL_ATTR_RW(profiling);\\n#endif\\n\\n#ifdef CONFIG_KEXEC_CORE\\nstatic ssize_t kexec_loaded_show(struct kobject *kobj,\\n\\t\\t\\t\\t struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", !!kexec_image);\\n}\\nKERNEL_ATTR_RO(kexec_loaded);\\n\\n#ifdef CONFIG_CRASH_DUMP\\nstatic ssize_t kexec_crash_loaded_show(struct kobject *kobj,\\n\\t\\t\\t\\t       struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", kexec_crash_loaded());\\n}\\nKERNEL_ATTR_RO(kexec_crash_loaded);\\n\\nstatic ssize_t kexec_crash_size_show(struct kobject *kobj,\\n\\t\\t\\t\\t       struct kobj_attribute *attr, char *buf)\\n{\\n\\tssize_t size = crash_get_memory_size();\\n\\n\\tif (size < 0)\\n\\t\\treturn size;\\n\\n\\treturn sysfs_emit(buf, \"%zd\\\\n\", size);\\n}\\nstatic ssize_t kexec_crash_size_store(struct kobject *kobj,\\n\\t\\t\\t\\t   struct kobj_attribute *attr,\\n\\t\\t\\t\\t   const char *buf, size_t count)\\n{\\n\\tunsigned long cnt;\\n\\tint ret;\\n\\n\\tif (kstrtoul(buf, 0, &cnt))\\n\\t\\treturn -EINVAL;\\n\\n\\tret = crash_shrink_memory(cnt);\\n\\treturn ret < 0 ? ret : count;\\n}\\nKERNEL_ATTR_RW(kexec_crash_size);\\n\\n#endif /* CONFIG_CRASH_DUMP*/\\n#endif /* CONFIG_KEXEC_CORE */\\n\\n#ifdef CONFIG_VMCORE_INFO\\n\\nstatic ssize_t vmcoreinfo_show(struct kobject *kobj,\\n\\t\\t\\t       struct kobj_attribute *attr, char *buf)\\n{\\n\\tphys_addr_t vmcore_base = paddr_vmcoreinfo_note();\\n\\treturn sysfs_emit(buf, \"%pa %x\\\\n\", &vmcore_base,\\n\\t\\t\\t  (unsigned int)VMCOREINFO_NOTE_SIZE);\\n}\\nKERNEL_ATTR_RO(vmcoreinfo);\\n\\n#ifdef CONFIG_CRASH_HOTPLUG\\nstatic ssize_t crash_elfcorehdr_size_show(struct kobject *kobj,\\n\\t\\t\\t       struct kobj_attribute *attr, char *buf)\\n{\\n\\tunsigned int sz = crash_get_elfcorehdr_size();\\n\\n\\treturn sysfs_emit(buf, \"%u\\\\n\", sz);\\n}\\nKERNEL_ATTR_RO(crash_elfcorehdr_size);\\n\\n#endif\\n\\n#endif /* CONFIG_VMCORE_INFO */\\n\\n/* whether file capabilities are enabled */\\nstatic ssize_t fscaps_show(struct kobject *kobj,\\n\\t\\t\\t\\t  struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", file_caps_enabled);\\n}\\nKERNEL_ATTR_RO(fscaps);\\n\\n#ifndef CONFIG_TINY_RCU\\nint rcu_expedited;\\nstatic ssize_t rcu_expedited_show(struct kobject *kobj,\\n\\t\\t\\t\\t  struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", READ_ONCE(rcu_expedited));\\n}\\nstatic ssize_t rcu_expedited_store(struct kobject *kobj,\\n\\t\\t\\t\\t   struct kobj_attribute *attr,\\n\\t\\t\\t\\t   const char *buf, size_t count)\\n{\\n\\tif (kstrtoint(buf, 0, &rcu_expedited))\\n\\t\\treturn -EINVAL;\\n\\n\\treturn count;\\n}\\nKERNEL_ATTR_RW(rcu_expedited);\\n\\nint rcu_normal;\\nstatic ssize_t rcu_normal_show(struct kobject *kobj,\\n\\t\\t\\t       struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", READ_ONCE(rcu_normal));\\n}\\nstatic ssize_t rcu_normal_store(struct kobject *kobj,\\n\\t\\t\\t\\tstruct kobj_attribute *attr,\\n\\t\\t\\t\\tconst char *buf, size_t count)\\n{\\n\\tif (kstrtoint(buf, 0, &rcu_normal))\\n\\t\\treturn -EINVAL;\\n\\n\\treturn count;\\n}\\nKERNEL_ATTR_RW(rcu_normal);\\n#endif /* #ifndef CONFIG_TINY_RCU */\\n\\n/*\\n * Make /sys/kernel/notes give the raw contents of our kernel .notes section.\\n */\\nextern const void __start_notes;\\nextern const void __stop_notes;\\n#define\\tnotes_size (&__stop_notes - &__start_notes)\\n\\nstatic ssize_t notes_read(struct file *filp, struct kobject *kobj,\\n\\t\\t\\t  struct bin_attribute *bin_attr,\\n\\t\\t\\t  char *buf, loff_t off, size_t count)\\n{\\n\\tmemcpy(buf, &__start_notes + off, count);\\n\\treturn count;\\n}\\n\\nstatic struct bin_attribute notes_attr __ro_after_init  = {\\n\\t.attr = {\\n\\t\\t.name = \"notes\",\\n\\t\\t.mode = S_IRUGO,\\n\\t},\\n\\t.read = &notes_read,\\n};\\n\\nstruct kobject *kernel_kobj;\\nEXPORT_SYMBOL_GPL(kernel_kobj);\\n\\nstatic struct attribute * kernel_attrs[] = {\\n\\t&fscaps_attr.attr,\\n\\t&uevent_seqnum_attr.attr,\\n\\t&cpu_byteorder_attr.attr,\\n\\t&address_bits_attr.attr,\\n#ifdef CONFIG_UEVENT_HELPER\\n\\t&uevent_helper_attr.attr,\\n#endif\\n#ifdef CONFIG_PROFILING\\n\\t&profiling_attr.attr,\\n#endif\\n#ifdef CONFIG_KEXEC_CORE\\n\\t&kexec_loaded_attr.attr,\\n#ifdef CONFIG_CRASH_DUMP\\n\\t&kexec_crash_loaded_attr.attr,\\n\\t&kexec_crash_size_attr.attr,\\n#endif\\n#endif\\n#ifdef CONFIG_VMCORE_INFO\\n\\t&vmcoreinfo_attr.attr,\\n#ifdef CONFIG_CRASH_HOTPLUG\\n\\t&crash_elfcorehdr_size_attr.attr,\\n#endif\\n#endif\\n#ifndef CONFIG_TINY_RCU\\n\\t&rcu_expedited_attr.attr,\\n\\t&rcu_normal_attr.attr,\\n#endif\\n\\tNULL\\n};\\n\\nstatic const struct attribute_group kernel_attr_group = {\\n\\t.attrs = kernel_attrs,\\n};\\n\\nstatic int __init ksysfs_init(void)\\n{\\n\\tint error;\\n\\n\\tkernel_kobj = kobject_create_and_add(\"kernel\", NULL);\\n\\tif (!kernel_kobj) {\\n\\t\\terror = -ENOMEM;\\n\\t\\tgoto exit;\\n\\t}\\n\\terror = sysfs_create_group(kernel_kobj, &kernel_attr_group);\\n\\tif (error)\\n\\t\\tgoto kset_exit;\\n\\n\\tif (notes_size > 0) {\\n\\t\\tnotes_attr.size = notes_size;\\n\\t\\terror = sysfs_create_bin_file(kernel_kobj, &notes_attr);\\n\\t\\tif (error)\\n\\t\\t\\tgoto group_exit;\\n\\t}\\n\\n\\treturn 0;\\n\\ngroup_exit:\\n\\tsysfs_remove_group(kernel_kobj, &kernel_attr_group);\\nkset_exit:\\n\\tkobject_put(kernel_kobj);\\nexit:\\n\\treturn error;\\n}\\n\\ncore_initcall(ksysfs_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *\\tlinux/kernel/softirq.c\\n *\\n *\\tCopyright (C) 1992 Linus Torvalds\\n *\\n *\\tRewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/export.h>\\n#include <linux/kernel_stat.h>\\n#include <linux/interrupt.h>\\n#include <linux/init.h>\\n#include <linux/local_lock.h>\\n#include <linux/mm.h>\\n#include <linux/notifier.h>\\n#include <linux/percpu.h>\\n#include <linux/cpu.h>\\n#include <linux/freezer.h>\\n#include <linux/kthread.h>\\n#include <linux/rcupdate.h>\\n#include <linux/ftrace.h>\\n#include <linux/smp.h>\\n#include <linux/smpboot.h>\\n#include <linux/tick.h>\\n#include <linux/irq.h>\\n#include <linux/wait_bit.h>\\n#include <linux/workqueue.h>\\n\\n#include <asm/softirq_stack.h>\\n\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/irq.h>\\n\\n/*\\n   - No shared variables, all the data are CPU local.\\n   - If a softirq needs serialization, let it serialize itself\\n     by its own spinlocks.\\n   - Even if softirq is serialized, only local cpu is marked for\\n     execution. Hence, we get something sort of weak cpu binding.\\n     Though it is still not clear, will it result in better locality\\n     or will not.\\n\\n   Examples:\\n   - NET RX softirq. It is multithreaded and does not require\\n     any global serialization.\\n   - NET TX softirq. It kicks software netdevice queues, hence\\n     it is logically serialized per device, but this serialization\\n     is invisible to common code.\\n   - Tasklets: serialized wrt itself.\\n */\\n\\n#ifndef __ARCH_IRQ_STAT\\nDEFINE_PER_CPU_ALIGNED(irq_cpustat_t, irq_stat);\\nEXPORT_PER_CPU_SYMBOL(irq_stat);\\n#endif\\n\\nstatic struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;\\n\\nDEFINE_PER_CPU(struct task_struct *, ksoftirqd);\\n\\nconst char * const softirq_to_name[NR_SOFTIRQS] = {\\n\\t\"HI\", \"TIMER\", \"NET_TX\", \"NET_RX\", \"BLOCK\", \"IRQ_POLL\",\\n\\t\"TASKLET\", \"SCHED\", \"HRTIMER\", \"RCU\"\\n};\\n\\n/*\\n * we cannot loop indefinitely here to avoid userspace starvation,\\n * but we also don\\'t want to introduce a worst case 1/HZ latency\\n * to the pending events, so lets the scheduler to balance\\n * the softirq load for us.\\n */\\nstatic void wakeup_softirqd(void)\\n{\\n\\t/* Interrupts are disabled: no need to stop preemption */\\n\\tstruct task_struct *tsk = __this_cpu_read(ksoftirqd);\\n\\n\\tif (tsk)\\n\\t\\twake_up_process(tsk);\\n}\\n\\n#ifdef CONFIG_TRACE_IRQFLAGS\\nDEFINE_PER_CPU(int, hardirqs_enabled);\\nDEFINE_PER_CPU(int, hardirq_context);\\nEXPORT_PER_CPU_SYMBOL_GPL(hardirqs_enabled);\\nEXPORT_PER_CPU_SYMBOL_GPL(hardirq_context);\\n#endif\\n\\n/*\\n * SOFTIRQ_OFFSET usage:\\n *\\n * On !RT kernels \\'count\\' is the preempt counter, on RT kernels this applies\\n * to a per CPU counter and to task::softirqs_disabled_cnt.\\n *\\n * - count is changed by SOFTIRQ_OFFSET on entering or leaving softirq\\n *   processing.\\n *\\n * - count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)\\n *   on local_bh_disable or local_bh_enable.\\n *\\n * This lets us distinguish between whether we are currently processing\\n * softirq and whether we just have bh disabled.\\n */\\n#ifdef CONFIG_PREEMPT_RT\\n\\n/*\\n * RT accounts for BH disabled sections in task::softirqs_disabled_cnt and\\n * also in per CPU softirq_ctrl::cnt. This is necessary to allow tasks in a\\n * softirq disabled section to be preempted.\\n *\\n * The per task counter is used for softirq_count(), in_softirq() and\\n * in_serving_softirqs() because these counts are only valid when the task\\n * holding softirq_ctrl::lock is running.\\n *\\n * The per CPU counter prevents pointless wakeups of ksoftirqd in case that\\n * the task which is in a softirq disabled section is preempted or blocks.\\n */\\nstruct softirq_ctrl {\\n\\tlocal_lock_t\\tlock;\\n\\tint\\t\\tcnt;\\n};\\n\\nstatic DEFINE_PER_CPU(struct softirq_ctrl, softirq_ctrl) = {\\n\\t.lock\\t= INIT_LOCAL_LOCK(softirq_ctrl.lock),\\n};\\n\\n/**\\n * local_bh_blocked() - Check for idle whether BH processing is blocked\\n *\\n * Returns false if the per CPU softirq::cnt is 0 otherwise true.\\n *\\n * This is invoked from the idle task to guard against false positive\\n * softirq pending warnings, which would happen when the task which holds\\n * softirq_ctrl::lock was the only running task on the CPU and blocks on\\n * some other lock.\\n */\\nbool local_bh_blocked(void)\\n{\\n\\treturn __this_cpu_read(softirq_ctrl.cnt) != 0;\\n}\\n\\nvoid __local_bh_disable_ip(unsigned long ip, unsigned int cnt)\\n{\\n\\tunsigned long flags;\\n\\tint newcnt;\\n\\n\\tWARN_ON_ONCE(in_hardirq());\\n\\n\\t/* First entry of a task into a BH disabled section? */\\n\\tif (!current->softirq_disable_cnt) {\\n\\t\\tif (preemptible()) {\\n\\t\\t\\tlocal_lock(&softirq_ctrl.lock);\\n\\t\\t\\t/* Required to meet the RCU bottomhalf requirements. */\\n\\t\\t\\trcu_read_lock();\\n\\t\\t} else {\\n\\t\\t\\tDEBUG_LOCKS_WARN_ON(this_cpu_read(softirq_ctrl.cnt));\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * Track the per CPU softirq disabled state. On RT this is per CPU\\n\\t * state to allow preemption of bottom half disabled sections.\\n\\t */\\n\\tnewcnt = __this_cpu_add_return(softirq_ctrl.cnt, cnt);\\n\\t/*\\n\\t * Reflect the result in the task state to prevent recursion on the\\n\\t * local lock and to make softirq_count() & al work.\\n\\t */\\n\\tcurrent->softirq_disable_cnt = newcnt;\\n\\n\\tif (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && newcnt == cnt) {\\n\\t\\traw_local_irq_save(flags);\\n\\t\\tlockdep_softirqs_off(ip);\\n\\t\\traw_local_irq_restore(flags);\\n\\t}\\n}\\nEXPORT_SYMBOL(__local_bh_disable_ip);\\n\\nstatic void __local_bh_enable(unsigned int cnt, bool unlock)\\n{\\n\\tunsigned long flags;\\n\\tint newcnt;\\n\\n\\tDEBUG_LOCKS_WARN_ON(current->softirq_disable_cnt !=\\n\\t\\t\\t    this_cpu_read(softirq_ctrl.cnt));\\n\\n\\tif (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && softirq_count() == cnt) {\\n\\t\\traw_local_irq_save(flags);\\n\\t\\tlockdep_softirqs_on(_RET_IP_);\\n\\t\\traw_local_irq_restore(flags);\\n\\t}\\n\\n\\tnewcnt = __this_cpu_sub_return(softirq_ctrl.cnt, cnt);\\n\\tcurrent->softirq_disable_cnt = newcnt;\\n\\n\\tif (!newcnt && unlock) {\\n\\t\\trcu_read_unlock();\\n\\t\\tlocal_unlock(&softirq_ctrl.lock);\\n\\t}\\n}\\n\\nvoid __local_bh_enable_ip(unsigned long ip, unsigned int cnt)\\n{\\n\\tbool preempt_on = preemptible();\\n\\tunsigned long flags;\\n\\tu32 pending;\\n\\tint curcnt;\\n\\n\\tWARN_ON_ONCE(in_hardirq());\\n\\tlockdep_assert_irqs_enabled();\\n\\n\\tlocal_irq_save(flags);\\n\\tcurcnt = __this_cpu_read(softirq_ctrl.cnt);\\n\\n\\t/*\\n\\t * If this is not reenabling soft interrupts, no point in trying to\\n\\t * run pending ones.\\n\\t */\\n\\tif (curcnt != cnt)\\n\\t\\tgoto out;\\n\\n\\tpending = local_softirq_pending();\\n\\tif (!pending)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * If this was called from non preemptible context, wake up the\\n\\t * softirq daemon.\\n\\t */\\n\\tif (!preempt_on) {\\n\\t\\twakeup_softirqd();\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * Adjust softirq count to SOFTIRQ_OFFSET which makes\\n\\t * in_serving_softirq() become true.\\n\\t */\\n\\tcnt = SOFTIRQ_OFFSET;\\n\\t__local_bh_enable(cnt, false);\\n\\t__do_softirq();\\n\\nout:\\n\\t__local_bh_enable(cnt, preempt_on);\\n\\tlocal_irq_restore(flags);\\n}\\nEXPORT_SYMBOL(__local_bh_enable_ip);\\n\\n/*\\n * Invoked from ksoftirqd_run() outside of the interrupt disabled section\\n * to acquire the per CPU local lock for reentrancy protection.\\n */\\nstatic inline void ksoftirqd_run_begin(void)\\n{\\n\\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);\\n\\tlocal_irq_disable();\\n}\\n\\n/* Counterpart to ksoftirqd_run_begin() */\\nstatic inline void ksoftirqd_run_end(void)\\n{\\n\\t__local_bh_enable(SOFTIRQ_OFFSET, true);\\n\\tWARN_ON_ONCE(in_interrupt());\\n\\tlocal_irq_enable();\\n}\\n\\nstatic inline void softirq_handle_begin(void) { }\\nstatic inline void softirq_handle_end(void) { }\\n\\nstatic inline bool should_wake_ksoftirqd(void)\\n{\\n\\treturn !this_cpu_read(softirq_ctrl.cnt);\\n}\\n\\nstatic inline void invoke_softirq(void)\\n{\\n\\tif (should_wake_ksoftirqd())\\n\\t\\twakeup_softirqd();\\n}\\n\\n#define SCHED_SOFTIRQ_MASK\\tBIT(SCHED_SOFTIRQ)\\n\\n/*\\n * flush_smp_call_function_queue() can raise a soft interrupt in a function\\n * call. On RT kernels this is undesired and the only known functionalities\\n * are in the block layer which is disabled on RT, and in the scheduler for\\n * idle load balancing. If soft interrupts get raised which haven\\'t been\\n * raised before the flush, warn if it is not a SCHED_SOFTIRQ so it can be\\n * investigated.\\n */\\nvoid do_softirq_post_smp_call_flush(unsigned int was_pending)\\n{\\n\\tunsigned int is_pending = local_softirq_pending();\\n\\n\\tif (unlikely(was_pending != is_pending)) {\\n\\t\\tWARN_ON_ONCE(was_pending != (is_pending & ~SCHED_SOFTIRQ_MASK));\\n\\t\\tinvoke_softirq();\\n\\t}\\n}\\n\\n#else /* CONFIG_PREEMPT_RT */\\n\\n/*\\n * This one is for softirq.c-internal use, where hardirqs are disabled\\n * legitimately:\\n */\\n#ifdef CONFIG_TRACE_IRQFLAGS\\nvoid __local_bh_disable_ip(unsigned long ip, unsigned int cnt)\\n{\\n\\tunsigned long flags;\\n\\n\\tWARN_ON_ONCE(in_hardirq());\\n\\n\\traw_local_irq_save(flags);\\n\\t/*\\n\\t * The preempt tracer hooks into preempt_count_add and will break\\n\\t * lockdep because it calls back into lockdep after SOFTIRQ_OFFSET\\n\\t * is set and before current->softirq_enabled is cleared.\\n\\t * We must manually increment preempt_count here and manually\\n\\t * call the trace_preempt_off later.\\n\\t */\\n\\t__preempt_count_add(cnt);\\n\\t/*\\n\\t * Were softirqs turned off above:\\n\\t */\\n\\tif (softirq_count() == (cnt & SOFTIRQ_MASK))\\n\\t\\tlockdep_softirqs_off(ip);\\n\\traw_local_irq_restore(flags);\\n\\n\\tif (preempt_count() == cnt) {\\n#ifdef CONFIG_DEBUG_PREEMPT\\n\\t\\tcurrent->preempt_disable_ip = get_lock_parent_ip();\\n#endif\\n\\t\\ttrace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());\\n\\t}\\n}\\nEXPORT_SYMBOL(__local_bh_disable_ip);\\n#endif /* CONFIG_TRACE_IRQFLAGS */\\n\\nstatic void __local_bh_enable(unsigned int cnt)\\n{\\n\\tlockdep_assert_irqs_disabled();\\n\\n\\tif (preempt_count() == cnt)\\n\\t\\ttrace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());\\n\\n\\tif (softirq_count() == (cnt & SOFTIRQ_MASK))\\n\\t\\tlockdep_softirqs_on(_RET_IP_);\\n\\n\\t__preempt_count_sub(cnt);\\n}\\n\\n/*\\n * Special-case - softirqs can safely be enabled by __do_softirq(),\\n * without processing still-pending softirqs:\\n */\\nvoid _local_bh_enable(void)\\n{\\n\\tWARN_ON_ONCE(in_hardirq());\\n\\t__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);\\n}\\nEXPORT_SYMBOL(_local_bh_enable);\\n\\nvoid __local_bh_enable_ip(unsigned long ip, unsigned int cnt)\\n{\\n\\tWARN_ON_ONCE(in_hardirq());\\n\\tlockdep_assert_irqs_enabled();\\n#ifdef CONFIG_TRACE_IRQFLAGS\\n\\tlocal_irq_disable();\\n#endif\\n\\t/*\\n\\t * Are softirqs going to be turned on now:\\n\\t */\\n\\tif (softirq_count() == SOFTIRQ_DISABLE_OFFSET)\\n\\t\\tlockdep_softirqs_on(ip);\\n\\t/*\\n\\t * Keep preemption disabled until we are done with\\n\\t * softirq processing:\\n\\t */\\n\\t__preempt_count_sub(cnt - 1);\\n\\n\\tif (unlikely(!in_interrupt() && local_softirq_pending())) {\\n\\t\\t/*\\n\\t\\t * Run softirq if any pending. And do it in its own stack\\n\\t\\t * as we may be calling this deep in a task call stack already.\\n\\t\\t */\\n\\t\\tdo_softirq();\\n\\t}\\n\\n\\tpreempt_count_dec();\\n#ifdef CONFIG_TRACE_IRQFLAGS\\n\\tlocal_irq_enable();\\n#endif\\n\\tpreempt_check_resched();\\n}\\nEXPORT_SYMBOL(__local_bh_enable_ip);\\n\\nstatic inline void softirq_handle_begin(void)\\n{\\n\\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);\\n}\\n\\nstatic inline void softirq_handle_end(void)\\n{\\n\\t__local_bh_enable(SOFTIRQ_OFFSET);\\n\\tWARN_ON_ONCE(in_interrupt());\\n}\\n\\nstatic inline void ksoftirqd_run_begin(void)\\n{\\n\\tlocal_irq_disable();\\n}\\n\\nstatic inline void ksoftirqd_run_end(void)\\n{\\n\\tlocal_irq_enable();\\n}\\n\\nstatic inline bool should_wake_ksoftirqd(void)\\n{\\n\\treturn true;\\n}\\n\\nstatic inline void invoke_softirq(void)\\n{\\n\\tif (!force_irqthreads() || !__this_cpu_read(ksoftirqd)) {\\n#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK\\n\\t\\t/*\\n\\t\\t * We can safely execute softirq on the current stack if\\n\\t\\t * it is the irq stack, because it should be near empty\\n\\t\\t * at this stage.\\n\\t\\t */\\n\\t\\t__do_softirq();\\n#else\\n\\t\\t/*\\n\\t\\t * Otherwise, irq_exit() is called on the task stack that can\\n\\t\\t * be potentially deep already. So call softirq in its own stack\\n\\t\\t * to prevent from any overrun.\\n\\t\\t */\\n\\t\\tdo_softirq_own_stack();\\n#endif\\n\\t} else {\\n\\t\\twakeup_softirqd();\\n\\t}\\n}\\n\\nasmlinkage __visible void do_softirq(void)\\n{\\n\\t__u32 pending;\\n\\tunsigned long flags;\\n\\n\\tif (in_interrupt())\\n\\t\\treturn;\\n\\n\\tlocal_irq_save(flags);\\n\\n\\tpending = local_softirq_pending();\\n\\n\\tif (pending)\\n\\t\\tdo_softirq_own_stack();\\n\\n\\tlocal_irq_restore(flags);\\n}\\n\\n#endif /* !CONFIG_PREEMPT_RT */\\n\\n/*\\n * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,\\n * but break the loop if need_resched() is set or after 2 ms.\\n * The MAX_SOFTIRQ_TIME provides a nice upper bound in most cases, but in\\n * certain cases, such as stop_machine(), jiffies may cease to\\n * increment and so we need the MAX_SOFTIRQ_RESTART limit as\\n * well to make sure we eventually return from this method.\\n *\\n * These limits have been established via experimentation.\\n * The two things to balance is latency against fairness -\\n * we want to handle softirqs as soon as possible, but they\\n * should not be able to lock up the box.\\n */\\n#define MAX_SOFTIRQ_TIME  msecs_to_jiffies(2)\\n#define MAX_SOFTIRQ_RESTART 10\\n\\n#ifdef CONFIG_TRACE_IRQFLAGS\\n/*\\n * When we run softirqs from irq_exit() and thus on the hardirq stack we need\\n * to keep the lockdep irq context tracking as tight as possible in order to\\n * not miss-qualify lock contexts and miss possible deadlocks.\\n */\\n\\nstatic inline bool lockdep_softirq_start(void)\\n{\\n\\tbool in_hardirq = false;\\n\\n\\tif (lockdep_hardirq_context()) {\\n\\t\\tin_hardirq = true;\\n\\t\\tlockdep_hardirq_exit();\\n\\t}\\n\\n\\tlockdep_softirq_enter();\\n\\n\\treturn in_hardirq;\\n}\\n\\nstatic inline void lockdep_softirq_end(bool in_hardirq)\\n{\\n\\tlockdep_softirq_exit();\\n\\n\\tif (in_hardirq)\\n\\t\\tlockdep_hardirq_enter();\\n}\\n#else\\nstatic inline bool lockdep_softirq_start(void) { return false; }\\nstatic inline void lockdep_softirq_end(bool in_hardirq) { }\\n#endif\\n\\nstatic void handle_softirqs(bool ksirqd)\\n{\\n\\tunsigned long end = jiffies + MAX_SOFTIRQ_TIME;\\n\\tunsigned long old_flags = current->flags;\\n\\tint max_restart = MAX_SOFTIRQ_RESTART;\\n\\tstruct softirq_action *h;\\n\\tbool in_hardirq;\\n\\t__u32 pending;\\n\\tint softirq_bit;\\n\\n\\t/*\\n\\t * Mask out PF_MEMALLOC as the current task context is borrowed for the\\n\\t * softirq. A softirq handled, such as network RX, might set PF_MEMALLOC\\n\\t * again if the socket is related to swapping.\\n\\t */\\n\\tcurrent->flags &= ~PF_MEMALLOC;\\n\\n\\tpending = local_softirq_pending();\\n\\n\\tsoftirq_handle_begin();\\n\\tin_hardirq = lockdep_softirq_start();\\n\\taccount_softirq_enter(current);\\n\\nrestart:\\n\\t/* Reset the pending bitmask before enabling irqs */\\n\\tset_softirq_pending(0);\\n\\n\\tlocal_irq_enable();\\n\\n\\th = softirq_vec;\\n\\n\\twhile ((softirq_bit = ffs(pending))) {\\n\\t\\tunsigned int vec_nr;\\n\\t\\tint prev_count;\\n\\n\\t\\th += softirq_bit - 1;\\n\\n\\t\\tvec_nr = h - softirq_vec;\\n\\t\\tprev_count = preempt_count();\\n\\n\\t\\tkstat_incr_softirqs_this_cpu(vec_nr);\\n\\n\\t\\ttrace_softirq_entry(vec_nr);\\n\\t\\th->action();\\n\\t\\ttrace_softirq_exit(vec_nr);\\n\\t\\tif (unlikely(prev_count != preempt_count())) {\\n\\t\\t\\tpr_err(\"huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\\\\n\",\\n\\t\\t\\t       vec_nr, softirq_to_name[vec_nr], h->action,\\n\\t\\t\\t       prev_count, preempt_count());\\n\\t\\t\\tpreempt_count_set(prev_count);\\n\\t\\t}\\n\\t\\th++;\\n\\t\\tpending >>= softirq_bit;\\n\\t}\\n\\n\\tif (!IS_ENABLED(CONFIG_PREEMPT_RT) && ksirqd)\\n\\t\\trcu_softirq_qs();\\n\\n\\tlocal_irq_disable();\\n\\n\\tpending = local_softirq_pending();\\n\\tif (pending) {\\n\\t\\tif (time_before(jiffies, end) && !need_resched() &&\\n\\t\\t    --max_restart)\\n\\t\\t\\tgoto restart;\\n\\n\\t\\twakeup_softirqd();\\n\\t}\\n\\n\\taccount_softirq_exit(current);\\n\\tlockdep_softirq_end(in_hardirq);\\n\\tsoftirq_handle_end();\\n\\tcurrent_restore_flags(old_flags, PF_MEMALLOC);\\n}\\n\\nasmlinkage __visible void __softirq_entry __do_softirq(void)\\n{\\n\\thandle_softirqs(false);\\n}\\n\\n/**\\n * irq_enter_rcu - Enter an interrupt context with RCU watching\\n */\\nvoid irq_enter_rcu(void)\\n{\\n\\t__irq_enter_raw();\\n\\n\\tif (tick_nohz_full_cpu(smp_processor_id()) ||\\n\\t    (is_idle_task(current) && (irq_count() == HARDIRQ_OFFSET)))\\n\\t\\ttick_irq_enter();\\n\\n\\taccount_hardirq_enter(current);\\n}\\n\\n/**\\n * irq_enter - Enter an interrupt context including RCU update\\n */\\nvoid irq_enter(void)\\n{\\n\\tct_irq_enter();\\n\\tirq_enter_rcu();\\n}\\n\\nstatic inline void tick_irq_exit(void)\\n{\\n#ifdef CONFIG_NO_HZ_COMMON\\n\\tint cpu = smp_processor_id();\\n\\n\\t/* Make sure that timer wheel updates are propagated */\\n\\tif ((sched_core_idle_cpu(cpu) && !need_resched()) || tick_nohz_full_cpu(cpu)) {\\n\\t\\tif (!in_hardirq())\\n\\t\\t\\ttick_nohz_irq_exit();\\n\\t}\\n#endif\\n}\\n\\n#ifdef CONFIG_IRQ_FORCED_THREADING\\nDEFINE_PER_CPU(struct task_struct *, ktimerd);\\nDEFINE_PER_CPU(unsigned long, pending_timer_softirq);\\n\\nstatic void wake_timersd(void)\\n{\\n\\tstruct task_struct *tsk = __this_cpu_read(ktimerd);\\n\\n\\tif (tsk)\\n\\t\\twake_up_process(tsk);\\n}\\n\\n#else\\n\\nstatic inline void wake_timersd(void) { }\\n\\n#endif\\n\\nstatic inline void __irq_exit_rcu(void)\\n{\\n#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED\\n\\tlocal_irq_disable();\\n#else\\n\\tlockdep_assert_irqs_disabled();\\n#endif\\n\\taccount_hardirq_exit(current);\\n\\tpreempt_count_sub(HARDIRQ_OFFSET);\\n\\tif (!in_interrupt() && local_softirq_pending())\\n\\t\\tinvoke_softirq();\\n\\n\\tif (IS_ENABLED(CONFIG_IRQ_FORCED_THREADING) && force_irqthreads() &&\\n\\t    local_timers_pending_force_th() && !(in_nmi() | in_hardirq()))\\n\\t\\twake_timersd();\\n\\n\\ttick_irq_exit();\\n}\\n\\n/**\\n * irq_exit_rcu() - Exit an interrupt context without updating RCU\\n *\\n * Also processes softirqs if needed and possible.\\n */\\nvoid irq_exit_rcu(void)\\n{\\n\\t__irq_exit_rcu();\\n\\t /* must be last! */\\n\\tlockdep_hardirq_exit();\\n}\\n\\n/**\\n * irq_exit - Exit an interrupt context, update RCU and lockdep\\n *\\n * Also processes softirqs if needed and possible.\\n */\\nvoid irq_exit(void)\\n{\\n\\t__irq_exit_rcu();\\n\\tct_irq_exit();\\n\\t /* must be last! */\\n\\tlockdep_hardirq_exit();\\n}\\n\\n/*\\n * This function must run with irqs disabled!\\n */\\ninline void raise_softirq_irqoff(unsigned int nr)\\n{\\n\\t__raise_softirq_irqoff(nr);\\n\\n\\t/*\\n\\t * If we\\'re in an interrupt or softirq, we\\'re done\\n\\t * (this also catches softirq-disabled code). We will\\n\\t * actually run the softirq once we return from\\n\\t * the irq or softirq.\\n\\t *\\n\\t * Otherwise we wake up ksoftirqd to make sure we\\n\\t * schedule the softirq soon.\\n\\t */\\n\\tif (!in_interrupt() && should_wake_ksoftirqd())\\n\\t\\twakeup_softirqd();\\n}\\n\\nvoid raise_softirq(unsigned int nr)\\n{\\n\\tunsigned long flags;\\n\\n\\tlocal_irq_save(flags);\\n\\traise_softirq_irqoff(nr);\\n\\tlocal_irq_restore(flags);\\n}\\n\\nvoid __raise_softirq_irqoff(unsigned int nr)\\n{\\n\\tlockdep_assert_irqs_disabled();\\n\\ttrace_softirq_raise(nr);\\n\\tor_softirq_pending(1UL << nr);\\n}\\n\\nvoid open_softirq(int nr, void (*action)(void))\\n{\\n\\tsoftirq_vec[nr].action = action;\\n}\\n\\n/*\\n * Tasklets\\n */\\nstruct tasklet_head {\\n\\tstruct tasklet_struct *head;\\n\\tstruct tasklet_struct **tail;\\n};\\n\\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);\\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);\\n\\nstatic void __tasklet_schedule_common(struct tasklet_struct *t,\\n\\t\\t\\t\\t      struct tasklet_head __percpu *headp,\\n\\t\\t\\t\\t      unsigned int softirq_nr)\\n{\\n\\tstruct tasklet_head *head;\\n\\tunsigned long flags;\\n\\n\\tlocal_irq_save(flags);\\n\\thead = this_cpu_ptr(headp);\\n\\tt->next = NULL;\\n\\t*head->tail = t;\\n\\thead->tail = &(t->next);\\n\\traise_softirq_irqoff(softirq_nr);\\n\\tlocal_irq_restore(flags);\\n}\\n\\nvoid __tasklet_schedule(struct tasklet_struct *t)\\n{\\n\\t__tasklet_schedule_common(t, &tasklet_vec,\\n\\t\\t\\t\\t  TASKLET_SOFTIRQ);\\n}\\nEXPORT_SYMBOL(__tasklet_schedule);\\n\\nvoid __tasklet_hi_schedule(struct tasklet_struct *t)\\n{\\n\\t__tasklet_schedule_common(t, &tasklet_hi_vec,\\n\\t\\t\\t\\t  HI_SOFTIRQ);\\n}\\nEXPORT_SYMBOL(__tasklet_hi_schedule);\\n\\nstatic bool tasklet_clear_sched(struct tasklet_struct *t)\\n{\\n\\tif (test_and_clear_wake_up_bit(TASKLET_STATE_SCHED, &t->state))\\n\\t\\treturn true;\\n\\n\\tWARN_ONCE(1, \"tasklet SCHED state not set: %s %pS\\\\n\",\\n\\t\\t  t->use_callback ? \"callback\" : \"func\",\\n\\t\\t  t->use_callback ? (void *)t->callback : (void *)t->func);\\n\\n\\treturn false;\\n}\\n\\nstatic void tasklet_action_common(struct tasklet_head *tl_head,\\n\\t\\t\\t\\t  unsigned int softirq_nr)\\n{\\n\\tstruct tasklet_struct *list;\\n\\n\\tlocal_irq_disable();\\n\\tlist = tl_head->head;\\n\\ttl_head->head = NULL;\\n\\ttl_head->tail = &tl_head->head;\\n\\tlocal_irq_enable();\\n\\n\\twhile (list) {\\n\\t\\tstruct tasklet_struct *t = list;\\n\\n\\t\\tlist = list->next;\\n\\n\\t\\tif (tasklet_trylock(t)) {\\n\\t\\t\\tif (!atomic_read(&t->count)) {\\n\\t\\t\\t\\tif (tasklet_clear_sched(t)) {\\n\\t\\t\\t\\t\\tif (t->use_callback) {\\n\\t\\t\\t\\t\\t\\ttrace_tasklet_entry(t, t->callback);\\n\\t\\t\\t\\t\\t\\tt->callback(t);\\n\\t\\t\\t\\t\\t\\ttrace_tasklet_exit(t, t->callback);\\n\\t\\t\\t\\t\\t} else {\\n\\t\\t\\t\\t\\t\\ttrace_tasklet_entry(t, t->func);\\n\\t\\t\\t\\t\\t\\tt->func(t->data);\\n\\t\\t\\t\\t\\t\\ttrace_tasklet_exit(t, t->func);\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\ttasklet_unlock(t);\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\t\\t\\ttasklet_unlock(t);\\n\\t\\t}\\n\\n\\t\\tlocal_irq_disable();\\n\\t\\tt->next = NULL;\\n\\t\\t*tl_head->tail = t;\\n\\t\\ttl_head->tail = &t->next;\\n\\t\\t__raise_softirq_irqoff(softirq_nr);\\n\\t\\tlocal_irq_enable();\\n\\t}\\n}\\n\\nstatic __latent_entropy void tasklet_action(void)\\n{\\n\\tworkqueue_softirq_action(false);\\n\\ttasklet_action_common(this_cpu_ptr(&tasklet_vec), TASKLET_SOFTIRQ);\\n}\\n\\nstatic __latent_entropy void tasklet_hi_action(void)\\n{\\n\\tworkqueue_softirq_action(true);\\n\\ttasklet_action_common(this_cpu_ptr(&tasklet_hi_vec), HI_SOFTIRQ);\\n}\\n\\nvoid tasklet_setup(struct tasklet_struct *t,\\n\\t\\t   void (*callback)(struct tasklet_struct *))\\n{\\n\\tt->next = NULL;\\n\\tt->state = 0;\\n\\tatomic_set(&t->count, 0);\\n\\tt->callback = callback;\\n\\tt->use_callback = true;\\n\\tt->data = 0;\\n}\\nEXPORT_SYMBOL(tasklet_setup);\\n\\nvoid tasklet_init(struct tasklet_struct *t,\\n\\t\\t  void (*func)(unsigned long), unsigned long data)\\n{\\n\\tt->next = NULL;\\n\\tt->state = 0;\\n\\tatomic_set(&t->count, 0);\\n\\tt->func = func;\\n\\tt->use_callback = false;\\n\\tt->data = data;\\n}\\nEXPORT_SYMBOL(tasklet_init);\\n\\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)\\n/*\\n * Do not use in new code. Waiting for tasklets from atomic contexts is\\n * error prone and should be avoided.\\n */\\nvoid tasklet_unlock_spin_wait(struct tasklet_struct *t)\\n{\\n\\twhile (test_bit(TASKLET_STATE_RUN, &(t)->state)) {\\n\\t\\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Prevent a live lock when current preempted soft\\n\\t\\t\\t * interrupt processing or prevents ksoftirqd from\\n\\t\\t\\t * running. If the tasklet runs on a different CPU\\n\\t\\t\\t * then this has no effect other than doing the BH\\n\\t\\t\\t * disable/enable dance for nothing.\\n\\t\\t\\t */\\n\\t\\t\\tlocal_bh_disable();\\n\\t\\t\\tlocal_bh_enable();\\n\\t\\t} else {\\n\\t\\t\\tcpu_relax();\\n\\t\\t}\\n\\t}\\n}\\nEXPORT_SYMBOL(tasklet_unlock_spin_wait);\\n#endif\\n\\nvoid tasklet_kill(struct tasklet_struct *t)\\n{\\n\\tif (in_interrupt())\\n\\t\\tpr_notice(\"Attempt to kill tasklet from interrupt\\\\n\");\\n\\n\\twait_on_bit_lock(&t->state, TASKLET_STATE_SCHED, TASK_UNINTERRUPTIBLE);\\n\\n\\ttasklet_unlock_wait(t);\\n\\ttasklet_clear_sched(t);\\n}\\nEXPORT_SYMBOL(tasklet_kill);\\n\\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)\\nvoid tasklet_unlock(struct tasklet_struct *t)\\n{\\n\\tclear_and_wake_up_bit(TASKLET_STATE_RUN, &t->state);\\n}\\nEXPORT_SYMBOL_GPL(tasklet_unlock);\\n\\nvoid tasklet_unlock_wait(struct tasklet_struct *t)\\n{\\n\\twait_on_bit(&t->state, TASKLET_STATE_RUN, TASK_UNINTERRUPTIBLE);\\n}\\nEXPORT_SYMBOL_GPL(tasklet_unlock_wait);\\n#endif\\n\\nvoid __init softirq_init(void)\\n{\\n\\tint cpu;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tper_cpu(tasklet_vec, cpu).tail =\\n\\t\\t\\t&per_cpu(tasklet_vec, cpu).head;\\n\\t\\tper_cpu(tasklet_hi_vec, cpu).tail =\\n\\t\\t\\t&per_cpu(tasklet_hi_vec, cpu).head;\\n\\t}\\n\\n\\topen_softirq(TASKLET_SOFTIRQ, tasklet_action);\\n\\topen_softirq(HI_SOFTIRQ, tasklet_hi_action);\\n}\\n\\nstatic int ksoftirqd_should_run(unsigned int cpu)\\n{\\n\\treturn local_softirq_pending();\\n}\\n\\nstatic void run_ksoftirqd(unsigned int cpu)\\n{\\n\\tksoftirqd_run_begin();\\n\\tif (local_softirq_pending()) {\\n\\t\\t/*\\n\\t\\t * We can safely run softirq on inline stack, as we are not deep\\n\\t\\t * in the task stack here.\\n\\t\\t */\\n\\t\\thandle_softirqs(true);\\n\\t\\tksoftirqd_run_end();\\n\\t\\tcond_resched();\\n\\t\\treturn;\\n\\t}\\n\\tksoftirqd_run_end();\\n}\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\nstatic int takeover_tasklets(unsigned int cpu)\\n{\\n\\tworkqueue_softirq_dead(cpu);\\n\\n\\t/* CPU is dead, so no lock needed. */\\n\\tlocal_irq_disable();\\n\\n\\t/* Find end, append list for that CPU. */\\n\\tif (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {\\n\\t\\t*__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;\\n\\t\\t__this_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);\\n\\t\\tper_cpu(tasklet_vec, cpu).head = NULL;\\n\\t\\tper_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;\\n\\t}\\n\\traise_softirq_irqoff(TASKLET_SOFTIRQ);\\n\\n\\tif (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {\\n\\t\\t*__this_cpu_read(tasklet_hi_vec.tail) = per_cpu(tasklet_hi_vec, cpu).head;\\n\\t\\t__this_cpu_write(tasklet_hi_vec.tail, per_cpu(tasklet_hi_vec, cpu).tail);\\n\\t\\tper_cpu(tasklet_hi_vec, cpu).head = NULL;\\n\\t\\tper_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;\\n\\t}\\n\\traise_softirq_irqoff(HI_SOFTIRQ);\\n\\n\\tlocal_irq_enable();\\n\\treturn 0;\\n}\\n#else\\n#define takeover_tasklets\\tNULL\\n#endif /* CONFIG_HOTPLUG_CPU */\\n\\nstatic struct smp_hotplug_thread softirq_threads = {\\n\\t.store\\t\\t\\t= &ksoftirqd,\\n\\t.thread_should_run\\t= ksoftirqd_should_run,\\n\\t.thread_fn\\t\\t= run_ksoftirqd,\\n\\t.thread_comm\\t\\t= \"ksoftirqd/%u\",\\n};\\n\\n#ifdef CONFIG_IRQ_FORCED_THREADING\\nstatic void ktimerd_setup(unsigned int cpu)\\n{\\n\\t/* Above SCHED_NORMAL to handle timers before regular tasks. */\\n\\tsched_set_fifo_low(current);\\n}\\n\\nstatic int ktimerd_should_run(unsigned int cpu)\\n{\\n\\treturn local_timers_pending_force_th();\\n}\\n\\nvoid raise_ktimers_thread(unsigned int nr)\\n{\\n\\ttrace_softirq_raise(nr);\\n\\t__this_cpu_or(pending_timer_softirq, BIT(nr));\\n}\\n\\nstatic void run_ktimerd(unsigned int cpu)\\n{\\n\\tunsigned int timer_si;\\n\\n\\tksoftirqd_run_begin();\\n\\n\\ttimer_si = local_timers_pending_force_th();\\n\\t__this_cpu_write(pending_timer_softirq, 0);\\n\\tor_softirq_pending(timer_si);\\n\\n\\t__do_softirq();\\n\\n\\tksoftirqd_run_end();\\n}\\n\\nstatic struct smp_hotplug_thread timer_thread = {\\n\\t.store\\t\\t\\t= &ktimerd,\\n\\t.setup\\t\\t\\t= ktimerd_setup,\\n\\t.thread_should_run\\t= ktimerd_should_run,\\n\\t.thread_fn\\t\\t= run_ktimerd,\\n\\t.thread_comm\\t\\t= \"ktimers/%u\",\\n};\\n#endif\\n\\nstatic __init int spawn_ksoftirqd(void)\\n{\\n\\tcpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, \"softirq:dead\", NULL,\\n\\t\\t\\t\\t  takeover_tasklets);\\n\\tBUG_ON(smpboot_register_percpu_thread(&softirq_threads));\\n#ifdef CONFIG_IRQ_FORCED_THREADING\\n\\tif (force_irqthreads())\\n\\t\\tBUG_ON(smpboot_register_percpu_thread(&timer_thread));\\n#endif\\n\\treturn 0;\\n}\\nearly_initcall(spawn_ksoftirqd);\\n\\n/*\\n * [ These __weak aliases are kept in a separate compilation unit, so that\\n *   GCC does not inline them incorrectly. ]\\n */\\n\\nint __init __weak early_irq_init(void)\\n{\\n\\treturn 0;\\n}\\n\\nint __init __weak arch_probe_nr_irqs(void)\\n{\\n\\treturn NR_IRQS_LEGACY;\\n}\\n\\nint __init __weak arch_early_irq_init(void)\\n{\\n\\treturn 0;\\n}\\n\\nunsigned int __weak arch_dynirq_lower_bound(unsigned int from)\\n{\\n\\treturn from;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * padata.c - generic interface to process data streams in parallel\\n *\\n * See Documentation/core-api/padata.rst for more information.\\n *\\n * Copyright (C) 2008, 2009 secunet Security Networks AG\\n * Copyright (C) 2008, 2009 Steffen Klassert <steffen.klassert@secunet.com>\\n *\\n * Copyright (c) 2020 Oracle and/or its affiliates.\\n * Author: Daniel Jordan <daniel.m.jordan@oracle.com>\\n */\\n\\n#include <linux/completion.h>\\n#include <linux/export.h>\\n#include <linux/cpumask.h>\\n#include <linux/err.h>\\n#include <linux/cpu.h>\\n#include <linux/padata.h>\\n#include <linux/mutex.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/sysfs.h>\\n#include <linux/rcupdate.h>\\n\\n#define\\tPADATA_WORK_ONSTACK\\t1\\t/* Work\\'s memory is on stack */\\n\\nstruct padata_work {\\n\\tstruct work_struct\\tpw_work;\\n\\tstruct list_head\\tpw_list;  /* padata_free_works linkage */\\n\\tvoid\\t\\t\\t*pw_data;\\n};\\n\\nstatic DEFINE_SPINLOCK(padata_works_lock);\\nstatic struct padata_work *padata_works;\\nstatic LIST_HEAD(padata_free_works);\\n\\nstruct padata_mt_job_state {\\n\\tspinlock_t\\t\\tlock;\\n\\tstruct completion\\tcompletion;\\n\\tstruct padata_mt_job\\t*job;\\n\\tint\\t\\t\\tnworks;\\n\\tint\\t\\t\\tnworks_fini;\\n\\tunsigned long\\t\\tchunk_size;\\n};\\n\\nstatic void padata_free_pd(struct parallel_data *pd);\\nstatic void __init padata_mt_helper(struct work_struct *work);\\n\\nstatic int padata_index_to_cpu(struct parallel_data *pd, int cpu_index)\\n{\\n\\tint cpu, target_cpu;\\n\\n\\ttarget_cpu = cpumask_first(pd->cpumask.pcpu);\\n\\tfor (cpu = 0; cpu < cpu_index; cpu++)\\n\\t\\ttarget_cpu = cpumask_next(target_cpu, pd->cpumask.pcpu);\\n\\n\\treturn target_cpu;\\n}\\n\\nstatic int padata_cpu_hash(struct parallel_data *pd, unsigned int seq_nr)\\n{\\n\\t/*\\n\\t * Hash the sequence numbers to the cpus by taking\\n\\t * seq_nr mod. number of cpus in use.\\n\\t */\\n\\tint cpu_index = seq_nr % cpumask_weight(pd->cpumask.pcpu);\\n\\n\\treturn padata_index_to_cpu(pd, cpu_index);\\n}\\n\\nstatic struct padata_work *padata_work_alloc(void)\\n{\\n\\tstruct padata_work *pw;\\n\\n\\tlockdep_assert_held(&padata_works_lock);\\n\\n\\tif (list_empty(&padata_free_works))\\n\\t\\treturn NULL;\\t/* No more work items allowed to be queued. */\\n\\n\\tpw = list_first_entry(&padata_free_works, struct padata_work, pw_list);\\n\\tlist_del(&pw->pw_list);\\n\\treturn pw;\\n}\\n\\n/*\\n * This function is marked __ref because this function may be optimized in such\\n * a way that it directly refers to work_fn\\'s address, which causes modpost to\\n * complain when work_fn is marked __init. This scenario was observed with clang\\n * LTO, where padata_work_init() was optimized to refer directly to\\n * padata_mt_helper() because the calls to padata_work_init() with other work_fn\\n * values were eliminated or inlined.\\n */\\nstatic void __ref padata_work_init(struct padata_work *pw, work_func_t work_fn,\\n\\t\\t\\t\\t   void *data, int flags)\\n{\\n\\tif (flags & PADATA_WORK_ONSTACK)\\n\\t\\tINIT_WORK_ONSTACK(&pw->pw_work, work_fn);\\n\\telse\\n\\t\\tINIT_WORK(&pw->pw_work, work_fn);\\n\\tpw->pw_data = data;\\n}\\n\\nstatic int __init padata_work_alloc_mt(int nworks, void *data,\\n\\t\\t\\t\\t       struct list_head *head)\\n{\\n\\tint i;\\n\\n\\tspin_lock_bh(&padata_works_lock);\\n\\t/* Start at 1 because the current task participates in the job. */\\n\\tfor (i = 1; i < nworks; ++i) {\\n\\t\\tstruct padata_work *pw = padata_work_alloc();\\n\\n\\t\\tif (!pw)\\n\\t\\t\\tbreak;\\n\\t\\tpadata_work_init(pw, padata_mt_helper, data, 0);\\n\\t\\tlist_add(&pw->pw_list, head);\\n\\t}\\n\\tspin_unlock_bh(&padata_works_lock);\\n\\n\\treturn i;\\n}\\n\\nstatic void padata_work_free(struct padata_work *pw)\\n{\\n\\tlockdep_assert_held(&padata_works_lock);\\n\\tlist_add(&pw->pw_list, &padata_free_works);\\n}\\n\\nstatic void __init padata_works_free(struct list_head *works)\\n{\\n\\tstruct padata_work *cur, *next;\\n\\n\\tif (list_empty(works))\\n\\t\\treturn;\\n\\n\\tspin_lock_bh(&padata_works_lock);\\n\\tlist_for_each_entry_safe(cur, next, works, pw_list) {\\n\\t\\tlist_del(&cur->pw_list);\\n\\t\\tpadata_work_free(cur);\\n\\t}\\n\\tspin_unlock_bh(&padata_works_lock);\\n}\\n\\nstatic void padata_parallel_worker(struct work_struct *parallel_work)\\n{\\n\\tstruct padata_work *pw = container_of(parallel_work, struct padata_work,\\n\\t\\t\\t\\t\\t      pw_work);\\n\\tstruct padata_priv *padata = pw->pw_data;\\n\\n\\tlocal_bh_disable();\\n\\tpadata->parallel(padata);\\n\\tspin_lock(&padata_works_lock);\\n\\tpadata_work_free(pw);\\n\\tspin_unlock(&padata_works_lock);\\n\\tlocal_bh_enable();\\n}\\n\\n/**\\n * padata_do_parallel - padata parallelization function\\n *\\n * @ps: padatashell\\n * @padata: object to be parallelized\\n * @cb_cpu: pointer to the CPU that the serialization callback function should\\n *          run on.  If it\\'s not in the serial cpumask of @pinst\\n *          (i.e. cpumask.cbcpu), this function selects a fallback CPU and if\\n *          none found, returns -EINVAL.\\n *\\n * The parallelization callback function will run with BHs off.\\n * Note: Every object which is parallelized by padata_do_parallel\\n * must be seen by padata_do_serial.\\n *\\n * Return: 0 on success or else negative error code.\\n */\\nint padata_do_parallel(struct padata_shell *ps,\\n\\t\\t       struct padata_priv *padata, int *cb_cpu)\\n{\\n\\tstruct padata_instance *pinst = ps->pinst;\\n\\tint i, cpu, cpu_index, err;\\n\\tstruct parallel_data *pd;\\n\\tstruct padata_work *pw;\\n\\n\\trcu_read_lock_bh();\\n\\n\\tpd = rcu_dereference_bh(ps->pd);\\n\\n\\terr = -EINVAL;\\n\\tif (!(pinst->flags & PADATA_INIT) || pinst->flags & PADATA_INVALID)\\n\\t\\tgoto out;\\n\\n\\tif (!cpumask_test_cpu(*cb_cpu, pd->cpumask.cbcpu)) {\\n\\t\\tif (cpumask_empty(pd->cpumask.cbcpu))\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/* Select an alternate fallback CPU and notify the caller. */\\n\\t\\tcpu_index = *cb_cpu % cpumask_weight(pd->cpumask.cbcpu);\\n\\n\\t\\tcpu = cpumask_first(pd->cpumask.cbcpu);\\n\\t\\tfor (i = 0; i < cpu_index; i++)\\n\\t\\t\\tcpu = cpumask_next(cpu, pd->cpumask.cbcpu);\\n\\n\\t\\t*cb_cpu = cpu;\\n\\t}\\n\\n\\terr = -EBUSY;\\n\\tif ((pinst->flags & PADATA_RESET))\\n\\t\\tgoto out;\\n\\n\\trefcount_inc(&pd->refcnt);\\n\\tpadata->pd = pd;\\n\\tpadata->cb_cpu = *cb_cpu;\\n\\n\\tspin_lock(&padata_works_lock);\\n\\tpadata->seq_nr = ++pd->seq_nr;\\n\\tpw = padata_work_alloc();\\n\\tspin_unlock(&padata_works_lock);\\n\\n\\tif (!pw) {\\n\\t\\t/* Maximum works limit exceeded, run in the current task. */\\n\\t\\tpadata->parallel(padata);\\n\\t}\\n\\n\\trcu_read_unlock_bh();\\n\\n\\tif (pw) {\\n\\t\\tpadata_work_init(pw, padata_parallel_worker, padata, 0);\\n\\t\\tqueue_work(pinst->parallel_wq, &pw->pw_work);\\n\\t}\\n\\n\\treturn 0;\\nout:\\n\\trcu_read_unlock_bh();\\n\\n\\treturn err;\\n}\\nEXPORT_SYMBOL(padata_do_parallel);\\n\\n/*\\n * padata_find_next - Find the next object that needs serialization.\\n *\\n * Return:\\n * * A pointer to the control struct of the next object that needs\\n *   serialization, if present in one of the percpu reorder queues.\\n * * NULL, if the next object that needs serialization will\\n *   be parallel processed by another cpu and is not yet present in\\n *   the cpu\\'s reorder queue.\\n */\\nstatic struct padata_priv *padata_find_next(struct parallel_data *pd,\\n\\t\\t\\t\\t\\t    bool remove_object)\\n{\\n\\tstruct padata_priv *padata;\\n\\tstruct padata_list *reorder;\\n\\tint cpu = pd->cpu;\\n\\n\\treorder = per_cpu_ptr(pd->reorder_list, cpu);\\n\\n\\tspin_lock(&reorder->lock);\\n\\tif (list_empty(&reorder->list)) {\\n\\t\\tspin_unlock(&reorder->lock);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tpadata = list_entry(reorder->list.next, struct padata_priv, list);\\n\\n\\t/*\\n\\t * Checks the rare case where two or more parallel jobs have hashed to\\n\\t * the same CPU and one of the later ones finishes first.\\n\\t */\\n\\tif (padata->seq_nr != pd->processed) {\\n\\t\\tspin_unlock(&reorder->lock);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tif (remove_object) {\\n\\t\\tlist_del_init(&padata->list);\\n\\t\\t++pd->processed;\\n\\t\\tpd->cpu = cpumask_next_wrap(cpu, pd->cpumask.pcpu, -1, false);\\n\\t}\\n\\n\\tspin_unlock(&reorder->lock);\\n\\treturn padata;\\n}\\n\\nstatic void padata_reorder(struct parallel_data *pd)\\n{\\n\\tstruct padata_instance *pinst = pd->ps->pinst;\\n\\tint cb_cpu;\\n\\tstruct padata_priv *padata;\\n\\tstruct padata_serial_queue *squeue;\\n\\tstruct padata_list *reorder;\\n\\n\\t/*\\n\\t * We need to ensure that only one cpu can work on dequeueing of\\n\\t * the reorder queue the time. Calculating in which percpu reorder\\n\\t * queue the next object will arrive takes some time. A spinlock\\n\\t * would be highly contended. Also it is not clear in which order\\n\\t * the objects arrive to the reorder queues. So a cpu could wait to\\n\\t * get the lock just to notice that there is nothing to do at the\\n\\t * moment. Therefore we use a trylock and let the holder of the lock\\n\\t * care for all the objects enqueued during the holdtime of the lock.\\n\\t */\\n\\tif (!spin_trylock_bh(&pd->lock))\\n\\t\\treturn;\\n\\n\\twhile (1) {\\n\\t\\tpadata = padata_find_next(pd, true);\\n\\n\\t\\t/*\\n\\t\\t * If the next object that needs serialization is parallel\\n\\t\\t * processed by another cpu and is still on it\\'s way to the\\n\\t\\t * cpu\\'s reorder queue, nothing to do for now.\\n\\t\\t */\\n\\t\\tif (!padata)\\n\\t\\t\\tbreak;\\n\\n\\t\\tcb_cpu = padata->cb_cpu;\\n\\t\\tsqueue = per_cpu_ptr(pd->squeue, cb_cpu);\\n\\n\\t\\tspin_lock(&squeue->serial.lock);\\n\\t\\tlist_add_tail(&padata->list, &squeue->serial.list);\\n\\t\\tspin_unlock(&squeue->serial.lock);\\n\\n\\t\\tqueue_work_on(cb_cpu, pinst->serial_wq, &squeue->work);\\n\\t}\\n\\n\\tspin_unlock_bh(&pd->lock);\\n\\n\\t/*\\n\\t * The next object that needs serialization might have arrived to\\n\\t * the reorder queues in the meantime.\\n\\t *\\n\\t * Ensure reorder queue is read after pd->lock is dropped so we see\\n\\t * new objects from another task in padata_do_serial.  Pairs with\\n\\t * smp_mb in padata_do_serial.\\n\\t */\\n\\tsmp_mb();\\n\\n\\treorder = per_cpu_ptr(pd->reorder_list, pd->cpu);\\n\\tif (!list_empty(&reorder->list) && padata_find_next(pd, false))\\n\\t\\tqueue_work(pinst->serial_wq, &pd->reorder_work);\\n}\\n\\nstatic void invoke_padata_reorder(struct work_struct *work)\\n{\\n\\tstruct parallel_data *pd;\\n\\n\\tlocal_bh_disable();\\n\\tpd = container_of(work, struct parallel_data, reorder_work);\\n\\tpadata_reorder(pd);\\n\\tlocal_bh_enable();\\n}\\n\\nstatic void padata_serial_worker(struct work_struct *serial_work)\\n{\\n\\tstruct padata_serial_queue *squeue;\\n\\tstruct parallel_data *pd;\\n\\tLIST_HEAD(local_list);\\n\\tint cnt;\\n\\n\\tlocal_bh_disable();\\n\\tsqueue = container_of(serial_work, struct padata_serial_queue, work);\\n\\tpd = squeue->pd;\\n\\n\\tspin_lock(&squeue->serial.lock);\\n\\tlist_replace_init(&squeue->serial.list, &local_list);\\n\\tspin_unlock(&squeue->serial.lock);\\n\\n\\tcnt = 0;\\n\\n\\twhile (!list_empty(&local_list)) {\\n\\t\\tstruct padata_priv *padata;\\n\\n\\t\\tpadata = list_entry(local_list.next,\\n\\t\\t\\t\\t    struct padata_priv, list);\\n\\n\\t\\tlist_del_init(&padata->list);\\n\\n\\t\\tpadata->serial(padata);\\n\\t\\tcnt++;\\n\\t}\\n\\tlocal_bh_enable();\\n\\n\\tif (refcount_sub_and_test(cnt, &pd->refcnt))\\n\\t\\tpadata_free_pd(pd);\\n}\\n\\n/**\\n * padata_do_serial - padata serialization function\\n *\\n * @padata: object to be serialized.\\n *\\n * padata_do_serial must be called for every parallelized object.\\n * The serialization callback function will run with BHs off.\\n */\\nvoid padata_do_serial(struct padata_priv *padata)\\n{\\n\\tstruct parallel_data *pd = padata->pd;\\n\\tint hashed_cpu = padata_cpu_hash(pd, padata->seq_nr);\\n\\tstruct padata_list *reorder = per_cpu_ptr(pd->reorder_list, hashed_cpu);\\n\\tstruct padata_priv *cur;\\n\\tstruct list_head *pos;\\n\\n\\tspin_lock(&reorder->lock);\\n\\t/* Sort in ascending order of sequence number. */\\n\\tlist_for_each_prev(pos, &reorder->list) {\\n\\t\\tcur = list_entry(pos, struct padata_priv, list);\\n\\t\\t/* Compare by difference to consider integer wrap around */\\n\\t\\tif ((signed int)(cur->seq_nr - padata->seq_nr) < 0)\\n\\t\\t\\tbreak;\\n\\t}\\n\\tlist_add(&padata->list, pos);\\n\\tspin_unlock(&reorder->lock);\\n\\n\\t/*\\n\\t * Ensure the addition to the reorder list is ordered correctly\\n\\t * with the trylock of pd->lock in padata_reorder.  Pairs with smp_mb\\n\\t * in padata_reorder.\\n\\t */\\n\\tsmp_mb();\\n\\n\\tpadata_reorder(pd);\\n}\\nEXPORT_SYMBOL(padata_do_serial);\\n\\nstatic int padata_setup_cpumasks(struct padata_instance *pinst)\\n{\\n\\tstruct workqueue_attrs *attrs;\\n\\tint err;\\n\\n\\tattrs = alloc_workqueue_attrs();\\n\\tif (!attrs)\\n\\t\\treturn -ENOMEM;\\n\\n\\t/* Restrict parallel_wq workers to pd->cpumask.pcpu. */\\n\\tcpumask_copy(attrs->cpumask, pinst->cpumask.pcpu);\\n\\terr = apply_workqueue_attrs(pinst->parallel_wq, attrs);\\n\\tfree_workqueue_attrs(attrs);\\n\\n\\treturn err;\\n}\\n\\nstatic void __init padata_mt_helper(struct work_struct *w)\\n{\\n\\tstruct padata_work *pw = container_of(w, struct padata_work, pw_work);\\n\\tstruct padata_mt_job_state *ps = pw->pw_data;\\n\\tstruct padata_mt_job *job = ps->job;\\n\\tbool done;\\n\\n\\tspin_lock(&ps->lock);\\n\\n\\twhile (job->size > 0) {\\n\\t\\tunsigned long start, size, end;\\n\\n\\t\\tstart = job->start;\\n\\t\\t/* So end is chunk size aligned if enough work remains. */\\n\\t\\tsize = roundup(start + 1, ps->chunk_size) - start;\\n\\t\\tsize = min(size, job->size);\\n\\t\\tend = start + size;\\n\\n\\t\\tjob->start = end;\\n\\t\\tjob->size -= size;\\n\\n\\t\\tspin_unlock(&ps->lock);\\n\\t\\tjob->thread_fn(start, end, job->fn_arg);\\n\\t\\tspin_lock(&ps->lock);\\n\\t}\\n\\n\\t++ps->nworks_fini;\\n\\tdone = (ps->nworks_fini == ps->nworks);\\n\\tspin_unlock(&ps->lock);\\n\\n\\tif (done)\\n\\t\\tcomplete(&ps->completion);\\n}\\n\\n/**\\n * padata_do_multithreaded - run a multithreaded job\\n * @job: Description of the job.\\n *\\n * See the definition of struct padata_mt_job for more details.\\n */\\nvoid __init padata_do_multithreaded(struct padata_mt_job *job)\\n{\\n\\t/* In case threads finish at different times. */\\n\\tstatic const unsigned long load_balance_factor = 4;\\n\\tstruct padata_work my_work, *pw;\\n\\tstruct padata_mt_job_state ps;\\n\\tLIST_HEAD(works);\\n\\tint nworks, nid;\\n\\tstatic atomic_t last_used_nid __initdata;\\n\\n\\tif (job->size == 0)\\n\\t\\treturn;\\n\\n\\t/* Ensure at least one thread when size < min_chunk. */\\n\\tnworks = max(job->size / max(job->min_chunk, job->align), 1ul);\\n\\tnworks = min(nworks, job->max_threads);\\n\\n\\tif (nworks == 1) {\\n\\t\\t/* Single thread, no coordination needed, cut to the chase. */\\n\\t\\tjob->thread_fn(job->start, job->start + job->size, job->fn_arg);\\n\\t\\treturn;\\n\\t}\\n\\n\\tspin_lock_init(&ps.lock);\\n\\tinit_completion(&ps.completion);\\n\\tps.job\\t       = job;\\n\\tps.nworks      = padata_work_alloc_mt(nworks, &ps, &works);\\n\\tps.nworks_fini = 0;\\n\\n\\t/*\\n\\t * Chunk size is the amount of work a helper does per call to the\\n\\t * thread function.  Load balance large jobs between threads by\\n\\t * increasing the number of chunks, guarantee at least the minimum\\n\\t * chunk size from the caller, and honor the caller\\'s alignment.\\n\\t * Ensure chunk_size is at least 1 to prevent divide-by-0\\n\\t * panic in padata_mt_helper().\\n\\t */\\n\\tps.chunk_size = job->size / (ps.nworks * load_balance_factor);\\n\\tps.chunk_size = max(ps.chunk_size, job->min_chunk);\\n\\tps.chunk_size = max(ps.chunk_size, 1ul);\\n\\tps.chunk_size = roundup(ps.chunk_size, job->align);\\n\\n\\tlist_for_each_entry(pw, &works, pw_list)\\n\\t\\tif (job->numa_aware) {\\n\\t\\t\\tint old_node = atomic_read(&last_used_nid);\\n\\n\\t\\t\\tdo {\\n\\t\\t\\t\\tnid = next_node_in(old_node, node_states[N_CPU]);\\n\\t\\t\\t} while (!atomic_try_cmpxchg(&last_used_nid, &old_node, nid));\\n\\t\\t\\tqueue_work_node(nid, system_unbound_wq, &pw->pw_work);\\n\\t\\t} else {\\n\\t\\t\\tqueue_work(system_unbound_wq, &pw->pw_work);\\n\\t\\t}\\n\\n\\t/* Use the current thread, which saves starting a workqueue worker. */\\n\\tpadata_work_init(&my_work, padata_mt_helper, &ps, PADATA_WORK_ONSTACK);\\n\\tpadata_mt_helper(&my_work.pw_work);\\n\\n\\t/* Wait for all the helpers to finish. */\\n\\twait_for_completion(&ps.completion);\\n\\n\\tdestroy_work_on_stack(&my_work.pw_work);\\n\\tpadata_works_free(&works);\\n}\\n\\nstatic void __padata_list_init(struct padata_list *pd_list)\\n{\\n\\tINIT_LIST_HEAD(&pd_list->list);\\n\\tspin_lock_init(&pd_list->lock);\\n}\\n\\n/* Initialize all percpu queues used by serial workers */\\nstatic void padata_init_squeues(struct parallel_data *pd)\\n{\\n\\tint cpu;\\n\\tstruct padata_serial_queue *squeue;\\n\\n\\tfor_each_cpu(cpu, pd->cpumask.cbcpu) {\\n\\t\\tsqueue = per_cpu_ptr(pd->squeue, cpu);\\n\\t\\tsqueue->pd = pd;\\n\\t\\t__padata_list_init(&squeue->serial);\\n\\t\\tINIT_WORK(&squeue->work, padata_serial_worker);\\n\\t}\\n}\\n\\n/* Initialize per-CPU reorder lists */\\nstatic void padata_init_reorder_list(struct parallel_data *pd)\\n{\\n\\tint cpu;\\n\\tstruct padata_list *list;\\n\\n\\tfor_each_cpu(cpu, pd->cpumask.pcpu) {\\n\\t\\tlist = per_cpu_ptr(pd->reorder_list, cpu);\\n\\t\\t__padata_list_init(list);\\n\\t}\\n}\\n\\n/* Allocate and initialize the internal cpumask dependend resources. */\\nstatic struct parallel_data *padata_alloc_pd(struct padata_shell *ps)\\n{\\n\\tstruct padata_instance *pinst = ps->pinst;\\n\\tstruct parallel_data *pd;\\n\\n\\tpd = kzalloc(sizeof(struct parallel_data), GFP_KERNEL);\\n\\tif (!pd)\\n\\t\\tgoto err;\\n\\n\\tpd->reorder_list = alloc_percpu(struct padata_list);\\n\\tif (!pd->reorder_list)\\n\\t\\tgoto err_free_pd;\\n\\n\\tpd->squeue = alloc_percpu(struct padata_serial_queue);\\n\\tif (!pd->squeue)\\n\\t\\tgoto err_free_reorder_list;\\n\\n\\tpd->ps = ps;\\n\\n\\tif (!alloc_cpumask_var(&pd->cpumask.pcpu, GFP_KERNEL))\\n\\t\\tgoto err_free_squeue;\\n\\tif (!alloc_cpumask_var(&pd->cpumask.cbcpu, GFP_KERNEL))\\n\\t\\tgoto err_free_pcpu;\\n\\n\\tcpumask_and(pd->cpumask.pcpu, pinst->cpumask.pcpu, cpu_online_mask);\\n\\tcpumask_and(pd->cpumask.cbcpu, pinst->cpumask.cbcpu, cpu_online_mask);\\n\\n\\tpadata_init_reorder_list(pd);\\n\\tpadata_init_squeues(pd);\\n\\tpd->seq_nr = -1;\\n\\trefcount_set(&pd->refcnt, 1);\\n\\tspin_lock_init(&pd->lock);\\n\\tpd->cpu = cpumask_first(pd->cpumask.pcpu);\\n\\tINIT_WORK(&pd->reorder_work, invoke_padata_reorder);\\n\\n\\treturn pd;\\n\\nerr_free_pcpu:\\n\\tfree_cpumask_var(pd->cpumask.pcpu);\\nerr_free_squeue:\\n\\tfree_percpu(pd->squeue);\\nerr_free_reorder_list:\\n\\tfree_percpu(pd->reorder_list);\\nerr_free_pd:\\n\\tkfree(pd);\\nerr:\\n\\treturn NULL;\\n}\\n\\nstatic void padata_free_pd(struct parallel_data *pd)\\n{\\n\\tfree_cpumask_var(pd->cpumask.pcpu);\\n\\tfree_cpumask_var(pd->cpumask.cbcpu);\\n\\tfree_percpu(pd->reorder_list);\\n\\tfree_percpu(pd->squeue);\\n\\tkfree(pd);\\n}\\n\\nstatic void __padata_start(struct padata_instance *pinst)\\n{\\n\\tpinst->flags |= PADATA_INIT;\\n}\\n\\nstatic void __padata_stop(struct padata_instance *pinst)\\n{\\n\\tif (!(pinst->flags & PADATA_INIT))\\n\\t\\treturn;\\n\\n\\tpinst->flags &= ~PADATA_INIT;\\n\\n\\tsynchronize_rcu();\\n}\\n\\n/* Replace the internal control structure with a new one. */\\nstatic int padata_replace_one(struct padata_shell *ps)\\n{\\n\\tstruct parallel_data *pd_new;\\n\\n\\tpd_new = padata_alloc_pd(ps);\\n\\tif (!pd_new)\\n\\t\\treturn -ENOMEM;\\n\\n\\tps->opd = rcu_dereference_protected(ps->pd, 1);\\n\\trcu_assign_pointer(ps->pd, pd_new);\\n\\n\\treturn 0;\\n}\\n\\nstatic int padata_replace(struct padata_instance *pinst)\\n{\\n\\tstruct padata_shell *ps;\\n\\tint err = 0;\\n\\n\\tpinst->flags |= PADATA_RESET;\\n\\n\\tlist_for_each_entry(ps, &pinst->pslist, list) {\\n\\t\\terr = padata_replace_one(ps);\\n\\t\\tif (err)\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tsynchronize_rcu();\\n\\n\\tlist_for_each_entry_continue_reverse(ps, &pinst->pslist, list)\\n\\t\\tif (refcount_dec_and_test(&ps->opd->refcnt))\\n\\t\\t\\tpadata_free_pd(ps->opd);\\n\\n\\tpinst->flags &= ~PADATA_RESET;\\n\\n\\treturn err;\\n}\\n\\n/* If cpumask contains no active cpu, we mark the instance as invalid. */\\nstatic bool padata_validate_cpumask(struct padata_instance *pinst,\\n\\t\\t\\t\\t    const struct cpumask *cpumask)\\n{\\n\\tif (!cpumask_intersects(cpumask, cpu_online_mask)) {\\n\\t\\tpinst->flags |= PADATA_INVALID;\\n\\t\\treturn false;\\n\\t}\\n\\n\\tpinst->flags &= ~PADATA_INVALID;\\n\\treturn true;\\n}\\n\\nstatic int __padata_set_cpumasks(struct padata_instance *pinst,\\n\\t\\t\\t\\t cpumask_var_t pcpumask,\\n\\t\\t\\t\\t cpumask_var_t cbcpumask)\\n{\\n\\tint valid;\\n\\tint err;\\n\\n\\tvalid = padata_validate_cpumask(pinst, pcpumask);\\n\\tif (!valid) {\\n\\t\\t__padata_stop(pinst);\\n\\t\\tgoto out_replace;\\n\\t}\\n\\n\\tvalid = padata_validate_cpumask(pinst, cbcpumask);\\n\\tif (!valid)\\n\\t\\t__padata_stop(pinst);\\n\\nout_replace:\\n\\tcpumask_copy(pinst->cpumask.pcpu, pcpumask);\\n\\tcpumask_copy(pinst->cpumask.cbcpu, cbcpumask);\\n\\n\\terr = padata_setup_cpumasks(pinst) ?: padata_replace(pinst);\\n\\n\\tif (valid)\\n\\t\\t__padata_start(pinst);\\n\\n\\treturn err;\\n}\\n\\n/**\\n * padata_set_cpumask - Sets specified by @cpumask_type cpumask to the value\\n *                      equivalent to @cpumask.\\n * @pinst: padata instance\\n * @cpumask_type: PADATA_CPU_SERIAL or PADATA_CPU_PARALLEL corresponding\\n *                to parallel and serial cpumasks respectively.\\n * @cpumask: the cpumask to use\\n *\\n * Return: 0 on success or negative error code\\n */\\nint padata_set_cpumask(struct padata_instance *pinst, int cpumask_type,\\n\\t\\t       cpumask_var_t cpumask)\\n{\\n\\tstruct cpumask *serial_mask, *parallel_mask;\\n\\tint err = -EINVAL;\\n\\n\\tcpus_read_lock();\\n\\tmutex_lock(&pinst->lock);\\n\\n\\tswitch (cpumask_type) {\\n\\tcase PADATA_CPU_PARALLEL:\\n\\t\\tserial_mask = pinst->cpumask.cbcpu;\\n\\t\\tparallel_mask = cpumask;\\n\\t\\tbreak;\\n\\tcase PADATA_CPU_SERIAL:\\n\\t\\tparallel_mask = pinst->cpumask.pcpu;\\n\\t\\tserial_mask = cpumask;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\t goto out;\\n\\t}\\n\\n\\terr =  __padata_set_cpumasks(pinst, parallel_mask, serial_mask);\\n\\nout:\\n\\tmutex_unlock(&pinst->lock);\\n\\tcpus_read_unlock();\\n\\n\\treturn err;\\n}\\nEXPORT_SYMBOL(padata_set_cpumask);\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\nstatic int __padata_add_cpu(struct padata_instance *pinst, int cpu)\\n{\\n\\tint err = 0;\\n\\n\\tif (cpumask_test_cpu(cpu, cpu_online_mask)) {\\n\\t\\terr = padata_replace(pinst);\\n\\n\\t\\tif (padata_validate_cpumask(pinst, pinst->cpumask.pcpu) &&\\n\\t\\t    padata_validate_cpumask(pinst, pinst->cpumask.cbcpu))\\n\\t\\t\\t__padata_start(pinst);\\n\\t}\\n\\n\\treturn err;\\n}\\n\\nstatic int __padata_remove_cpu(struct padata_instance *pinst, int cpu)\\n{\\n\\tint err = 0;\\n\\n\\tif (!cpumask_test_cpu(cpu, cpu_online_mask)) {\\n\\t\\tif (!padata_validate_cpumask(pinst, pinst->cpumask.pcpu) ||\\n\\t\\t    !padata_validate_cpumask(pinst, pinst->cpumask.cbcpu))\\n\\t\\t\\t__padata_stop(pinst);\\n\\n\\t\\terr = padata_replace(pinst);\\n\\t}\\n\\n\\treturn err;\\n}\\n\\nstatic inline int pinst_has_cpu(struct padata_instance *pinst, int cpu)\\n{\\n\\treturn cpumask_test_cpu(cpu, pinst->cpumask.pcpu) ||\\n\\t\\tcpumask_test_cpu(cpu, pinst->cpumask.cbcpu);\\n}\\n\\nstatic int padata_cpu_online(unsigned int cpu, struct hlist_node *node)\\n{\\n\\tstruct padata_instance *pinst;\\n\\tint ret;\\n\\n\\tpinst = hlist_entry_safe(node, struct padata_instance, cpu_online_node);\\n\\tif (!pinst_has_cpu(pinst, cpu))\\n\\t\\treturn 0;\\n\\n\\tmutex_lock(&pinst->lock);\\n\\tret = __padata_add_cpu(pinst, cpu);\\n\\tmutex_unlock(&pinst->lock);\\n\\treturn ret;\\n}\\n\\nstatic int padata_cpu_dead(unsigned int cpu, struct hlist_node *node)\\n{\\n\\tstruct padata_instance *pinst;\\n\\tint ret;\\n\\n\\tpinst = hlist_entry_safe(node, struct padata_instance, cpu_dead_node);\\n\\tif (!pinst_has_cpu(pinst, cpu))\\n\\t\\treturn 0;\\n\\n\\tmutex_lock(&pinst->lock);\\n\\tret = __padata_remove_cpu(pinst, cpu);\\n\\tmutex_unlock(&pinst->lock);\\n\\treturn ret;\\n}\\n\\nstatic enum cpuhp_state hp_online;\\n#endif\\n\\nstatic void __padata_free(struct padata_instance *pinst)\\n{\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\tcpuhp_state_remove_instance_nocalls(CPUHP_PADATA_DEAD,\\n\\t\\t\\t\\t\\t    &pinst->cpu_dead_node);\\n\\tcpuhp_state_remove_instance_nocalls(hp_online, &pinst->cpu_online_node);\\n#endif\\n\\n\\tWARN_ON(!list_empty(&pinst->pslist));\\n\\n\\tfree_cpumask_var(pinst->cpumask.pcpu);\\n\\tfree_cpumask_var(pinst->cpumask.cbcpu);\\n\\tdestroy_workqueue(pinst->serial_wq);\\n\\tdestroy_workqueue(pinst->parallel_wq);\\n\\tkfree(pinst);\\n}\\n\\n#define kobj2pinst(_kobj)\\t\\t\\t\\t\\t\\\\\\n\\tcontainer_of(_kobj, struct padata_instance, kobj)\\n#define attr2pentry(_attr)\\t\\t\\t\\t\\t\\\\\\n\\tcontainer_of(_attr, struct padata_sysfs_entry, attr)\\n\\nstatic void padata_sysfs_release(struct kobject *kobj)\\n{\\n\\tstruct padata_instance *pinst = kobj2pinst(kobj);\\n\\t__padata_free(pinst);\\n}\\n\\nstruct padata_sysfs_entry {\\n\\tstruct attribute attr;\\n\\tssize_t (*show)(struct padata_instance *, struct attribute *, char *);\\n\\tssize_t (*store)(struct padata_instance *, struct attribute *,\\n\\t\\t\\t const char *, size_t);\\n};\\n\\nstatic ssize_t show_cpumask(struct padata_instance *pinst,\\n\\t\\t\\t    struct attribute *attr,  char *buf)\\n{\\n\\tstruct cpumask *cpumask;\\n\\tssize_t len;\\n\\n\\tmutex_lock(&pinst->lock);\\n\\tif (!strcmp(attr->name, \"serial_cpumask\"))\\n\\t\\tcpumask = pinst->cpumask.cbcpu;\\n\\telse\\n\\t\\tcpumask = pinst->cpumask.pcpu;\\n\\n\\tlen = snprintf(buf, PAGE_SIZE, \"%*pb\\\\n\",\\n\\t\\t       nr_cpu_ids, cpumask_bits(cpumask));\\n\\tmutex_unlock(&pinst->lock);\\n\\treturn len < PAGE_SIZE ? len : -EINVAL;\\n}\\n\\nstatic ssize_t store_cpumask(struct padata_instance *pinst,\\n\\t\\t\\t     struct attribute *attr,\\n\\t\\t\\t     const char *buf, size_t count)\\n{\\n\\tcpumask_var_t new_cpumask;\\n\\tssize_t ret;\\n\\tint mask_type;\\n\\n\\tif (!alloc_cpumask_var(&new_cpumask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\n\\tret = bitmap_parse(buf, count, cpumask_bits(new_cpumask),\\n\\t\\t\\t   nr_cpumask_bits);\\n\\tif (ret < 0)\\n\\t\\tgoto out;\\n\\n\\tmask_type = !strcmp(attr->name, \"serial_cpumask\") ?\\n\\t\\tPADATA_CPU_SERIAL : PADATA_CPU_PARALLEL;\\n\\tret = padata_set_cpumask(pinst, mask_type, new_cpumask);\\n\\tif (!ret)\\n\\t\\tret = count;\\n\\nout:\\n\\tfree_cpumask_var(new_cpumask);\\n\\treturn ret;\\n}\\n\\n#define PADATA_ATTR_RW(_name, _show_name, _store_name)\\t\\t\\\\\\n\\tstatic struct padata_sysfs_entry _name##_attr =\\t\\t\\\\\\n\\t\\t__ATTR(_name, 0644, _show_name, _store_name)\\n#define PADATA_ATTR_RO(_name, _show_name)\\t\\t\\\\\\n\\tstatic struct padata_sysfs_entry _name##_attr = \\\\\\n\\t\\t__ATTR(_name, 0400, _show_name, NULL)\\n\\nPADATA_ATTR_RW(serial_cpumask, show_cpumask, store_cpumask);\\nPADATA_ATTR_RW(parallel_cpumask, show_cpumask, store_cpumask);\\n\\n/*\\n * Padata sysfs provides the following objects:\\n * serial_cpumask   [RW] - cpumask for serial workers\\n * parallel_cpumask [RW] - cpumask for parallel workers\\n */\\nstatic struct attribute *padata_default_attrs[] = {\\n\\t&serial_cpumask_attr.attr,\\n\\t&parallel_cpumask_attr.attr,\\n\\tNULL,\\n};\\nATTRIBUTE_GROUPS(padata_default);\\n\\nstatic ssize_t padata_sysfs_show(struct kobject *kobj,\\n\\t\\t\\t\\t struct attribute *attr, char *buf)\\n{\\n\\tstruct padata_instance *pinst;\\n\\tstruct padata_sysfs_entry *pentry;\\n\\tssize_t ret = -EIO;\\n\\n\\tpinst = kobj2pinst(kobj);\\n\\tpentry = attr2pentry(attr);\\n\\tif (pentry->show)\\n\\t\\tret = pentry->show(pinst, attr, buf);\\n\\n\\treturn ret;\\n}\\n\\nstatic ssize_t padata_sysfs_store(struct kobject *kobj, struct attribute *attr,\\n\\t\\t\\t\\t  const char *buf, size_t count)\\n{\\n\\tstruct padata_instance *pinst;\\n\\tstruct padata_sysfs_entry *pentry;\\n\\tssize_t ret = -EIO;\\n\\n\\tpinst = kobj2pinst(kobj);\\n\\tpentry = attr2pentry(attr);\\n\\tif (pentry->show)\\n\\t\\tret = pentry->store(pinst, attr, buf, count);\\n\\n\\treturn ret;\\n}\\n\\nstatic const struct sysfs_ops padata_sysfs_ops = {\\n\\t.show = padata_sysfs_show,\\n\\t.store = padata_sysfs_store,\\n};\\n\\nstatic const struct kobj_type padata_attr_type = {\\n\\t.sysfs_ops = &padata_sysfs_ops,\\n\\t.default_groups = padata_default_groups,\\n\\t.release = padata_sysfs_release,\\n};\\n\\n/**\\n * padata_alloc - allocate and initialize a padata instance\\n * @name: used to identify the instance\\n *\\n * Return: new instance on success, NULL on error\\n */\\nstruct padata_instance *padata_alloc(const char *name)\\n{\\n\\tstruct padata_instance *pinst;\\n\\n\\tpinst = kzalloc(sizeof(struct padata_instance), GFP_KERNEL);\\n\\tif (!pinst)\\n\\t\\tgoto err;\\n\\n\\tpinst->parallel_wq = alloc_workqueue(\"%s_parallel\", WQ_UNBOUND, 0,\\n\\t\\t\\t\\t\\t     name);\\n\\tif (!pinst->parallel_wq)\\n\\t\\tgoto err_free_inst;\\n\\n\\tcpus_read_lock();\\n\\n\\tpinst->serial_wq = alloc_workqueue(\"%s_serial\", WQ_MEM_RECLAIM |\\n\\t\\t\\t\\t\\t   WQ_CPU_INTENSIVE, 1, name);\\n\\tif (!pinst->serial_wq)\\n\\t\\tgoto err_put_cpus;\\n\\n\\tif (!alloc_cpumask_var(&pinst->cpumask.pcpu, GFP_KERNEL))\\n\\t\\tgoto err_free_serial_wq;\\n\\tif (!alloc_cpumask_var(&pinst->cpumask.cbcpu, GFP_KERNEL)) {\\n\\t\\tfree_cpumask_var(pinst->cpumask.pcpu);\\n\\t\\tgoto err_free_serial_wq;\\n\\t}\\n\\n\\tINIT_LIST_HEAD(&pinst->pslist);\\n\\n\\tcpumask_copy(pinst->cpumask.pcpu, cpu_possible_mask);\\n\\tcpumask_copy(pinst->cpumask.cbcpu, cpu_possible_mask);\\n\\n\\tif (padata_setup_cpumasks(pinst))\\n\\t\\tgoto err_free_masks;\\n\\n\\t__padata_start(pinst);\\n\\n\\tkobject_init(&pinst->kobj, &padata_attr_type);\\n\\tmutex_init(&pinst->lock);\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\tcpuhp_state_add_instance_nocalls_cpuslocked(hp_online,\\n\\t\\t\\t\\t\\t\\t    &pinst->cpu_online_node);\\n\\tcpuhp_state_add_instance_nocalls_cpuslocked(CPUHP_PADATA_DEAD,\\n\\t\\t\\t\\t\\t\\t    &pinst->cpu_dead_node);\\n#endif\\n\\n\\tcpus_read_unlock();\\n\\n\\treturn pinst;\\n\\nerr_free_masks:\\n\\tfree_cpumask_var(pinst->cpumask.pcpu);\\n\\tfree_cpumask_var(pinst->cpumask.cbcpu);\\nerr_free_serial_wq:\\n\\tdestroy_workqueue(pinst->serial_wq);\\nerr_put_cpus:\\n\\tcpus_read_unlock();\\n\\tdestroy_workqueue(pinst->parallel_wq);\\nerr_free_inst:\\n\\tkfree(pinst);\\nerr:\\n\\treturn NULL;\\n}\\nEXPORT_SYMBOL(padata_alloc);\\n\\n/**\\n * padata_free - free a padata instance\\n *\\n * @pinst: padata instance to free\\n */\\nvoid padata_free(struct padata_instance *pinst)\\n{\\n\\tkobject_put(&pinst->kobj);\\n}\\nEXPORT_SYMBOL(padata_free);\\n\\n/**\\n * padata_alloc_shell - Allocate and initialize padata shell.\\n *\\n * @pinst: Parent padata_instance object.\\n *\\n * Return: new shell on success, NULL on error\\n */\\nstruct padata_shell *padata_alloc_shell(struct padata_instance *pinst)\\n{\\n\\tstruct parallel_data *pd;\\n\\tstruct padata_shell *ps;\\n\\n\\tps = kzalloc(sizeof(*ps), GFP_KERNEL);\\n\\tif (!ps)\\n\\t\\tgoto out;\\n\\n\\tps->pinst = pinst;\\n\\n\\tcpus_read_lock();\\n\\tpd = padata_alloc_pd(ps);\\n\\tcpus_read_unlock();\\n\\n\\tif (!pd)\\n\\t\\tgoto out_free_ps;\\n\\n\\tmutex_lock(&pinst->lock);\\n\\tRCU_INIT_POINTER(ps->pd, pd);\\n\\tlist_add(&ps->list, &pinst->pslist);\\n\\tmutex_unlock(&pinst->lock);\\n\\n\\treturn ps;\\n\\nout_free_ps:\\n\\tkfree(ps);\\nout:\\n\\treturn NULL;\\n}\\nEXPORT_SYMBOL(padata_alloc_shell);\\n\\n/**\\n * padata_free_shell - free a padata shell\\n *\\n * @ps: padata shell to free\\n */\\nvoid padata_free_shell(struct padata_shell *ps)\\n{\\n\\tstruct parallel_data *pd;\\n\\n\\tif (!ps)\\n\\t\\treturn;\\n\\n\\tmutex_lock(&ps->pinst->lock);\\n\\tlist_del(&ps->list);\\n\\tpd = rcu_dereference_protected(ps->pd, 1);\\n\\tif (refcount_dec_and_test(&pd->refcnt))\\n\\t\\tpadata_free_pd(pd);\\n\\tmutex_unlock(&ps->pinst->lock);\\n\\n\\tkfree(ps);\\n}\\nEXPORT_SYMBOL(padata_free_shell);\\n\\nvoid __init padata_init(void)\\n{\\n\\tunsigned int i, possible_cpus;\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\tint ret;\\n\\n\\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN, \"padata:online\",\\n\\t\\t\\t\\t      padata_cpu_online, NULL);\\n\\tif (ret < 0)\\n\\t\\tgoto err;\\n\\thp_online = ret;\\n\\n\\tret = cpuhp_setup_state_multi(CPUHP_PADATA_DEAD, \"padata:dead\",\\n\\t\\t\\t\\t      NULL, padata_cpu_dead);\\n\\tif (ret < 0)\\n\\t\\tgoto remove_online_state;\\n#endif\\n\\n\\tpossible_cpus = num_possible_cpus();\\n\\tpadata_works = kmalloc_array(possible_cpus, sizeof(struct padata_work),\\n\\t\\t\\t\\t     GFP_KERNEL);\\n\\tif (!padata_works)\\n\\t\\tgoto remove_dead_state;\\n\\n\\tfor (i = 0; i < possible_cpus; ++i)\\n\\t\\tlist_add(&padata_works[i].pw_list, &padata_free_works);\\n\\n\\treturn;\\n\\nremove_dead_state:\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\tcpuhp_remove_multi_state(CPUHP_PADATA_DEAD);\\nremove_online_state:\\n\\tcpuhp_remove_multi_state(hp_online);\\nerr:\\n#endif\\n\\tpr_warn(\"padata: initialization failed\\\\n\");\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#include <linux/init.h>\\n#include <linux/static_call.h>\\n#include <linux/bug.h>\\n#include <linux/smp.h>\\n#include <linux/sort.h>\\n#include <linux/slab.h>\\n#include <linux/module.h>\\n#include <linux/cpu.h>\\n#include <linux/processor.h>\\n#include <asm/sections.h>\\n\\nextern struct static_call_site __start_static_call_sites[],\\n\\t\\t\\t       __stop_static_call_sites[];\\nextern struct static_call_tramp_key __start_static_call_tramp_key[],\\n\\t\\t\\t\\t    __stop_static_call_tramp_key[];\\n\\nint static_call_initialized;\\n\\n/*\\n * Must be called before early_initcall() to be effective.\\n */\\nvoid static_call_force_reinit(void)\\n{\\n\\tif (WARN_ON_ONCE(!static_call_initialized))\\n\\t\\treturn;\\n\\n\\tstatic_call_initialized++;\\n}\\n\\n/* mutex to protect key modules/sites */\\nstatic DEFINE_MUTEX(static_call_mutex);\\n\\nstatic void static_call_lock(void)\\n{\\n\\tmutex_lock(&static_call_mutex);\\n}\\n\\nstatic void static_call_unlock(void)\\n{\\n\\tmutex_unlock(&static_call_mutex);\\n}\\n\\nstatic inline void *static_call_addr(struct static_call_site *site)\\n{\\n\\treturn (void *)((long)site->addr + (long)&site->addr);\\n}\\n\\nstatic inline unsigned long __static_call_key(const struct static_call_site *site)\\n{\\n\\treturn (long)site->key + (long)&site->key;\\n}\\n\\nstatic inline struct static_call_key *static_call_key(const struct static_call_site *site)\\n{\\n\\treturn (void *)(__static_call_key(site) & ~STATIC_CALL_SITE_FLAGS);\\n}\\n\\n/* These assume the key is word-aligned. */\\nstatic inline bool static_call_is_init(struct static_call_site *site)\\n{\\n\\treturn __static_call_key(site) & STATIC_CALL_SITE_INIT;\\n}\\n\\nstatic inline bool static_call_is_tail(struct static_call_site *site)\\n{\\n\\treturn __static_call_key(site) & STATIC_CALL_SITE_TAIL;\\n}\\n\\nstatic inline void static_call_set_init(struct static_call_site *site)\\n{\\n\\tsite->key = (__static_call_key(site) | STATIC_CALL_SITE_INIT) -\\n\\t\\t    (long)&site->key;\\n}\\n\\nstatic int static_call_site_cmp(const void *_a, const void *_b)\\n{\\n\\tconst struct static_call_site *a = _a;\\n\\tconst struct static_call_site *b = _b;\\n\\tconst struct static_call_key *key_a = static_call_key(a);\\n\\tconst struct static_call_key *key_b = static_call_key(b);\\n\\n\\tif (key_a < key_b)\\n\\t\\treturn -1;\\n\\n\\tif (key_a > key_b)\\n\\t\\treturn 1;\\n\\n\\treturn 0;\\n}\\n\\nstatic void static_call_site_swap(void *_a, void *_b, int size)\\n{\\n\\tlong delta = (unsigned long)_a - (unsigned long)_b;\\n\\tstruct static_call_site *a = _a;\\n\\tstruct static_call_site *b = _b;\\n\\tstruct static_call_site tmp = *a;\\n\\n\\ta->addr = b->addr  - delta;\\n\\ta->key  = b->key   - delta;\\n\\n\\tb->addr = tmp.addr + delta;\\n\\tb->key  = tmp.key  + delta;\\n}\\n\\nstatic inline void static_call_sort_entries(struct static_call_site *start,\\n\\t\\t\\t\\t\\t    struct static_call_site *stop)\\n{\\n\\tsort(start, stop - start, sizeof(struct static_call_site),\\n\\t     static_call_site_cmp, static_call_site_swap);\\n}\\n\\nstatic inline bool static_call_key_has_mods(struct static_call_key *key)\\n{\\n\\treturn !(key->type & 1);\\n}\\n\\nstatic inline struct static_call_mod *static_call_key_next(struct static_call_key *key)\\n{\\n\\tif (!static_call_key_has_mods(key))\\n\\t\\treturn NULL;\\n\\n\\treturn key->mods;\\n}\\n\\nstatic inline struct static_call_site *static_call_key_sites(struct static_call_key *key)\\n{\\n\\tif (static_call_key_has_mods(key))\\n\\t\\treturn NULL;\\n\\n\\treturn (struct static_call_site *)(key->type & ~1);\\n}\\n\\nvoid __static_call_update(struct static_call_key *key, void *tramp, void *func)\\n{\\n\\tstruct static_call_site *site, *stop;\\n\\tstruct static_call_mod *site_mod, first;\\n\\n\\tcpus_read_lock();\\n\\tstatic_call_lock();\\n\\n\\tif (key->func == func)\\n\\t\\tgoto done;\\n\\n\\tkey->func = func;\\n\\n\\tarch_static_call_transform(NULL, tramp, func, false);\\n\\n\\t/*\\n\\t * If uninitialized, we\\'ll not update the callsites, but they still\\n\\t * point to the trampoline and we just patched that.\\n\\t */\\n\\tif (WARN_ON_ONCE(!static_call_initialized))\\n\\t\\tgoto done;\\n\\n\\tfirst = (struct static_call_mod){\\n\\t\\t.next = static_call_key_next(key),\\n\\t\\t.mod = NULL,\\n\\t\\t.sites = static_call_key_sites(key),\\n\\t};\\n\\n\\tfor (site_mod = &first; site_mod; site_mod = site_mod->next) {\\n\\t\\tbool init = system_state < SYSTEM_RUNNING;\\n\\t\\tstruct module *mod = site_mod->mod;\\n\\n\\t\\tif (!site_mod->sites) {\\n\\t\\t\\t/*\\n\\t\\t\\t * This can happen if the static call key is defined in\\n\\t\\t\\t * a module which doesn\\'t use it.\\n\\t\\t\\t *\\n\\t\\t\\t * It also happens in the has_mods case, where the\\n\\t\\t\\t * \\'first\\' entry has no sites associated with it.\\n\\t\\t\\t */\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tstop = __stop_static_call_sites;\\n\\n\\t\\tif (mod) {\\n#ifdef CONFIG_MODULES\\n\\t\\t\\tstop = mod->static_call_sites +\\n\\t\\t\\t       mod->num_static_call_sites;\\n\\t\\t\\tinit = mod->state == MODULE_STATE_COMING;\\n#endif\\n\\t\\t}\\n\\n\\t\\tfor (site = site_mod->sites;\\n\\t\\t     site < stop && static_call_key(site) == key; site++) {\\n\\t\\t\\tvoid *site_addr = static_call_addr(site);\\n\\n\\t\\t\\tif (!init && static_call_is_init(site))\\n\\t\\t\\t\\tcontinue;\\n\\n\\t\\t\\tif (!kernel_text_address((unsigned long)site_addr)) {\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * This skips patching built-in __exit, which\\n\\t\\t\\t\\t * is part of init_section_contains() but is\\n\\t\\t\\t\\t * not part of kernel_text_address().\\n\\t\\t\\t\\t *\\n\\t\\t\\t\\t * Skipping built-in __exit is fine since it\\n\\t\\t\\t\\t * will never be executed.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\tWARN_ONCE(!static_call_is_init(site),\\n\\t\\t\\t\\t\\t  \"can\\'t patch static call site at %pS\",\\n\\t\\t\\t\\t\\t  site_addr);\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\n\\t\\t\\tarch_static_call_transform(site_addr, NULL, func,\\n\\t\\t\\t\\t\\t\\t   static_call_is_tail(site));\\n\\t\\t}\\n\\t}\\n\\ndone:\\n\\tstatic_call_unlock();\\n\\tcpus_read_unlock();\\n}\\nEXPORT_SYMBOL_GPL(__static_call_update);\\n\\nstatic int __static_call_init(struct module *mod,\\n\\t\\t\\t      struct static_call_site *start,\\n\\t\\t\\t      struct static_call_site *stop)\\n{\\n\\tstruct static_call_site *site;\\n\\tstruct static_call_key *key, *prev_key = NULL;\\n\\tstruct static_call_mod *site_mod;\\n\\n\\tif (start == stop)\\n\\t\\treturn 0;\\n\\n\\tstatic_call_sort_entries(start, stop);\\n\\n\\tfor (site = start; site < stop; site++) {\\n\\t\\tvoid *site_addr = static_call_addr(site);\\n\\n\\t\\tif ((mod && within_module_init((unsigned long)site_addr, mod)) ||\\n\\t\\t    (!mod && init_section_contains(site_addr, 1)))\\n\\t\\t\\tstatic_call_set_init(site);\\n\\n\\t\\tkey = static_call_key(site);\\n\\t\\tif (key != prev_key) {\\n\\t\\t\\tprev_key = key;\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * For vmlinux (!mod) avoid the allocation by storing\\n\\t\\t\\t * the sites pointer in the key itself. Also see\\n\\t\\t\\t * __static_call_update()\\'s @first.\\n\\t\\t\\t *\\n\\t\\t\\t * This allows architectures (eg. x86) to call\\n\\t\\t\\t * static_call_init() before memory allocation works.\\n\\t\\t\\t */\\n\\t\\t\\tif (!mod) {\\n\\t\\t\\t\\tkey->sites = site;\\n\\t\\t\\t\\tkey->type |= 1;\\n\\t\\t\\t\\tgoto do_transform;\\n\\t\\t\\t}\\n\\n\\t\\t\\tsite_mod = kzalloc(sizeof(*site_mod), GFP_KERNEL);\\n\\t\\t\\tif (!site_mod)\\n\\t\\t\\t\\treturn -ENOMEM;\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * When the key has a direct sites pointer, extract\\n\\t\\t\\t * that into an explicit struct static_call_mod, so we\\n\\t\\t\\t * can have a list of modules.\\n\\t\\t\\t */\\n\\t\\t\\tif (static_call_key_sites(key)) {\\n\\t\\t\\t\\tsite_mod->mod = NULL;\\n\\t\\t\\t\\tsite_mod->next = NULL;\\n\\t\\t\\t\\tsite_mod->sites = static_call_key_sites(key);\\n\\n\\t\\t\\t\\tkey->mods = site_mod;\\n\\n\\t\\t\\t\\tsite_mod = kzalloc(sizeof(*site_mod), GFP_KERNEL);\\n\\t\\t\\t\\tif (!site_mod)\\n\\t\\t\\t\\t\\treturn -ENOMEM;\\n\\t\\t\\t}\\n\\n\\t\\t\\tsite_mod->mod = mod;\\n\\t\\t\\tsite_mod->sites = site;\\n\\t\\t\\tsite_mod->next = static_call_key_next(key);\\n\\t\\t\\tkey->mods = site_mod;\\n\\t\\t}\\n\\ndo_transform:\\n\\t\\tarch_static_call_transform(site_addr, NULL, key->func,\\n\\t\\t\\t\\tstatic_call_is_tail(site));\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic int addr_conflict(struct static_call_site *site, void *start, void *end)\\n{\\n\\tunsigned long addr = (unsigned long)static_call_addr(site);\\n\\n\\tif (addr <= (unsigned long)end &&\\n\\t    addr + CALL_INSN_SIZE > (unsigned long)start)\\n\\t\\treturn 1;\\n\\n\\treturn 0;\\n}\\n\\nstatic int __static_call_text_reserved(struct static_call_site *iter_start,\\n\\t\\t\\t\\t       struct static_call_site *iter_stop,\\n\\t\\t\\t\\t       void *start, void *end, bool init)\\n{\\n\\tstruct static_call_site *iter = iter_start;\\n\\n\\twhile (iter < iter_stop) {\\n\\t\\tif (init || !static_call_is_init(iter)) {\\n\\t\\t\\tif (addr_conflict(iter, start, end))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t}\\n\\t\\titer++;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_MODULES\\n\\nstatic int __static_call_mod_text_reserved(void *start, void *end)\\n{\\n\\tstruct module *mod;\\n\\tint ret;\\n\\n\\tpreempt_disable();\\n\\tmod = __module_text_address((unsigned long)start);\\n\\tWARN_ON_ONCE(__module_text_address((unsigned long)end) != mod);\\n\\tif (!try_module_get(mod))\\n\\t\\tmod = NULL;\\n\\tpreempt_enable();\\n\\n\\tif (!mod)\\n\\t\\treturn 0;\\n\\n\\tret = __static_call_text_reserved(mod->static_call_sites,\\n\\t\\t\\tmod->static_call_sites + mod->num_static_call_sites,\\n\\t\\t\\tstart, end, mod->state == MODULE_STATE_COMING);\\n\\n\\tmodule_put(mod);\\n\\n\\treturn ret;\\n}\\n\\nstatic unsigned long tramp_key_lookup(unsigned long addr)\\n{\\n\\tstruct static_call_tramp_key *start = __start_static_call_tramp_key;\\n\\tstruct static_call_tramp_key *stop = __stop_static_call_tramp_key;\\n\\tstruct static_call_tramp_key *tramp_key;\\n\\n\\tfor (tramp_key = start; tramp_key != stop; tramp_key++) {\\n\\t\\tunsigned long tramp;\\n\\n\\t\\ttramp = (long)tramp_key->tramp + (long)&tramp_key->tramp;\\n\\t\\tif (tramp == addr)\\n\\t\\t\\treturn (long)tramp_key->key + (long)&tramp_key->key;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic int static_call_add_module(struct module *mod)\\n{\\n\\tstruct static_call_site *start = mod->static_call_sites;\\n\\tstruct static_call_site *stop = start + mod->num_static_call_sites;\\n\\tstruct static_call_site *site;\\n\\n\\tfor (site = start; site != stop; site++) {\\n\\t\\tunsigned long s_key = __static_call_key(site);\\n\\t\\tunsigned long addr = s_key & ~STATIC_CALL_SITE_FLAGS;\\n\\t\\tunsigned long key;\\n\\n\\t\\t/*\\n\\t\\t * Is the key is exported, \\'addr\\' points to the key, which\\n\\t\\t * means modules are allowed to call static_call_update() on\\n\\t\\t * it.\\n\\t\\t *\\n\\t\\t * Otherwise, the key isn\\'t exported, and \\'addr\\' points to the\\n\\t\\t * trampoline so we need to lookup the key.\\n\\t\\t *\\n\\t\\t * We go through this dance to prevent crazy modules from\\n\\t\\t * abusing sensitive static calls.\\n\\t\\t */\\n\\t\\tif (!kernel_text_address(addr))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tkey = tramp_key_lookup(addr);\\n\\t\\tif (!key) {\\n\\t\\t\\tpr_warn(\"Failed to fixup __raw_static_call() usage at: %ps\\\\n\",\\n\\t\\t\\t\\tstatic_call_addr(site));\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\n\\t\\tkey |= s_key & STATIC_CALL_SITE_FLAGS;\\n\\t\\tsite->key = key - (long)&site->key;\\n\\t}\\n\\n\\treturn __static_call_init(mod, start, stop);\\n}\\n\\nstatic void static_call_del_module(struct module *mod)\\n{\\n\\tstruct static_call_site *start = mod->static_call_sites;\\n\\tstruct static_call_site *stop = mod->static_call_sites +\\n\\t\\t\\t\\t\\tmod->num_static_call_sites;\\n\\tstruct static_call_key *key, *prev_key = NULL;\\n\\tstruct static_call_mod *site_mod, **prev;\\n\\tstruct static_call_site *site;\\n\\n\\tfor (site = start; site < stop; site++) {\\n\\t\\tkey = static_call_key(site);\\n\\n\\t\\t/*\\n\\t\\t * If the key was not updated due to a memory allocation\\n\\t\\t * failure in __static_call_init() then treating key::sites\\n\\t\\t * as key::mods in the code below would cause random memory\\n\\t\\t * access and #GP. In that case all subsequent sites have\\n\\t\\t * not been touched either, so stop iterating.\\n\\t\\t */\\n\\t\\tif (!static_call_key_has_mods(key))\\n\\t\\t\\tbreak;\\n\\n\\t\\tif (key == prev_key)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tprev_key = key;\\n\\n\\t\\tfor (prev = &key->mods, site_mod = key->mods;\\n\\t\\t     site_mod && site_mod->mod != mod;\\n\\t\\t     prev = &site_mod->next, site_mod = site_mod->next)\\n\\t\\t\\t;\\n\\n\\t\\tif (!site_mod)\\n\\t\\t\\tcontinue;\\n\\n\\t\\t*prev = site_mod->next;\\n\\t\\tkfree(site_mod);\\n\\t}\\n}\\n\\nstatic int static_call_module_notify(struct notifier_block *nb,\\n\\t\\t\\t\\t     unsigned long val, void *data)\\n{\\n\\tstruct module *mod = data;\\n\\tint ret = 0;\\n\\n\\tcpus_read_lock();\\n\\tstatic_call_lock();\\n\\n\\tswitch (val) {\\n\\tcase MODULE_STATE_COMING:\\n\\t\\tret = static_call_add_module(mod);\\n\\t\\tif (ret) {\\n\\t\\t\\tpr_warn(\"Failed to allocate memory for static calls\\\\n\");\\n\\t\\t\\tstatic_call_del_module(mod);\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase MODULE_STATE_GOING:\\n\\t\\tstatic_call_del_module(mod);\\n\\t\\tbreak;\\n\\t}\\n\\n\\tstatic_call_unlock();\\n\\tcpus_read_unlock();\\n\\n\\treturn notifier_from_errno(ret);\\n}\\n\\nstatic struct notifier_block static_call_module_nb = {\\n\\t.notifier_call = static_call_module_notify,\\n};\\n\\n#else\\n\\nstatic inline int __static_call_mod_text_reserved(void *start, void *end)\\n{\\n\\treturn 0;\\n}\\n\\n#endif /* CONFIG_MODULES */\\n\\nint static_call_text_reserved(void *start, void *end)\\n{\\n\\tbool init = system_state < SYSTEM_RUNNING;\\n\\tint ret = __static_call_text_reserved(__start_static_call_sites,\\n\\t\\t\\t__stop_static_call_sites, start, end, init);\\n\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\treturn __static_call_mod_text_reserved(start, end);\\n}\\n\\nint __init static_call_init(void)\\n{\\n\\tint ret;\\n\\n\\t/* See static_call_force_reinit(). */\\n\\tif (static_call_initialized == 1)\\n\\t\\treturn 0;\\n\\n\\tcpus_read_lock();\\n\\tstatic_call_lock();\\n\\tret = __static_call_init(NULL, __start_static_call_sites,\\n\\t\\t\\t\\t __stop_static_call_sites);\\n\\tstatic_call_unlock();\\n\\tcpus_read_unlock();\\n\\n\\tif (ret) {\\n\\t\\tpr_err(\"Failed to allocate memory for static_call!\\\\n\");\\n\\t\\tBUG();\\n\\t}\\n\\n#ifdef CONFIG_MODULES\\n\\tif (!static_call_initialized)\\n\\t\\tregister_module_notifier(&static_call_module_nb);\\n#endif\\n\\n\\tstatic_call_initialized = 1;\\n\\treturn 0;\\n}\\nearly_initcall(static_call_init);\\n\\n#ifdef CONFIG_STATIC_CALL_SELFTEST\\n\\nstatic int func_a(int x)\\n{\\n\\treturn x+1;\\n}\\n\\nstatic int func_b(int x)\\n{\\n\\treturn x+2;\\n}\\n\\nDEFINE_STATIC_CALL(sc_selftest, func_a);\\n\\nstatic struct static_call_data {\\n      int (*func)(int);\\n      int val;\\n      int expect;\\n} static_call_data [] __initdata = {\\n      { NULL,   2, 3 },\\n      { func_b, 2, 4 },\\n      { func_a, 2, 3 }\\n};\\n\\nstatic int __init test_static_call_init(void)\\n{\\n      int i;\\n\\n      for (i = 0; i < ARRAY_SIZE(static_call_data); i++ ) {\\n\\t      struct static_call_data *scd = &static_call_data[i];\\n\\n              if (scd->func)\\n                      static_call_update(sc_selftest, scd->func);\\n\\n              WARN_ON(static_call(sc_selftest)(scd->val) != scd->expect);\\n      }\\n\\n      return 0;\\n}\\nearly_initcall(test_static_call_init);\\n\\n#endif /* CONFIG_STATIC_CALL_SELFTEST */\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#include <linux/static_call.h>\\n\\nlong __static_call_return0(void)\\n{\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(__static_call_return0);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n#include <linux/kdebug.h>\\n#include <linux/kprobes.h>\\n#include <linux/export.h>\\n#include <linux/notifier.h>\\n#include <linux/rcupdate.h>\\n#include <linux/vmalloc.h>\\n\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/notifier.h>\\n\\n/*\\n *\\tNotifier chain core routines.  The exported routines below\\n *\\tare layered on top of these, with appropriate locking added.\\n */\\n\\nstatic int notifier_chain_register(struct notifier_block **nl,\\n\\t\\t\\t\\t   struct notifier_block *n,\\n\\t\\t\\t\\t   bool unique_priority)\\n{\\n\\twhile ((*nl) != NULL) {\\n\\t\\tif (unlikely((*nl) == n)) {\\n\\t\\t\\tWARN(1, \"notifier callback %ps already registered\",\\n\\t\\t\\t     n->notifier_call);\\n\\t\\t\\treturn -EEXIST;\\n\\t\\t}\\n\\t\\tif (n->priority > (*nl)->priority)\\n\\t\\t\\tbreak;\\n\\t\\tif (n->priority == (*nl)->priority && unique_priority)\\n\\t\\t\\treturn -EBUSY;\\n\\t\\tnl = &((*nl)->next);\\n\\t}\\n\\tn->next = *nl;\\n\\trcu_assign_pointer(*nl, n);\\n\\ttrace_notifier_register((void *)n->notifier_call);\\n\\treturn 0;\\n}\\n\\nstatic int notifier_chain_unregister(struct notifier_block **nl,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\twhile ((*nl) != NULL) {\\n\\t\\tif ((*nl) == n) {\\n\\t\\t\\trcu_assign_pointer(*nl, n->next);\\n\\t\\t\\ttrace_notifier_unregister((void *)n->notifier_call);\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\t\\tnl = &((*nl)->next);\\n\\t}\\n\\treturn -ENOENT;\\n}\\n\\n/**\\n * notifier_call_chain - Informs the registered notifiers about an event.\\n *\\t@nl:\\t\\tPointer to head of the blocking notifier chain\\n *\\t@val:\\t\\tValue passed unmodified to notifier function\\n *\\t@v:\\t\\tPointer passed unmodified to notifier function\\n *\\t@nr_to_call:\\tNumber of notifier functions to be called. Don\\'t care\\n *\\t\\t\\tvalue of this parameter is -1.\\n *\\t@nr_calls:\\tRecords the number of notifications sent. Don\\'t care\\n *\\t\\t\\tvalue of this field is NULL.\\n *\\tReturn:\\t\\tnotifier_call_chain returns the value returned by the\\n *\\t\\t\\tlast notifier function called.\\n */\\nstatic int notifier_call_chain(struct notifier_block **nl,\\n\\t\\t\\t       unsigned long val, void *v,\\n\\t\\t\\t       int nr_to_call, int *nr_calls)\\n{\\n\\tint ret = NOTIFY_DONE;\\n\\tstruct notifier_block *nb, *next_nb;\\n\\n\\tnb = rcu_dereference_raw(*nl);\\n\\n\\twhile (nb && nr_to_call) {\\n\\t\\tnext_nb = rcu_dereference_raw(nb->next);\\n\\n#ifdef CONFIG_DEBUG_NOTIFIERS\\n\\t\\tif (unlikely(!func_ptr_is_kernel_text(nb->notifier_call))) {\\n\\t\\t\\tWARN(1, \"Invalid notifier called!\");\\n\\t\\t\\tnb = next_nb;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n#endif\\n\\t\\ttrace_notifier_run((void *)nb->notifier_call);\\n\\t\\tret = nb->notifier_call(nb, val, v);\\n\\n\\t\\tif (nr_calls)\\n\\t\\t\\t(*nr_calls)++;\\n\\n\\t\\tif (ret & NOTIFY_STOP_MASK)\\n\\t\\t\\tbreak;\\n\\t\\tnb = next_nb;\\n\\t\\tnr_to_call--;\\n\\t}\\n\\treturn ret;\\n}\\nNOKPROBE_SYMBOL(notifier_call_chain);\\n\\n/**\\n * notifier_call_chain_robust - Inform the registered notifiers about an event\\n *                              and rollback on error.\\n * @nl:\\t\\tPointer to head of the blocking notifier chain\\n * @val_up:\\tValue passed unmodified to the notifier function\\n * @val_down:\\tValue passed unmodified to the notifier function when recovering\\n *              from an error on @val_up\\n * @v:\\t\\tPointer passed unmodified to the notifier function\\n *\\n * NOTE:\\tIt is important the @nl chain doesn\\'t change between the two\\n *\\t\\tinvocations of notifier_call_chain() such that we visit the\\n *\\t\\texact same notifier callbacks; this rules out any RCU usage.\\n *\\n * Return:\\tthe return value of the @val_up call.\\n */\\nstatic int notifier_call_chain_robust(struct notifier_block **nl,\\n\\t\\t\\t\\t     unsigned long val_up, unsigned long val_down,\\n\\t\\t\\t\\t     void *v)\\n{\\n\\tint ret, nr = 0;\\n\\n\\tret = notifier_call_chain(nl, val_up, v, -1, &nr);\\n\\tif (ret & NOTIFY_STOP_MASK)\\n\\t\\tnotifier_call_chain(nl, val_down, v, nr-1, NULL);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n *\\tAtomic notifier chain routines.  Registration and unregistration\\n *\\tuse a spinlock, and call_chain is synchronized by RCU (no locks).\\n */\\n\\n/**\\n *\\tatomic_notifier_chain_register - Add notifier to an atomic notifier chain\\n *\\t@nh: Pointer to head of the atomic notifier chain\\n *\\t@n: New entry in notifier chain\\n *\\n *\\tAdds a notifier to an atomic notifier chain.\\n *\\n *\\tReturns 0 on success, %-EEXIST on error.\\n */\\nint atomic_notifier_chain_register(struct atomic_notifier_head *nh,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\tunsigned long flags;\\n\\tint ret;\\n\\n\\tspin_lock_irqsave(&nh->lock, flags);\\n\\tret = notifier_chain_register(&nh->head, n, false);\\n\\tspin_unlock_irqrestore(&nh->lock, flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(atomic_notifier_chain_register);\\n\\n/**\\n *\\tatomic_notifier_chain_register_unique_prio - Add notifier to an atomic notifier chain\\n *\\t@nh: Pointer to head of the atomic notifier chain\\n *\\t@n: New entry in notifier chain\\n *\\n *\\tAdds a notifier to an atomic notifier chain if there is no other\\n *\\tnotifier registered using the same priority.\\n *\\n *\\tReturns 0 on success, %-EEXIST or %-EBUSY on error.\\n */\\nint atomic_notifier_chain_register_unique_prio(struct atomic_notifier_head *nh,\\n\\t\\t\\t\\t\\t       struct notifier_block *n)\\n{\\n\\tunsigned long flags;\\n\\tint ret;\\n\\n\\tspin_lock_irqsave(&nh->lock, flags);\\n\\tret = notifier_chain_register(&nh->head, n, true);\\n\\tspin_unlock_irqrestore(&nh->lock, flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(atomic_notifier_chain_register_unique_prio);\\n\\n/**\\n *\\tatomic_notifier_chain_unregister - Remove notifier from an atomic notifier chain\\n *\\t@nh: Pointer to head of the atomic notifier chain\\n *\\t@n: Entry to remove from notifier chain\\n *\\n *\\tRemoves a notifier from an atomic notifier chain.\\n *\\n *\\tReturns zero on success or %-ENOENT on failure.\\n */\\nint atomic_notifier_chain_unregister(struct atomic_notifier_head *nh,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\tunsigned long flags;\\n\\tint ret;\\n\\n\\tspin_lock_irqsave(&nh->lock, flags);\\n\\tret = notifier_chain_unregister(&nh->head, n);\\n\\tspin_unlock_irqrestore(&nh->lock, flags);\\n\\tsynchronize_rcu();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(atomic_notifier_chain_unregister);\\n\\n/**\\n *\\tatomic_notifier_call_chain - Call functions in an atomic notifier chain\\n *\\t@nh: Pointer to head of the atomic notifier chain\\n *\\t@val: Value passed unmodified to notifier function\\n *\\t@v: Pointer passed unmodified to notifier function\\n *\\n *\\tCalls each function in a notifier chain in turn.  The functions\\n *\\trun in an atomic context, so they must not block.\\n *\\tThis routine uses RCU to synchronize with changes to the chain.\\n *\\n *\\tIf the return value of the notifier can be and\\'ed\\n *\\twith %NOTIFY_STOP_MASK then atomic_notifier_call_chain()\\n *\\twill return immediately, with the return value of\\n *\\tthe notifier function which halted execution.\\n *\\tOtherwise the return value is the return value\\n *\\tof the last notifier function called.\\n */\\nint atomic_notifier_call_chain(struct atomic_notifier_head *nh,\\n\\t\\t\\t       unsigned long val, void *v)\\n{\\n\\tint ret;\\n\\n\\trcu_read_lock();\\n\\tret = notifier_call_chain(&nh->head, val, v, -1, NULL);\\n\\trcu_read_unlock();\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(atomic_notifier_call_chain);\\nNOKPROBE_SYMBOL(atomic_notifier_call_chain);\\n\\n/**\\n *\\tatomic_notifier_call_chain_is_empty - Check whether notifier chain is empty\\n *\\t@nh: Pointer to head of the atomic notifier chain\\n *\\n *\\tChecks whether notifier chain is empty.\\n *\\n *\\tReturns true is notifier chain is empty, false otherwise.\\n */\\nbool atomic_notifier_call_chain_is_empty(struct atomic_notifier_head *nh)\\n{\\n\\treturn !rcu_access_pointer(nh->head);\\n}\\n\\n/*\\n *\\tBlocking notifier chain routines.  All access to the chain is\\n *\\tsynchronized by an rwsem.\\n */\\n\\nstatic int __blocking_notifier_chain_register(struct blocking_notifier_head *nh,\\n\\t\\t\\t\\t\\t      struct notifier_block *n,\\n\\t\\t\\t\\t\\t      bool unique_priority)\\n{\\n\\tint ret;\\n\\n\\t/*\\n\\t * This code gets used during boot-up, when task switching is\\n\\t * not yet working and interrupts must remain disabled.  At\\n\\t * such times we must not call down_write().\\n\\t */\\n\\tif (unlikely(system_state == SYSTEM_BOOTING))\\n\\t\\treturn notifier_chain_register(&nh->head, n, unique_priority);\\n\\n\\tdown_write(&nh->rwsem);\\n\\tret = notifier_chain_register(&nh->head, n, unique_priority);\\n\\tup_write(&nh->rwsem);\\n\\treturn ret;\\n}\\n\\n/**\\n *\\tblocking_notifier_chain_register - Add notifier to a blocking notifier chain\\n *\\t@nh: Pointer to head of the blocking notifier chain\\n *\\t@n: New entry in notifier chain\\n *\\n *\\tAdds a notifier to a blocking notifier chain.\\n *\\tMust be called in process context.\\n *\\n *\\tReturns 0 on success, %-EEXIST on error.\\n */\\nint blocking_notifier_chain_register(struct blocking_notifier_head *nh,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\treturn __blocking_notifier_chain_register(nh, n, false);\\n}\\nEXPORT_SYMBOL_GPL(blocking_notifier_chain_register);\\n\\n/**\\n *\\tblocking_notifier_chain_register_unique_prio - Add notifier to a blocking notifier chain\\n *\\t@nh: Pointer to head of the blocking notifier chain\\n *\\t@n: New entry in notifier chain\\n *\\n *\\tAdds a notifier to an blocking notifier chain if there is no other\\n *\\tnotifier registered using the same priority.\\n *\\n *\\tReturns 0 on success, %-EEXIST or %-EBUSY on error.\\n */\\nint blocking_notifier_chain_register_unique_prio(struct blocking_notifier_head *nh,\\n\\t\\t\\t\\t\\t\\t struct notifier_block *n)\\n{\\n\\treturn __blocking_notifier_chain_register(nh, n, true);\\n}\\nEXPORT_SYMBOL_GPL(blocking_notifier_chain_register_unique_prio);\\n\\n/**\\n *\\tblocking_notifier_chain_unregister - Remove notifier from a blocking notifier chain\\n *\\t@nh: Pointer to head of the blocking notifier chain\\n *\\t@n: Entry to remove from notifier chain\\n *\\n *\\tRemoves a notifier from a blocking notifier chain.\\n *\\tMust be called from process context.\\n *\\n *\\tReturns zero on success or %-ENOENT on failure.\\n */\\nint blocking_notifier_chain_unregister(struct blocking_notifier_head *nh,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\tint ret;\\n\\n\\t/*\\n\\t * This code gets used during boot-up, when task switching is\\n\\t * not yet working and interrupts must remain disabled.  At\\n\\t * such times we must not call down_write().\\n\\t */\\n\\tif (unlikely(system_state == SYSTEM_BOOTING))\\n\\t\\treturn notifier_chain_unregister(&nh->head, n);\\n\\n\\tdown_write(&nh->rwsem);\\n\\tret = notifier_chain_unregister(&nh->head, n);\\n\\tup_write(&nh->rwsem);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(blocking_notifier_chain_unregister);\\n\\nint blocking_notifier_call_chain_robust(struct blocking_notifier_head *nh,\\n\\t\\tunsigned long val_up, unsigned long val_down, void *v)\\n{\\n\\tint ret = NOTIFY_DONE;\\n\\n\\t/*\\n\\t * We check the head outside the lock, but if this access is\\n\\t * racy then it does not matter what the result of the test\\n\\t * is, we re-check the list after having taken the lock anyway:\\n\\t */\\n\\tif (rcu_access_pointer(nh->head)) {\\n\\t\\tdown_read(&nh->rwsem);\\n\\t\\tret = notifier_call_chain_robust(&nh->head, val_up, val_down, v);\\n\\t\\tup_read(&nh->rwsem);\\n\\t}\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(blocking_notifier_call_chain_robust);\\n\\n/**\\n *\\tblocking_notifier_call_chain - Call functions in a blocking notifier chain\\n *\\t@nh: Pointer to head of the blocking notifier chain\\n *\\t@val: Value passed unmodified to notifier function\\n *\\t@v: Pointer passed unmodified to notifier function\\n *\\n *\\tCalls each function in a notifier chain in turn.  The functions\\n *\\trun in a process context, so they are allowed to block.\\n *\\n *\\tIf the return value of the notifier can be and\\'ed\\n *\\twith %NOTIFY_STOP_MASK then blocking_notifier_call_chain()\\n *\\twill return immediately, with the return value of\\n *\\tthe notifier function which halted execution.\\n *\\tOtherwise the return value is the return value\\n *\\tof the last notifier function called.\\n */\\nint blocking_notifier_call_chain(struct blocking_notifier_head *nh,\\n\\t\\tunsigned long val, void *v)\\n{\\n\\tint ret = NOTIFY_DONE;\\n\\n\\t/*\\n\\t * We check the head outside the lock, but if this access is\\n\\t * racy then it does not matter what the result of the test\\n\\t * is, we re-check the list after having taken the lock anyway:\\n\\t */\\n\\tif (rcu_access_pointer(nh->head)) {\\n\\t\\tdown_read(&nh->rwsem);\\n\\t\\tret = notifier_call_chain(&nh->head, val, v, -1, NULL);\\n\\t\\tup_read(&nh->rwsem);\\n\\t}\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(blocking_notifier_call_chain);\\n\\n/*\\n *\\tRaw notifier chain routines.  There is no protection;\\n *\\tthe caller must provide it.  Use at your own risk!\\n */\\n\\n/**\\n *\\traw_notifier_chain_register - Add notifier to a raw notifier chain\\n *\\t@nh: Pointer to head of the raw notifier chain\\n *\\t@n: New entry in notifier chain\\n *\\n *\\tAdds a notifier to a raw notifier chain.\\n *\\tAll locking must be provided by the caller.\\n *\\n *\\tReturns 0 on success, %-EEXIST on error.\\n */\\nint raw_notifier_chain_register(struct raw_notifier_head *nh,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\treturn notifier_chain_register(&nh->head, n, false);\\n}\\nEXPORT_SYMBOL_GPL(raw_notifier_chain_register);\\n\\n/**\\n *\\traw_notifier_chain_unregister - Remove notifier from a raw notifier chain\\n *\\t@nh: Pointer to head of the raw notifier chain\\n *\\t@n: Entry to remove from notifier chain\\n *\\n *\\tRemoves a notifier from a raw notifier chain.\\n *\\tAll locking must be provided by the caller.\\n *\\n *\\tReturns zero on success or %-ENOENT on failure.\\n */\\nint raw_notifier_chain_unregister(struct raw_notifier_head *nh,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\treturn notifier_chain_unregister(&nh->head, n);\\n}\\nEXPORT_SYMBOL_GPL(raw_notifier_chain_unregister);\\n\\nint raw_notifier_call_chain_robust(struct raw_notifier_head *nh,\\n\\t\\tunsigned long val_up, unsigned long val_down, void *v)\\n{\\n\\treturn notifier_call_chain_robust(&nh->head, val_up, val_down, v);\\n}\\nEXPORT_SYMBOL_GPL(raw_notifier_call_chain_robust);\\n\\n/**\\n *\\traw_notifier_call_chain - Call functions in a raw notifier chain\\n *\\t@nh: Pointer to head of the raw notifier chain\\n *\\t@val: Value passed unmodified to notifier function\\n *\\t@v: Pointer passed unmodified to notifier function\\n *\\n *\\tCalls each function in a notifier chain in turn.  The functions\\n *\\trun in an undefined context.\\n *\\tAll locking must be provided by the caller.\\n *\\n *\\tIf the return value of the notifier can be and\\'ed\\n *\\twith %NOTIFY_STOP_MASK then raw_notifier_call_chain()\\n *\\twill return immediately, with the return value of\\n *\\tthe notifier function which halted execution.\\n *\\tOtherwise the return value is the return value\\n *\\tof the last notifier function called.\\n */\\nint raw_notifier_call_chain(struct raw_notifier_head *nh,\\n\\t\\tunsigned long val, void *v)\\n{\\n\\treturn notifier_call_chain(&nh->head, val, v, -1, NULL);\\n}\\nEXPORT_SYMBOL_GPL(raw_notifier_call_chain);\\n\\n/*\\n *\\tSRCU notifier chain routines.    Registration and unregistration\\n *\\tuse a mutex, and call_chain is synchronized by SRCU (no locks).\\n */\\n\\n/**\\n *\\tsrcu_notifier_chain_register - Add notifier to an SRCU notifier chain\\n *\\t@nh: Pointer to head of the SRCU notifier chain\\n *\\t@n: New entry in notifier chain\\n *\\n *\\tAdds a notifier to an SRCU notifier chain.\\n *\\tMust be called in process context.\\n *\\n *\\tReturns 0 on success, %-EEXIST on error.\\n */\\nint srcu_notifier_chain_register(struct srcu_notifier_head *nh,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\tint ret;\\n\\n\\t/*\\n\\t * This code gets used during boot-up, when task switching is\\n\\t * not yet working and interrupts must remain disabled.  At\\n\\t * such times we must not call mutex_lock().\\n\\t */\\n\\tif (unlikely(system_state == SYSTEM_BOOTING))\\n\\t\\treturn notifier_chain_register(&nh->head, n, false);\\n\\n\\tmutex_lock(&nh->mutex);\\n\\tret = notifier_chain_register(&nh->head, n, false);\\n\\tmutex_unlock(&nh->mutex);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(srcu_notifier_chain_register);\\n\\n/**\\n *\\tsrcu_notifier_chain_unregister - Remove notifier from an SRCU notifier chain\\n *\\t@nh: Pointer to head of the SRCU notifier chain\\n *\\t@n: Entry to remove from notifier chain\\n *\\n *\\tRemoves a notifier from an SRCU notifier chain.\\n *\\tMust be called from process context.\\n *\\n *\\tReturns zero on success or %-ENOENT on failure.\\n */\\nint srcu_notifier_chain_unregister(struct srcu_notifier_head *nh,\\n\\t\\tstruct notifier_block *n)\\n{\\n\\tint ret;\\n\\n\\t/*\\n\\t * This code gets used during boot-up, when task switching is\\n\\t * not yet working and interrupts must remain disabled.  At\\n\\t * such times we must not call mutex_lock().\\n\\t */\\n\\tif (unlikely(system_state == SYSTEM_BOOTING))\\n\\t\\treturn notifier_chain_unregister(&nh->head, n);\\n\\n\\tmutex_lock(&nh->mutex);\\n\\tret = notifier_chain_unregister(&nh->head, n);\\n\\tmutex_unlock(&nh->mutex);\\n\\tsynchronize_srcu(&nh->srcu);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(srcu_notifier_chain_unregister);\\n\\n/**\\n *\\tsrcu_notifier_call_chain - Call functions in an SRCU notifier chain\\n *\\t@nh: Pointer to head of the SRCU notifier chain\\n *\\t@val: Value passed unmodified to notifier function\\n *\\t@v: Pointer passed unmodified to notifier function\\n *\\n *\\tCalls each function in a notifier chain in turn.  The functions\\n *\\trun in a process context, so they are allowed to block.\\n *\\n *\\tIf the return value of the notifier can be and\\'ed\\n *\\twith %NOTIFY_STOP_MASK then srcu_notifier_call_chain()\\n *\\twill return immediately, with the return value of\\n *\\tthe notifier function which halted execution.\\n *\\tOtherwise the return value is the return value\\n *\\tof the last notifier function called.\\n */\\nint srcu_notifier_call_chain(struct srcu_notifier_head *nh,\\n\\t\\tunsigned long val, void *v)\\n{\\n\\tint ret;\\n\\tint idx;\\n\\n\\tidx = srcu_read_lock(&nh->srcu);\\n\\tret = notifier_call_chain(&nh->head, val, v, -1, NULL);\\n\\tsrcu_read_unlock(&nh->srcu, idx);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(srcu_notifier_call_chain);\\n\\n/**\\n *\\tsrcu_init_notifier_head - Initialize an SRCU notifier head\\n *\\t@nh: Pointer to head of the srcu notifier chain\\n *\\n *\\tUnlike other sorts of notifier heads, SRCU notifier heads require\\n *\\tdynamic initialization.  Be sure to call this routine before\\n *\\tcalling any of the other SRCU notifier routines for this head.\\n *\\n *\\tIf an SRCU notifier head is deallocated, it must first be cleaned\\n *\\tup by calling srcu_cleanup_notifier_head().  Otherwise the head\\'s\\n *\\tper-cpu data (used by the SRCU mechanism) will leak.\\n */\\nvoid srcu_init_notifier_head(struct srcu_notifier_head *nh)\\n{\\n\\tmutex_init(&nh->mutex);\\n\\tif (init_srcu_struct(&nh->srcu) < 0)\\n\\t\\tBUG();\\n\\tnh->head = NULL;\\n}\\nEXPORT_SYMBOL_GPL(srcu_init_notifier_head);\\n\\nstatic ATOMIC_NOTIFIER_HEAD(die_chain);\\n\\nint notrace notify_die(enum die_val val, const char *str,\\n\\t       struct pt_regs *regs, long err, int trap, int sig)\\n{\\n\\tstruct die_args args = {\\n\\t\\t.regs\\t= regs,\\n\\t\\t.str\\t= str,\\n\\t\\t.err\\t= err,\\n\\t\\t.trapnr\\t= trap,\\n\\t\\t.signr\\t= sig,\\n\\n\\t};\\n\\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\\n\\t\\t\\t   \"notify_die called but RCU thinks we\\'re quiescent\");\\n\\treturn atomic_notifier_call_chain(&die_chain, val, &args);\\n}\\nNOKPROBE_SYMBOL(notify_die);\\n\\nint register_die_notifier(struct notifier_block *nb)\\n{\\n\\treturn atomic_notifier_chain_register(&die_chain, nb);\\n}\\nEXPORT_SYMBOL_GPL(register_die_notifier);\\n\\nint unregister_die_notifier(struct notifier_block *nb)\\n{\\n\\treturn atomic_notifier_chain_unregister(&die_chain, nb);\\n}\\nEXPORT_SYMBOL_GPL(unregister_die_notifier);\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/*\\n * taskstats.c - Export per-task statistics to userland\\n *\\n * Copyright (C) Shailabh Nagar, IBM Corp. 2006\\n *           (C) Balbir Singh,   IBM Corp. 2006\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/taskstats_kern.h>\\n#include <linux/tsacct_kern.h>\\n#include <linux/acct.h>\\n#include <linux/delayacct.h>\\n#include <linux/cpumask.h>\\n#include <linux/percpu.h>\\n#include <linux/slab.h>\\n#include <linux/cgroupstats.h>\\n#include <linux/cgroup.h>\\n#include <linux/fs.h>\\n#include <linux/file.h>\\n#include <linux/pid_namespace.h>\\n#include <net/genetlink.h>\\n#include <linux/atomic.h>\\n#include <linux/sched/cputime.h>\\n\\n/*\\n * Maximum length of a cpumask that can be specified in\\n * the TASKSTATS_CMD_ATTR_REGISTER/DEREGISTER_CPUMASK attribute\\n */\\n#define TASKSTATS_CPUMASK_MAXLEN\\t(100+6*NR_CPUS)\\n\\nstatic DEFINE_PER_CPU(__u32, taskstats_seqnum);\\nstatic int family_registered;\\nstruct kmem_cache *taskstats_cache;\\n\\nstatic struct genl_family family;\\n\\nstatic const struct nla_policy taskstats_cmd_get_policy[] = {\\n\\t[TASKSTATS_CMD_ATTR_PID]  = { .type = NLA_U32 },\\n\\t[TASKSTATS_CMD_ATTR_TGID] = { .type = NLA_U32 },\\n\\t[TASKSTATS_CMD_ATTR_REGISTER_CPUMASK] = { .type = NLA_STRING },\\n\\t[TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK] = { .type = NLA_STRING },};\\n\\nstatic const struct nla_policy cgroupstats_cmd_get_policy[] = {\\n\\t[CGROUPSTATS_CMD_ATTR_FD] = { .type = NLA_U32 },\\n};\\n\\nstruct listener {\\n\\tstruct list_head list;\\n\\tpid_t pid;\\n\\tchar valid;\\n};\\n\\nstruct listener_list {\\n\\tstruct rw_semaphore sem;\\n\\tstruct list_head list;\\n};\\nstatic DEFINE_PER_CPU(struct listener_list, listener_array);\\n\\nenum actions {\\n\\tREGISTER,\\n\\tDEREGISTER,\\n\\tCPU_DONT_CARE\\n};\\n\\nstatic int prepare_reply(struct genl_info *info, u8 cmd, struct sk_buff **skbp,\\n\\t\\t\\t\\tsize_t size)\\n{\\n\\tstruct sk_buff *skb;\\n\\tvoid *reply;\\n\\n\\t/*\\n\\t * If new attributes are added, please revisit this allocation\\n\\t */\\n\\tskb = genlmsg_new(size, GFP_KERNEL);\\n\\tif (!skb)\\n\\t\\treturn -ENOMEM;\\n\\n\\tif (!info) {\\n\\t\\tint seq = this_cpu_inc_return(taskstats_seqnum) - 1;\\n\\n\\t\\treply = genlmsg_put(skb, 0, seq, &family, 0, cmd);\\n\\t} else\\n\\t\\treply = genlmsg_put_reply(skb, info, &family, 0, cmd);\\n\\tif (reply == NULL) {\\n\\t\\tnlmsg_free(skb);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\t*skbp = skb;\\n\\treturn 0;\\n}\\n\\n/*\\n * Send taskstats data in @skb to listener with nl_pid @pid\\n */\\nstatic int send_reply(struct sk_buff *skb, struct genl_info *info)\\n{\\n\\tstruct genlmsghdr *genlhdr = nlmsg_data(nlmsg_hdr(skb));\\n\\tvoid *reply = genlmsg_data(genlhdr);\\n\\n\\tgenlmsg_end(skb, reply);\\n\\n\\treturn genlmsg_reply(skb, info);\\n}\\n\\n/*\\n * Send taskstats data in @skb to listeners registered for @cpu\\'s exit data\\n */\\nstatic void send_cpu_listeners(struct sk_buff *skb,\\n\\t\\t\\t\\t\\tstruct listener_list *listeners)\\n{\\n\\tstruct genlmsghdr *genlhdr = nlmsg_data(nlmsg_hdr(skb));\\n\\tstruct listener *s, *tmp;\\n\\tstruct sk_buff *skb_next, *skb_cur = skb;\\n\\tvoid *reply = genlmsg_data(genlhdr);\\n\\tint delcount = 0;\\n\\n\\tgenlmsg_end(skb, reply);\\n\\n\\tdown_read(&listeners->sem);\\n\\tlist_for_each_entry(s, &listeners->list, list) {\\n\\t\\tint rc;\\n\\n\\t\\tskb_next = NULL;\\n\\t\\tif (!list_is_last(&s->list, &listeners->list)) {\\n\\t\\t\\tskb_next = skb_clone(skb_cur, GFP_KERNEL);\\n\\t\\t\\tif (!skb_next)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\trc = genlmsg_unicast(&init_net, skb_cur, s->pid);\\n\\t\\tif (rc == -ECONNREFUSED) {\\n\\t\\t\\ts->valid = 0;\\n\\t\\t\\tdelcount++;\\n\\t\\t}\\n\\t\\tskb_cur = skb_next;\\n\\t}\\n\\tup_read(&listeners->sem);\\n\\n\\tif (skb_cur)\\n\\t\\tnlmsg_free(skb_cur);\\n\\n\\tif (!delcount)\\n\\t\\treturn;\\n\\n\\t/* Delete invalidated entries */\\n\\tdown_write(&listeners->sem);\\n\\tlist_for_each_entry_safe(s, tmp, &listeners->list, list) {\\n\\t\\tif (!s->valid) {\\n\\t\\t\\tlist_del(&s->list);\\n\\t\\t\\tkfree(s);\\n\\t\\t}\\n\\t}\\n\\tup_write(&listeners->sem);\\n}\\n\\nstatic void exe_add_tsk(struct taskstats *stats, struct task_struct *tsk)\\n{\\n\\t/* No idea if I\\'m allowed to access that here, now. */\\n\\tstruct file *exe_file = get_task_exe_file(tsk);\\n\\n\\tif (exe_file) {\\n\\t\\t/* Following cp_new_stat64() in stat.c . */\\n\\t\\tstats->ac_exe_dev =\\n\\t\\t\\thuge_encode_dev(exe_file->f_inode->i_sb->s_dev);\\n\\t\\tstats->ac_exe_inode = exe_file->f_inode->i_ino;\\n\\t\\tfput(exe_file);\\n\\t} else {\\n\\t\\tstats->ac_exe_dev = 0;\\n\\t\\tstats->ac_exe_inode = 0;\\n\\t}\\n}\\n\\nstatic void fill_stats(struct user_namespace *user_ns,\\n\\t\\t       struct pid_namespace *pid_ns,\\n\\t\\t       struct task_struct *tsk, struct taskstats *stats)\\n{\\n\\tmemset(stats, 0, sizeof(*stats));\\n\\t/*\\n\\t * Each accounting subsystem adds calls to its functions to\\n\\t * fill in relevant parts of struct taskstsats as follows\\n\\t *\\n\\t *\\tper-task-foo(stats, tsk);\\n\\t */\\n\\n\\tdelayacct_add_tsk(stats, tsk);\\n\\n\\t/* fill in basic acct fields */\\n\\tstats->version = TASKSTATS_VERSION;\\n\\tstats->nvcsw = tsk->nvcsw;\\n\\tstats->nivcsw = tsk->nivcsw;\\n\\tbacct_add_tsk(user_ns, pid_ns, stats, tsk);\\n\\n\\t/* fill in extended acct fields */\\n\\txacct_add_tsk(stats, tsk);\\n\\n\\t/* add executable info */\\n\\texe_add_tsk(stats, tsk);\\n}\\n\\nstatic int fill_stats_for_pid(pid_t pid, struct taskstats *stats)\\n{\\n\\tstruct task_struct *tsk;\\n\\n\\ttsk = find_get_task_by_vpid(pid);\\n\\tif (!tsk)\\n\\t\\treturn -ESRCH;\\n\\tfill_stats(current_user_ns(), task_active_pid_ns(current), tsk, stats);\\n\\tput_task_struct(tsk);\\n\\treturn 0;\\n}\\n\\nstatic int fill_stats_for_tgid(pid_t tgid, struct taskstats *stats)\\n{\\n\\tstruct task_struct *tsk, *first;\\n\\tunsigned long flags;\\n\\tint rc = -ESRCH;\\n\\tu64 delta, utime, stime;\\n\\tu64 start_time;\\n\\n\\t/*\\n\\t * Add additional stats from live tasks except zombie thread group\\n\\t * leaders who are already counted with the dead tasks\\n\\t */\\n\\trcu_read_lock();\\n\\tfirst = find_task_by_vpid(tgid);\\n\\n\\tif (!first || !lock_task_sighand(first, &flags))\\n\\t\\tgoto out;\\n\\n\\tif (first->signal->stats)\\n\\t\\tmemcpy(stats, first->signal->stats, sizeof(*stats));\\n\\telse\\n\\t\\tmemset(stats, 0, sizeof(*stats));\\n\\n\\tstart_time = ktime_get_ns();\\n\\tfor_each_thread(first, tsk) {\\n\\t\\tif (tsk->exit_state)\\n\\t\\t\\tcontinue;\\n\\t\\t/*\\n\\t\\t * Accounting subsystem can call its functions here to\\n\\t\\t * fill in relevant parts of struct taskstsats as follows\\n\\t\\t *\\n\\t\\t *\\tper-task-foo(stats, tsk);\\n\\t\\t */\\n\\t\\tdelayacct_add_tsk(stats, tsk);\\n\\n\\t\\t/* calculate task elapsed time in nsec */\\n\\t\\tdelta = start_time - tsk->start_time;\\n\\t\\t/* Convert to micro seconds */\\n\\t\\tdo_div(delta, NSEC_PER_USEC);\\n\\t\\tstats->ac_etime += delta;\\n\\n\\t\\ttask_cputime(tsk, &utime, &stime);\\n\\t\\tstats->ac_utime += div_u64(utime, NSEC_PER_USEC);\\n\\t\\tstats->ac_stime += div_u64(stime, NSEC_PER_USEC);\\n\\n\\t\\tstats->nvcsw += tsk->nvcsw;\\n\\t\\tstats->nivcsw += tsk->nivcsw;\\n\\t}\\n\\n\\tunlock_task_sighand(first, &flags);\\n\\trc = 0;\\nout:\\n\\trcu_read_unlock();\\n\\n\\tstats->version = TASKSTATS_VERSION;\\n\\t/*\\n\\t * Accounting subsystems can also add calls here to modify\\n\\t * fields of taskstats.\\n\\t */\\n\\treturn rc;\\n}\\n\\nstatic void fill_tgid_exit(struct task_struct *tsk)\\n{\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\\n\\tif (!tsk->signal->stats)\\n\\t\\tgoto ret;\\n\\n\\t/*\\n\\t * Each accounting subsystem calls its functions here to\\n\\t * accumalate its per-task stats for tsk, into the per-tgid structure\\n\\t *\\n\\t *\\tper-task-foo(tsk->signal->stats, tsk);\\n\\t */\\n\\tdelayacct_add_tsk(tsk->signal->stats, tsk);\\nret:\\n\\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\\n\\treturn;\\n}\\n\\nstatic int add_del_listener(pid_t pid, const struct cpumask *mask, int isadd)\\n{\\n\\tstruct listener_list *listeners;\\n\\tstruct listener *s, *tmp, *s2;\\n\\tunsigned int cpu;\\n\\tint ret = 0;\\n\\n\\tif (!cpumask_subset(mask, cpu_possible_mask))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (current_user_ns() != &init_user_ns)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (task_active_pid_ns(current) != &init_pid_ns)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (isadd == REGISTER) {\\n\\t\\tfor_each_cpu(cpu, mask) {\\n\\t\\t\\ts = kmalloc_node(sizeof(struct listener),\\n\\t\\t\\t\\t\\tGFP_KERNEL, cpu_to_node(cpu));\\n\\t\\t\\tif (!s) {\\n\\t\\t\\t\\tret = -ENOMEM;\\n\\t\\t\\t\\tgoto cleanup;\\n\\t\\t\\t}\\n\\t\\t\\ts->pid = pid;\\n\\t\\t\\ts->valid = 1;\\n\\n\\t\\t\\tlisteners = &per_cpu(listener_array, cpu);\\n\\t\\t\\tdown_write(&listeners->sem);\\n\\t\\t\\tlist_for_each_entry(s2, &listeners->list, list) {\\n\\t\\t\\t\\tif (s2->pid == pid && s2->valid)\\n\\t\\t\\t\\t\\tgoto exists;\\n\\t\\t\\t}\\n\\t\\t\\tlist_add(&s->list, &listeners->list);\\n\\t\\t\\ts = NULL;\\nexists:\\n\\t\\t\\tup_write(&listeners->sem);\\n\\t\\t\\tkfree(s); /* nop if NULL */\\n\\t\\t}\\n\\t\\treturn 0;\\n\\t}\\n\\n\\t/* Deregister or cleanup */\\ncleanup:\\n\\tfor_each_cpu(cpu, mask) {\\n\\t\\tlisteners = &per_cpu(listener_array, cpu);\\n\\t\\tdown_write(&listeners->sem);\\n\\t\\tlist_for_each_entry_safe(s, tmp, &listeners->list, list) {\\n\\t\\t\\tif (s->pid == pid) {\\n\\t\\t\\t\\tlist_del(&s->list);\\n\\t\\t\\t\\tkfree(s);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tup_write(&listeners->sem);\\n\\t}\\n\\treturn ret;\\n}\\n\\nstatic int parse(struct nlattr *na, struct cpumask *mask)\\n{\\n\\tchar *data;\\n\\tint len;\\n\\tint ret;\\n\\n\\tif (na == NULL)\\n\\t\\treturn 1;\\n\\tlen = nla_len(na);\\n\\tif (len > TASKSTATS_CPUMASK_MAXLEN)\\n\\t\\treturn -E2BIG;\\n\\tif (len < 1)\\n\\t\\treturn -EINVAL;\\n\\tdata = kmalloc(len, GFP_KERNEL);\\n\\tif (!data)\\n\\t\\treturn -ENOMEM;\\n\\tnla_strscpy(data, na, len);\\n\\tret = cpulist_parse(data, mask);\\n\\tkfree(data);\\n\\treturn ret;\\n}\\n\\nstatic struct taskstats *mk_reply(struct sk_buff *skb, int type, u32 pid)\\n{\\n\\tstruct nlattr *na, *ret;\\n\\tint aggr;\\n\\n\\taggr = (type == TASKSTATS_TYPE_PID)\\n\\t\\t\\t? TASKSTATS_TYPE_AGGR_PID\\n\\t\\t\\t: TASKSTATS_TYPE_AGGR_TGID;\\n\\n\\tna = nla_nest_start_noflag(skb, aggr);\\n\\tif (!na)\\n\\t\\tgoto err;\\n\\n\\tif (nla_put(skb, type, sizeof(pid), &pid) < 0) {\\n\\t\\tnla_nest_cancel(skb, na);\\n\\t\\tgoto err;\\n\\t}\\n\\tret = nla_reserve_64bit(skb, TASKSTATS_TYPE_STATS,\\n\\t\\t\\t\\tsizeof(struct taskstats), TASKSTATS_TYPE_NULL);\\n\\tif (!ret) {\\n\\t\\tnla_nest_cancel(skb, na);\\n\\t\\tgoto err;\\n\\t}\\n\\tnla_nest_end(skb, na);\\n\\n\\treturn nla_data(ret);\\nerr:\\n\\treturn NULL;\\n}\\n\\nstatic int cgroupstats_user_cmd(struct sk_buff *skb, struct genl_info *info)\\n{\\n\\tint rc = 0;\\n\\tstruct sk_buff *rep_skb;\\n\\tstruct cgroupstats *stats;\\n\\tstruct nlattr *na;\\n\\tsize_t size;\\n\\tu32 fd;\\n\\n\\tna = info->attrs[CGROUPSTATS_CMD_ATTR_FD];\\n\\tif (!na)\\n\\t\\treturn -EINVAL;\\n\\n\\tfd = nla_get_u32(info->attrs[CGROUPSTATS_CMD_ATTR_FD]);\\n\\tCLASS(fd, f)(fd);\\n\\tif (fd_empty(f))\\n\\t\\treturn 0;\\n\\n\\tsize = nla_total_size(sizeof(struct cgroupstats));\\n\\n\\trc = prepare_reply(info, CGROUPSTATS_CMD_NEW, &rep_skb,\\n\\t\\t\\t\\tsize);\\n\\tif (rc < 0)\\n\\t\\treturn rc;\\n\\n\\tna = nla_reserve(rep_skb, CGROUPSTATS_TYPE_CGROUP_STATS,\\n\\t\\t\\t\\tsizeof(struct cgroupstats));\\n\\tif (na == NULL) {\\n\\t\\tnlmsg_free(rep_skb);\\n\\t\\treturn -EMSGSIZE;\\n\\t}\\n\\n\\tstats = nla_data(na);\\n\\tmemset(stats, 0, sizeof(*stats));\\n\\n\\trc = cgroupstats_build(stats, fd_file(f)->f_path.dentry);\\n\\tif (rc < 0) {\\n\\t\\tnlmsg_free(rep_skb);\\n\\t\\treturn rc;\\n\\t}\\n\\n\\treturn send_reply(rep_skb, info);\\n}\\n\\nstatic int cmd_attr_register_cpumask(struct genl_info *info)\\n{\\n\\tcpumask_var_t mask;\\n\\tint rc;\\n\\n\\tif (!alloc_cpumask_var(&mask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\trc = parse(info->attrs[TASKSTATS_CMD_ATTR_REGISTER_CPUMASK], mask);\\n\\tif (rc < 0)\\n\\t\\tgoto out;\\n\\trc = add_del_listener(info->snd_portid, mask, REGISTER);\\nout:\\n\\tfree_cpumask_var(mask);\\n\\treturn rc;\\n}\\n\\nstatic int cmd_attr_deregister_cpumask(struct genl_info *info)\\n{\\n\\tcpumask_var_t mask;\\n\\tint rc;\\n\\n\\tif (!alloc_cpumask_var(&mask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\trc = parse(info->attrs[TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK], mask);\\n\\tif (rc < 0)\\n\\t\\tgoto out;\\n\\trc = add_del_listener(info->snd_portid, mask, DEREGISTER);\\nout:\\n\\tfree_cpumask_var(mask);\\n\\treturn rc;\\n}\\n\\nstatic size_t taskstats_packet_size(void)\\n{\\n\\tsize_t size;\\n\\n\\tsize = nla_total_size(sizeof(u32)) +\\n\\t\\tnla_total_size_64bit(sizeof(struct taskstats)) +\\n\\t\\tnla_total_size(0);\\n\\n\\treturn size;\\n}\\n\\nstatic int cmd_attr_pid(struct genl_info *info)\\n{\\n\\tstruct taskstats *stats;\\n\\tstruct sk_buff *rep_skb;\\n\\tsize_t size;\\n\\tu32 pid;\\n\\tint rc;\\n\\n\\tsize = taskstats_packet_size();\\n\\n\\trc = prepare_reply(info, TASKSTATS_CMD_NEW, &rep_skb, size);\\n\\tif (rc < 0)\\n\\t\\treturn rc;\\n\\n\\trc = -EINVAL;\\n\\tpid = nla_get_u32(info->attrs[TASKSTATS_CMD_ATTR_PID]);\\n\\tstats = mk_reply(rep_skb, TASKSTATS_TYPE_PID, pid);\\n\\tif (!stats)\\n\\t\\tgoto err;\\n\\n\\trc = fill_stats_for_pid(pid, stats);\\n\\tif (rc < 0)\\n\\t\\tgoto err;\\n\\treturn send_reply(rep_skb, info);\\nerr:\\n\\tnlmsg_free(rep_skb);\\n\\treturn rc;\\n}\\n\\nstatic int cmd_attr_tgid(struct genl_info *info)\\n{\\n\\tstruct taskstats *stats;\\n\\tstruct sk_buff *rep_skb;\\n\\tsize_t size;\\n\\tu32 tgid;\\n\\tint rc;\\n\\n\\tsize = taskstats_packet_size();\\n\\n\\trc = prepare_reply(info, TASKSTATS_CMD_NEW, &rep_skb, size);\\n\\tif (rc < 0)\\n\\t\\treturn rc;\\n\\n\\trc = -EINVAL;\\n\\ttgid = nla_get_u32(info->attrs[TASKSTATS_CMD_ATTR_TGID]);\\n\\tstats = mk_reply(rep_skb, TASKSTATS_TYPE_TGID, tgid);\\n\\tif (!stats)\\n\\t\\tgoto err;\\n\\n\\trc = fill_stats_for_tgid(tgid, stats);\\n\\tif (rc < 0)\\n\\t\\tgoto err;\\n\\treturn send_reply(rep_skb, info);\\nerr:\\n\\tnlmsg_free(rep_skb);\\n\\treturn rc;\\n}\\n\\nstatic int taskstats_user_cmd(struct sk_buff *skb, struct genl_info *info)\\n{\\n\\tif (info->attrs[TASKSTATS_CMD_ATTR_REGISTER_CPUMASK])\\n\\t\\treturn cmd_attr_register_cpumask(info);\\n\\telse if (info->attrs[TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK])\\n\\t\\treturn cmd_attr_deregister_cpumask(info);\\n\\telse if (info->attrs[TASKSTATS_CMD_ATTR_PID])\\n\\t\\treturn cmd_attr_pid(info);\\n\\telse if (info->attrs[TASKSTATS_CMD_ATTR_TGID])\\n\\t\\treturn cmd_attr_tgid(info);\\n\\telse\\n\\t\\treturn -EINVAL;\\n}\\n\\nstatic struct taskstats *taskstats_tgid_alloc(struct task_struct *tsk)\\n{\\n\\tstruct signal_struct *sig = tsk->signal;\\n\\tstruct taskstats *stats_new, *stats;\\n\\n\\t/* Pairs with smp_store_release() below. */\\n\\tstats = smp_load_acquire(&sig->stats);\\n\\tif (stats || thread_group_empty(tsk))\\n\\t\\treturn stats;\\n\\n\\t/* No problem if kmem_cache_zalloc() fails */\\n\\tstats_new = kmem_cache_zalloc(taskstats_cache, GFP_KERNEL);\\n\\n\\tspin_lock_irq(&tsk->sighand->siglock);\\n\\tstats = sig->stats;\\n\\tif (!stats) {\\n\\t\\t/*\\n\\t\\t * Pairs with smp_store_release() above and order the\\n\\t\\t * kmem_cache_zalloc().\\n\\t\\t */\\n\\t\\tsmp_store_release(&sig->stats, stats_new);\\n\\t\\tstats = stats_new;\\n\\t\\tstats_new = NULL;\\n\\t}\\n\\tspin_unlock_irq(&tsk->sighand->siglock);\\n\\n\\tif (stats_new)\\n\\t\\tkmem_cache_free(taskstats_cache, stats_new);\\n\\n\\treturn stats;\\n}\\n\\n/* Send pid data out on exit */\\nvoid taskstats_exit(struct task_struct *tsk, int group_dead)\\n{\\n\\tint rc;\\n\\tstruct listener_list *listeners;\\n\\tstruct taskstats *stats;\\n\\tstruct sk_buff *rep_skb;\\n\\tsize_t size;\\n\\tint is_thread_group;\\n\\n\\tif (!family_registered)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Size includes space for nested attributes\\n\\t */\\n\\tsize = taskstats_packet_size();\\n\\n\\tis_thread_group = !!taskstats_tgid_alloc(tsk);\\n\\tif (is_thread_group) {\\n\\t\\t/* PID + STATS + TGID + STATS */\\n\\t\\tsize = 2 * size;\\n\\t\\t/* fill the tsk->signal->stats structure */\\n\\t\\tfill_tgid_exit(tsk);\\n\\t}\\n\\n\\tlisteners = raw_cpu_ptr(&listener_array);\\n\\tif (list_empty(&listeners->list))\\n\\t\\treturn;\\n\\n\\trc = prepare_reply(NULL, TASKSTATS_CMD_NEW, &rep_skb, size);\\n\\tif (rc < 0)\\n\\t\\treturn;\\n\\n\\tstats = mk_reply(rep_skb, TASKSTATS_TYPE_PID,\\n\\t\\t\\t task_pid_nr_ns(tsk, &init_pid_ns));\\n\\tif (!stats)\\n\\t\\tgoto err;\\n\\n\\tfill_stats(&init_user_ns, &init_pid_ns, tsk, stats);\\n\\tif (group_dead)\\n\\t\\tstats->ac_flag |= AGROUP;\\n\\n\\t/*\\n\\t * Doesn\\'t matter if tsk is the leader or the last group member leaving\\n\\t */\\n\\tif (!is_thread_group || !group_dead)\\n\\t\\tgoto send;\\n\\n\\tstats = mk_reply(rep_skb, TASKSTATS_TYPE_TGID,\\n\\t\\t\\t task_tgid_nr_ns(tsk, &init_pid_ns));\\n\\tif (!stats)\\n\\t\\tgoto err;\\n\\n\\tmemcpy(stats, tsk->signal->stats, sizeof(*stats));\\n\\nsend:\\n\\tsend_cpu_listeners(rep_skb, listeners);\\n\\treturn;\\nerr:\\n\\tnlmsg_free(rep_skb);\\n}\\n\\nstatic const struct genl_ops taskstats_ops[] = {\\n\\t{\\n\\t\\t.cmd\\t\\t= TASKSTATS_CMD_GET,\\n\\t\\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\\n\\t\\t.doit\\t\\t= taskstats_user_cmd,\\n\\t\\t.policy\\t\\t= taskstats_cmd_get_policy,\\n\\t\\t.maxattr\\t= ARRAY_SIZE(taskstats_cmd_get_policy) - 1,\\n\\t\\t.flags\\t\\t= GENL_ADMIN_PERM,\\n\\t},\\n\\t{\\n\\t\\t.cmd\\t\\t= CGROUPSTATS_CMD_GET,\\n\\t\\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\\n\\t\\t.doit\\t\\t= cgroupstats_user_cmd,\\n\\t\\t.policy\\t\\t= cgroupstats_cmd_get_policy,\\n\\t\\t.maxattr\\t= ARRAY_SIZE(cgroupstats_cmd_get_policy) - 1,\\n\\t},\\n};\\n\\nstatic struct genl_family family __ro_after_init = {\\n\\t.name\\t\\t= TASKSTATS_GENL_NAME,\\n\\t.version\\t= TASKSTATS_GENL_VERSION,\\n\\t.module\\t\\t= THIS_MODULE,\\n\\t.ops\\t\\t= taskstats_ops,\\n\\t.n_ops\\t\\t= ARRAY_SIZE(taskstats_ops),\\n\\t.resv_start_op\\t= CGROUPSTATS_CMD_GET + 1,\\n\\t.netnsok\\t= true,\\n};\\n\\n/* Needed early in initialization */\\nvoid __init taskstats_init_early(void)\\n{\\n\\tunsigned int i;\\n\\n\\ttaskstats_cache = KMEM_CACHE(taskstats, SLAB_PANIC);\\n\\tfor_each_possible_cpu(i) {\\n\\t\\tINIT_LIST_HEAD(&(per_cpu(listener_array, i).list));\\n\\t\\tinit_rwsem(&(per_cpu(listener_array, i).sem));\\n\\t}\\n}\\n\\nstatic int __init taskstats_init(void)\\n{\\n\\tint rc;\\n\\n\\trc = genl_register_family(&family);\\n\\tif (rc)\\n\\t\\treturn rc;\\n\\n\\tfamily_registered = 1;\\n\\tpr_info(\"registered taskstats version %d\\\\n\", TASKSTATS_GENL_VERSION);\\n\\treturn 0;\\n}\\n\\n/*\\n * late initcall ensures initialization of statistics collection\\n * mechanisms precedes initialization of the taskstats interface\\n */\\nlate_initcall(taskstats_init);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n *  linux/kernel/sys.c\\n *\\n *  Copyright (C) 1991, 1992  Linus Torvalds\\n */\\n\\n#include <linux/export.h>\\n#include <linux/mm.h>\\n#include <linux/mm_inline.h>\\n#include <linux/utsname.h>\\n#include <linux/mman.h>\\n#include <linux/reboot.h>\\n#include <linux/prctl.h>\\n#include <linux/highuid.h>\\n#include <linux/fs.h>\\n#include <linux/kmod.h>\\n#include <linux/ksm.h>\\n#include <linux/perf_event.h>\\n#include <linux/resource.h>\\n#include <linux/kernel.h>\\n#include <linux/workqueue.h>\\n#include <linux/capability.h>\\n#include <linux/device.h>\\n#include <linux/key.h>\\n#include <linux/times.h>\\n#include <linux/posix-timers.h>\\n#include <linux/security.h>\\n#include <linux/random.h>\\n#include <linux/suspend.h>\\n#include <linux/tty.h>\\n#include <linux/signal.h>\\n#include <linux/cn_proc.h>\\n#include <linux/getcpu.h>\\n#include <linux/task_io_accounting_ops.h>\\n#include <linux/seccomp.h>\\n#include <linux/cpu.h>\\n#include <linux/personality.h>\\n#include <linux/ptrace.h>\\n#include <linux/fs_struct.h>\\n#include <linux/file.h>\\n#include <linux/mount.h>\\n#include <linux/gfp.h>\\n#include <linux/syscore_ops.h>\\n#include <linux/version.h>\\n#include <linux/ctype.h>\\n#include <linux/syscall_user_dispatch.h>\\n\\n#include <linux/compat.h>\\n#include <linux/syscalls.h>\\n#include <linux/kprobes.h>\\n#include <linux/user_namespace.h>\\n#include <linux/time_namespace.h>\\n#include <linux/binfmts.h>\\n\\n#include <linux/sched.h>\\n#include <linux/sched/autogroup.h>\\n#include <linux/sched/loadavg.h>\\n#include <linux/sched/stat.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/coredump.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/rcupdate.h>\\n#include <linux/uidgid.h>\\n#include <linux/cred.h>\\n\\n#include <linux/nospec.h>\\n\\n#include <linux/kmsg_dump.h>\\n/* Move somewhere else to avoid recompiling? */\\n#include <generated/utsrelease.h>\\n\\n#include <linux/uaccess.h>\\n#include <asm/io.h>\\n#include <asm/unistd.h>\\n\\n#include \"uid16.h\"\\n\\n#ifndef SET_UNALIGN_CTL\\n# define SET_UNALIGN_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_UNALIGN_CTL\\n# define GET_UNALIGN_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef SET_FPEMU_CTL\\n# define SET_FPEMU_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_FPEMU_CTL\\n# define GET_FPEMU_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef SET_FPEXC_CTL\\n# define SET_FPEXC_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_FPEXC_CTL\\n# define GET_FPEXC_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_ENDIAN\\n# define GET_ENDIAN(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef SET_ENDIAN\\n# define SET_ENDIAN(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_TSC_CTL\\n# define GET_TSC_CTL(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef SET_TSC_CTL\\n# define SET_TSC_CTL(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef GET_FP_MODE\\n# define GET_FP_MODE(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef SET_FP_MODE\\n# define SET_FP_MODE(a,b)\\t(-EINVAL)\\n#endif\\n#ifndef SVE_SET_VL\\n# define SVE_SET_VL(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef SVE_GET_VL\\n# define SVE_GET_VL()\\t\\t(-EINVAL)\\n#endif\\n#ifndef SME_SET_VL\\n# define SME_SET_VL(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef SME_GET_VL\\n# define SME_GET_VL()\\t\\t(-EINVAL)\\n#endif\\n#ifndef PAC_RESET_KEYS\\n# define PAC_RESET_KEYS(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef PAC_SET_ENABLED_KEYS\\n# define PAC_SET_ENABLED_KEYS(a, b, c)\\t(-EINVAL)\\n#endif\\n#ifndef PAC_GET_ENABLED_KEYS\\n# define PAC_GET_ENABLED_KEYS(a)\\t(-EINVAL)\\n#endif\\n#ifndef SET_TAGGED_ADDR_CTRL\\n# define SET_TAGGED_ADDR_CTRL(a)\\t(-EINVAL)\\n#endif\\n#ifndef GET_TAGGED_ADDR_CTRL\\n# define GET_TAGGED_ADDR_CTRL()\\t\\t(-EINVAL)\\n#endif\\n#ifndef RISCV_V_SET_CONTROL\\n# define RISCV_V_SET_CONTROL(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef RISCV_V_GET_CONTROL\\n# define RISCV_V_GET_CONTROL()\\t\\t(-EINVAL)\\n#endif\\n#ifndef RISCV_SET_ICACHE_FLUSH_CTX\\n# define RISCV_SET_ICACHE_FLUSH_CTX(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef PPC_GET_DEXCR_ASPECT\\n# define PPC_GET_DEXCR_ASPECT(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef PPC_SET_DEXCR_ASPECT\\n# define PPC_SET_DEXCR_ASPECT(a, b, c)\\t(-EINVAL)\\n#endif\\n\\n/*\\n * this is where the system-wide overflow UID and GID are defined, for\\n * architectures that now have 32-bit UID/GID but didn\\'t in the past\\n */\\n\\nint overflowuid = DEFAULT_OVERFLOWUID;\\nint overflowgid = DEFAULT_OVERFLOWGID;\\n\\nEXPORT_SYMBOL(overflowuid);\\nEXPORT_SYMBOL(overflowgid);\\n\\n/*\\n * the same as above, but for filesystems which can only store a 16-bit\\n * UID and GID. as such, this is needed on all architectures\\n */\\n\\nint fs_overflowuid = DEFAULT_FS_OVERFLOWUID;\\nint fs_overflowgid = DEFAULT_FS_OVERFLOWGID;\\n\\nEXPORT_SYMBOL(fs_overflowuid);\\nEXPORT_SYMBOL(fs_overflowgid);\\n\\n/*\\n * Returns true if current\\'s euid is same as p\\'s uid or euid,\\n * or has CAP_SYS_NICE to p\\'s user_ns.\\n *\\n * Called with rcu_read_lock, creds are safe\\n */\\nstatic bool set_one_prio_perm(struct task_struct *p)\\n{\\n\\tconst struct cred *cred = current_cred(), *pcred = __task_cred(p);\\n\\n\\tif (uid_eq(pcred->uid,  cred->euid) ||\\n\\t    uid_eq(pcred->euid, cred->euid))\\n\\t\\treturn true;\\n\\tif (ns_capable(pcred->user_ns, CAP_SYS_NICE))\\n\\t\\treturn true;\\n\\treturn false;\\n}\\n\\n/*\\n * set the priority of a task\\n * - the caller must hold the RCU read lock\\n */\\nstatic int set_one_prio(struct task_struct *p, int niceval, int error)\\n{\\n\\tint no_nice;\\n\\n\\tif (!set_one_prio_perm(p)) {\\n\\t\\terror = -EPERM;\\n\\t\\tgoto out;\\n\\t}\\n\\tif (niceval < task_nice(p) && !can_nice(p, niceval)) {\\n\\t\\terror = -EACCES;\\n\\t\\tgoto out;\\n\\t}\\n\\tno_nice = security_task_setnice(p, niceval);\\n\\tif (no_nice) {\\n\\t\\terror = no_nice;\\n\\t\\tgoto out;\\n\\t}\\n\\tif (error == -ESRCH)\\n\\t\\terror = 0;\\n\\tset_user_nice(p, niceval);\\nout:\\n\\treturn error;\\n}\\n\\nSYSCALL_DEFINE3(setpriority, int, which, int, who, int, niceval)\\n{\\n\\tstruct task_struct *g, *p;\\n\\tstruct user_struct *user;\\n\\tconst struct cred *cred = current_cred();\\n\\tint error = -EINVAL;\\n\\tstruct pid *pgrp;\\n\\tkuid_t uid;\\n\\n\\tif (which > PRIO_USER || which < PRIO_PROCESS)\\n\\t\\tgoto out;\\n\\n\\t/* normalize: avoid signed division (rounding problems) */\\n\\terror = -ESRCH;\\n\\tif (niceval < MIN_NICE)\\n\\t\\tniceval = MIN_NICE;\\n\\tif (niceval > MAX_NICE)\\n\\t\\tniceval = MAX_NICE;\\n\\n\\trcu_read_lock();\\n\\tswitch (which) {\\n\\tcase PRIO_PROCESS:\\n\\t\\tif (who)\\n\\t\\t\\tp = find_task_by_vpid(who);\\n\\t\\telse\\n\\t\\t\\tp = current;\\n\\t\\tif (p)\\n\\t\\t\\terror = set_one_prio(p, niceval, error);\\n\\t\\tbreak;\\n\\tcase PRIO_PGRP:\\n\\t\\tif (who)\\n\\t\\t\\tpgrp = find_vpid(who);\\n\\t\\telse\\n\\t\\t\\tpgrp = task_pgrp(current);\\n\\t\\tread_lock(&tasklist_lock);\\n\\t\\tdo_each_pid_thread(pgrp, PIDTYPE_PGID, p) {\\n\\t\\t\\terror = set_one_prio(p, niceval, error);\\n\\t\\t} while_each_pid_thread(pgrp, PIDTYPE_PGID, p);\\n\\t\\tread_unlock(&tasklist_lock);\\n\\t\\tbreak;\\n\\tcase PRIO_USER:\\n\\t\\tuid = make_kuid(cred->user_ns, who);\\n\\t\\tuser = cred->user;\\n\\t\\tif (!who)\\n\\t\\t\\tuid = cred->uid;\\n\\t\\telse if (!uid_eq(uid, cred->uid)) {\\n\\t\\t\\tuser = find_user(uid);\\n\\t\\t\\tif (!user)\\n\\t\\t\\t\\tgoto out_unlock;\\t/* No processes for this user */\\n\\t\\t}\\n\\t\\tfor_each_process_thread(g, p) {\\n\\t\\t\\tif (uid_eq(task_uid(p), uid) && task_pid_vnr(p))\\n\\t\\t\\t\\terror = set_one_prio(p, niceval, error);\\n\\t\\t}\\n\\t\\tif (!uid_eq(uid, cred->uid))\\n\\t\\t\\tfree_uid(user);\\t\\t/* For find_user() */\\n\\t\\tbreak;\\n\\t}\\nout_unlock:\\n\\trcu_read_unlock();\\nout:\\n\\treturn error;\\n}\\n\\n/*\\n * Ugh. To avoid negative return values, \"getpriority()\" will\\n * not return the normal nice-value, but a negated value that\\n * has been offset by 20 (ie it returns 40..1 instead of -20..19)\\n * to stay compatible.\\n */\\nSYSCALL_DEFINE2(getpriority, int, which, int, who)\\n{\\n\\tstruct task_struct *g, *p;\\n\\tstruct user_struct *user;\\n\\tconst struct cred *cred = current_cred();\\n\\tlong niceval, retval = -ESRCH;\\n\\tstruct pid *pgrp;\\n\\tkuid_t uid;\\n\\n\\tif (which > PRIO_USER || which < PRIO_PROCESS)\\n\\t\\treturn -EINVAL;\\n\\n\\trcu_read_lock();\\n\\tswitch (which) {\\n\\tcase PRIO_PROCESS:\\n\\t\\tif (who)\\n\\t\\t\\tp = find_task_by_vpid(who);\\n\\t\\telse\\n\\t\\t\\tp = current;\\n\\t\\tif (p) {\\n\\t\\t\\tniceval = nice_to_rlimit(task_nice(p));\\n\\t\\t\\tif (niceval > retval)\\n\\t\\t\\t\\tretval = niceval;\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase PRIO_PGRP:\\n\\t\\tif (who)\\n\\t\\t\\tpgrp = find_vpid(who);\\n\\t\\telse\\n\\t\\t\\tpgrp = task_pgrp(current);\\n\\t\\tread_lock(&tasklist_lock);\\n\\t\\tdo_each_pid_thread(pgrp, PIDTYPE_PGID, p) {\\n\\t\\t\\tniceval = nice_to_rlimit(task_nice(p));\\n\\t\\t\\tif (niceval > retval)\\n\\t\\t\\t\\tretval = niceval;\\n\\t\\t} while_each_pid_thread(pgrp, PIDTYPE_PGID, p);\\n\\t\\tread_unlock(&tasklist_lock);\\n\\t\\tbreak;\\n\\tcase PRIO_USER:\\n\\t\\tuid = make_kuid(cred->user_ns, who);\\n\\t\\tuser = cred->user;\\n\\t\\tif (!who)\\n\\t\\t\\tuid = cred->uid;\\n\\t\\telse if (!uid_eq(uid, cred->uid)) {\\n\\t\\t\\tuser = find_user(uid);\\n\\t\\t\\tif (!user)\\n\\t\\t\\t\\tgoto out_unlock;\\t/* No processes for this user */\\n\\t\\t}\\n\\t\\tfor_each_process_thread(g, p) {\\n\\t\\t\\tif (uid_eq(task_uid(p), uid) && task_pid_vnr(p)) {\\n\\t\\t\\t\\tniceval = nice_to_rlimit(task_nice(p));\\n\\t\\t\\t\\tif (niceval > retval)\\n\\t\\t\\t\\t\\tretval = niceval;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif (!uid_eq(uid, cred->uid))\\n\\t\\t\\tfree_uid(user);\\t\\t/* for find_user() */\\n\\t\\tbreak;\\n\\t}\\nout_unlock:\\n\\trcu_read_unlock();\\n\\n\\treturn retval;\\n}\\n\\n/*\\n * Unprivileged users may change the real gid to the effective gid\\n * or vice versa.  (BSD-style)\\n *\\n * If you set the real gid at all, or set the effective gid to a value not\\n * equal to the real gid, then the saved gid is set to the new effective gid.\\n *\\n * This makes it possible for a setgid program to completely drop its\\n * privileges, which is often a useful assertion to make when you are doing\\n * a security audit over a program.\\n *\\n * The general idea is that a program which uses just setregid() will be\\n * 100% compatible with BSD.  A program which uses just setgid() will be\\n * 100% compatible with POSIX with saved IDs.\\n *\\n * SMP: There are not races, the GIDs are checked only by filesystem\\n *      operations (as far as semantic preservation is concerned).\\n */\\n#ifdef CONFIG_MULTIUSER\\nlong __sys_setregid(gid_t rgid, gid_t egid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkgid_t krgid, kegid;\\n\\n\\tkrgid = make_kgid(ns, rgid);\\n\\tkegid = make_kgid(ns, egid);\\n\\n\\tif ((rgid != (gid_t) -1) && !gid_valid(krgid))\\n\\t\\treturn -EINVAL;\\n\\tif ((egid != (gid_t) -1) && !gid_valid(kegid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (rgid != (gid_t) -1) {\\n\\t\\tif (gid_eq(old->gid, krgid) ||\\n\\t\\t    gid_eq(old->egid, krgid) ||\\n\\t\\t    ns_capable_setid(old->user_ns, CAP_SETGID))\\n\\t\\t\\tnew->gid = krgid;\\n\\t\\telse\\n\\t\\t\\tgoto error;\\n\\t}\\n\\tif (egid != (gid_t) -1) {\\n\\t\\tif (gid_eq(old->gid, kegid) ||\\n\\t\\t    gid_eq(old->egid, kegid) ||\\n\\t\\t    gid_eq(old->sgid, kegid) ||\\n\\t\\t    ns_capable_setid(old->user_ns, CAP_SETGID))\\n\\t\\t\\tnew->egid = kegid;\\n\\t\\telse\\n\\t\\t\\tgoto error;\\n\\t}\\n\\n\\tif (rgid != (gid_t) -1 ||\\n\\t    (egid != (gid_t) -1 && !gid_eq(kegid, old->gid)))\\n\\t\\tnew->sgid = new->egid;\\n\\tnew->fsgid = new->egid;\\n\\n\\tretval = security_task_fix_setgid(new, old, LSM_SETID_RE);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE2(setregid, gid_t, rgid, gid_t, egid)\\n{\\n\\treturn __sys_setregid(rgid, egid);\\n}\\n\\n/*\\n * setgid() is implemented like SysV w/ SAVED_IDS\\n *\\n * SMP: Same implicit races as above.\\n */\\nlong __sys_setgid(gid_t gid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkgid_t kgid;\\n\\n\\tkgid = make_kgid(ns, gid);\\n\\tif (!gid_valid(kgid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (ns_capable_setid(old->user_ns, CAP_SETGID))\\n\\t\\tnew->gid = new->egid = new->sgid = new->fsgid = kgid;\\n\\telse if (gid_eq(kgid, old->gid) || gid_eq(kgid, old->sgid))\\n\\t\\tnew->egid = new->fsgid = kgid;\\n\\telse\\n\\t\\tgoto error;\\n\\n\\tretval = security_task_fix_setgid(new, old, LSM_SETID_ID);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE1(setgid, gid_t, gid)\\n{\\n\\treturn __sys_setgid(gid);\\n}\\n\\n/*\\n * change the user struct in a credentials set to match the new UID\\n */\\nstatic int set_user(struct cred *new)\\n{\\n\\tstruct user_struct *new_user;\\n\\n\\tnew_user = alloc_uid(new->uid);\\n\\tif (!new_user)\\n\\t\\treturn -EAGAIN;\\n\\n\\tfree_uid(new->user);\\n\\tnew->user = new_user;\\n\\treturn 0;\\n}\\n\\nstatic void flag_nproc_exceeded(struct cred *new)\\n{\\n\\tif (new->ucounts == current_ucounts())\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * We don\\'t fail in case of NPROC limit excess here because too many\\n\\t * poorly written programs don\\'t check set*uid() return code, assuming\\n\\t * it never fails if called by root.  We may still enforce NPROC limit\\n\\t * for programs doing set*uid()+execve() by harmlessly deferring the\\n\\t * failure to the execve() stage.\\n\\t */\\n\\tif (is_rlimit_overlimit(new->ucounts, UCOUNT_RLIMIT_NPROC, rlimit(RLIMIT_NPROC)) &&\\n\\t\\t\\tnew->user != INIT_USER)\\n\\t\\tcurrent->flags |= PF_NPROC_EXCEEDED;\\n\\telse\\n\\t\\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\\n}\\n\\n/*\\n * Unprivileged users may change the real uid to the effective uid\\n * or vice versa.  (BSD-style)\\n *\\n * If you set the real uid at all, or set the effective uid to a value not\\n * equal to the real uid, then the saved uid is set to the new effective uid.\\n *\\n * This makes it possible for a setuid program to completely drop its\\n * privileges, which is often a useful assertion to make when you are doing\\n * a security audit over a program.\\n *\\n * The general idea is that a program which uses just setreuid() will be\\n * 100% compatible with BSD.  A program which uses just setuid() will be\\n * 100% compatible with POSIX with saved IDs.\\n */\\nlong __sys_setreuid(uid_t ruid, uid_t euid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkuid_t kruid, keuid;\\n\\n\\tkruid = make_kuid(ns, ruid);\\n\\tkeuid = make_kuid(ns, euid);\\n\\n\\tif ((ruid != (uid_t) -1) && !uid_valid(kruid))\\n\\t\\treturn -EINVAL;\\n\\tif ((euid != (uid_t) -1) && !uid_valid(keuid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (ruid != (uid_t) -1) {\\n\\t\\tnew->uid = kruid;\\n\\t\\tif (!uid_eq(old->uid, kruid) &&\\n\\t\\t    !uid_eq(old->euid, kruid) &&\\n\\t\\t    !ns_capable_setid(old->user_ns, CAP_SETUID))\\n\\t\\t\\tgoto error;\\n\\t}\\n\\n\\tif (euid != (uid_t) -1) {\\n\\t\\tnew->euid = keuid;\\n\\t\\tif (!uid_eq(old->uid, keuid) &&\\n\\t\\t    !uid_eq(old->euid, keuid) &&\\n\\t\\t    !uid_eq(old->suid, keuid) &&\\n\\t\\t    !ns_capable_setid(old->user_ns, CAP_SETUID))\\n\\t\\t\\tgoto error;\\n\\t}\\n\\n\\tif (!uid_eq(new->uid, old->uid)) {\\n\\t\\tretval = set_user(new);\\n\\t\\tif (retval < 0)\\n\\t\\t\\tgoto error;\\n\\t}\\n\\tif (ruid != (uid_t) -1 ||\\n\\t    (euid != (uid_t) -1 && !uid_eq(keuid, old->uid)))\\n\\t\\tnew->suid = new->euid;\\n\\tnew->fsuid = new->euid;\\n\\n\\tretval = security_task_fix_setuid(new, old, LSM_SETID_RE);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\tretval = set_cred_ucounts(new);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\tflag_nproc_exceeded(new);\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE2(setreuid, uid_t, ruid, uid_t, euid)\\n{\\n\\treturn __sys_setreuid(ruid, euid);\\n}\\n\\n/*\\n * setuid() is implemented like SysV with SAVED_IDS\\n *\\n * Note that SAVED_ID\\'s is deficient in that a setuid root program\\n * like sendmail, for example, cannot set its uid to be a normal\\n * user and then switch back, because if you\\'re root, setuid() sets\\n * the saved uid too.  If you don\\'t like this, blame the bright people\\n * in the POSIX committee and/or USG.  Note that the BSD-style setreuid()\\n * will allow a root program to temporarily drop privileges and be able to\\n * regain them by swapping the real and effective uid.\\n */\\nlong __sys_setuid(uid_t uid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkuid_t kuid;\\n\\n\\tkuid = make_kuid(ns, uid);\\n\\tif (!uid_valid(kuid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (ns_capable_setid(old->user_ns, CAP_SETUID)) {\\n\\t\\tnew->suid = new->uid = kuid;\\n\\t\\tif (!uid_eq(kuid, old->uid)) {\\n\\t\\t\\tretval = set_user(new);\\n\\t\\t\\tif (retval < 0)\\n\\t\\t\\t\\tgoto error;\\n\\t\\t}\\n\\t} else if (!uid_eq(kuid, old->uid) && !uid_eq(kuid, new->suid)) {\\n\\t\\tgoto error;\\n\\t}\\n\\n\\tnew->fsuid = new->euid = kuid;\\n\\n\\tretval = security_task_fix_setuid(new, old, LSM_SETID_ID);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\tretval = set_cred_ucounts(new);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\tflag_nproc_exceeded(new);\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE1(setuid, uid_t, uid)\\n{\\n\\treturn __sys_setuid(uid);\\n}\\n\\n\\n/*\\n * This function implements a generic ability to update ruid, euid,\\n * and suid.  This allows you to implement the 4.4 compatible seteuid().\\n */\\nlong __sys_setresuid(uid_t ruid, uid_t euid, uid_t suid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkuid_t kruid, keuid, ksuid;\\n\\tbool ruid_new, euid_new, suid_new;\\n\\n\\tkruid = make_kuid(ns, ruid);\\n\\tkeuid = make_kuid(ns, euid);\\n\\tksuid = make_kuid(ns, suid);\\n\\n\\tif ((ruid != (uid_t) -1) && !uid_valid(kruid))\\n\\t\\treturn -EINVAL;\\n\\n\\tif ((euid != (uid_t) -1) && !uid_valid(keuid))\\n\\t\\treturn -EINVAL;\\n\\n\\tif ((suid != (uid_t) -1) && !uid_valid(ksuid))\\n\\t\\treturn -EINVAL;\\n\\n\\told = current_cred();\\n\\n\\t/* check for no-op */\\n\\tif ((ruid == (uid_t) -1 || uid_eq(kruid, old->uid)) &&\\n\\t    (euid == (uid_t) -1 || (uid_eq(keuid, old->euid) &&\\n\\t\\t\\t\\t    uid_eq(keuid, old->fsuid))) &&\\n\\t    (suid == (uid_t) -1 || uid_eq(ksuid, old->suid)))\\n\\t\\treturn 0;\\n\\n\\truid_new = ruid != (uid_t) -1        && !uid_eq(kruid, old->uid) &&\\n\\t\\t   !uid_eq(kruid, old->euid) && !uid_eq(kruid, old->suid);\\n\\teuid_new = euid != (uid_t) -1        && !uid_eq(keuid, old->uid) &&\\n\\t\\t   !uid_eq(keuid, old->euid) && !uid_eq(keuid, old->suid);\\n\\tsuid_new = suid != (uid_t) -1        && !uid_eq(ksuid, old->uid) &&\\n\\t\\t   !uid_eq(ksuid, old->euid) && !uid_eq(ksuid, old->suid);\\n\\tif ((ruid_new || euid_new || suid_new) &&\\n\\t    !ns_capable_setid(old->user_ns, CAP_SETUID))\\n\\t\\treturn -EPERM;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\n\\tif (ruid != (uid_t) -1) {\\n\\t\\tnew->uid = kruid;\\n\\t\\tif (!uid_eq(kruid, old->uid)) {\\n\\t\\t\\tretval = set_user(new);\\n\\t\\t\\tif (retval < 0)\\n\\t\\t\\t\\tgoto error;\\n\\t\\t}\\n\\t}\\n\\tif (euid != (uid_t) -1)\\n\\t\\tnew->euid = keuid;\\n\\tif (suid != (uid_t) -1)\\n\\t\\tnew->suid = ksuid;\\n\\tnew->fsuid = new->euid;\\n\\n\\tretval = security_task_fix_setuid(new, old, LSM_SETID_RES);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\tretval = set_cred_ucounts(new);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\tflag_nproc_exceeded(new);\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE3(setresuid, uid_t, ruid, uid_t, euid, uid_t, suid)\\n{\\n\\treturn __sys_setresuid(ruid, euid, suid);\\n}\\n\\nSYSCALL_DEFINE3(getresuid, uid_t __user *, ruidp, uid_t __user *, euidp, uid_t __user *, suidp)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint retval;\\n\\tuid_t ruid, euid, suid;\\n\\n\\truid = from_kuid_munged(cred->user_ns, cred->uid);\\n\\teuid = from_kuid_munged(cred->user_ns, cred->euid);\\n\\tsuid = from_kuid_munged(cred->user_ns, cred->suid);\\n\\n\\tretval = put_user(ruid, ruidp);\\n\\tif (!retval) {\\n\\t\\tretval = put_user(euid, euidp);\\n\\t\\tif (!retval)\\n\\t\\t\\treturn put_user(suid, suidp);\\n\\t}\\n\\treturn retval;\\n}\\n\\n/*\\n * Same as above, but for rgid, egid, sgid.\\n */\\nlong __sys_setresgid(gid_t rgid, gid_t egid, gid_t sgid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkgid_t krgid, kegid, ksgid;\\n\\tbool rgid_new, egid_new, sgid_new;\\n\\n\\tkrgid = make_kgid(ns, rgid);\\n\\tkegid = make_kgid(ns, egid);\\n\\tksgid = make_kgid(ns, sgid);\\n\\n\\tif ((rgid != (gid_t) -1) && !gid_valid(krgid))\\n\\t\\treturn -EINVAL;\\n\\tif ((egid != (gid_t) -1) && !gid_valid(kegid))\\n\\t\\treturn -EINVAL;\\n\\tif ((sgid != (gid_t) -1) && !gid_valid(ksgid))\\n\\t\\treturn -EINVAL;\\n\\n\\told = current_cred();\\n\\n\\t/* check for no-op */\\n\\tif ((rgid == (gid_t) -1 || gid_eq(krgid, old->gid)) &&\\n\\t    (egid == (gid_t) -1 || (gid_eq(kegid, old->egid) &&\\n\\t\\t\\t\\t    gid_eq(kegid, old->fsgid))) &&\\n\\t    (sgid == (gid_t) -1 || gid_eq(ksgid, old->sgid)))\\n\\t\\treturn 0;\\n\\n\\trgid_new = rgid != (gid_t) -1        && !gid_eq(krgid, old->gid) &&\\n\\t\\t   !gid_eq(krgid, old->egid) && !gid_eq(krgid, old->sgid);\\n\\tegid_new = egid != (gid_t) -1        && !gid_eq(kegid, old->gid) &&\\n\\t\\t   !gid_eq(kegid, old->egid) && !gid_eq(kegid, old->sgid);\\n\\tsgid_new = sgid != (gid_t) -1        && !gid_eq(ksgid, old->gid) &&\\n\\t\\t   !gid_eq(ksgid, old->egid) && !gid_eq(ksgid, old->sgid);\\n\\tif ((rgid_new || egid_new || sgid_new) &&\\n\\t    !ns_capable_setid(old->user_ns, CAP_SETGID))\\n\\t\\treturn -EPERM;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\n\\tif (rgid != (gid_t) -1)\\n\\t\\tnew->gid = krgid;\\n\\tif (egid != (gid_t) -1)\\n\\t\\tnew->egid = kegid;\\n\\tif (sgid != (gid_t) -1)\\n\\t\\tnew->sgid = ksgid;\\n\\tnew->fsgid = new->egid;\\n\\n\\tretval = security_task_fix_setgid(new, old, LSM_SETID_RES);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE3(setresgid, gid_t, rgid, gid_t, egid, gid_t, sgid)\\n{\\n\\treturn __sys_setresgid(rgid, egid, sgid);\\n}\\n\\nSYSCALL_DEFINE3(getresgid, gid_t __user *, rgidp, gid_t __user *, egidp, gid_t __user *, sgidp)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint retval;\\n\\tgid_t rgid, egid, sgid;\\n\\n\\trgid = from_kgid_munged(cred->user_ns, cred->gid);\\n\\tegid = from_kgid_munged(cred->user_ns, cred->egid);\\n\\tsgid = from_kgid_munged(cred->user_ns, cred->sgid);\\n\\n\\tretval = put_user(rgid, rgidp);\\n\\tif (!retval) {\\n\\t\\tretval = put_user(egid, egidp);\\n\\t\\tif (!retval)\\n\\t\\t\\tretval = put_user(sgid, sgidp);\\n\\t}\\n\\n\\treturn retval;\\n}\\n\\n\\n/*\\n * \"setfsuid()\" sets the fsuid - the uid used for filesystem checks. This\\n * is used for \"access()\" and for the NFS daemon (letting nfsd stay at\\n * whatever uid it wants to). It normally shadows \"euid\", except when\\n * explicitly set by setfsuid() or for access..\\n */\\nlong __sys_setfsuid(uid_t uid)\\n{\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tuid_t old_fsuid;\\n\\tkuid_t kuid;\\n\\n\\told = current_cred();\\n\\told_fsuid = from_kuid_munged(old->user_ns, old->fsuid);\\n\\n\\tkuid = make_kuid(old->user_ns, uid);\\n\\tif (!uid_valid(kuid))\\n\\t\\treturn old_fsuid;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn old_fsuid;\\n\\n\\tif (uid_eq(kuid, old->uid)  || uid_eq(kuid, old->euid)  ||\\n\\t    uid_eq(kuid, old->suid) || uid_eq(kuid, old->fsuid) ||\\n\\t    ns_capable_setid(old->user_ns, CAP_SETUID)) {\\n\\t\\tif (!uid_eq(kuid, old->fsuid)) {\\n\\t\\t\\tnew->fsuid = kuid;\\n\\t\\t\\tif (security_task_fix_setuid(new, old, LSM_SETID_FS) == 0)\\n\\t\\t\\t\\tgoto change_okay;\\n\\t\\t}\\n\\t}\\n\\n\\tabort_creds(new);\\n\\treturn old_fsuid;\\n\\nchange_okay:\\n\\tcommit_creds(new);\\n\\treturn old_fsuid;\\n}\\n\\nSYSCALL_DEFINE1(setfsuid, uid_t, uid)\\n{\\n\\treturn __sys_setfsuid(uid);\\n}\\n\\n/*\\n * Samma på svenska..\\n */\\nlong __sys_setfsgid(gid_t gid)\\n{\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tgid_t old_fsgid;\\n\\tkgid_t kgid;\\n\\n\\told = current_cred();\\n\\told_fsgid = from_kgid_munged(old->user_ns, old->fsgid);\\n\\n\\tkgid = make_kgid(old->user_ns, gid);\\n\\tif (!gid_valid(kgid))\\n\\t\\treturn old_fsgid;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn old_fsgid;\\n\\n\\tif (gid_eq(kgid, old->gid)  || gid_eq(kgid, old->egid)  ||\\n\\t    gid_eq(kgid, old->sgid) || gid_eq(kgid, old->fsgid) ||\\n\\t    ns_capable_setid(old->user_ns, CAP_SETGID)) {\\n\\t\\tif (!gid_eq(kgid, old->fsgid)) {\\n\\t\\t\\tnew->fsgid = kgid;\\n\\t\\t\\tif (security_task_fix_setgid(new,old,LSM_SETID_FS) == 0)\\n\\t\\t\\t\\tgoto change_okay;\\n\\t\\t}\\n\\t}\\n\\n\\tabort_creds(new);\\n\\treturn old_fsgid;\\n\\nchange_okay:\\n\\tcommit_creds(new);\\n\\treturn old_fsgid;\\n}\\n\\nSYSCALL_DEFINE1(setfsgid, gid_t, gid)\\n{\\n\\treturn __sys_setfsgid(gid);\\n}\\n#endif /* CONFIG_MULTIUSER */\\n\\n/**\\n * sys_getpid - return the thread group id of the current process\\n *\\n * Note, despite the name, this returns the tgid not the pid.  The tgid and\\n * the pid are identical unless CLONE_THREAD was specified on clone() in\\n * which case the tgid is the same in all threads of the same group.\\n *\\n * This is SMP safe as current->tgid does not change.\\n */\\nSYSCALL_DEFINE0(getpid)\\n{\\n\\treturn task_tgid_vnr(current);\\n}\\n\\n/* Thread ID - the internal kernel \"pid\" */\\nSYSCALL_DEFINE0(gettid)\\n{\\n\\treturn task_pid_vnr(current);\\n}\\n\\n/*\\n * Accessing ->real_parent is not SMP-safe, it could\\n * change from under us. However, we can use a stale\\n * value of ->real_parent under rcu_read_lock(), see\\n * release_task()->call_rcu(delayed_put_task_struct).\\n */\\nSYSCALL_DEFINE0(getppid)\\n{\\n\\tint pid;\\n\\n\\trcu_read_lock();\\n\\tpid = task_tgid_vnr(rcu_dereference(current->real_parent));\\n\\trcu_read_unlock();\\n\\n\\treturn pid;\\n}\\n\\nSYSCALL_DEFINE0(getuid)\\n{\\n\\t/* Only we change this so SMP safe */\\n\\treturn from_kuid_munged(current_user_ns(), current_uid());\\n}\\n\\nSYSCALL_DEFINE0(geteuid)\\n{\\n\\t/* Only we change this so SMP safe */\\n\\treturn from_kuid_munged(current_user_ns(), current_euid());\\n}\\n\\nSYSCALL_DEFINE0(getgid)\\n{\\n\\t/* Only we change this so SMP safe */\\n\\treturn from_kgid_munged(current_user_ns(), current_gid());\\n}\\n\\nSYSCALL_DEFINE0(getegid)\\n{\\n\\t/* Only we change this so SMP safe */\\n\\treturn from_kgid_munged(current_user_ns(), current_egid());\\n}\\n\\nstatic void do_sys_times(struct tms *tms)\\n{\\n\\tu64 tgutime, tgstime, cutime, cstime;\\n\\n\\tthread_group_cputime_adjusted(current, &tgutime, &tgstime);\\n\\tcutime = current->signal->cutime;\\n\\tcstime = current->signal->cstime;\\n\\ttms->tms_utime = nsec_to_clock_t(tgutime);\\n\\ttms->tms_stime = nsec_to_clock_t(tgstime);\\n\\ttms->tms_cutime = nsec_to_clock_t(cutime);\\n\\ttms->tms_cstime = nsec_to_clock_t(cstime);\\n}\\n\\nSYSCALL_DEFINE1(times, struct tms __user *, tbuf)\\n{\\n\\tif (tbuf) {\\n\\t\\tstruct tms tmp;\\n\\n\\t\\tdo_sys_times(&tmp);\\n\\t\\tif (copy_to_user(tbuf, &tmp, sizeof(struct tms)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\tforce_successful_syscall_return();\\n\\treturn (long) jiffies_64_to_clock_t(get_jiffies_64());\\n}\\n\\n#ifdef CONFIG_COMPAT\\nstatic compat_clock_t clock_t_to_compat_clock_t(clock_t x)\\n{\\n\\treturn compat_jiffies_to_clock_t(clock_t_to_jiffies(x));\\n}\\n\\nCOMPAT_SYSCALL_DEFINE1(times, struct compat_tms __user *, tbuf)\\n{\\n\\tif (tbuf) {\\n\\t\\tstruct tms tms;\\n\\t\\tstruct compat_tms tmp;\\n\\n\\t\\tdo_sys_times(&tms);\\n\\t\\t/* Convert our struct tms to the compat version. */\\n\\t\\ttmp.tms_utime = clock_t_to_compat_clock_t(tms.tms_utime);\\n\\t\\ttmp.tms_stime = clock_t_to_compat_clock_t(tms.tms_stime);\\n\\t\\ttmp.tms_cutime = clock_t_to_compat_clock_t(tms.tms_cutime);\\n\\t\\ttmp.tms_cstime = clock_t_to_compat_clock_t(tms.tms_cstime);\\n\\t\\tif (copy_to_user(tbuf, &tmp, sizeof(tmp)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\tforce_successful_syscall_return();\\n\\treturn compat_jiffies_to_clock_t(jiffies);\\n}\\n#endif\\n\\n/*\\n * This needs some heavy checking ...\\n * I just haven\\'t the stomach for it. I also don\\'t fully\\n * understand sessions/pgrp etc. Let somebody who does explain it.\\n *\\n * OK, I think I have the protection semantics right.... this is really\\n * only important on a multi-user system anyway, to make sure one user\\n * can\\'t send a signal to a process owned by another.  -TYT, 12/12/91\\n *\\n * !PF_FORKNOEXEC check to conform completely to POSIX.\\n */\\nSYSCALL_DEFINE2(setpgid, pid_t, pid, pid_t, pgid)\\n{\\n\\tstruct task_struct *p;\\n\\tstruct task_struct *group_leader = current->group_leader;\\n\\tstruct pid *pgrp;\\n\\tint err;\\n\\n\\tif (!pid)\\n\\t\\tpid = task_pid_vnr(group_leader);\\n\\tif (!pgid)\\n\\t\\tpgid = pid;\\n\\tif (pgid < 0)\\n\\t\\treturn -EINVAL;\\n\\trcu_read_lock();\\n\\n\\t/* From this point forward we keep holding onto the tasklist lock\\n\\t * so that our parent does not change from under us. -DaveM\\n\\t */\\n\\twrite_lock_irq(&tasklist_lock);\\n\\n\\terr = -ESRCH;\\n\\tp = find_task_by_vpid(pid);\\n\\tif (!p)\\n\\t\\tgoto out;\\n\\n\\terr = -EINVAL;\\n\\tif (!thread_group_leader(p))\\n\\t\\tgoto out;\\n\\n\\tif (same_thread_group(p->real_parent, group_leader)) {\\n\\t\\terr = -EPERM;\\n\\t\\tif (task_session(p) != task_session(group_leader))\\n\\t\\t\\tgoto out;\\n\\t\\terr = -EACCES;\\n\\t\\tif (!(p->flags & PF_FORKNOEXEC))\\n\\t\\t\\tgoto out;\\n\\t} else {\\n\\t\\terr = -ESRCH;\\n\\t\\tif (p != group_leader)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\terr = -EPERM;\\n\\tif (p->signal->leader)\\n\\t\\tgoto out;\\n\\n\\tpgrp = task_pid(p);\\n\\tif (pgid != pid) {\\n\\t\\tstruct task_struct *g;\\n\\n\\t\\tpgrp = find_vpid(pgid);\\n\\t\\tg = pid_task(pgrp, PIDTYPE_PGID);\\n\\t\\tif (!g || task_session(g) != task_session(group_leader))\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\terr = security_task_setpgid(p, pgid);\\n\\tif (err)\\n\\t\\tgoto out;\\n\\n\\tif (task_pgrp(p) != pgrp)\\n\\t\\tchange_pid(p, PIDTYPE_PGID, pgrp);\\n\\n\\terr = 0;\\nout:\\n\\t/* All paths lead to here, thus we are safe. -DaveM */\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\trcu_read_unlock();\\n\\treturn err;\\n}\\n\\nstatic int do_getpgid(pid_t pid)\\n{\\n\\tstruct task_struct *p;\\n\\tstruct pid *grp;\\n\\tint retval;\\n\\n\\trcu_read_lock();\\n\\tif (!pid)\\n\\t\\tgrp = task_pgrp(current);\\n\\telse {\\n\\t\\tretval = -ESRCH;\\n\\t\\tp = find_task_by_vpid(pid);\\n\\t\\tif (!p)\\n\\t\\t\\tgoto out;\\n\\t\\tgrp = task_pgrp(p);\\n\\t\\tif (!grp)\\n\\t\\t\\tgoto out;\\n\\n\\t\\tretval = security_task_getpgid(p);\\n\\t\\tif (retval)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\tretval = pid_vnr(grp);\\nout:\\n\\trcu_read_unlock();\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE1(getpgid, pid_t, pid)\\n{\\n\\treturn do_getpgid(pid);\\n}\\n\\n#ifdef __ARCH_WANT_SYS_GETPGRP\\n\\nSYSCALL_DEFINE0(getpgrp)\\n{\\n\\treturn do_getpgid(0);\\n}\\n\\n#endif\\n\\nSYSCALL_DEFINE1(getsid, pid_t, pid)\\n{\\n\\tstruct task_struct *p;\\n\\tstruct pid *sid;\\n\\tint retval;\\n\\n\\trcu_read_lock();\\n\\tif (!pid)\\n\\t\\tsid = task_session(current);\\n\\telse {\\n\\t\\tretval = -ESRCH;\\n\\t\\tp = find_task_by_vpid(pid);\\n\\t\\tif (!p)\\n\\t\\t\\tgoto out;\\n\\t\\tsid = task_session(p);\\n\\t\\tif (!sid)\\n\\t\\t\\tgoto out;\\n\\n\\t\\tretval = security_task_getsid(p);\\n\\t\\tif (retval)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\tretval = pid_vnr(sid);\\nout:\\n\\trcu_read_unlock();\\n\\treturn retval;\\n}\\n\\nstatic void set_special_pids(struct pid *pid)\\n{\\n\\tstruct task_struct *curr = current->group_leader;\\n\\n\\tif (task_session(curr) != pid)\\n\\t\\tchange_pid(curr, PIDTYPE_SID, pid);\\n\\n\\tif (task_pgrp(curr) != pid)\\n\\t\\tchange_pid(curr, PIDTYPE_PGID, pid);\\n}\\n\\nint ksys_setsid(void)\\n{\\n\\tstruct task_struct *group_leader = current->group_leader;\\n\\tstruct pid *sid = task_pid(group_leader);\\n\\tpid_t session = pid_vnr(sid);\\n\\tint err = -EPERM;\\n\\n\\twrite_lock_irq(&tasklist_lock);\\n\\t/* Fail if I am already a session leader */\\n\\tif (group_leader->signal->leader)\\n\\t\\tgoto out;\\n\\n\\t/* Fail if a process group id already exists that equals the\\n\\t * proposed session id.\\n\\t */\\n\\tif (pid_task(sid, PIDTYPE_PGID))\\n\\t\\tgoto out;\\n\\n\\tgroup_leader->signal->leader = 1;\\n\\tset_special_pids(sid);\\n\\n\\tproc_clear_tty(group_leader);\\n\\n\\terr = session;\\nout:\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\tif (err > 0) {\\n\\t\\tproc_sid_connector(group_leader);\\n\\t\\tsched_autogroup_create_attach(group_leader);\\n\\t}\\n\\treturn err;\\n}\\n\\nSYSCALL_DEFINE0(setsid)\\n{\\n\\treturn ksys_setsid();\\n}\\n\\nDECLARE_RWSEM(uts_sem);\\n\\n#ifdef COMPAT_UTS_MACHINE\\n#define override_architecture(name) \\\\\\n\\t(personality(current->personality) == PER_LINUX32 && \\\\\\n\\t copy_to_user(name->machine, COMPAT_UTS_MACHINE, \\\\\\n\\t\\t      sizeof(COMPAT_UTS_MACHINE)))\\n#else\\n#define override_architecture(name)\\t0\\n#endif\\n\\n/*\\n * Work around broken programs that cannot handle \"Linux 3.0\".\\n * Instead we map 3.x to 2.6.40+x, so e.g. 3.0 would be 2.6.40\\n * And we map 4.x and later versions to 2.6.60+x, so 4.0/5.0/6.0/... would be\\n * 2.6.60.\\n */\\nstatic int override_release(char __user *release, size_t len)\\n{\\n\\tint ret = 0;\\n\\n\\tif (current->personality & UNAME26) {\\n\\t\\tconst char *rest = UTS_RELEASE;\\n\\t\\tchar buf[65] = { 0 };\\n\\t\\tint ndots = 0;\\n\\t\\tunsigned v;\\n\\t\\tsize_t copy;\\n\\n\\t\\twhile (*rest) {\\n\\t\\t\\tif (*rest == \\'.\\' && ++ndots >= 3)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tif (!isdigit(*rest) && *rest != \\'.\\')\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\trest++;\\n\\t\\t}\\n\\t\\tv = LINUX_VERSION_PATCHLEVEL + 60;\\n\\t\\tcopy = clamp_t(size_t, len, 1, sizeof(buf));\\n\\t\\tcopy = scnprintf(buf, copy, \"2.6.%u%s\", v, rest);\\n\\t\\tret = copy_to_user(release, buf, copy + 1);\\n\\t}\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE1(newuname, struct new_utsname __user *, name)\\n{\\n\\tstruct new_utsname tmp;\\n\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(&tmp, utsname(), sizeof(tmp));\\n\\tup_read(&uts_sem);\\n\\tif (copy_to_user(name, &tmp, sizeof(tmp)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (override_release(name->release, sizeof(name->release)))\\n\\t\\treturn -EFAULT;\\n\\tif (override_architecture(name))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\n#ifdef __ARCH_WANT_SYS_OLD_UNAME\\n/*\\n * Old cruft\\n */\\nSYSCALL_DEFINE1(uname, struct old_utsname __user *, name)\\n{\\n\\tstruct old_utsname tmp;\\n\\n\\tif (!name)\\n\\t\\treturn -EFAULT;\\n\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(&tmp, utsname(), sizeof(tmp));\\n\\tup_read(&uts_sem);\\n\\tif (copy_to_user(name, &tmp, sizeof(tmp)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (override_release(name->release, sizeof(name->release)))\\n\\t\\treturn -EFAULT;\\n\\tif (override_architecture(name))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\nSYSCALL_DEFINE1(olduname, struct oldold_utsname __user *, name)\\n{\\n\\tstruct oldold_utsname tmp;\\n\\n\\tif (!name)\\n\\t\\treturn -EFAULT;\\n\\n\\tmemset(&tmp, 0, sizeof(tmp));\\n\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(&tmp.sysname, &utsname()->sysname, __OLD_UTS_LEN);\\n\\tmemcpy(&tmp.nodename, &utsname()->nodename, __OLD_UTS_LEN);\\n\\tmemcpy(&tmp.release, &utsname()->release, __OLD_UTS_LEN);\\n\\tmemcpy(&tmp.version, &utsname()->version, __OLD_UTS_LEN);\\n\\tmemcpy(&tmp.machine, &utsname()->machine, __OLD_UTS_LEN);\\n\\tup_read(&uts_sem);\\n\\tif (copy_to_user(name, &tmp, sizeof(tmp)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (override_architecture(name))\\n\\t\\treturn -EFAULT;\\n\\tif (override_release(name->release, sizeof(name->release)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n#endif\\n\\nSYSCALL_DEFINE2(sethostname, char __user *, name, int, len)\\n{\\n\\tint errno;\\n\\tchar tmp[__NEW_UTS_LEN];\\n\\n\\tif (!ns_capable(current->nsproxy->uts_ns->user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tif (len < 0 || len > __NEW_UTS_LEN)\\n\\t\\treturn -EINVAL;\\n\\terrno = -EFAULT;\\n\\tif (!copy_from_user(tmp, name, len)) {\\n\\t\\tstruct new_utsname *u;\\n\\n\\t\\tadd_device_randomness(tmp, len);\\n\\t\\tdown_write(&uts_sem);\\n\\t\\tu = utsname();\\n\\t\\tmemcpy(u->nodename, tmp, len);\\n\\t\\tmemset(u->nodename + len, 0, sizeof(u->nodename) - len);\\n\\t\\terrno = 0;\\n\\t\\tuts_proc_notify(UTS_PROC_HOSTNAME);\\n\\t\\tup_write(&uts_sem);\\n\\t}\\n\\treturn errno;\\n}\\n\\n#ifdef __ARCH_WANT_SYS_GETHOSTNAME\\n\\nSYSCALL_DEFINE2(gethostname, char __user *, name, int, len)\\n{\\n\\tint i;\\n\\tstruct new_utsname *u;\\n\\tchar tmp[__NEW_UTS_LEN + 1];\\n\\n\\tif (len < 0)\\n\\t\\treturn -EINVAL;\\n\\tdown_read(&uts_sem);\\n\\tu = utsname();\\n\\ti = 1 + strlen(u->nodename);\\n\\tif (i > len)\\n\\t\\ti = len;\\n\\tmemcpy(tmp, u->nodename, i);\\n\\tup_read(&uts_sem);\\n\\tif (copy_to_user(name, tmp, i))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\n#endif\\n\\n/*\\n * Only setdomainname; getdomainname can be implemented by calling\\n * uname()\\n */\\nSYSCALL_DEFINE2(setdomainname, char __user *, name, int, len)\\n{\\n\\tint errno;\\n\\tchar tmp[__NEW_UTS_LEN];\\n\\n\\tif (!ns_capable(current->nsproxy->uts_ns->user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\tif (len < 0 || len > __NEW_UTS_LEN)\\n\\t\\treturn -EINVAL;\\n\\n\\terrno = -EFAULT;\\n\\tif (!copy_from_user(tmp, name, len)) {\\n\\t\\tstruct new_utsname *u;\\n\\n\\t\\tadd_device_randomness(tmp, len);\\n\\t\\tdown_write(&uts_sem);\\n\\t\\tu = utsname();\\n\\t\\tmemcpy(u->domainname, tmp, len);\\n\\t\\tmemset(u->domainname + len, 0, sizeof(u->domainname) - len);\\n\\t\\terrno = 0;\\n\\t\\tuts_proc_notify(UTS_PROC_DOMAINNAME);\\n\\t\\tup_write(&uts_sem);\\n\\t}\\n\\treturn errno;\\n}\\n\\n/* make sure you are allowed to change @tsk limits before calling this */\\nstatic int do_prlimit(struct task_struct *tsk, unsigned int resource,\\n\\t\\t      struct rlimit *new_rlim, struct rlimit *old_rlim)\\n{\\n\\tstruct rlimit *rlim;\\n\\tint retval = 0;\\n\\n\\tif (resource >= RLIM_NLIMITS)\\n\\t\\treturn -EINVAL;\\n\\tresource = array_index_nospec(resource, RLIM_NLIMITS);\\n\\n\\tif (new_rlim) {\\n\\t\\tif (new_rlim->rlim_cur > new_rlim->rlim_max)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (resource == RLIMIT_NOFILE &&\\n\\t\\t\\t\\tnew_rlim->rlim_max > sysctl_nr_open)\\n\\t\\t\\treturn -EPERM;\\n\\t}\\n\\n\\t/* Holding a refcount on tsk protects tsk->signal from disappearing. */\\n\\trlim = tsk->signal->rlim + resource;\\n\\ttask_lock(tsk->group_leader);\\n\\tif (new_rlim) {\\n\\t\\t/*\\n\\t\\t * Keep the capable check against init_user_ns until cgroups can\\n\\t\\t * contain all limits.\\n\\t\\t */\\n\\t\\tif (new_rlim->rlim_max > rlim->rlim_max &&\\n\\t\\t\\t\\t!capable(CAP_SYS_RESOURCE))\\n\\t\\t\\tretval = -EPERM;\\n\\t\\tif (!retval)\\n\\t\\t\\tretval = security_task_setrlimit(tsk, resource, new_rlim);\\n\\t}\\n\\tif (!retval) {\\n\\t\\tif (old_rlim)\\n\\t\\t\\t*old_rlim = *rlim;\\n\\t\\tif (new_rlim)\\n\\t\\t\\t*rlim = *new_rlim;\\n\\t}\\n\\ttask_unlock(tsk->group_leader);\\n\\n\\t/*\\n\\t * RLIMIT_CPU handling. Arm the posix CPU timer if the limit is not\\n\\t * infinite. In case of RLIM_INFINITY the posix CPU timer code\\n\\t * ignores the rlimit.\\n\\t */\\n\\tif (!retval && new_rlim && resource == RLIMIT_CPU &&\\n\\t    new_rlim->rlim_cur != RLIM_INFINITY &&\\n\\t    IS_ENABLED(CONFIG_POSIX_TIMERS)) {\\n\\t\\t/*\\n\\t\\t * update_rlimit_cpu can fail if the task is exiting, but there\\n\\t\\t * may be other tasks in the thread group that are not exiting,\\n\\t\\t * and they need their cpu timers adjusted.\\n\\t\\t *\\n\\t\\t * The group_leader is the last task to be released, so if we\\n\\t\\t * cannot update_rlimit_cpu on it, then the entire process is\\n\\t\\t * exiting and we do not need to update at all.\\n\\t\\t */\\n\\t\\tupdate_rlimit_cpu(tsk->group_leader, new_rlim->rlim_cur);\\n\\t}\\n\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE2(getrlimit, unsigned int, resource, struct rlimit __user *, rlim)\\n{\\n\\tstruct rlimit value;\\n\\tint ret;\\n\\n\\tret = do_prlimit(current, resource, NULL, &value);\\n\\tif (!ret)\\n\\t\\tret = copy_to_user(rlim, &value, sizeof(*rlim)) ? -EFAULT : 0;\\n\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_COMPAT\\n\\nCOMPAT_SYSCALL_DEFINE2(setrlimit, unsigned int, resource,\\n\\t\\t       struct compat_rlimit __user *, rlim)\\n{\\n\\tstruct rlimit r;\\n\\tstruct compat_rlimit r32;\\n\\n\\tif (copy_from_user(&r32, rlim, sizeof(struct compat_rlimit)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (r32.rlim_cur == COMPAT_RLIM_INFINITY)\\n\\t\\tr.rlim_cur = RLIM_INFINITY;\\n\\telse\\n\\t\\tr.rlim_cur = r32.rlim_cur;\\n\\tif (r32.rlim_max == COMPAT_RLIM_INFINITY)\\n\\t\\tr.rlim_max = RLIM_INFINITY;\\n\\telse\\n\\t\\tr.rlim_max = r32.rlim_max;\\n\\treturn do_prlimit(current, resource, &r, NULL);\\n}\\n\\nCOMPAT_SYSCALL_DEFINE2(getrlimit, unsigned int, resource,\\n\\t\\t       struct compat_rlimit __user *, rlim)\\n{\\n\\tstruct rlimit r;\\n\\tint ret;\\n\\n\\tret = do_prlimit(current, resource, NULL, &r);\\n\\tif (!ret) {\\n\\t\\tstruct compat_rlimit r32;\\n\\t\\tif (r.rlim_cur > COMPAT_RLIM_INFINITY)\\n\\t\\t\\tr32.rlim_cur = COMPAT_RLIM_INFINITY;\\n\\t\\telse\\n\\t\\t\\tr32.rlim_cur = r.rlim_cur;\\n\\t\\tif (r.rlim_max > COMPAT_RLIM_INFINITY)\\n\\t\\t\\tr32.rlim_max = COMPAT_RLIM_INFINITY;\\n\\t\\telse\\n\\t\\t\\tr32.rlim_max = r.rlim_max;\\n\\n\\t\\tif (copy_to_user(rlim, &r32, sizeof(struct compat_rlimit)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\treturn ret;\\n}\\n\\n#endif\\n\\n#ifdef __ARCH_WANT_SYS_OLD_GETRLIMIT\\n\\n/*\\n *\\tBack compatibility for getrlimit. Needed for some apps.\\n */\\nSYSCALL_DEFINE2(old_getrlimit, unsigned int, resource,\\n\\t\\tstruct rlimit __user *, rlim)\\n{\\n\\tstruct rlimit x;\\n\\tif (resource >= RLIM_NLIMITS)\\n\\t\\treturn -EINVAL;\\n\\n\\tresource = array_index_nospec(resource, RLIM_NLIMITS);\\n\\ttask_lock(current->group_leader);\\n\\tx = current->signal->rlim[resource];\\n\\ttask_unlock(current->group_leader);\\n\\tif (x.rlim_cur > 0x7FFFFFFF)\\n\\t\\tx.rlim_cur = 0x7FFFFFFF;\\n\\tif (x.rlim_max > 0x7FFFFFFF)\\n\\t\\tx.rlim_max = 0x7FFFFFFF;\\n\\treturn copy_to_user(rlim, &x, sizeof(x)) ? -EFAULT : 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE2(old_getrlimit, unsigned int, resource,\\n\\t\\t       struct compat_rlimit __user *, rlim)\\n{\\n\\tstruct rlimit r;\\n\\n\\tif (resource >= RLIM_NLIMITS)\\n\\t\\treturn -EINVAL;\\n\\n\\tresource = array_index_nospec(resource, RLIM_NLIMITS);\\n\\ttask_lock(current->group_leader);\\n\\tr = current->signal->rlim[resource];\\n\\ttask_unlock(current->group_leader);\\n\\tif (r.rlim_cur > 0x7FFFFFFF)\\n\\t\\tr.rlim_cur = 0x7FFFFFFF;\\n\\tif (r.rlim_max > 0x7FFFFFFF)\\n\\t\\tr.rlim_max = 0x7FFFFFFF;\\n\\n\\tif (put_user(r.rlim_cur, &rlim->rlim_cur) ||\\n\\t    put_user(r.rlim_max, &rlim->rlim_max))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n#endif\\n\\n#endif\\n\\nstatic inline bool rlim64_is_infinity(__u64 rlim64)\\n{\\n#if BITS_PER_LONG < 64\\n\\treturn rlim64 >= ULONG_MAX;\\n#else\\n\\treturn rlim64 == RLIM64_INFINITY;\\n#endif\\n}\\n\\nstatic void rlim_to_rlim64(const struct rlimit *rlim, struct rlimit64 *rlim64)\\n{\\n\\tif (rlim->rlim_cur == RLIM_INFINITY)\\n\\t\\trlim64->rlim_cur = RLIM64_INFINITY;\\n\\telse\\n\\t\\trlim64->rlim_cur = rlim->rlim_cur;\\n\\tif (rlim->rlim_max == RLIM_INFINITY)\\n\\t\\trlim64->rlim_max = RLIM64_INFINITY;\\n\\telse\\n\\t\\trlim64->rlim_max = rlim->rlim_max;\\n}\\n\\nstatic void rlim64_to_rlim(const struct rlimit64 *rlim64, struct rlimit *rlim)\\n{\\n\\tif (rlim64_is_infinity(rlim64->rlim_cur))\\n\\t\\trlim->rlim_cur = RLIM_INFINITY;\\n\\telse\\n\\t\\trlim->rlim_cur = (unsigned long)rlim64->rlim_cur;\\n\\tif (rlim64_is_infinity(rlim64->rlim_max))\\n\\t\\trlim->rlim_max = RLIM_INFINITY;\\n\\telse\\n\\t\\trlim->rlim_max = (unsigned long)rlim64->rlim_max;\\n}\\n\\n/* rcu lock must be held */\\nstatic int check_prlimit_permission(struct task_struct *task,\\n\\t\\t\\t\\t    unsigned int flags)\\n{\\n\\tconst struct cred *cred = current_cred(), *tcred;\\n\\tbool id_match;\\n\\n\\tif (current == task)\\n\\t\\treturn 0;\\n\\n\\ttcred = __task_cred(task);\\n\\tid_match = (uid_eq(cred->uid, tcred->euid) &&\\n\\t\\t    uid_eq(cred->uid, tcred->suid) &&\\n\\t\\t    uid_eq(cred->uid, tcred->uid)  &&\\n\\t\\t    gid_eq(cred->gid, tcred->egid) &&\\n\\t\\t    gid_eq(cred->gid, tcred->sgid) &&\\n\\t\\t    gid_eq(cred->gid, tcred->gid));\\n\\tif (!id_match && !ns_capable(tcred->user_ns, CAP_SYS_RESOURCE))\\n\\t\\treturn -EPERM;\\n\\n\\treturn security_task_prlimit(cred, tcred, flags);\\n}\\n\\nSYSCALL_DEFINE4(prlimit64, pid_t, pid, unsigned int, resource,\\n\\t\\tconst struct rlimit64 __user *, new_rlim,\\n\\t\\tstruct rlimit64 __user *, old_rlim)\\n{\\n\\tstruct rlimit64 old64, new64;\\n\\tstruct rlimit old, new;\\n\\tstruct task_struct *tsk;\\n\\tunsigned int checkflags = 0;\\n\\tint ret;\\n\\n\\tif (old_rlim)\\n\\t\\tcheckflags |= LSM_PRLIMIT_READ;\\n\\n\\tif (new_rlim) {\\n\\t\\tif (copy_from_user(&new64, new_rlim, sizeof(new64)))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\trlim64_to_rlim(&new64, &new);\\n\\t\\tcheckflags |= LSM_PRLIMIT_WRITE;\\n\\t}\\n\\n\\trcu_read_lock();\\n\\ttsk = pid ? find_task_by_vpid(pid) : current;\\n\\tif (!tsk) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn -ESRCH;\\n\\t}\\n\\tret = check_prlimit_permission(tsk, checkflags);\\n\\tif (ret) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn ret;\\n\\t}\\n\\tget_task_struct(tsk);\\n\\trcu_read_unlock();\\n\\n\\tret = do_prlimit(tsk, resource, new_rlim ? &new : NULL,\\n\\t\\t\\told_rlim ? &old : NULL);\\n\\n\\tif (!ret && old_rlim) {\\n\\t\\trlim_to_rlim64(&old, &old64);\\n\\t\\tif (copy_to_user(old_rlim, &old64, sizeof(old64)))\\n\\t\\t\\tret = -EFAULT;\\n\\t}\\n\\n\\tput_task_struct(tsk);\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE2(setrlimit, unsigned int, resource, struct rlimit __user *, rlim)\\n{\\n\\tstruct rlimit new_rlim;\\n\\n\\tif (copy_from_user(&new_rlim, rlim, sizeof(*rlim)))\\n\\t\\treturn -EFAULT;\\n\\treturn do_prlimit(current, resource, &new_rlim, NULL);\\n}\\n\\n/*\\n * It would make sense to put struct rusage in the task_struct,\\n * except that would make the task_struct be *really big*.  After\\n * task_struct gets moved into malloc\\'ed memory, it would\\n * make sense to do this.  It will make moving the rest of the information\\n * a lot simpler!  (Which we\\'re not doing right now because we\\'re not\\n * measuring them yet).\\n *\\n * When sampling multiple threads for RUSAGE_SELF, under SMP we might have\\n * races with threads incrementing their own counters.  But since word\\n * reads are atomic, we either get new values or old values and we don\\'t\\n * care which for the sums.  We always take the siglock to protect reading\\n * the c* fields from p->signal from races with exit.c updating those\\n * fields when reaping, so a sample either gets all the additions of a\\n * given child after it\\'s reaped, or none so this sample is before reaping.\\n *\\n * Locking:\\n * We need to take the siglock for CHILDEREN, SELF and BOTH\\n * for  the cases current multithreaded, non-current single threaded\\n * non-current multithreaded.  Thread traversal is now safe with\\n * the siglock held.\\n * Strictly speaking, we donot need to take the siglock if we are current and\\n * single threaded,  as no one else can take our signal_struct away, no one\\n * else can  reap the  children to update signal->c* counters, and no one else\\n * can race with the signal-> fields. If we do not take any lock, the\\n * signal-> fields could be read out of order while another thread was just\\n * exiting. So we should  place a read memory barrier when we avoid the lock.\\n * On the writer side,  write memory barrier is implied in  __exit_signal\\n * as __exit_signal releases  the siglock spinlock after updating the signal->\\n * fields. But we don\\'t do this yet to keep things simple.\\n *\\n */\\n\\nstatic void accumulate_thread_rusage(struct task_struct *t, struct rusage *r)\\n{\\n\\tr->ru_nvcsw += t->nvcsw;\\n\\tr->ru_nivcsw += t->nivcsw;\\n\\tr->ru_minflt += t->min_flt;\\n\\tr->ru_majflt += t->maj_flt;\\n\\tr->ru_inblock += task_io_get_inblock(t);\\n\\tr->ru_oublock += task_io_get_oublock(t);\\n}\\n\\nvoid getrusage(struct task_struct *p, int who, struct rusage *r)\\n{\\n\\tstruct task_struct *t;\\n\\tunsigned long flags;\\n\\tu64 tgutime, tgstime, utime, stime;\\n\\tunsigned long maxrss;\\n\\tstruct mm_struct *mm;\\n\\tstruct signal_struct *sig = p->signal;\\n\\tunsigned int seq = 0;\\n\\nretry:\\n\\tmemset(r, 0, sizeof(*r));\\n\\tutime = stime = 0;\\n\\tmaxrss = 0;\\n\\n\\tif (who == RUSAGE_THREAD) {\\n\\t\\ttask_cputime_adjusted(current, &utime, &stime);\\n\\t\\taccumulate_thread_rusage(p, r);\\n\\t\\tmaxrss = sig->maxrss;\\n\\t\\tgoto out_thread;\\n\\t}\\n\\n\\tflags = read_seqbegin_or_lock_irqsave(&sig->stats_lock, &seq);\\n\\n\\tswitch (who) {\\n\\tcase RUSAGE_BOTH:\\n\\tcase RUSAGE_CHILDREN:\\n\\t\\tutime = sig->cutime;\\n\\t\\tstime = sig->cstime;\\n\\t\\tr->ru_nvcsw = sig->cnvcsw;\\n\\t\\tr->ru_nivcsw = sig->cnivcsw;\\n\\t\\tr->ru_minflt = sig->cmin_flt;\\n\\t\\tr->ru_majflt = sig->cmaj_flt;\\n\\t\\tr->ru_inblock = sig->cinblock;\\n\\t\\tr->ru_oublock = sig->coublock;\\n\\t\\tmaxrss = sig->cmaxrss;\\n\\n\\t\\tif (who == RUSAGE_CHILDREN)\\n\\t\\t\\tbreak;\\n\\t\\tfallthrough;\\n\\n\\tcase RUSAGE_SELF:\\n\\t\\tr->ru_nvcsw += sig->nvcsw;\\n\\t\\tr->ru_nivcsw += sig->nivcsw;\\n\\t\\tr->ru_minflt += sig->min_flt;\\n\\t\\tr->ru_majflt += sig->maj_flt;\\n\\t\\tr->ru_inblock += sig->inblock;\\n\\t\\tr->ru_oublock += sig->oublock;\\n\\t\\tif (maxrss < sig->maxrss)\\n\\t\\t\\tmaxrss = sig->maxrss;\\n\\n\\t\\trcu_read_lock();\\n\\t\\t__for_each_thread(sig, t)\\n\\t\\t\\taccumulate_thread_rusage(t, r);\\n\\t\\trcu_read_unlock();\\n\\n\\t\\tbreak;\\n\\n\\tdefault:\\n\\t\\tBUG();\\n\\t}\\n\\n\\tif (need_seqretry(&sig->stats_lock, seq)) {\\n\\t\\tseq = 1;\\n\\t\\tgoto retry;\\n\\t}\\n\\tdone_seqretry_irqrestore(&sig->stats_lock, seq, flags);\\n\\n\\tif (who == RUSAGE_CHILDREN)\\n\\t\\tgoto out_children;\\n\\n\\tthread_group_cputime_adjusted(p, &tgutime, &tgstime);\\n\\tutime += tgutime;\\n\\tstime += tgstime;\\n\\nout_thread:\\n\\tmm = get_task_mm(p);\\n\\tif (mm) {\\n\\t\\tsetmax_mm_hiwater_rss(&maxrss, mm);\\n\\t\\tmmput(mm);\\n\\t}\\n\\nout_children:\\n\\tr->ru_maxrss = maxrss * (PAGE_SIZE / 1024); /* convert pages to KBs */\\n\\tr->ru_utime = ns_to_kernel_old_timeval(utime);\\n\\tr->ru_stime = ns_to_kernel_old_timeval(stime);\\n}\\n\\nSYSCALL_DEFINE2(getrusage, int, who, struct rusage __user *, ru)\\n{\\n\\tstruct rusage r;\\n\\n\\tif (who != RUSAGE_SELF && who != RUSAGE_CHILDREN &&\\n\\t    who != RUSAGE_THREAD)\\n\\t\\treturn -EINVAL;\\n\\n\\tgetrusage(current, who, &r);\\n\\treturn copy_to_user(ru, &r, sizeof(r)) ? -EFAULT : 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE2(getrusage, int, who, struct compat_rusage __user *, ru)\\n{\\n\\tstruct rusage r;\\n\\n\\tif (who != RUSAGE_SELF && who != RUSAGE_CHILDREN &&\\n\\t    who != RUSAGE_THREAD)\\n\\t\\treturn -EINVAL;\\n\\n\\tgetrusage(current, who, &r);\\n\\treturn put_compat_rusage(&r, ru);\\n}\\n#endif\\n\\nSYSCALL_DEFINE1(umask, int, mask)\\n{\\n\\tmask = xchg(&current->fs->umask, mask & S_IRWXUGO);\\n\\treturn mask;\\n}\\n\\nstatic int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)\\n{\\n\\tCLASS(fd, exe)(fd);\\n\\tstruct inode *inode;\\n\\tint err;\\n\\n\\tif (fd_empty(exe))\\n\\t\\treturn -EBADF;\\n\\n\\tinode = file_inode(fd_file(exe));\\n\\n\\t/*\\n\\t * Because the original mm->exe_file points to executable file, make\\n\\t * sure that this one is executable as well, to avoid breaking an\\n\\t * overall picture.\\n\\t */\\n\\tif (!S_ISREG(inode->i_mode) || path_noexec(&fd_file(exe)->f_path))\\n\\t\\treturn -EACCES;\\n\\n\\terr = file_permission(fd_file(exe), MAY_EXEC);\\n\\tif (err)\\n\\t\\treturn err;\\n\\n\\treturn replace_mm_exe_file(mm, fd_file(exe));\\n}\\n\\n/*\\n * Check arithmetic relations of passed addresses.\\n *\\n * WARNING: we don\\'t require any capability here so be very careful\\n * in what is allowed for modification from userspace.\\n */\\nstatic int validate_prctl_map_addr(struct prctl_mm_map *prctl_map)\\n{\\n\\tunsigned long mmap_max_addr = TASK_SIZE;\\n\\tint error = -EINVAL, i;\\n\\n\\tstatic const unsigned char offsets[] = {\\n\\t\\toffsetof(struct prctl_mm_map, start_code),\\n\\t\\toffsetof(struct prctl_mm_map, end_code),\\n\\t\\toffsetof(struct prctl_mm_map, start_data),\\n\\t\\toffsetof(struct prctl_mm_map, end_data),\\n\\t\\toffsetof(struct prctl_mm_map, start_brk),\\n\\t\\toffsetof(struct prctl_mm_map, brk),\\n\\t\\toffsetof(struct prctl_mm_map, start_stack),\\n\\t\\toffsetof(struct prctl_mm_map, arg_start),\\n\\t\\toffsetof(struct prctl_mm_map, arg_end),\\n\\t\\toffsetof(struct prctl_mm_map, env_start),\\n\\t\\toffsetof(struct prctl_mm_map, env_end),\\n\\t};\\n\\n\\t/*\\n\\t * Make sure the members are not somewhere outside\\n\\t * of allowed address space.\\n\\t */\\n\\tfor (i = 0; i < ARRAY_SIZE(offsets); i++) {\\n\\t\\tu64 val = *(u64 *)((char *)prctl_map + offsets[i]);\\n\\n\\t\\tif ((unsigned long)val >= mmap_max_addr ||\\n\\t\\t    (unsigned long)val < mmap_min_addr)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * Make sure the pairs are ordered.\\n\\t */\\n#define __prctl_check_order(__m1, __op, __m2)\\t\\t\\t\\t\\\\\\n\\t((unsigned long)prctl_map->__m1 __op\\t\\t\\t\\t\\\\\\n\\t (unsigned long)prctl_map->__m2) ? 0 : -EINVAL\\n\\terror  = __prctl_check_order(start_code, <, end_code);\\n\\terror |= __prctl_check_order(start_data,<=, end_data);\\n\\terror |= __prctl_check_order(start_brk, <=, brk);\\n\\terror |= __prctl_check_order(arg_start, <=, arg_end);\\n\\terror |= __prctl_check_order(env_start, <=, env_end);\\n\\tif (error)\\n\\t\\tgoto out;\\n#undef __prctl_check_order\\n\\n\\terror = -EINVAL;\\n\\n\\t/*\\n\\t * Neither we should allow to override limits if they set.\\n\\t */\\n\\tif (check_data_rlimit(rlimit(RLIMIT_DATA), prctl_map->brk,\\n\\t\\t\\t      prctl_map->start_brk, prctl_map->end_data,\\n\\t\\t\\t      prctl_map->start_data))\\n\\t\\t\\tgoto out;\\n\\n\\terror = 0;\\nout:\\n\\treturn error;\\n}\\n\\n#ifdef CONFIG_CHECKPOINT_RESTORE\\nstatic int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data_size)\\n{\\n\\tstruct prctl_mm_map prctl_map = { .exe_fd = (u32)-1, };\\n\\tunsigned long user_auxv[AT_VECTOR_SIZE];\\n\\tstruct mm_struct *mm = current->mm;\\n\\tint error;\\n\\n\\tBUILD_BUG_ON(sizeof(user_auxv) != sizeof(mm->saved_auxv));\\n\\tBUILD_BUG_ON(sizeof(struct prctl_mm_map) > 256);\\n\\n\\tif (opt == PR_SET_MM_MAP_SIZE)\\n\\t\\treturn put_user((unsigned int)sizeof(prctl_map),\\n\\t\\t\\t\\t(unsigned int __user *)addr);\\n\\n\\tif (data_size != sizeof(prctl_map))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (copy_from_user(&prctl_map, addr, sizeof(prctl_map)))\\n\\t\\treturn -EFAULT;\\n\\n\\terror = validate_prctl_map_addr(&prctl_map);\\n\\tif (error)\\n\\t\\treturn error;\\n\\n\\tif (prctl_map.auxv_size) {\\n\\t\\t/*\\n\\t\\t * Someone is trying to cheat the auxv vector.\\n\\t\\t */\\n\\t\\tif (!prctl_map.auxv ||\\n\\t\\t\\t\\tprctl_map.auxv_size > sizeof(mm->saved_auxv))\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tmemset(user_auxv, 0, sizeof(user_auxv));\\n\\t\\tif (copy_from_user(user_auxv,\\n\\t\\t\\t\\t   (const void __user *)prctl_map.auxv,\\n\\t\\t\\t\\t   prctl_map.auxv_size))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\t/* Last entry must be AT_NULL as specification requires */\\n\\t\\tuser_auxv[AT_VECTOR_SIZE - 2] = AT_NULL;\\n\\t\\tuser_auxv[AT_VECTOR_SIZE - 1] = AT_NULL;\\n\\t}\\n\\n\\tif (prctl_map.exe_fd != (u32)-1) {\\n\\t\\t/*\\n\\t\\t * Check if the current user is checkpoint/restore capable.\\n\\t\\t * At the time of this writing, it checks for CAP_SYS_ADMIN\\n\\t\\t * or CAP_CHECKPOINT_RESTORE.\\n\\t\\t * Note that a user with access to ptrace can masquerade an\\n\\t\\t * arbitrary program as any executable, even setuid ones.\\n\\t\\t * This may have implications in the tomoyo subsystem.\\n\\t\\t */\\n\\t\\tif (!checkpoint_restore_ns_capable(current_user_ns()))\\n\\t\\t\\treturn -EPERM;\\n\\n\\t\\terror = prctl_set_mm_exe_file(mm, prctl_map.exe_fd);\\n\\t\\tif (error)\\n\\t\\t\\treturn error;\\n\\t}\\n\\n\\t/*\\n\\t * arg_lock protects concurrent updates but we still need mmap_lock for\\n\\t * read to exclude races with sys_brk.\\n\\t */\\n\\tmmap_read_lock(mm);\\n\\n\\t/*\\n\\t * We don\\'t validate if these members are pointing to\\n\\t * real present VMAs because application may have correspond\\n\\t * VMAs already unmapped and kernel uses these members for statistics\\n\\t * output in procfs mostly, except\\n\\t *\\n\\t *  - @start_brk/@brk which are used in do_brk_flags but kernel lookups\\n\\t *    for VMAs when updating these members so anything wrong written\\n\\t *    here cause kernel to swear at userspace program but won\\'t lead\\n\\t *    to any problem in kernel itself\\n\\t */\\n\\n\\tspin_lock(&mm->arg_lock);\\n\\tmm->start_code\\t= prctl_map.start_code;\\n\\tmm->end_code\\t= prctl_map.end_code;\\n\\tmm->start_data\\t= prctl_map.start_data;\\n\\tmm->end_data\\t= prctl_map.end_data;\\n\\tmm->start_brk\\t= prctl_map.start_brk;\\n\\tmm->brk\\t\\t= prctl_map.brk;\\n\\tmm->start_stack\\t= prctl_map.start_stack;\\n\\tmm->arg_start\\t= prctl_map.arg_start;\\n\\tmm->arg_end\\t= prctl_map.arg_end;\\n\\tmm->env_start\\t= prctl_map.env_start;\\n\\tmm->env_end\\t= prctl_map.env_end;\\n\\tspin_unlock(&mm->arg_lock);\\n\\n\\t/*\\n\\t * Note this update of @saved_auxv is lockless thus\\n\\t * if someone reads this member in procfs while we\\'re\\n\\t * updating -- it may get partly updated results. It\\'s\\n\\t * known and acceptable trade off: we leave it as is to\\n\\t * not introduce additional locks here making the kernel\\n\\t * more complex.\\n\\t */\\n\\tif (prctl_map.auxv_size)\\n\\t\\tmemcpy(mm->saved_auxv, user_auxv, sizeof(user_auxv));\\n\\n\\tmmap_read_unlock(mm);\\n\\treturn 0;\\n}\\n#endif /* CONFIG_CHECKPOINT_RESTORE */\\n\\nstatic int prctl_set_auxv(struct mm_struct *mm, unsigned long addr,\\n\\t\\t\\t  unsigned long len)\\n{\\n\\t/*\\n\\t * This doesn\\'t move the auxiliary vector itself since it\\'s pinned to\\n\\t * mm_struct, but it permits filling the vector with new values.  It\\'s\\n\\t * up to the caller to provide sane values here, otherwise userspace\\n\\t * tools which use this vector might be unhappy.\\n\\t */\\n\\tunsigned long user_auxv[AT_VECTOR_SIZE] = {};\\n\\n\\tif (len > sizeof(user_auxv))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (copy_from_user(user_auxv, (const void __user *)addr, len))\\n\\t\\treturn -EFAULT;\\n\\n\\t/* Make sure the last entry is always AT_NULL */\\n\\tuser_auxv[AT_VECTOR_SIZE - 2] = 0;\\n\\tuser_auxv[AT_VECTOR_SIZE - 1] = 0;\\n\\n\\tBUILD_BUG_ON(sizeof(user_auxv) != sizeof(mm->saved_auxv));\\n\\n\\ttask_lock(current);\\n\\tmemcpy(mm->saved_auxv, user_auxv, len);\\n\\ttask_unlock(current);\\n\\n\\treturn 0;\\n}\\n\\nstatic int prctl_set_mm(int opt, unsigned long addr,\\n\\t\\t\\tunsigned long arg4, unsigned long arg5)\\n{\\n\\tstruct mm_struct *mm = current->mm;\\n\\tstruct prctl_mm_map prctl_map = {\\n\\t\\t.auxv = NULL,\\n\\t\\t.auxv_size = 0,\\n\\t\\t.exe_fd = -1,\\n\\t};\\n\\tstruct vm_area_struct *vma;\\n\\tint error;\\n\\n\\tif (arg5 || (arg4 && (opt != PR_SET_MM_AUXV &&\\n\\t\\t\\t      opt != PR_SET_MM_MAP &&\\n\\t\\t\\t      opt != PR_SET_MM_MAP_SIZE)))\\n\\t\\treturn -EINVAL;\\n\\n#ifdef CONFIG_CHECKPOINT_RESTORE\\n\\tif (opt == PR_SET_MM_MAP || opt == PR_SET_MM_MAP_SIZE)\\n\\t\\treturn prctl_set_mm_map(opt, (const void __user *)addr, arg4);\\n#endif\\n\\n\\tif (!capable(CAP_SYS_RESOURCE))\\n\\t\\treturn -EPERM;\\n\\n\\tif (opt == PR_SET_MM_EXE_FILE)\\n\\t\\treturn prctl_set_mm_exe_file(mm, (unsigned int)addr);\\n\\n\\tif (opt == PR_SET_MM_AUXV)\\n\\t\\treturn prctl_set_auxv(mm, addr, arg4);\\n\\n\\tif (addr >= TASK_SIZE || addr < mmap_min_addr)\\n\\t\\treturn -EINVAL;\\n\\n\\terror = -EINVAL;\\n\\n\\t/*\\n\\t * arg_lock protects concurrent updates of arg boundaries, we need\\n\\t * mmap_lock for a) concurrent sys_brk, b) finding VMA for addr\\n\\t * validation.\\n\\t */\\n\\tmmap_read_lock(mm);\\n\\tvma = find_vma(mm, addr);\\n\\n\\tspin_lock(&mm->arg_lock);\\n\\tprctl_map.start_code\\t= mm->start_code;\\n\\tprctl_map.end_code\\t= mm->end_code;\\n\\tprctl_map.start_data\\t= mm->start_data;\\n\\tprctl_map.end_data\\t= mm->end_data;\\n\\tprctl_map.start_brk\\t= mm->start_brk;\\n\\tprctl_map.brk\\t\\t= mm->brk;\\n\\tprctl_map.start_stack\\t= mm->start_stack;\\n\\tprctl_map.arg_start\\t= mm->arg_start;\\n\\tprctl_map.arg_end\\t= mm->arg_end;\\n\\tprctl_map.env_start\\t= mm->env_start;\\n\\tprctl_map.env_end\\t= mm->env_end;\\n\\n\\tswitch (opt) {\\n\\tcase PR_SET_MM_START_CODE:\\n\\t\\tprctl_map.start_code = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_END_CODE:\\n\\t\\tprctl_map.end_code = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_START_DATA:\\n\\t\\tprctl_map.start_data = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_END_DATA:\\n\\t\\tprctl_map.end_data = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_START_STACK:\\n\\t\\tprctl_map.start_stack = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_START_BRK:\\n\\t\\tprctl_map.start_brk = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_BRK:\\n\\t\\tprctl_map.brk = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_ARG_START:\\n\\t\\tprctl_map.arg_start = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_ARG_END:\\n\\t\\tprctl_map.arg_end = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_ENV_START:\\n\\t\\tprctl_map.env_start = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_ENV_END:\\n\\t\\tprctl_map.env_end = addr;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tgoto out;\\n\\t}\\n\\n\\terror = validate_prctl_map_addr(&prctl_map);\\n\\tif (error)\\n\\t\\tgoto out;\\n\\n\\tswitch (opt) {\\n\\t/*\\n\\t * If command line arguments and environment\\n\\t * are placed somewhere else on stack, we can\\n\\t * set them up here, ARG_START/END to setup\\n\\t * command line arguments and ENV_START/END\\n\\t * for environment.\\n\\t */\\n\\tcase PR_SET_MM_START_STACK:\\n\\tcase PR_SET_MM_ARG_START:\\n\\tcase PR_SET_MM_ARG_END:\\n\\tcase PR_SET_MM_ENV_START:\\n\\tcase PR_SET_MM_ENV_END:\\n\\t\\tif (!vma) {\\n\\t\\t\\terror = -EFAULT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\n\\tmm->start_code\\t= prctl_map.start_code;\\n\\tmm->end_code\\t= prctl_map.end_code;\\n\\tmm->start_data\\t= prctl_map.start_data;\\n\\tmm->end_data\\t= prctl_map.end_data;\\n\\tmm->start_brk\\t= prctl_map.start_brk;\\n\\tmm->brk\\t\\t= prctl_map.brk;\\n\\tmm->start_stack\\t= prctl_map.start_stack;\\n\\tmm->arg_start\\t= prctl_map.arg_start;\\n\\tmm->arg_end\\t= prctl_map.arg_end;\\n\\tmm->env_start\\t= prctl_map.env_start;\\n\\tmm->env_end\\t= prctl_map.env_end;\\n\\n\\terror = 0;\\nout:\\n\\tspin_unlock(&mm->arg_lock);\\n\\tmmap_read_unlock(mm);\\n\\treturn error;\\n}\\n\\n#ifdef CONFIG_CHECKPOINT_RESTORE\\nstatic int prctl_get_tid_address(struct task_struct *me, int __user * __user *tid_addr)\\n{\\n\\treturn put_user(me->clear_child_tid, tid_addr);\\n}\\n#else\\nstatic int prctl_get_tid_address(struct task_struct *me, int __user * __user *tid_addr)\\n{\\n\\treturn -EINVAL;\\n}\\n#endif\\n\\nstatic int propagate_has_child_subreaper(struct task_struct *p, void *data)\\n{\\n\\t/*\\n\\t * If task has has_child_subreaper - all its descendants\\n\\t * already have these flag too and new descendants will\\n\\t * inherit it on fork, skip them.\\n\\t *\\n\\t * If we\\'ve found child_reaper - skip descendants in\\n\\t * it\\'s subtree as they will never get out pidns.\\n\\t */\\n\\tif (p->signal->has_child_subreaper ||\\n\\t    is_child_reaper(task_pid(p)))\\n\\t\\treturn 0;\\n\\n\\tp->signal->has_child_subreaper = 1;\\n\\treturn 1;\\n}\\n\\nint __weak arch_prctl_spec_ctrl_get(struct task_struct *t, unsigned long which)\\n{\\n\\treturn -EINVAL;\\n}\\n\\nint __weak arch_prctl_spec_ctrl_set(struct task_struct *t, unsigned long which,\\n\\t\\t\\t\\t    unsigned long ctrl)\\n{\\n\\treturn -EINVAL;\\n}\\n\\nint __weak arch_get_shadow_stack_status(struct task_struct *t, unsigned long __user *status)\\n{\\n\\treturn -EINVAL;\\n}\\n\\nint __weak arch_set_shadow_stack_status(struct task_struct *t, unsigned long status)\\n{\\n\\treturn -EINVAL;\\n}\\n\\nint __weak arch_lock_shadow_stack_status(struct task_struct *t, unsigned long status)\\n{\\n\\treturn -EINVAL;\\n}\\n\\n#define PR_IO_FLUSHER (PF_MEMALLOC_NOIO | PF_LOCAL_THROTTLE)\\n\\n#ifdef CONFIG_ANON_VMA_NAME\\n\\n#define ANON_VMA_NAME_MAX_LEN\\t\\t80\\n#define ANON_VMA_NAME_INVALID_CHARS\\t\"\\\\\\\\`$[]\"\\n\\nstatic inline bool is_valid_name_char(char ch)\\n{\\n\\t/* printable ascii characters, excluding ANON_VMA_NAME_INVALID_CHARS */\\n\\treturn ch > 0x1f && ch < 0x7f &&\\n\\t\\t!strchr(ANON_VMA_NAME_INVALID_CHARS, ch);\\n}\\n\\nstatic int prctl_set_vma(unsigned long opt, unsigned long addr,\\n\\t\\t\\t unsigned long size, unsigned long arg)\\n{\\n\\tstruct mm_struct *mm = current->mm;\\n\\tconst char __user *uname;\\n\\tstruct anon_vma_name *anon_name = NULL;\\n\\tint error;\\n\\n\\tswitch (opt) {\\n\\tcase PR_SET_VMA_ANON_NAME:\\n\\t\\tuname = (const char __user *)arg;\\n\\t\\tif (uname) {\\n\\t\\t\\tchar *name, *pch;\\n\\n\\t\\t\\tname = strndup_user(uname, ANON_VMA_NAME_MAX_LEN);\\n\\t\\t\\tif (IS_ERR(name))\\n\\t\\t\\t\\treturn PTR_ERR(name);\\n\\n\\t\\t\\tfor (pch = name; *pch != \\'\\\\0\\'; pch++) {\\n\\t\\t\\t\\tif (!is_valid_name_char(*pch)) {\\n\\t\\t\\t\\t\\tkfree(name);\\n\\t\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\t/* anon_vma has its own copy */\\n\\t\\t\\tanon_name = anon_vma_name_alloc(name);\\n\\t\\t\\tkfree(name);\\n\\t\\t\\tif (!anon_name)\\n\\t\\t\\t\\treturn -ENOMEM;\\n\\n\\t\\t}\\n\\n\\t\\tmmap_write_lock(mm);\\n\\t\\terror = madvise_set_anon_name(mm, addr, size, anon_name);\\n\\t\\tmmap_write_unlock(mm);\\n\\t\\tanon_vma_name_put(anon_name);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\terror = -EINVAL;\\n\\t}\\n\\n\\treturn error;\\n}\\n\\n#else /* CONFIG_ANON_VMA_NAME */\\nstatic int prctl_set_vma(unsigned long opt, unsigned long start,\\n\\t\\t\\t unsigned long size, unsigned long arg)\\n{\\n\\treturn -EINVAL;\\n}\\n#endif /* CONFIG_ANON_VMA_NAME */\\n\\nstatic inline unsigned long get_current_mdwe(void)\\n{\\n\\tunsigned long ret = 0;\\n\\n\\tif (test_bit(MMF_HAS_MDWE, &current->mm->flags))\\n\\t\\tret |= PR_MDWE_REFUSE_EXEC_GAIN;\\n\\tif (test_bit(MMF_HAS_MDWE_NO_INHERIT, &current->mm->flags))\\n\\t\\tret |= PR_MDWE_NO_INHERIT;\\n\\n\\treturn ret;\\n}\\n\\nstatic inline int prctl_set_mdwe(unsigned long bits, unsigned long arg3,\\n\\t\\t\\t\\t unsigned long arg4, unsigned long arg5)\\n{\\n\\tunsigned long current_bits;\\n\\n\\tif (arg3 || arg4 || arg5)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (bits & ~(PR_MDWE_REFUSE_EXEC_GAIN | PR_MDWE_NO_INHERIT))\\n\\t\\treturn -EINVAL;\\n\\n\\t/* NO_INHERIT only makes sense with REFUSE_EXEC_GAIN */\\n\\tif (bits & PR_MDWE_NO_INHERIT && !(bits & PR_MDWE_REFUSE_EXEC_GAIN))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * EOPNOTSUPP might be more appropriate here in principle, but\\n\\t * existing userspace depends on EINVAL specifically.\\n\\t */\\n\\tif (!arch_memory_deny_write_exec_supported())\\n\\t\\treturn -EINVAL;\\n\\n\\tcurrent_bits = get_current_mdwe();\\n\\tif (current_bits && current_bits != bits)\\n\\t\\treturn -EPERM; /* Cannot unset the flags */\\n\\n\\tif (bits & PR_MDWE_NO_INHERIT)\\n\\t\\tset_bit(MMF_HAS_MDWE_NO_INHERIT, &current->mm->flags);\\n\\tif (bits & PR_MDWE_REFUSE_EXEC_GAIN)\\n\\t\\tset_bit(MMF_HAS_MDWE, &current->mm->flags);\\n\\n\\treturn 0;\\n}\\n\\nstatic inline int prctl_get_mdwe(unsigned long arg2, unsigned long arg3,\\n\\t\\t\\t\\t unsigned long arg4, unsigned long arg5)\\n{\\n\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\treturn -EINVAL;\\n\\treturn get_current_mdwe();\\n}\\n\\nstatic int prctl_get_auxv(void __user *addr, unsigned long len)\\n{\\n\\tstruct mm_struct *mm = current->mm;\\n\\tunsigned long size = min_t(unsigned long, sizeof(mm->saved_auxv), len);\\n\\n\\tif (size && copy_to_user(addr, mm->saved_auxv, size))\\n\\t\\treturn -EFAULT;\\n\\treturn sizeof(mm->saved_auxv);\\n}\\n\\nSYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,\\n\\t\\tunsigned long, arg4, unsigned long, arg5)\\n{\\n\\tstruct task_struct *me = current;\\n\\tunsigned char comm[sizeof(me->comm)];\\n\\tlong error;\\n\\n\\terror = security_task_prctl(option, arg2, arg3, arg4, arg5);\\n\\tif (error != -ENOSYS)\\n\\t\\treturn error;\\n\\n\\terror = 0;\\n\\tswitch (option) {\\n\\tcase PR_SET_PDEATHSIG:\\n\\t\\tif (!valid_signal(arg2)) {\\n\\t\\t\\terror = -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tme->pdeath_signal = arg2;\\n\\t\\tbreak;\\n\\tcase PR_GET_PDEATHSIG:\\n\\t\\terror = put_user(me->pdeath_signal, (int __user *)arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_DUMPABLE:\\n\\t\\terror = get_dumpable(me->mm);\\n\\t\\tbreak;\\n\\tcase PR_SET_DUMPABLE:\\n\\t\\tif (arg2 != SUID_DUMP_DISABLE && arg2 != SUID_DUMP_USER) {\\n\\t\\t\\terror = -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tset_dumpable(me->mm, arg2);\\n\\t\\tbreak;\\n\\n\\tcase PR_SET_UNALIGN:\\n\\t\\terror = SET_UNALIGN_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_UNALIGN:\\n\\t\\terror = GET_UNALIGN_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_FPEMU:\\n\\t\\terror = SET_FPEMU_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_FPEMU:\\n\\t\\terror = GET_FPEMU_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_FPEXC:\\n\\t\\terror = SET_FPEXC_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_FPEXC:\\n\\t\\terror = GET_FPEXC_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_TIMING:\\n\\t\\terror = PR_TIMING_STATISTICAL;\\n\\t\\tbreak;\\n\\tcase PR_SET_TIMING:\\n\\t\\tif (arg2 != PR_TIMING_STATISTICAL)\\n\\t\\t\\terror = -EINVAL;\\n\\t\\tbreak;\\n\\tcase PR_SET_NAME:\\n\\t\\tcomm[sizeof(me->comm) - 1] = 0;\\n\\t\\tif (strncpy_from_user(comm, (char __user *)arg2,\\n\\t\\t\\t\\t      sizeof(me->comm) - 1) < 0)\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tset_task_comm(me, comm);\\n\\t\\tproc_comm_connector(me);\\n\\t\\tbreak;\\n\\tcase PR_GET_NAME:\\n\\t\\tget_task_comm(comm, me);\\n\\t\\tif (copy_to_user((char __user *)arg2, comm, sizeof(comm)))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tbreak;\\n\\tcase PR_GET_ENDIAN:\\n\\t\\terror = GET_ENDIAN(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_ENDIAN:\\n\\t\\terror = SET_ENDIAN(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_SECCOMP:\\n\\t\\terror = prctl_get_seccomp();\\n\\t\\tbreak;\\n\\tcase PR_SET_SECCOMP:\\n\\t\\terror = prctl_set_seccomp(arg2, (char __user *)arg3);\\n\\t\\tbreak;\\n\\tcase PR_GET_TSC:\\n\\t\\terror = GET_TSC_CTL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_TSC:\\n\\t\\terror = SET_TSC_CTL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_TASK_PERF_EVENTS_DISABLE:\\n\\t\\terror = perf_event_task_disable();\\n\\t\\tbreak;\\n\\tcase PR_TASK_PERF_EVENTS_ENABLE:\\n\\t\\terror = perf_event_task_enable();\\n\\t\\tbreak;\\n\\tcase PR_GET_TIMERSLACK:\\n\\t\\tif (current->timer_slack_ns > ULONG_MAX)\\n\\t\\t\\terror = ULONG_MAX;\\n\\t\\telse\\n\\t\\t\\terror = current->timer_slack_ns;\\n\\t\\tbreak;\\n\\tcase PR_SET_TIMERSLACK:\\n\\t\\tif (rt_or_dl_task_policy(current))\\n\\t\\t\\tbreak;\\n\\t\\tif (arg2 <= 0)\\n\\t\\t\\tcurrent->timer_slack_ns =\\n\\t\\t\\t\\t\\tcurrent->default_timer_slack_ns;\\n\\t\\telse\\n\\t\\t\\tcurrent->timer_slack_ns = arg2;\\n\\t\\tbreak;\\n\\tcase PR_MCE_KILL:\\n\\t\\tif (arg4 | arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tswitch (arg2) {\\n\\t\\tcase PR_MCE_KILL_CLEAR:\\n\\t\\t\\tif (arg3 != 0)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tcurrent->flags &= ~PF_MCE_PROCESS;\\n\\t\\t\\tbreak;\\n\\t\\tcase PR_MCE_KILL_SET:\\n\\t\\t\\tcurrent->flags |= PF_MCE_PROCESS;\\n\\t\\t\\tif (arg3 == PR_MCE_KILL_EARLY)\\n\\t\\t\\t\\tcurrent->flags |= PF_MCE_EARLY;\\n\\t\\t\\telse if (arg3 == PR_MCE_KILL_LATE)\\n\\t\\t\\t\\tcurrent->flags &= ~PF_MCE_EARLY;\\n\\t\\t\\telse if (arg3 == PR_MCE_KILL_DEFAULT)\\n\\t\\t\\t\\tcurrent->flags &=\\n\\t\\t\\t\\t\\t\\t~(PF_MCE_EARLY|PF_MCE_PROCESS);\\n\\t\\t\\telse\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase PR_MCE_KILL_GET:\\n\\t\\tif (arg2 | arg3 | arg4 | arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (current->flags & PF_MCE_PROCESS)\\n\\t\\t\\terror = (current->flags & PF_MCE_EARLY) ?\\n\\t\\t\\t\\tPR_MCE_KILL_EARLY : PR_MCE_KILL_LATE;\\n\\t\\telse\\n\\t\\t\\terror = PR_MCE_KILL_DEFAULT;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM:\\n\\t\\terror = prctl_set_mm(arg2, arg3, arg4, arg5);\\n\\t\\tbreak;\\n\\tcase PR_GET_TID_ADDRESS:\\n\\t\\terror = prctl_get_tid_address(me, (int __user * __user *)arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_CHILD_SUBREAPER:\\n\\t\\tme->signal->is_child_subreaper = !!arg2;\\n\\t\\tif (!arg2)\\n\\t\\t\\tbreak;\\n\\n\\t\\twalk_process_tree(me, propagate_has_child_subreaper, NULL);\\n\\t\\tbreak;\\n\\tcase PR_GET_CHILD_SUBREAPER:\\n\\t\\terror = put_user(me->signal->is_child_subreaper,\\n\\t\\t\\t\\t (int __user *)arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_NO_NEW_PRIVS:\\n\\t\\tif (arg2 != 1 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\ttask_set_no_new_privs(current);\\n\\t\\tbreak;\\n\\tcase PR_GET_NO_NEW_PRIVS:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\treturn task_no_new_privs(current) ? 1 : 0;\\n\\tcase PR_GET_THP_DISABLE:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = !!test_bit(MMF_DISABLE_THP, &me->mm->flags);\\n\\t\\tbreak;\\n\\tcase PR_SET_THP_DISABLE:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (mmap_write_lock_killable(me->mm))\\n\\t\\t\\treturn -EINTR;\\n\\t\\tif (arg2)\\n\\t\\t\\tset_bit(MMF_DISABLE_THP, &me->mm->flags);\\n\\t\\telse\\n\\t\\t\\tclear_bit(MMF_DISABLE_THP, &me->mm->flags);\\n\\t\\tmmap_write_unlock(me->mm);\\n\\t\\tbreak;\\n\\tcase PR_MPX_ENABLE_MANAGEMENT:\\n\\tcase PR_MPX_DISABLE_MANAGEMENT:\\n\\t\\t/* No longer implemented: */\\n\\t\\treturn -EINVAL;\\n\\tcase PR_SET_FP_MODE:\\n\\t\\terror = SET_FP_MODE(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_FP_MODE:\\n\\t\\terror = GET_FP_MODE(me);\\n\\t\\tbreak;\\n\\tcase PR_SVE_SET_VL:\\n\\t\\terror = SVE_SET_VL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_SVE_GET_VL:\\n\\t\\terror = SVE_GET_VL();\\n\\t\\tbreak;\\n\\tcase PR_SME_SET_VL:\\n\\t\\terror = SME_SET_VL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_SME_GET_VL:\\n\\t\\terror = SME_GET_VL();\\n\\t\\tbreak;\\n\\tcase PR_GET_SPECULATION_CTRL:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = arch_prctl_spec_ctrl_get(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_SPECULATION_CTRL:\\n\\t\\tif (arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = arch_prctl_spec_ctrl_set(me, arg2, arg3);\\n\\t\\tbreak;\\n\\tcase PR_PAC_RESET_KEYS:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = PAC_RESET_KEYS(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_PAC_SET_ENABLED_KEYS:\\n\\t\\tif (arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = PAC_SET_ENABLED_KEYS(me, arg2, arg3);\\n\\t\\tbreak;\\n\\tcase PR_PAC_GET_ENABLED_KEYS:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = PAC_GET_ENABLED_KEYS(me);\\n\\t\\tbreak;\\n\\tcase PR_SET_TAGGED_ADDR_CTRL:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = SET_TAGGED_ADDR_CTRL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_TAGGED_ADDR_CTRL:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = GET_TAGGED_ADDR_CTRL();\\n\\t\\tbreak;\\n\\tcase PR_SET_IO_FLUSHER:\\n\\t\\tif (!capable(CAP_SYS_RESOURCE))\\n\\t\\t\\treturn -EPERM;\\n\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tif (arg2 == 1)\\n\\t\\t\\tcurrent->flags |= PR_IO_FLUSHER;\\n\\t\\telse if (!arg2)\\n\\t\\t\\tcurrent->flags &= ~PR_IO_FLUSHER;\\n\\t\\telse\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tbreak;\\n\\tcase PR_GET_IO_FLUSHER:\\n\\t\\tif (!capable(CAP_SYS_RESOURCE))\\n\\t\\t\\treturn -EPERM;\\n\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\terror = (current->flags & PR_IO_FLUSHER) == PR_IO_FLUSHER;\\n\\t\\tbreak;\\n\\tcase PR_SET_SYSCALL_USER_DISPATCH:\\n\\t\\terror = set_syscall_user_dispatch(arg2, arg3, arg4,\\n\\t\\t\\t\\t\\t\\t  (char __user *) arg5);\\n\\t\\tbreak;\\n#ifdef CONFIG_SCHED_CORE\\n\\tcase PR_SCHED_CORE:\\n\\t\\terror = sched_core_share_pid(arg2, arg3, arg4, arg5);\\n\\t\\tbreak;\\n#endif\\n\\tcase PR_SET_MDWE:\\n\\t\\terror = prctl_set_mdwe(arg2, arg3, arg4, arg5);\\n\\t\\tbreak;\\n\\tcase PR_GET_MDWE:\\n\\t\\terror = prctl_get_mdwe(arg2, arg3, arg4, arg5);\\n\\t\\tbreak;\\n\\tcase PR_PPC_GET_DEXCR:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = PPC_GET_DEXCR_ASPECT(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_PPC_SET_DEXCR:\\n\\t\\tif (arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = PPC_SET_DEXCR_ASPECT(me, arg2, arg3);\\n\\t\\tbreak;\\n\\tcase PR_SET_VMA:\\n\\t\\terror = prctl_set_vma(arg2, arg3, arg4, arg5);\\n\\t\\tbreak;\\n\\tcase PR_GET_AUXV:\\n\\t\\tif (arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = prctl_get_auxv((void __user *)arg2, arg3);\\n\\t\\tbreak;\\n#ifdef CONFIG_KSM\\n\\tcase PR_SET_MEMORY_MERGE:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (mmap_write_lock_killable(me->mm))\\n\\t\\t\\treturn -EINTR;\\n\\n\\t\\tif (arg2)\\n\\t\\t\\terror = ksm_enable_merge_any(me->mm);\\n\\t\\telse\\n\\t\\t\\terror = ksm_disable_merge_any(me->mm);\\n\\t\\tmmap_write_unlock(me->mm);\\n\\t\\tbreak;\\n\\tcase PR_GET_MEMORY_MERGE:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\terror = !!test_bit(MMF_VM_MERGE_ANY, &me->mm->flags);\\n\\t\\tbreak;\\n#endif\\n\\tcase PR_RISCV_V_SET_CONTROL:\\n\\t\\terror = RISCV_V_SET_CONTROL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_RISCV_V_GET_CONTROL:\\n\\t\\terror = RISCV_V_GET_CONTROL();\\n\\t\\tbreak;\\n\\tcase PR_RISCV_SET_ICACHE_FLUSH_CTX:\\n\\t\\terror = RISCV_SET_ICACHE_FLUSH_CTX(arg2, arg3);\\n\\t\\tbreak;\\n\\tcase PR_GET_SHADOW_STACK_STATUS:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = arch_get_shadow_stack_status(me, (unsigned long __user *) arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_SHADOW_STACK_STATUS:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = arch_set_shadow_stack_status(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_LOCK_SHADOW_STACK_STATUS:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = arch_lock_shadow_stack_status(me, arg2);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\terror = -EINVAL;\\n\\t\\tbreak;\\n\\t}\\n\\treturn error;\\n}\\n\\nSYSCALL_DEFINE3(getcpu, unsigned __user *, cpup, unsigned __user *, nodep,\\n\\t\\tstruct getcpu_cache __user *, unused)\\n{\\n\\tint err = 0;\\n\\tint cpu = raw_smp_processor_id();\\n\\n\\tif (cpup)\\n\\t\\terr |= put_user(cpu, cpup);\\n\\tif (nodep)\\n\\t\\terr |= put_user(cpu_to_node(cpu), nodep);\\n\\treturn err ? -EFAULT : 0;\\n}\\n\\n/**\\n * do_sysinfo - fill in sysinfo struct\\n * @info: pointer to buffer to fill\\n */\\nstatic int do_sysinfo(struct sysinfo *info)\\n{\\n\\tunsigned long mem_total, sav_total;\\n\\tunsigned int mem_unit, bitcount;\\n\\tstruct timespec64 tp;\\n\\n\\tmemset(info, 0, sizeof(struct sysinfo));\\n\\n\\tktime_get_boottime_ts64(&tp);\\n\\ttimens_add_boottime(&tp);\\n\\tinfo->uptime = tp.tv_sec + (tp.tv_nsec ? 1 : 0);\\n\\n\\tget_avenrun(info->loads, 0, SI_LOAD_SHIFT - FSHIFT);\\n\\n\\tinfo->procs = nr_threads;\\n\\n\\tsi_meminfo(info);\\n\\tsi_swapinfo(info);\\n\\n\\t/*\\n\\t * If the sum of all the available memory (i.e. ram + swap)\\n\\t * is less than can be stored in a 32 bit unsigned long then\\n\\t * we can be binary compatible with 2.2.x kernels.  If not,\\n\\t * well, in that case 2.2.x was broken anyways...\\n\\t *\\n\\t *  -Erik Andersen <andersee@debian.org>\\n\\t */\\n\\n\\tmem_total = info->totalram + info->totalswap;\\n\\tif (mem_total < info->totalram || mem_total < info->totalswap)\\n\\t\\tgoto out;\\n\\tbitcount = 0;\\n\\tmem_unit = info->mem_unit;\\n\\twhile (mem_unit > 1) {\\n\\t\\tbitcount++;\\n\\t\\tmem_unit >>= 1;\\n\\t\\tsav_total = mem_total;\\n\\t\\tmem_total <<= 1;\\n\\t\\tif (mem_total < sav_total)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * If mem_total did not overflow, multiply all memory values by\\n\\t * info->mem_unit and set it to 1.  This leaves things compatible\\n\\t * with 2.2.x, and also retains compatibility with earlier 2.4.x\\n\\t * kernels...\\n\\t */\\n\\n\\tinfo->mem_unit = 1;\\n\\tinfo->totalram <<= bitcount;\\n\\tinfo->freeram <<= bitcount;\\n\\tinfo->sharedram <<= bitcount;\\n\\tinfo->bufferram <<= bitcount;\\n\\tinfo->totalswap <<= bitcount;\\n\\tinfo->freeswap <<= bitcount;\\n\\tinfo->totalhigh <<= bitcount;\\n\\tinfo->freehigh <<= bitcount;\\n\\nout:\\n\\treturn 0;\\n}\\n\\nSYSCALL_DEFINE1(sysinfo, struct sysinfo __user *, info)\\n{\\n\\tstruct sysinfo val;\\n\\n\\tdo_sysinfo(&val);\\n\\n\\tif (copy_to_user(info, &val, sizeof(struct sysinfo)))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nstruct compat_sysinfo {\\n\\ts32 uptime;\\n\\tu32 loads[3];\\n\\tu32 totalram;\\n\\tu32 freeram;\\n\\tu32 sharedram;\\n\\tu32 bufferram;\\n\\tu32 totalswap;\\n\\tu32 freeswap;\\n\\tu16 procs;\\n\\tu16 pad;\\n\\tu32 totalhigh;\\n\\tu32 freehigh;\\n\\tu32 mem_unit;\\n\\tchar _f[20-2*sizeof(u32)-sizeof(int)];\\n};\\n\\nCOMPAT_SYSCALL_DEFINE1(sysinfo, struct compat_sysinfo __user *, info)\\n{\\n\\tstruct sysinfo s;\\n\\tstruct compat_sysinfo s_32;\\n\\n\\tdo_sysinfo(&s);\\n\\n\\t/* Check to see if any memory value is too large for 32-bit and scale\\n\\t *  down if needed\\n\\t */\\n\\tif (upper_32_bits(s.totalram) || upper_32_bits(s.totalswap)) {\\n\\t\\tint bitcount = 0;\\n\\n\\t\\twhile (s.mem_unit < PAGE_SIZE) {\\n\\t\\t\\ts.mem_unit <<= 1;\\n\\t\\t\\tbitcount++;\\n\\t\\t}\\n\\n\\t\\ts.totalram >>= bitcount;\\n\\t\\ts.freeram >>= bitcount;\\n\\t\\ts.sharedram >>= bitcount;\\n\\t\\ts.bufferram >>= bitcount;\\n\\t\\ts.totalswap >>= bitcount;\\n\\t\\ts.freeswap >>= bitcount;\\n\\t\\ts.totalhigh >>= bitcount;\\n\\t\\ts.freehigh >>= bitcount;\\n\\t}\\n\\n\\tmemset(&s_32, 0, sizeof(s_32));\\n\\ts_32.uptime = s.uptime;\\n\\ts_32.loads[0] = s.loads[0];\\n\\ts_32.loads[1] = s.loads[1];\\n\\ts_32.loads[2] = s.loads[2];\\n\\ts_32.totalram = s.totalram;\\n\\ts_32.freeram = s.freeram;\\n\\ts_32.sharedram = s.sharedram;\\n\\ts_32.bufferram = s.bufferram;\\n\\ts_32.totalswap = s.totalswap;\\n\\ts_32.freeswap = s.freeswap;\\n\\ts_32.procs = s.procs;\\n\\ts_32.totalhigh = s.totalhigh;\\n\\ts_32.freehigh = s.freehigh;\\n\\ts_32.mem_unit = s.mem_unit;\\n\\tif (copy_to_user(info, &s_32, sizeof(s_32)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n#endif /* CONFIG_COMPAT */\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/*\\n * Helpers for initial module or kernel cmdline parsing\\n * Copyright (C) 2001 Rusty Russell.\\n */\\n#include <linux/ctype.h>\\n#include <linux/device.h>\\n#include <linux/err.h>\\n#include <linux/errno.h>\\n#include <linux/kernel.h>\\n#include <linux/kstrtox.h>\\n#include <linux/module.h>\\n#include <linux/moduleparam.h>\\n#include <linux/overflow.h>\\n#include <linux/security.h>\\n#include <linux/slab.h>\\n#include <linux/string.h>\\n\\n#ifdef CONFIG_SYSFS\\n/* Protects all built-in parameters, modules use their own param_lock */\\nstatic DEFINE_MUTEX(param_lock);\\n\\n/* Use the module\\'s mutex, or if built-in use the built-in mutex */\\n#ifdef CONFIG_MODULES\\n#define KPARAM_MUTEX(mod)\\t((mod) ? &(mod)->param_lock : &param_lock)\\n#else\\n#define KPARAM_MUTEX(mod)\\t(&param_lock)\\n#endif\\n\\nstatic inline void check_kparam_locked(struct module *mod)\\n{\\n\\tBUG_ON(!mutex_is_locked(KPARAM_MUTEX(mod)));\\n}\\n#else\\nstatic inline void check_kparam_locked(struct module *mod)\\n{\\n}\\n#endif /* !CONFIG_SYSFS */\\n\\n/* This just allows us to keep track of which parameters are kmalloced. */\\nstruct kmalloced_param {\\n\\tstruct list_head list;\\n\\tchar val[];\\n};\\nstatic LIST_HEAD(kmalloced_params);\\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\\n\\nstatic void *kmalloc_parameter(unsigned int size)\\n{\\n\\tstruct kmalloced_param *p;\\n\\n\\tp = kmalloc(size_add(sizeof(*p), size), GFP_KERNEL);\\n\\tif (!p)\\n\\t\\treturn NULL;\\n\\n\\tspin_lock(&kmalloced_params_lock);\\n\\tlist_add(&p->list, &kmalloced_params);\\n\\tspin_unlock(&kmalloced_params_lock);\\n\\n\\treturn p->val;\\n}\\n\\n/* Does nothing if parameter wasn\\'t kmalloced above. */\\nstatic void maybe_kfree_parameter(void *param)\\n{\\n\\tstruct kmalloced_param *p;\\n\\n\\tspin_lock(&kmalloced_params_lock);\\n\\tlist_for_each_entry(p, &kmalloced_params, list) {\\n\\t\\tif (p->val == param) {\\n\\t\\t\\tlist_del(&p->list);\\n\\t\\t\\tkfree(p);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\tspin_unlock(&kmalloced_params_lock);\\n}\\n\\nstatic char dash2underscore(char c)\\n{\\n\\tif (c == \\'-\\')\\n\\t\\treturn \\'_\\';\\n\\treturn c;\\n}\\n\\nbool parameqn(const char *a, const char *b, size_t n)\\n{\\n\\tsize_t i;\\n\\n\\tfor (i = 0; i < n; i++) {\\n\\t\\tif (dash2underscore(a[i]) != dash2underscore(b[i]))\\n\\t\\t\\treturn false;\\n\\t}\\n\\treturn true;\\n}\\n\\nbool parameq(const char *a, const char *b)\\n{\\n\\treturn parameqn(a, b, strlen(a)+1);\\n}\\n\\nstatic bool param_check_unsafe(const struct kernel_param *kp)\\n{\\n\\tif (kp->flags & KERNEL_PARAM_FL_HWPARAM &&\\n\\t    security_locked_down(LOCKDOWN_MODULE_PARAMETERS))\\n\\t\\treturn false;\\n\\n\\tif (kp->flags & KERNEL_PARAM_FL_UNSAFE) {\\n\\t\\tpr_notice(\"Setting dangerous option %s - tainting kernel\\\\n\",\\n\\t\\t\\t  kp->name);\\n\\t\\tadd_taint(TAINT_USER, LOCKDEP_STILL_OK);\\n\\t}\\n\\n\\treturn true;\\n}\\n\\nstatic int parse_one(char *param,\\n\\t\\t     char *val,\\n\\t\\t     const char *doing,\\n\\t\\t     const struct kernel_param *params,\\n\\t\\t     unsigned num_params,\\n\\t\\t     s16 min_level,\\n\\t\\t     s16 max_level,\\n\\t\\t     void *arg, parse_unknown_fn handle_unknown)\\n{\\n\\tunsigned int i;\\n\\tint err;\\n\\n\\t/* Find parameter */\\n\\tfor (i = 0; i < num_params; i++) {\\n\\t\\tif (parameq(param, params[i].name)) {\\n\\t\\t\\tif (params[i].level < min_level\\n\\t\\t\\t    || params[i].level > max_level)\\n\\t\\t\\t\\treturn 0;\\n\\t\\t\\t/* No one handled NULL, so do it here. */\\n\\t\\t\\tif (!val &&\\n\\t\\t\\t    !(params[i].ops->flags & KERNEL_PARAM_OPS_FL_NOARG))\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tpr_debug(\"handling %s with %p\\\\n\", param,\\n\\t\\t\\t\\tparams[i].ops->set);\\n\\t\\t\\tkernel_param_lock(params[i].mod);\\n\\t\\t\\tif (param_check_unsafe(&params[i]))\\n\\t\\t\\t\\terr = params[i].ops->set(val, &params[i]);\\n\\t\\t\\telse\\n\\t\\t\\t\\terr = -EPERM;\\n\\t\\t\\tkernel_param_unlock(params[i].mod);\\n\\t\\t\\treturn err;\\n\\t\\t}\\n\\t}\\n\\n\\tif (handle_unknown) {\\n\\t\\tpr_debug(\"doing %s: %s=\\'%s\\'\\\\n\", doing, param, val);\\n\\t\\treturn handle_unknown(param, val, doing, arg);\\n\\t}\\n\\n\\tpr_debug(\"Unknown argument \\'%s\\'\\\\n\", param);\\n\\treturn -ENOENT;\\n}\\n\\n/* Args looks like \"foo=bar,bar2 baz=fuz wiz\". */\\nchar *parse_args(const char *doing,\\n\\t\\t char *args,\\n\\t\\t const struct kernel_param *params,\\n\\t\\t unsigned num,\\n\\t\\t s16 min_level,\\n\\t\\t s16 max_level,\\n\\t\\t void *arg, parse_unknown_fn unknown)\\n{\\n\\tchar *param, *val, *err = NULL;\\n\\n\\t/* Chew leading spaces */\\n\\targs = skip_spaces(args);\\n\\n\\tif (*args)\\n\\t\\tpr_debug(\"doing %s, parsing ARGS: \\'%s\\'\\\\n\", doing, args);\\n\\n\\twhile (*args) {\\n\\t\\tint ret;\\n\\t\\tint irq_was_disabled;\\n\\n\\t\\targs = next_arg(args, &param, &val);\\n\\t\\t/* Stop at -- */\\n\\t\\tif (!val && strcmp(param, \"--\") == 0)\\n\\t\\t\\treturn err ?: args;\\n\\t\\tirq_was_disabled = irqs_disabled();\\n\\t\\tret = parse_one(param, val, doing, params, num,\\n\\t\\t\\t\\tmin_level, max_level, arg, unknown);\\n\\t\\tif (irq_was_disabled && !irqs_disabled())\\n\\t\\t\\tpr_warn(\"%s: option \\'%s\\' enabled irq\\'s!\\\\n\",\\n\\t\\t\\t\\tdoing, param);\\n\\n\\t\\tswitch (ret) {\\n\\t\\tcase 0:\\n\\t\\t\\tcontinue;\\n\\t\\tcase -ENOENT:\\n\\t\\t\\tpr_err(\"%s: Unknown parameter `%s\\'\\\\n\", doing, param);\\n\\t\\t\\tbreak;\\n\\t\\tcase -ENOSPC:\\n\\t\\t\\tpr_err(\"%s: `%s\\' too large for parameter `%s\\'\\\\n\",\\n\\t\\t\\t       doing, val ?: \"\", param);\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\tpr_err(\"%s: `%s\\' invalid for parameter `%s\\'\\\\n\",\\n\\t\\t\\t       doing, val ?: \"\", param);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\terr = ERR_PTR(ret);\\n\\t}\\n\\n\\treturn err;\\n}\\n\\n/* Lazy bastard, eh? */\\n#define STANDARD_PARAM_DEF(name, type, format, strtolfn)      \\t\\t\\\\\\n\\tint param_set_##name(const char *val, const struct kernel_param *kp) \\\\\\n\\t{\\t\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\t\\treturn strtolfn(val, 0, (type *)kp->arg);\\t\\t\\\\\\n\\t}\\t\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\tint param_get_##name(char *buffer, const struct kernel_param *kp) \\\\\\n\\t{\\t\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\t\\treturn scnprintf(buffer, PAGE_SIZE, format \"\\\\n\",\\t\\\\\\n\\t\\t\\t\\t*((type *)kp->arg));\\t\\t\\t\\\\\\n\\t}\\t\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\tconst struct kernel_param_ops param_ops_##name = {\\t\\t\\t\\\\\\n\\t\\t.set = param_set_##name,\\t\\t\\t\\t\\\\\\n\\t\\t.get = param_get_##name,\\t\\t\\t\\t\\\\\\n\\t};\\t\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\tEXPORT_SYMBOL(param_set_##name);\\t\\t\\t\\t\\\\\\n\\tEXPORT_SYMBOL(param_get_##name);\\t\\t\\t\\t\\\\\\n\\tEXPORT_SYMBOL(param_ops_##name)\\n\\n\\nSTANDARD_PARAM_DEF(byte,\\tunsigned char,\\t\\t\"%hhu\",\\t\\tkstrtou8);\\nSTANDARD_PARAM_DEF(short,\\tshort,\\t\\t\\t\"%hi\",\\t\\tkstrtos16);\\nSTANDARD_PARAM_DEF(ushort,\\tunsigned short,\\t\\t\"%hu\",\\t\\tkstrtou16);\\nSTANDARD_PARAM_DEF(int,\\t\\tint,\\t\\t\\t\"%i\",\\t\\tkstrtoint);\\nSTANDARD_PARAM_DEF(uint,\\tunsigned int,\\t\\t\"%u\",\\t\\tkstrtouint);\\nSTANDARD_PARAM_DEF(long,\\tlong,\\t\\t\\t\"%li\",\\t\\tkstrtol);\\nSTANDARD_PARAM_DEF(ulong,\\tunsigned long,\\t\\t\"%lu\",\\t\\tkstrtoul);\\nSTANDARD_PARAM_DEF(ullong,\\tunsigned long long,\\t\"%llu\",\\t\\tkstrtoull);\\nSTANDARD_PARAM_DEF(hexint,\\tunsigned int,\\t\\t\"%#08x\", \\tkstrtouint);\\n\\nint param_set_uint_minmax(const char *val, const struct kernel_param *kp,\\n\\t\\tunsigned int min, unsigned int max)\\n{\\n\\tunsigned int num;\\n\\tint ret;\\n\\n\\tif (!val)\\n\\t\\treturn -EINVAL;\\n\\tret = kstrtouint(val, 0, &num);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\tif (num < min || num > max)\\n\\t\\treturn -EINVAL;\\n\\t*((unsigned int *)kp->arg) = num;\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(param_set_uint_minmax);\\n\\nint param_set_charp(const char *val, const struct kernel_param *kp)\\n{\\n\\tsize_t len, maxlen = 1024;\\n\\n\\tlen = strnlen(val, maxlen + 1);\\n\\tif (len == maxlen + 1) {\\n\\t\\tpr_err(\"%s: string parameter too long\\\\n\", kp->name);\\n\\t\\treturn -ENOSPC;\\n\\t}\\n\\n\\tmaybe_kfree_parameter(*(char **)kp->arg);\\n\\n\\t/*\\n\\t * This is a hack. We can\\'t kmalloc() in early boot, and we\\n\\t * don\\'t need to; this mangled commandline is preserved.\\n\\t */\\n\\tif (slab_is_available()) {\\n\\t\\t*(char **)kp->arg = kmalloc_parameter(len + 1);\\n\\t\\tif (!*(char **)kp->arg)\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\tstrcpy(*(char **)kp->arg, val);\\n\\t} else\\n\\t\\t*(const char **)kp->arg = val;\\n\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(param_set_charp);\\n\\nint param_get_charp(char *buffer, const struct kernel_param *kp)\\n{\\n\\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\\\n\", *((char **)kp->arg));\\n}\\nEXPORT_SYMBOL(param_get_charp);\\n\\nvoid param_free_charp(void *arg)\\n{\\n\\tmaybe_kfree_parameter(*((char **)arg));\\n}\\nEXPORT_SYMBOL(param_free_charp);\\n\\nconst struct kernel_param_ops param_ops_charp = {\\n\\t.set = param_set_charp,\\n\\t.get = param_get_charp,\\n\\t.free = param_free_charp,\\n};\\nEXPORT_SYMBOL(param_ops_charp);\\n\\n/* Actually could be a bool or an int, for historical reasons. */\\nint param_set_bool(const char *val, const struct kernel_param *kp)\\n{\\n\\t/* No equals means \"set\"... */\\n\\tif (!val) val = \"1\";\\n\\n\\t/* One of =[yYnN01] */\\n\\treturn kstrtobool(val, kp->arg);\\n}\\nEXPORT_SYMBOL(param_set_bool);\\n\\nint param_get_bool(char *buffer, const struct kernel_param *kp)\\n{\\n\\t/* Y and N chosen as being relatively non-coder friendly */\\n\\treturn sprintf(buffer, \"%c\\\\n\", *(bool *)kp->arg ? \\'Y\\' : \\'N\\');\\n}\\nEXPORT_SYMBOL(param_get_bool);\\n\\nconst struct kernel_param_ops param_ops_bool = {\\n\\t.flags = KERNEL_PARAM_OPS_FL_NOARG,\\n\\t.set = param_set_bool,\\n\\t.get = param_get_bool,\\n};\\nEXPORT_SYMBOL(param_ops_bool);\\n\\nint param_set_bool_enable_only(const char *val, const struct kernel_param *kp)\\n{\\n\\tint err;\\n\\tbool new_value;\\n\\tbool orig_value = *(bool *)kp->arg;\\n\\tstruct kernel_param dummy_kp = *kp;\\n\\n\\tdummy_kp.arg = &new_value;\\n\\n\\terr = param_set_bool(val, &dummy_kp);\\n\\tif (err)\\n\\t\\treturn err;\\n\\n\\t/* Don\\'t let them unset it once it\\'s set! */\\n\\tif (!new_value && orig_value)\\n\\t\\treturn -EROFS;\\n\\n\\tif (new_value)\\n\\t\\terr = param_set_bool(val, kp);\\n\\n\\treturn err;\\n}\\nEXPORT_SYMBOL_GPL(param_set_bool_enable_only);\\n\\nconst struct kernel_param_ops param_ops_bool_enable_only = {\\n\\t.flags = KERNEL_PARAM_OPS_FL_NOARG,\\n\\t.set = param_set_bool_enable_only,\\n\\t.get = param_get_bool,\\n};\\nEXPORT_SYMBOL_GPL(param_ops_bool_enable_only);\\n\\n/* This one must be bool. */\\nint param_set_invbool(const char *val, const struct kernel_param *kp)\\n{\\n\\tint ret;\\n\\tbool boolval;\\n\\tstruct kernel_param dummy;\\n\\n\\tdummy.arg = &boolval;\\n\\tret = param_set_bool(val, &dummy);\\n\\tif (ret == 0)\\n\\t\\t*(bool *)kp->arg = !boolval;\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(param_set_invbool);\\n\\nint param_get_invbool(char *buffer, const struct kernel_param *kp)\\n{\\n\\treturn sprintf(buffer, \"%c\\\\n\", (*(bool *)kp->arg) ? \\'N\\' : \\'Y\\');\\n}\\nEXPORT_SYMBOL(param_get_invbool);\\n\\nconst struct kernel_param_ops param_ops_invbool = {\\n\\t.set = param_set_invbool,\\n\\t.get = param_get_invbool,\\n};\\nEXPORT_SYMBOL(param_ops_invbool);\\n\\nint param_set_bint(const char *val, const struct kernel_param *kp)\\n{\\n\\t/* Match bool exactly, by re-using it. */\\n\\tstruct kernel_param boolkp = *kp;\\n\\tbool v;\\n\\tint ret;\\n\\n\\tboolkp.arg = &v;\\n\\n\\tret = param_set_bool(val, &boolkp);\\n\\tif (ret == 0)\\n\\t\\t*(int *)kp->arg = v;\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(param_set_bint);\\n\\nconst struct kernel_param_ops param_ops_bint = {\\n\\t.flags = KERNEL_PARAM_OPS_FL_NOARG,\\n\\t.set = param_set_bint,\\n\\t.get = param_get_int,\\n};\\nEXPORT_SYMBOL(param_ops_bint);\\n\\n/* We break the rule and mangle the string. */\\nstatic int param_array(struct module *mod,\\n\\t\\t       const char *name,\\n\\t\\t       const char *val,\\n\\t\\t       unsigned int min, unsigned int max,\\n\\t\\t       void *elem, int elemsize,\\n\\t\\t       int (*set)(const char *, const struct kernel_param *kp),\\n\\t\\t       s16 level,\\n\\t\\t       unsigned int *num)\\n{\\n\\tint ret;\\n\\tstruct kernel_param kp;\\n\\tchar save;\\n\\n\\t/* Get the name right for errors. */\\n\\tkp.name = name;\\n\\tkp.arg = elem;\\n\\tkp.level = level;\\n\\n\\t*num = 0;\\n\\t/* We expect a comma-separated list of values. */\\n\\tdo {\\n\\t\\tint len;\\n\\n\\t\\tif (*num == max) {\\n\\t\\t\\tpr_err(\"%s: can only take %i arguments\\\\n\", name, max);\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\tlen = strcspn(val, \",\");\\n\\n\\t\\t/* nul-terminate and parse */\\n\\t\\tsave = val[len];\\n\\t\\t((char *)val)[len] = \\'\\\\0\\';\\n\\t\\tcheck_kparam_locked(mod);\\n\\t\\tret = set(val, &kp);\\n\\n\\t\\tif (ret != 0)\\n\\t\\t\\treturn ret;\\n\\t\\tkp.arg += elemsize;\\n\\t\\tval += len+1;\\n\\t\\t(*num)++;\\n\\t} while (save == \\',\\');\\n\\n\\tif (*num < min) {\\n\\t\\tpr_err(\"%s: needs at least %i arguments\\\\n\", name, min);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int param_array_set(const char *val, const struct kernel_param *kp)\\n{\\n\\tconst struct kparam_array *arr = kp->arr;\\n\\tunsigned int temp_num;\\n\\n\\treturn param_array(kp->mod, kp->name, val, 1, arr->max, arr->elem,\\n\\t\\t\\t   arr->elemsize, arr->ops->set, kp->level,\\n\\t\\t\\t   arr->num ?: &temp_num);\\n}\\n\\nstatic int param_array_get(char *buffer, const struct kernel_param *kp)\\n{\\n\\tint i, off, ret;\\n\\tconst struct kparam_array *arr = kp->arr;\\n\\tstruct kernel_param p = *kp;\\n\\n\\tfor (i = off = 0; i < (arr->num ? *arr->num : arr->max); i++) {\\n\\t\\t/* Replace \\\\n with comma */\\n\\t\\tif (i)\\n\\t\\t\\tbuffer[off - 1] = \\',\\';\\n\\t\\tp.arg = arr->elem + arr->elemsize * i;\\n\\t\\tcheck_kparam_locked(p.mod);\\n\\t\\tret = arr->ops->get(buffer + off, &p);\\n\\t\\tif (ret < 0)\\n\\t\\t\\treturn ret;\\n\\t\\toff += ret;\\n\\t}\\n\\tbuffer[off] = \\'\\\\0\\';\\n\\treturn off;\\n}\\n\\nstatic void param_array_free(void *arg)\\n{\\n\\tunsigned int i;\\n\\tconst struct kparam_array *arr = arg;\\n\\n\\tif (arr->ops->free)\\n\\t\\tfor (i = 0; i < (arr->num ? *arr->num : arr->max); i++)\\n\\t\\t\\tarr->ops->free(arr->elem + arr->elemsize * i);\\n}\\n\\nconst struct kernel_param_ops param_array_ops = {\\n\\t.set = param_array_set,\\n\\t.get = param_array_get,\\n\\t.free = param_array_free,\\n};\\nEXPORT_SYMBOL(param_array_ops);\\n\\nint param_set_copystring(const char *val, const struct kernel_param *kp)\\n{\\n\\tconst struct kparam_string *kps = kp->str;\\n\\n\\tif (strnlen(val, kps->maxlen) == kps->maxlen) {\\n\\t\\tpr_err(\"%s: string doesn\\'t fit in %u chars.\\\\n\",\\n\\t\\t       kp->name, kps->maxlen-1);\\n\\t\\treturn -ENOSPC;\\n\\t}\\n\\tstrcpy(kps->string, val);\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(param_set_copystring);\\n\\nint param_get_string(char *buffer, const struct kernel_param *kp)\\n{\\n\\tconst struct kparam_string *kps = kp->str;\\n\\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\\\n\", kps->string);\\n}\\nEXPORT_SYMBOL(param_get_string);\\n\\nconst struct kernel_param_ops param_ops_string = {\\n\\t.set = param_set_copystring,\\n\\t.get = param_get_string,\\n};\\nEXPORT_SYMBOL(param_ops_string);\\n\\n/* sysfs output in /sys/modules/XYZ/parameters/ */\\n#define to_module_attr(n) container_of(n, struct module_attribute, attr)\\n#define to_module_kobject(n) container_of(n, struct module_kobject, kobj)\\n\\nstruct param_attribute\\n{\\n\\tstruct module_attribute mattr;\\n\\tconst struct kernel_param *param;\\n};\\n\\nstruct module_param_attrs\\n{\\n\\tunsigned int num;\\n\\tstruct attribute_group grp;\\n\\tstruct param_attribute attrs[];\\n};\\n\\n#ifdef CONFIG_SYSFS\\n#define to_param_attr(n) container_of(n, struct param_attribute, mattr)\\n\\nstatic ssize_t param_attr_show(struct module_attribute *mattr,\\n\\t\\t\\t       struct module_kobject *mk, char *buf)\\n{\\n\\tint count;\\n\\tstruct param_attribute *attribute = to_param_attr(mattr);\\n\\n\\tif (!attribute->param->ops->get)\\n\\t\\treturn -EPERM;\\n\\n\\tkernel_param_lock(mk->mod);\\n\\tcount = attribute->param->ops->get(buf, attribute->param);\\n\\tkernel_param_unlock(mk->mod);\\n\\treturn count;\\n}\\n\\n/* sysfs always hands a nul-terminated string in buf.  We rely on that. */\\nstatic ssize_t param_attr_store(struct module_attribute *mattr,\\n\\t\\t\\t\\tstruct module_kobject *mk,\\n\\t\\t\\t\\tconst char *buf, size_t len)\\n{\\n \\tint err;\\n\\tstruct param_attribute *attribute = to_param_attr(mattr);\\n\\n\\tif (!attribute->param->ops->set)\\n\\t\\treturn -EPERM;\\n\\n\\tkernel_param_lock(mk->mod);\\n\\tif (param_check_unsafe(attribute->param))\\n\\t\\terr = attribute->param->ops->set(buf, attribute->param);\\n\\telse\\n\\t\\terr = -EPERM;\\n\\tkernel_param_unlock(mk->mod);\\n\\tif (!err)\\n\\t\\treturn len;\\n\\treturn err;\\n}\\n#endif\\n\\n#ifdef CONFIG_MODULES\\n#define __modinit\\n#else\\n#define __modinit __init\\n#endif\\n\\n#ifdef CONFIG_SYSFS\\nvoid kernel_param_lock(struct module *mod)\\n{\\n\\tmutex_lock(KPARAM_MUTEX(mod));\\n}\\n\\nvoid kernel_param_unlock(struct module *mod)\\n{\\n\\tmutex_unlock(KPARAM_MUTEX(mod));\\n}\\n\\nEXPORT_SYMBOL(kernel_param_lock);\\nEXPORT_SYMBOL(kernel_param_unlock);\\n\\n/*\\n * add_sysfs_param - add a parameter to sysfs\\n * @mk: struct module_kobject\\n * @kp: the actual parameter definition to add to sysfs\\n * @name: name of parameter\\n *\\n * Create a kobject if for a (per-module) parameter if mp NULL, and\\n * create file in sysfs.  Returns an error on out of memory.  Always cleans up\\n * if there\\'s an error.\\n */\\nstatic __modinit int add_sysfs_param(struct module_kobject *mk,\\n\\t\\t\\t\\t     const struct kernel_param *kp,\\n\\t\\t\\t\\t     const char *name)\\n{\\n\\tstruct module_param_attrs *new_mp;\\n\\tstruct attribute **new_attrs;\\n\\tunsigned int i;\\n\\n\\t/* We don\\'t bother calling this with invisible parameters. */\\n\\tBUG_ON(!kp->perm);\\n\\n\\tif (!mk->mp) {\\n\\t\\t/* First allocation. */\\n\\t\\tmk->mp = kzalloc(sizeof(*mk->mp), GFP_KERNEL);\\n\\t\\tif (!mk->mp)\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\tmk->mp->grp.name = \"parameters\";\\n\\t\\t/* NULL-terminated attribute array. */\\n\\t\\tmk->mp->grp.attrs = kzalloc(sizeof(mk->mp->grp.attrs[0]),\\n\\t\\t\\t\\t\\t    GFP_KERNEL);\\n\\t\\t/* Caller will cleanup via free_module_param_attrs */\\n\\t\\tif (!mk->mp->grp.attrs)\\n\\t\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\t/* Enlarge allocations. */\\n\\tnew_mp = krealloc(mk->mp,\\n\\t\\t\\t  sizeof(*mk->mp) +\\n\\t\\t\\t  sizeof(mk->mp->attrs[0]) * (mk->mp->num + 1),\\n\\t\\t\\t  GFP_KERNEL);\\n\\tif (!new_mp)\\n\\t\\treturn -ENOMEM;\\n\\tmk->mp = new_mp;\\n\\n\\t/* Extra pointer for NULL terminator */\\n\\tnew_attrs = krealloc(mk->mp->grp.attrs,\\n\\t\\t\\t     sizeof(mk->mp->grp.attrs[0]) * (mk->mp->num + 2),\\n\\t\\t\\t     GFP_KERNEL);\\n\\tif (!new_attrs)\\n\\t\\treturn -ENOMEM;\\n\\tmk->mp->grp.attrs = new_attrs;\\n\\n\\t/* Tack new one on the end. */\\n\\tmemset(&mk->mp->attrs[mk->mp->num], 0, sizeof(mk->mp->attrs[0]));\\n\\tsysfs_attr_init(&mk->mp->attrs[mk->mp->num].mattr.attr);\\n\\tmk->mp->attrs[mk->mp->num].param = kp;\\n\\tmk->mp->attrs[mk->mp->num].mattr.show = param_attr_show;\\n\\t/* Do not allow runtime DAC changes to make param writable. */\\n\\tif ((kp->perm & (S_IWUSR | S_IWGRP | S_IWOTH)) != 0)\\n\\t\\tmk->mp->attrs[mk->mp->num].mattr.store = param_attr_store;\\n\\telse\\n\\t\\tmk->mp->attrs[mk->mp->num].mattr.store = NULL;\\n\\tmk->mp->attrs[mk->mp->num].mattr.attr.name = (char *)name;\\n\\tmk->mp->attrs[mk->mp->num].mattr.attr.mode = kp->perm;\\n\\tmk->mp->num++;\\n\\n\\t/* Fix up all the pointers, since krealloc can move us */\\n\\tfor (i = 0; i < mk->mp->num; i++)\\n\\t\\tmk->mp->grp.attrs[i] = &mk->mp->attrs[i].mattr.attr;\\n\\tmk->mp->grp.attrs[mk->mp->num] = NULL;\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_MODULES\\nstatic void free_module_param_attrs(struct module_kobject *mk)\\n{\\n\\tif (mk->mp)\\n\\t\\tkfree(mk->mp->grp.attrs);\\n\\tkfree(mk->mp);\\n\\tmk->mp = NULL;\\n}\\n\\n/*\\n * module_param_sysfs_setup - setup sysfs support for one module\\n * @mod: module\\n * @kparam: module parameters (array)\\n * @num_params: number of module parameters\\n *\\n * Adds sysfs entries for module parameters under\\n * /sys/module/[mod->name]/parameters/\\n */\\nint module_param_sysfs_setup(struct module *mod,\\n\\t\\t\\t     const struct kernel_param *kparam,\\n\\t\\t\\t     unsigned int num_params)\\n{\\n\\tint i, err;\\n\\tbool params = false;\\n\\n\\tfor (i = 0; i < num_params; i++) {\\n\\t\\tif (kparam[i].perm == 0)\\n\\t\\t\\tcontinue;\\n\\t\\terr = add_sysfs_param(&mod->mkobj, &kparam[i], kparam[i].name);\\n\\t\\tif (err) {\\n\\t\\t\\tfree_module_param_attrs(&mod->mkobj);\\n\\t\\t\\treturn err;\\n\\t\\t}\\n\\t\\tparams = true;\\n\\t}\\n\\n\\tif (!params)\\n\\t\\treturn 0;\\n\\n\\t/* Create the param group. */\\n\\terr = sysfs_create_group(&mod->mkobj.kobj, &mod->mkobj.mp->grp);\\n\\tif (err)\\n\\t\\tfree_module_param_attrs(&mod->mkobj);\\n\\treturn err;\\n}\\n\\n/*\\n * module_param_sysfs_remove - remove sysfs support for one module\\n * @mod: module\\n *\\n * Remove sysfs entries for module parameters and the corresponding\\n * kobject.\\n */\\nvoid module_param_sysfs_remove(struct module *mod)\\n{\\n\\tif (mod->mkobj.mp) {\\n\\t\\tsysfs_remove_group(&mod->mkobj.kobj, &mod->mkobj.mp->grp);\\n\\t\\t/*\\n\\t\\t * We are positive that no one is using any param\\n\\t\\t * attrs at this point. Deallocate immediately.\\n\\t\\t */\\n\\t\\tfree_module_param_attrs(&mod->mkobj);\\n\\t}\\n}\\n#endif\\n\\nvoid destroy_params(const struct kernel_param *params, unsigned num)\\n{\\n\\tunsigned int i;\\n\\n\\tfor (i = 0; i < num; i++)\\n\\t\\tif (params[i].ops->free)\\n\\t\\t\\tparams[i].ops->free(params[i].arg);\\n}\\n\\nstatic struct module_kobject * __init locate_module_kobject(const char *name)\\n{\\n\\tstruct module_kobject *mk;\\n\\tstruct kobject *kobj;\\n\\tint err;\\n\\n\\tkobj = kset_find_obj(module_kset, name);\\n\\tif (kobj) {\\n\\t\\tmk = to_module_kobject(kobj);\\n\\t} else {\\n\\t\\tmk = kzalloc(sizeof(struct module_kobject), GFP_KERNEL);\\n\\t\\tBUG_ON(!mk);\\n\\n\\t\\tmk->mod = THIS_MODULE;\\n\\t\\tmk->kobj.kset = module_kset;\\n\\t\\terr = kobject_init_and_add(&mk->kobj, &module_ktype, NULL,\\n\\t\\t\\t\\t\\t   \"%s\", name);\\n#ifdef CONFIG_MODULES\\n\\t\\tif (!err)\\n\\t\\t\\terr = sysfs_create_file(&mk->kobj, &module_uevent.attr);\\n#endif\\n\\t\\tif (err) {\\n\\t\\t\\tkobject_put(&mk->kobj);\\n\\t\\t\\tpr_crit(\"Adding module \\'%s\\' to sysfs failed (%d), the system may be unstable.\\\\n\",\\n\\t\\t\\t\\tname, err);\\n\\t\\t\\treturn NULL;\\n\\t\\t}\\n\\n\\t\\t/* So that we hold reference in both cases. */\\n\\t\\tkobject_get(&mk->kobj);\\n\\t}\\n\\n\\treturn mk;\\n}\\n\\nstatic void __init kernel_add_sysfs_param(const char *name,\\n\\t\\t\\t\\t\\t  const struct kernel_param *kparam,\\n\\t\\t\\t\\t\\t  unsigned int name_skip)\\n{\\n\\tstruct module_kobject *mk;\\n\\tint err;\\n\\n\\tmk = locate_module_kobject(name);\\n\\tif (!mk)\\n\\t\\treturn;\\n\\n\\t/* We need to remove old parameters before adding more. */\\n\\tif (mk->mp)\\n\\t\\tsysfs_remove_group(&mk->kobj, &mk->mp->grp);\\n\\n\\t/* These should not fail at boot. */\\n\\terr = add_sysfs_param(mk, kparam, kparam->name + name_skip);\\n\\tBUG_ON(err);\\n\\terr = sysfs_create_group(&mk->kobj, &mk->mp->grp);\\n\\tBUG_ON(err);\\n\\tkobject_uevent(&mk->kobj, KOBJ_ADD);\\n\\tkobject_put(&mk->kobj);\\n}\\n\\n/*\\n * param_sysfs_builtin - add sysfs parameters for built-in modules\\n *\\n * Add module_parameters to sysfs for \"modules\" built into the kernel.\\n *\\n * The \"module\" name (KBUILD_MODNAME) is stored before a dot, the\\n * \"parameter\" name is stored behind a dot in kernel_param->name. So,\\n * extract the \"module\" name for all built-in kernel_param-eters,\\n * and for all who have the same, call kernel_add_sysfs_param.\\n */\\nstatic void __init param_sysfs_builtin(void)\\n{\\n\\tconst struct kernel_param *kp;\\n\\tunsigned int name_len;\\n\\tchar modname[MODULE_NAME_LEN];\\n\\n\\tfor (kp = __start___param; kp < __stop___param; kp++) {\\n\\t\\tchar *dot;\\n\\n\\t\\tif (kp->perm == 0)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tdot = strchr(kp->name, \\'.\\');\\n\\t\\tif (!dot) {\\n\\t\\t\\t/* This happens for core_param() */\\n\\t\\t\\tstrcpy(modname, \"kernel\");\\n\\t\\t\\tname_len = 0;\\n\\t\\t} else {\\n\\t\\t\\tname_len = dot - kp->name + 1;\\n\\t\\t\\tstrscpy(modname, kp->name, name_len);\\n\\t\\t}\\n\\t\\tkernel_add_sysfs_param(modname, kp, name_len);\\n\\t}\\n}\\n\\nssize_t __modver_version_show(struct module_attribute *mattr,\\n\\t\\t\\t      struct module_kobject *mk, char *buf)\\n{\\n\\tstruct module_version_attribute *vattr =\\n\\t\\tcontainer_of(mattr, struct module_version_attribute, mattr);\\n\\n\\treturn scnprintf(buf, PAGE_SIZE, \"%s\\\\n\", vattr->version);\\n}\\n\\nextern const struct module_version_attribute __start___modver[];\\nextern const struct module_version_attribute __stop___modver[];\\n\\nstatic void __init version_sysfs_builtin(void)\\n{\\n\\tconst struct module_version_attribute *vattr;\\n\\tstruct module_kobject *mk;\\n\\tint err;\\n\\n\\tfor (vattr = __start___modver; vattr < __stop___modver; vattr++) {\\n\\t\\tmk = locate_module_kobject(vattr->module_name);\\n\\t\\tif (mk) {\\n\\t\\t\\terr = sysfs_create_file(&mk->kobj, &vattr->mattr.attr);\\n\\t\\t\\tWARN_ON_ONCE(err);\\n\\t\\t\\tkobject_uevent(&mk->kobj, KOBJ_ADD);\\n\\t\\t\\tkobject_put(&mk->kobj);\\n\\t\\t}\\n\\t}\\n}\\n\\n/* module-related sysfs stuff */\\n\\nstatic ssize_t module_attr_show(struct kobject *kobj,\\n\\t\\t\\t\\tstruct attribute *attr,\\n\\t\\t\\t\\tchar *buf)\\n{\\n\\tstruct module_attribute *attribute;\\n\\tstruct module_kobject *mk;\\n\\tint ret;\\n\\n\\tattribute = to_module_attr(attr);\\n\\tmk = to_module_kobject(kobj);\\n\\n\\tif (!attribute->show)\\n\\t\\treturn -EIO;\\n\\n\\tret = attribute->show(attribute, mk, buf);\\n\\n\\treturn ret;\\n}\\n\\nstatic ssize_t module_attr_store(struct kobject *kobj,\\n\\t\\t\\t\\tstruct attribute *attr,\\n\\t\\t\\t\\tconst char *buf, size_t len)\\n{\\n\\tstruct module_attribute *attribute;\\n\\tstruct module_kobject *mk;\\n\\tint ret;\\n\\n\\tattribute = to_module_attr(attr);\\n\\tmk = to_module_kobject(kobj);\\n\\n\\tif (!attribute->store)\\n\\t\\treturn -EIO;\\n\\n\\tret = attribute->store(attribute, mk, buf, len);\\n\\n\\treturn ret;\\n}\\n\\nstatic const struct sysfs_ops module_sysfs_ops = {\\n\\t.show = module_attr_show,\\n\\t.store = module_attr_store,\\n};\\n\\nstatic int uevent_filter(const struct kobject *kobj)\\n{\\n\\tconst struct kobj_type *ktype = get_ktype(kobj);\\n\\n\\tif (ktype == &module_ktype)\\n\\t\\treturn 1;\\n\\treturn 0;\\n}\\n\\nstatic const struct kset_uevent_ops module_uevent_ops = {\\n\\t.filter = uevent_filter,\\n};\\n\\nstruct kset *module_kset;\\n\\nstatic void module_kobj_release(struct kobject *kobj)\\n{\\n\\tstruct module_kobject *mk = to_module_kobject(kobj);\\n\\tcomplete(mk->kobj_completion);\\n}\\n\\nconst struct kobj_type module_ktype = {\\n\\t.release   =\\tmodule_kobj_release,\\n\\t.sysfs_ops =\\t&module_sysfs_ops,\\n};\\n\\n/*\\n * param_sysfs_init - create \"module\" kset\\n *\\n * This must be done before the initramfs is unpacked and\\n * request_module() thus becomes possible, because otherwise the\\n * module load would fail in mod_sysfs_init.\\n */\\nstatic int __init param_sysfs_init(void)\\n{\\n\\tmodule_kset = kset_create_and_add(\"module\", &module_uevent_ops, NULL);\\n\\tif (!module_kset) {\\n\\t\\tprintk(KERN_WARNING \"%s (%d): error creating kset\\\\n\",\\n\\t\\t\\t__FILE__, __LINE__);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\treturn 0;\\n}\\nsubsys_initcall(param_sysfs_init);\\n\\n/*\\n * param_sysfs_builtin_init - add sysfs version and parameter\\n * attributes for built-in modules\\n */\\nstatic int __init param_sysfs_builtin_init(void)\\n{\\n\\tif (!module_kset)\\n\\t\\treturn -ENOMEM;\\n\\n\\tversion_sysfs_builtin();\\n\\tparam_sysfs_builtin();\\n\\n\\treturn 0;\\n}\\nlate_initcall(param_sysfs_builtin_init);\\n\\n#endif /* CONFIG_SYSFS */\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/* Kernel thread helper functions.\\n *   Copyright (C) 2004 IBM Corporation, Rusty Russell.\\n *   Copyright (C) 2009 Red Hat, Inc.\\n *\\n * Creation is done via kthreadd, so that we get a clean environment\\n * even if we\\'re invoked from userspace (think modprobe, hotplug cpu,\\n * etc.).\\n */\\n#include <uapi/linux/sched/types.h>\\n#include <linux/mm.h>\\n#include <linux/mmu_context.h>\\n#include <linux/sched.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/task.h>\\n#include <linux/kthread.h>\\n#include <linux/completion.h>\\n#include <linux/err.h>\\n#include <linux/cgroup.h>\\n#include <linux/cpuset.h>\\n#include <linux/unistd.h>\\n#include <linux/file.h>\\n#include <linux/export.h>\\n#include <linux/mutex.h>\\n#include <linux/slab.h>\\n#include <linux/freezer.h>\\n#include <linux/ptrace.h>\\n#include <linux/uaccess.h>\\n#include <linux/numa.h>\\n#include <linux/sched/isolation.h>\\n#include <trace/events/sched.h>\\n\\n\\nstatic DEFINE_SPINLOCK(kthread_create_lock);\\nstatic LIST_HEAD(kthread_create_list);\\nstruct task_struct *kthreadd_task;\\n\\nstruct kthread_create_info\\n{\\n\\t/* Information passed to kthread() from kthreadd. */\\n\\tchar *full_name;\\n\\tint (*threadfn)(void *data);\\n\\tvoid *data;\\n\\tint node;\\n\\n\\t/* Result passed back to kthread_create() from kthreadd. */\\n\\tstruct task_struct *result;\\n\\tstruct completion *done;\\n\\n\\tstruct list_head list;\\n};\\n\\nstruct kthread {\\n\\tunsigned long flags;\\n\\tunsigned int cpu;\\n\\tint result;\\n\\tint (*threadfn)(void *);\\n\\tvoid *data;\\n\\tstruct completion parked;\\n\\tstruct completion exited;\\n#ifdef CONFIG_BLK_CGROUP\\n\\tstruct cgroup_subsys_state *blkcg_css;\\n#endif\\n\\t/* To store the full name if task comm is truncated. */\\n\\tchar *full_name;\\n};\\n\\nenum KTHREAD_BITS {\\n\\tKTHREAD_IS_PER_CPU = 0,\\n\\tKTHREAD_SHOULD_STOP,\\n\\tKTHREAD_SHOULD_PARK,\\n};\\n\\nstatic inline struct kthread *to_kthread(struct task_struct *k)\\n{\\n\\tWARN_ON(!(k->flags & PF_KTHREAD));\\n\\treturn k->worker_private;\\n}\\n\\n/*\\n * Variant of to_kthread() that doesn\\'t assume @p is a kthread.\\n *\\n * Per construction; when:\\n *\\n *   (p->flags & PF_KTHREAD) && p->worker_private\\n *\\n * the task is both a kthread and struct kthread is persistent. However\\n * PF_KTHREAD on it\\'s own is not, kernel_thread() can exec() (See umh.c and\\n * begin_new_exec()).\\n */\\nstatic inline struct kthread *__to_kthread(struct task_struct *p)\\n{\\n\\tvoid *kthread = p->worker_private;\\n\\tif (kthread && !(p->flags & PF_KTHREAD))\\n\\t\\tkthread = NULL;\\n\\treturn kthread;\\n}\\n\\nvoid get_kthread_comm(char *buf, size_t buf_size, struct task_struct *tsk)\\n{\\n\\tstruct kthread *kthread = to_kthread(tsk);\\n\\n\\tif (!kthread || !kthread->full_name) {\\n\\t\\tstrscpy(buf, tsk->comm, buf_size);\\n\\t\\treturn;\\n\\t}\\n\\n\\tstrscpy_pad(buf, kthread->full_name, buf_size);\\n}\\n\\nbool set_kthread_struct(struct task_struct *p)\\n{\\n\\tstruct kthread *kthread;\\n\\n\\tif (WARN_ON_ONCE(to_kthread(p)))\\n\\t\\treturn false;\\n\\n\\tkthread = kzalloc(sizeof(*kthread), GFP_KERNEL);\\n\\tif (!kthread)\\n\\t\\treturn false;\\n\\n\\tinit_completion(&kthread->exited);\\n\\tinit_completion(&kthread->parked);\\n\\tp->vfork_done = &kthread->exited;\\n\\n\\tp->worker_private = kthread;\\n\\treturn true;\\n}\\n\\nvoid free_kthread_struct(struct task_struct *k)\\n{\\n\\tstruct kthread *kthread;\\n\\n\\t/*\\n\\t * Can be NULL if kmalloc() in set_kthread_struct() failed.\\n\\t */\\n\\tkthread = to_kthread(k);\\n\\tif (!kthread)\\n\\t\\treturn;\\n\\n#ifdef CONFIG_BLK_CGROUP\\n\\tWARN_ON_ONCE(kthread->blkcg_css);\\n#endif\\n\\tk->worker_private = NULL;\\n\\tkfree(kthread->full_name);\\n\\tkfree(kthread);\\n}\\n\\n/**\\n * kthread_should_stop - should this kthread return now?\\n *\\n * When someone calls kthread_stop() on your kthread, it will be woken\\n * and this will return true.  You should then return, and your return\\n * value will be passed through to kthread_stop().\\n */\\nbool kthread_should_stop(void)\\n{\\n\\treturn test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);\\n}\\nEXPORT_SYMBOL(kthread_should_stop);\\n\\nstatic bool __kthread_should_park(struct task_struct *k)\\n{\\n\\treturn test_bit(KTHREAD_SHOULD_PARK, &to_kthread(k)->flags);\\n}\\n\\n/**\\n * kthread_should_park - should this kthread park now?\\n *\\n * When someone calls kthread_park() on your kthread, it will be woken\\n * and this will return true.  You should then do the necessary\\n * cleanup and call kthread_parkme()\\n *\\n * Similar to kthread_should_stop(), but this keeps the thread alive\\n * and in a park position. kthread_unpark() \"restarts\" the thread and\\n * calls the thread function again.\\n */\\nbool kthread_should_park(void)\\n{\\n\\treturn __kthread_should_park(current);\\n}\\nEXPORT_SYMBOL_GPL(kthread_should_park);\\n\\nbool kthread_should_stop_or_park(void)\\n{\\n\\tstruct kthread *kthread = __to_kthread(current);\\n\\n\\tif (!kthread)\\n\\t\\treturn false;\\n\\n\\treturn kthread->flags & (BIT(KTHREAD_SHOULD_STOP) | BIT(KTHREAD_SHOULD_PARK));\\n}\\n\\n/**\\n * kthread_freezable_should_stop - should this freezable kthread return now?\\n * @was_frozen: optional out parameter, indicates whether %current was frozen\\n *\\n * kthread_should_stop() for freezable kthreads, which will enter\\n * refrigerator if necessary.  This function is safe from kthread_stop() /\\n * freezer deadlock and freezable kthreads should use this function instead\\n * of calling try_to_freeze() directly.\\n */\\nbool kthread_freezable_should_stop(bool *was_frozen)\\n{\\n\\tbool frozen = false;\\n\\n\\tmight_sleep();\\n\\n\\tif (unlikely(freezing(current)))\\n\\t\\tfrozen = __refrigerator(true);\\n\\n\\tif (was_frozen)\\n\\t\\t*was_frozen = frozen;\\n\\n\\treturn kthread_should_stop();\\n}\\nEXPORT_SYMBOL_GPL(kthread_freezable_should_stop);\\n\\n/**\\n * kthread_func - return the function specified on kthread creation\\n * @task: kthread task in question\\n *\\n * Returns NULL if the task is not a kthread.\\n */\\nvoid *kthread_func(struct task_struct *task)\\n{\\n\\tstruct kthread *kthread = __to_kthread(task);\\n\\tif (kthread)\\n\\t\\treturn kthread->threadfn;\\n\\treturn NULL;\\n}\\nEXPORT_SYMBOL_GPL(kthread_func);\\n\\n/**\\n * kthread_data - return data value specified on kthread creation\\n * @task: kthread task in question\\n *\\n * Return the data value specified when kthread @task was created.\\n * The caller is responsible for ensuring the validity of @task when\\n * calling this function.\\n */\\nvoid *kthread_data(struct task_struct *task)\\n{\\n\\treturn to_kthread(task)->data;\\n}\\nEXPORT_SYMBOL_GPL(kthread_data);\\n\\n/**\\n * kthread_probe_data - speculative version of kthread_data()\\n * @task: possible kthread task in question\\n *\\n * @task could be a kthread task.  Return the data value specified when it\\n * was created if accessible.  If @task isn\\'t a kthread task or its data is\\n * inaccessible for any reason, %NULL is returned.  This function requires\\n * that @task itself is safe to dereference.\\n */\\nvoid *kthread_probe_data(struct task_struct *task)\\n{\\n\\tstruct kthread *kthread = __to_kthread(task);\\n\\tvoid *data = NULL;\\n\\n\\tif (kthread)\\n\\t\\tcopy_from_kernel_nofault(&data, &kthread->data, sizeof(data));\\n\\treturn data;\\n}\\n\\nstatic void __kthread_parkme(struct kthread *self)\\n{\\n\\tfor (;;) {\\n\\t\\t/*\\n\\t\\t * TASK_PARKED is a special state; we must serialize against\\n\\t\\t * possible pending wakeups to avoid store-store collisions on\\n\\t\\t * task->state.\\n\\t\\t *\\n\\t\\t * Such a collision might possibly result in the task state\\n\\t\\t * changin from TASK_PARKED and us failing the\\n\\t\\t * wait_task_inactive() in kthread_park().\\n\\t\\t */\\n\\t\\tset_special_state(TASK_PARKED);\\n\\t\\tif (!test_bit(KTHREAD_SHOULD_PARK, &self->flags))\\n\\t\\t\\tbreak;\\n\\n\\t\\t/*\\n\\t\\t * Thread is going to call schedule(), do not preempt it,\\n\\t\\t * or the caller of kthread_park() may spend more time in\\n\\t\\t * wait_task_inactive().\\n\\t\\t */\\n\\t\\tpreempt_disable();\\n\\t\\tcomplete(&self->parked);\\n\\t\\tschedule_preempt_disabled();\\n\\t\\tpreempt_enable();\\n\\t}\\n\\t__set_current_state(TASK_RUNNING);\\n}\\n\\nvoid kthread_parkme(void)\\n{\\n\\t__kthread_parkme(to_kthread(current));\\n}\\nEXPORT_SYMBOL_GPL(kthread_parkme);\\n\\n/**\\n * kthread_exit - Cause the current kthread return @result to kthread_stop().\\n * @result: The integer value to return to kthread_stop().\\n *\\n * While kthread_exit can be called directly, it exists so that\\n * functions which do some additional work in non-modular code such as\\n * module_put_and_kthread_exit can be implemented.\\n *\\n * Does not return.\\n */\\nvoid __noreturn kthread_exit(long result)\\n{\\n\\tstruct kthread *kthread = to_kthread(current);\\n\\tkthread->result = result;\\n\\tdo_exit(0);\\n}\\nEXPORT_SYMBOL(kthread_exit);\\n\\n/**\\n * kthread_complete_and_exit - Exit the current kthread.\\n * @comp: Completion to complete\\n * @code: The integer value to return to kthread_stop().\\n *\\n * If present, complete @comp and then return code to kthread_stop().\\n *\\n * A kernel thread whose module may be removed after the completion of\\n * @comp can use this function to exit safely.\\n *\\n * Does not return.\\n */\\nvoid __noreturn kthread_complete_and_exit(struct completion *comp, long code)\\n{\\n\\tif (comp)\\n\\t\\tcomplete(comp);\\n\\n\\tkthread_exit(code);\\n}\\nEXPORT_SYMBOL(kthread_complete_and_exit);\\n\\nstatic int kthread(void *_create)\\n{\\n\\tstatic const struct sched_param param = { .sched_priority = 0 };\\n\\t/* Copy data: it\\'s on kthread\\'s stack */\\n\\tstruct kthread_create_info *create = _create;\\n\\tint (*threadfn)(void *data) = create->threadfn;\\n\\tvoid *data = create->data;\\n\\tstruct completion *done;\\n\\tstruct kthread *self;\\n\\tint ret;\\n\\n\\tself = to_kthread(current);\\n\\n\\t/* Release the structure when caller killed by a fatal signal. */\\n\\tdone = xchg(&create->done, NULL);\\n\\tif (!done) {\\n\\t\\tkfree(create->full_name);\\n\\t\\tkfree(create);\\n\\t\\tkthread_exit(-EINTR);\\n\\t}\\n\\n\\tself->full_name = create->full_name;\\n\\tself->threadfn = threadfn;\\n\\tself->data = data;\\n\\n\\t/*\\n\\t * The new thread inherited kthreadd\\'s priority and CPU mask. Reset\\n\\t * back to default in case they have been changed.\\n\\t */\\n\\tsched_setscheduler_nocheck(current, SCHED_NORMAL, &param);\\n\\tset_cpus_allowed_ptr(current, housekeeping_cpumask(HK_TYPE_KTHREAD));\\n\\n\\t/* OK, tell user we\\'re spawned, wait for stop or wakeup */\\n\\t__set_current_state(TASK_UNINTERRUPTIBLE);\\n\\tcreate->result = current;\\n\\t/*\\n\\t * Thread is going to call schedule(), do not preempt it,\\n\\t * or the creator may spend more time in wait_task_inactive().\\n\\t */\\n\\tpreempt_disable();\\n\\tcomplete(done);\\n\\tschedule_preempt_disabled();\\n\\tpreempt_enable();\\n\\n\\tret = -EINTR;\\n\\tif (!test_bit(KTHREAD_SHOULD_STOP, &self->flags)) {\\n\\t\\tcgroup_kthread_ready();\\n\\t\\t__kthread_parkme(self);\\n\\t\\tret = threadfn(data);\\n\\t}\\n\\tkthread_exit(ret);\\n}\\n\\n/* called from kernel_clone() to get node information for about to be created task */\\nint tsk_fork_get_node(struct task_struct *tsk)\\n{\\n#ifdef CONFIG_NUMA\\n\\tif (tsk == kthreadd_task)\\n\\t\\treturn tsk->pref_node_fork;\\n#endif\\n\\treturn NUMA_NO_NODE;\\n}\\n\\nstatic void create_kthread(struct kthread_create_info *create)\\n{\\n\\tint pid;\\n\\n#ifdef CONFIG_NUMA\\n\\tcurrent->pref_node_fork = create->node;\\n#endif\\n\\t/* We want our own signal handler (we take no signals by default). */\\n\\tpid = kernel_thread(kthread, create, create->full_name,\\n\\t\\t\\t    CLONE_FS | CLONE_FILES | SIGCHLD);\\n\\tif (pid < 0) {\\n\\t\\t/* Release the structure when caller killed by a fatal signal. */\\n\\t\\tstruct completion *done = xchg(&create->done, NULL);\\n\\n\\t\\tkfree(create->full_name);\\n\\t\\tif (!done) {\\n\\t\\t\\tkfree(create);\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tcreate->result = ERR_PTR(pid);\\n\\t\\tcomplete(done);\\n\\t}\\n}\\n\\nstatic __printf(4, 0)\\nstruct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),\\n\\t\\t\\t\\t\\t\\t    void *data, int node,\\n\\t\\t\\t\\t\\t\\t    const char namefmt[],\\n\\t\\t\\t\\t\\t\\t    va_list args)\\n{\\n\\tDECLARE_COMPLETION_ONSTACK(done);\\n\\tstruct task_struct *task;\\n\\tstruct kthread_create_info *create = kmalloc(sizeof(*create),\\n\\t\\t\\t\\t\\t\\t     GFP_KERNEL);\\n\\n\\tif (!create)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\tcreate->threadfn = threadfn;\\n\\tcreate->data = data;\\n\\tcreate->node = node;\\n\\tcreate->done = &done;\\n\\tcreate->full_name = kvasprintf(GFP_KERNEL, namefmt, args);\\n\\tif (!create->full_name) {\\n\\t\\ttask = ERR_PTR(-ENOMEM);\\n\\t\\tgoto free_create;\\n\\t}\\n\\n\\tspin_lock(&kthread_create_lock);\\n\\tlist_add_tail(&create->list, &kthread_create_list);\\n\\tspin_unlock(&kthread_create_lock);\\n\\n\\twake_up_process(kthreadd_task);\\n\\t/*\\n\\t * Wait for completion in killable state, for I might be chosen by\\n\\t * the OOM killer while kthreadd is trying to allocate memory for\\n\\t * new kernel thread.\\n\\t */\\n\\tif (unlikely(wait_for_completion_killable(&done))) {\\n\\t\\t/*\\n\\t\\t * If I was killed by a fatal signal before kthreadd (or new\\n\\t\\t * kernel thread) calls complete(), leave the cleanup of this\\n\\t\\t * structure to that thread.\\n\\t\\t */\\n\\t\\tif (xchg(&create->done, NULL))\\n\\t\\t\\treturn ERR_PTR(-EINTR);\\n\\t\\t/*\\n\\t\\t * kthreadd (or new kernel thread) will call complete()\\n\\t\\t * shortly.\\n\\t\\t */\\n\\t\\twait_for_completion(&done);\\n\\t}\\n\\ttask = create->result;\\nfree_create:\\n\\tkfree(create);\\n\\treturn task;\\n}\\n\\n/**\\n * kthread_create_on_node - create a kthread.\\n * @threadfn: the function to run until signal_pending(current).\\n * @data: data ptr for @threadfn.\\n * @node: task and thread structures for the thread are allocated on this node\\n * @namefmt: printf-style name for the thread.\\n *\\n * Description: This helper function creates and names a kernel\\n * thread.  The thread will be stopped: use wake_up_process() to start\\n * it.  See also kthread_run().  The new thread has SCHED_NORMAL policy and\\n * is affine to all CPUs.\\n *\\n * If thread is going to be bound on a particular cpu, give its node\\n * in @node, to get NUMA affinity for kthread stack, or else give NUMA_NO_NODE.\\n * When woken, the thread will run @threadfn() with @data as its\\n * argument. @threadfn() can either return directly if it is a\\n * standalone thread for which no one will call kthread_stop(), or\\n * return when \\'kthread_should_stop()\\' is true (which means\\n * kthread_stop() has been called).  The return value should be zero\\n * or a negative error number; it will be passed to kthread_stop().\\n *\\n * Returns a task_struct or ERR_PTR(-ENOMEM) or ERR_PTR(-EINTR).\\n */\\nstruct task_struct *kthread_create_on_node(int (*threadfn)(void *data),\\n\\t\\t\\t\\t\\t   void *data, int node,\\n\\t\\t\\t\\t\\t   const char namefmt[],\\n\\t\\t\\t\\t\\t   ...)\\n{\\n\\tstruct task_struct *task;\\n\\tva_list args;\\n\\n\\tva_start(args, namefmt);\\n\\ttask = __kthread_create_on_node(threadfn, data, node, namefmt, args);\\n\\tva_end(args);\\n\\n\\treturn task;\\n}\\nEXPORT_SYMBOL(kthread_create_on_node);\\n\\nstatic void __kthread_bind_mask(struct task_struct *p, const struct cpumask *mask, unsigned int state)\\n{\\n\\tunsigned long flags;\\n\\n\\tif (!wait_task_inactive(p, state)) {\\n\\t\\tWARN_ON(1);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* It\\'s safe because the task is inactive. */\\n\\traw_spin_lock_irqsave(&p->pi_lock, flags);\\n\\tdo_set_cpus_allowed(p, mask);\\n\\tp->flags |= PF_NO_SETAFFINITY;\\n\\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\\n}\\n\\nstatic void __kthread_bind(struct task_struct *p, unsigned int cpu, unsigned int state)\\n{\\n\\t__kthread_bind_mask(p, cpumask_of(cpu), state);\\n}\\n\\nvoid kthread_bind_mask(struct task_struct *p, const struct cpumask *mask)\\n{\\n\\t__kthread_bind_mask(p, mask, TASK_UNINTERRUPTIBLE);\\n}\\n\\n/**\\n * kthread_bind - bind a just-created kthread to a cpu.\\n * @p: thread created by kthread_create().\\n * @cpu: cpu (might not be online, must be possible) for @k to run on.\\n *\\n * Description: This function is equivalent to set_cpus_allowed(),\\n * except that @cpu doesn\\'t need to be online, and the thread must be\\n * stopped (i.e., just returned from kthread_create()).\\n */\\nvoid kthread_bind(struct task_struct *p, unsigned int cpu)\\n{\\n\\t__kthread_bind(p, cpu, TASK_UNINTERRUPTIBLE);\\n}\\nEXPORT_SYMBOL(kthread_bind);\\n\\n/**\\n * kthread_create_on_cpu - Create a cpu bound kthread\\n * @threadfn: the function to run until signal_pending(current).\\n * @data: data ptr for @threadfn.\\n * @cpu: The cpu on which the thread should be bound,\\n * @namefmt: printf-style name for the thread. Format is restricted\\n *\\t     to \"name.*%u\". Code fills in cpu number.\\n *\\n * Description: This helper function creates and names a kernel thread\\n */\\nstruct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),\\n\\t\\t\\t\\t\\t  void *data, unsigned int cpu,\\n\\t\\t\\t\\t\\t  const char *namefmt)\\n{\\n\\tstruct task_struct *p;\\n\\n\\tp = kthread_create_on_node(threadfn, data, cpu_to_node(cpu), namefmt,\\n\\t\\t\\t\\t   cpu);\\n\\tif (IS_ERR(p))\\n\\t\\treturn p;\\n\\tkthread_bind(p, cpu);\\n\\t/* CPU hotplug need to bind once again when unparking the thread. */\\n\\tto_kthread(p)->cpu = cpu;\\n\\treturn p;\\n}\\nEXPORT_SYMBOL(kthread_create_on_cpu);\\n\\nvoid kthread_set_per_cpu(struct task_struct *k, int cpu)\\n{\\n\\tstruct kthread *kthread = to_kthread(k);\\n\\tif (!kthread)\\n\\t\\treturn;\\n\\n\\tWARN_ON_ONCE(!(k->flags & PF_NO_SETAFFINITY));\\n\\n\\tif (cpu < 0) {\\n\\t\\tclear_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\\n\\t\\treturn;\\n\\t}\\n\\n\\tkthread->cpu = cpu;\\n\\tset_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\\n}\\n\\nbool kthread_is_per_cpu(struct task_struct *p)\\n{\\n\\tstruct kthread *kthread = __to_kthread(p);\\n\\tif (!kthread)\\n\\t\\treturn false;\\n\\n\\treturn test_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\\n}\\n\\n/**\\n * kthread_unpark - unpark a thread created by kthread_create().\\n * @k:\\t\\tthread created by kthread_create().\\n *\\n * Sets kthread_should_park() for @k to return false, wakes it, and\\n * waits for it to return. If the thread is marked percpu then its\\n * bound to the cpu again.\\n */\\nvoid kthread_unpark(struct task_struct *k)\\n{\\n\\tstruct kthread *kthread = to_kthread(k);\\n\\n\\tif (!test_bit(KTHREAD_SHOULD_PARK, &kthread->flags))\\n\\t\\treturn;\\n\\t/*\\n\\t * Newly created kthread was parked when the CPU was offline.\\n\\t * The binding was lost and we need to set it again.\\n\\t */\\n\\tif (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))\\n\\t\\t__kthread_bind(k, kthread->cpu, TASK_PARKED);\\n\\n\\tclear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);\\n\\t/*\\n\\t * __kthread_parkme() will either see !SHOULD_PARK or get the wakeup.\\n\\t */\\n\\twake_up_state(k, TASK_PARKED);\\n}\\nEXPORT_SYMBOL_GPL(kthread_unpark);\\n\\n/**\\n * kthread_park - park a thread created by kthread_create().\\n * @k: thread created by kthread_create().\\n *\\n * Sets kthread_should_park() for @k to return true, wakes it, and\\n * waits for it to return. This can also be called after kthread_create()\\n * instead of calling wake_up_process(): the thread will park without\\n * calling threadfn().\\n *\\n * Returns 0 if the thread is parked, -ENOSYS if the thread exited.\\n * If called by the kthread itself just the park bit is set.\\n */\\nint kthread_park(struct task_struct *k)\\n{\\n\\tstruct kthread *kthread = to_kthread(k);\\n\\n\\tif (WARN_ON(k->flags & PF_EXITING))\\n\\t\\treturn -ENOSYS;\\n\\n\\tif (WARN_ON_ONCE(test_bit(KTHREAD_SHOULD_PARK, &kthread->flags)))\\n\\t\\treturn -EBUSY;\\n\\n\\tset_bit(KTHREAD_SHOULD_PARK, &kthread->flags);\\n\\tif (k != current) {\\n\\t\\twake_up_process(k);\\n\\t\\t/*\\n\\t\\t * Wait for __kthread_parkme() to complete(), this means we\\n\\t\\t * _will_ have TASK_PARKED and are about to call schedule().\\n\\t\\t */\\n\\t\\twait_for_completion(&kthread->parked);\\n\\t\\t/*\\n\\t\\t * Now wait for that schedule() to complete and the task to\\n\\t\\t * get scheduled out.\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(!wait_task_inactive(k, TASK_PARKED));\\n\\t}\\n\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(kthread_park);\\n\\n/**\\n * kthread_stop - stop a thread created by kthread_create().\\n * @k: thread created by kthread_create().\\n *\\n * Sets kthread_should_stop() for @k to return true, wakes it, and\\n * waits for it to exit. This can also be called after kthread_create()\\n * instead of calling wake_up_process(): the thread will exit without\\n * calling threadfn().\\n *\\n * If threadfn() may call kthread_exit() itself, the caller must ensure\\n * task_struct can\\'t go away.\\n *\\n * Returns the result of threadfn(), or %-EINTR if wake_up_process()\\n * was never called.\\n */\\nint kthread_stop(struct task_struct *k)\\n{\\n\\tstruct kthread *kthread;\\n\\tint ret;\\n\\n\\ttrace_sched_kthread_stop(k);\\n\\n\\tget_task_struct(k);\\n\\tkthread = to_kthread(k);\\n\\tset_bit(KTHREAD_SHOULD_STOP, &kthread->flags);\\n\\tkthread_unpark(k);\\n\\tset_tsk_thread_flag(k, TIF_NOTIFY_SIGNAL);\\n\\twake_up_process(k);\\n\\twait_for_completion(&kthread->exited);\\n\\tret = kthread->result;\\n\\tput_task_struct(k);\\n\\n\\ttrace_sched_kthread_stop_ret(ret);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(kthread_stop);\\n\\n/**\\n * kthread_stop_put - stop a thread and put its task struct\\n * @k: thread created by kthread_create().\\n *\\n * Stops a thread created by kthread_create() and put its task_struct.\\n * Only use when holding an extra task struct reference obtained by\\n * calling get_task_struct().\\n */\\nint kthread_stop_put(struct task_struct *k)\\n{\\n\\tint ret;\\n\\n\\tret = kthread_stop(k);\\n\\tput_task_struct(k);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(kthread_stop_put);\\n\\nint kthreadd(void *unused)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\n\\t/* Setup a clean context for our children to inherit. */\\n\\tset_task_comm(tsk, \"kthreadd\");\\n\\tignore_signals(tsk);\\n\\tset_cpus_allowed_ptr(tsk, housekeeping_cpumask(HK_TYPE_KTHREAD));\\n\\tset_mems_allowed(node_states[N_MEMORY]);\\n\\n\\tcurrent->flags |= PF_NOFREEZE;\\n\\tcgroup_init_kthreadd();\\n\\n\\tfor (;;) {\\n\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\t\\tif (list_empty(&kthread_create_list))\\n\\t\\t\\tschedule();\\n\\t\\t__set_current_state(TASK_RUNNING);\\n\\n\\t\\tspin_lock(&kthread_create_lock);\\n\\t\\twhile (!list_empty(&kthread_create_list)) {\\n\\t\\t\\tstruct kthread_create_info *create;\\n\\n\\t\\t\\tcreate = list_entry(kthread_create_list.next,\\n\\t\\t\\t\\t\\t    struct kthread_create_info, list);\\n\\t\\t\\tlist_del_init(&create->list);\\n\\t\\t\\tspin_unlock(&kthread_create_lock);\\n\\n\\t\\t\\tcreate_kthread(create);\\n\\n\\t\\t\\tspin_lock(&kthread_create_lock);\\n\\t\\t}\\n\\t\\tspin_unlock(&kthread_create_lock);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nvoid __kthread_init_worker(struct kthread_worker *worker,\\n\\t\\t\\t\\tconst char *name,\\n\\t\\t\\t\\tstruct lock_class_key *key)\\n{\\n\\tmemset(worker, 0, sizeof(struct kthread_worker));\\n\\traw_spin_lock_init(&worker->lock);\\n\\tlockdep_set_class_and_name(&worker->lock, key, name);\\n\\tINIT_LIST_HEAD(&worker->work_list);\\n\\tINIT_LIST_HEAD(&worker->delayed_work_list);\\n}\\nEXPORT_SYMBOL_GPL(__kthread_init_worker);\\n\\n/**\\n * kthread_worker_fn - kthread function to process kthread_worker\\n * @worker_ptr: pointer to initialized kthread_worker\\n *\\n * This function implements the main cycle of kthread worker. It processes\\n * work_list until it is stopped with kthread_stop(). It sleeps when the queue\\n * is empty.\\n *\\n * The works are not allowed to keep any locks, disable preemption or interrupts\\n * when they finish. There is defined a safe point for freezing when one work\\n * finishes and before a new one is started.\\n *\\n * Also the works must not be handled by more than one worker at the same time,\\n * see also kthread_queue_work().\\n */\\nint kthread_worker_fn(void *worker_ptr)\\n{\\n\\tstruct kthread_worker *worker = worker_ptr;\\n\\tstruct kthread_work *work;\\n\\n\\t/*\\n\\t * FIXME: Update the check and remove the assignment when all kthread\\n\\t * worker users are created using kthread_create_worker*() functions.\\n\\t */\\n\\tWARN_ON(worker->task && worker->task != current);\\n\\tworker->task = current;\\n\\n\\tif (worker->flags & KTW_FREEZABLE)\\n\\t\\tset_freezable();\\n\\nrepeat:\\n\\tset_current_state(TASK_INTERRUPTIBLE);\\t/* mb paired w/ kthread_stop */\\n\\n\\tif (kthread_should_stop()) {\\n\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\traw_spin_lock_irq(&worker->lock);\\n\\t\\tworker->task = NULL;\\n\\t\\traw_spin_unlock_irq(&worker->lock);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\twork = NULL;\\n\\traw_spin_lock_irq(&worker->lock);\\n\\tif (!list_empty(&worker->work_list)) {\\n\\t\\twork = list_first_entry(&worker->work_list,\\n\\t\\t\\t\\t\\tstruct kthread_work, node);\\n\\t\\tlist_del_init(&work->node);\\n\\t}\\n\\tworker->current_work = work;\\n\\traw_spin_unlock_irq(&worker->lock);\\n\\n\\tif (work) {\\n\\t\\tkthread_work_func_t func = work->func;\\n\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\ttrace_sched_kthread_work_execute_start(work);\\n\\t\\twork->func(work);\\n\\t\\t/*\\n\\t\\t * Avoid dereferencing work after this point.  The trace\\n\\t\\t * event only cares about the address.\\n\\t\\t */\\n\\t\\ttrace_sched_kthread_work_execute_end(work, func);\\n\\t} else if (!freezing(current)) {\\n\\t\\tschedule();\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * Handle the case where the current remains\\n\\t\\t * TASK_INTERRUPTIBLE. try_to_freeze() expects\\n\\t\\t * the current to be TASK_RUNNING.\\n\\t\\t */\\n\\t\\t__set_current_state(TASK_RUNNING);\\n\\t}\\n\\n\\ttry_to_freeze();\\n\\tcond_resched();\\n\\tgoto repeat;\\n}\\nEXPORT_SYMBOL_GPL(kthread_worker_fn);\\n\\nstatic __printf(3, 0) struct kthread_worker *\\n__kthread_create_worker(int cpu, unsigned int flags,\\n\\t\\t\\tconst char namefmt[], va_list args)\\n{\\n\\tstruct kthread_worker *worker;\\n\\tstruct task_struct *task;\\n\\tint node = NUMA_NO_NODE;\\n\\n\\tworker = kzalloc(sizeof(*worker), GFP_KERNEL);\\n\\tif (!worker)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tkthread_init_worker(worker);\\n\\n\\tif (cpu >= 0)\\n\\t\\tnode = cpu_to_node(cpu);\\n\\n\\ttask = __kthread_create_on_node(kthread_worker_fn, worker,\\n\\t\\t\\t\\t\\t\\tnode, namefmt, args);\\n\\tif (IS_ERR(task))\\n\\t\\tgoto fail_task;\\n\\n\\tif (cpu >= 0)\\n\\t\\tkthread_bind(task, cpu);\\n\\n\\tworker->flags = flags;\\n\\tworker->task = task;\\n\\twake_up_process(task);\\n\\treturn worker;\\n\\nfail_task:\\n\\tkfree(worker);\\n\\treturn ERR_CAST(task);\\n}\\n\\n/**\\n * kthread_create_worker - create a kthread worker\\n * @flags: flags modifying the default behavior of the worker\\n * @namefmt: printf-style name for the kthread worker (task).\\n *\\n * Returns a pointer to the allocated worker on success, ERR_PTR(-ENOMEM)\\n * when the needed structures could not get allocated, and ERR_PTR(-EINTR)\\n * when the caller was killed by a fatal signal.\\n */\\nstruct kthread_worker *\\nkthread_create_worker(unsigned int flags, const char namefmt[], ...)\\n{\\n\\tstruct kthread_worker *worker;\\n\\tva_list args;\\n\\n\\tva_start(args, namefmt);\\n\\tworker = __kthread_create_worker(-1, flags, namefmt, args);\\n\\tva_end(args);\\n\\n\\treturn worker;\\n}\\nEXPORT_SYMBOL(kthread_create_worker);\\n\\n/**\\n * kthread_create_worker_on_cpu - create a kthread worker and bind it\\n *\\tto a given CPU and the associated NUMA node.\\n * @cpu: CPU number\\n * @flags: flags modifying the default behavior of the worker\\n * @namefmt: printf-style name for the kthread worker (task).\\n *\\n * Use a valid CPU number if you want to bind the kthread worker\\n * to the given CPU and the associated NUMA node.\\n *\\n * A good practice is to add the cpu number also into the worker name.\\n * For example, use kthread_create_worker_on_cpu(cpu, \"helper/%d\", cpu).\\n *\\n * CPU hotplug:\\n * The kthread worker API is simple and generic. It just provides a way\\n * to create, use, and destroy workers.\\n *\\n * It is up to the API user how to handle CPU hotplug. They have to decide\\n * how to handle pending work items, prevent queuing new ones, and\\n * restore the functionality when the CPU goes off and on. There are a\\n * few catches:\\n *\\n *    - CPU affinity gets lost when it is scheduled on an offline CPU.\\n *\\n *    - The worker might not exist when the CPU was off when the user\\n *      created the workers.\\n *\\n * Good practice is to implement two CPU hotplug callbacks and to\\n * destroy/create the worker when the CPU goes down/up.\\n *\\n * Return:\\n * The pointer to the allocated worker on success, ERR_PTR(-ENOMEM)\\n * when the needed structures could not get allocated, and ERR_PTR(-EINTR)\\n * when the caller was killed by a fatal signal.\\n */\\nstruct kthread_worker *\\nkthread_create_worker_on_cpu(int cpu, unsigned int flags,\\n\\t\\t\\t     const char namefmt[], ...)\\n{\\n\\tstruct kthread_worker *worker;\\n\\tva_list args;\\n\\n\\tva_start(args, namefmt);\\n\\tworker = __kthread_create_worker(cpu, flags, namefmt, args);\\n\\tva_end(args);\\n\\n\\treturn worker;\\n}\\nEXPORT_SYMBOL(kthread_create_worker_on_cpu);\\n\\n/*\\n * Returns true when the work could not be queued at the moment.\\n * It happens when it is already pending in a worker list\\n * or when it is being cancelled.\\n */\\nstatic inline bool queuing_blocked(struct kthread_worker *worker,\\n\\t\\t\\t\\t   struct kthread_work *work)\\n{\\n\\tlockdep_assert_held(&worker->lock);\\n\\n\\treturn !list_empty(&work->node) || work->canceling;\\n}\\n\\nstatic void kthread_insert_work_sanity_check(struct kthread_worker *worker,\\n\\t\\t\\t\\t\\t     struct kthread_work *work)\\n{\\n\\tlockdep_assert_held(&worker->lock);\\n\\tWARN_ON_ONCE(!list_empty(&work->node));\\n\\t/* Do not use a work with >1 worker, see kthread_queue_work() */\\n\\tWARN_ON_ONCE(work->worker && work->worker != worker);\\n}\\n\\n/* insert @work before @pos in @worker */\\nstatic void kthread_insert_work(struct kthread_worker *worker,\\n\\t\\t\\t\\tstruct kthread_work *work,\\n\\t\\t\\t\\tstruct list_head *pos)\\n{\\n\\tkthread_insert_work_sanity_check(worker, work);\\n\\n\\ttrace_sched_kthread_work_queue_work(worker, work);\\n\\n\\tlist_add_tail(&work->node, pos);\\n\\twork->worker = worker;\\n\\tif (!worker->current_work && likely(worker->task))\\n\\t\\twake_up_process(worker->task);\\n}\\n\\n/**\\n * kthread_queue_work - queue a kthread_work\\n * @worker: target kthread_worker\\n * @work: kthread_work to queue\\n *\\n * Queue @work to work processor @task for async execution.  @task\\n * must have been created with kthread_worker_create().  Returns %true\\n * if @work was successfully queued, %false if it was already pending.\\n *\\n * Reinitialize the work if it needs to be used by another worker.\\n * For example, when the worker was stopped and started again.\\n */\\nbool kthread_queue_work(struct kthread_worker *worker,\\n\\t\\t\\tstruct kthread_work *work)\\n{\\n\\tbool ret = false;\\n\\tunsigned long flags;\\n\\n\\traw_spin_lock_irqsave(&worker->lock, flags);\\n\\tif (!queuing_blocked(worker, work)) {\\n\\t\\tkthread_insert_work(worker, work, &worker->work_list);\\n\\t\\tret = true;\\n\\t}\\n\\traw_spin_unlock_irqrestore(&worker->lock, flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(kthread_queue_work);\\n\\n/**\\n * kthread_delayed_work_timer_fn - callback that queues the associated kthread\\n *\\tdelayed work when the timer expires.\\n * @t: pointer to the expired timer\\n *\\n * The format of the function is defined by struct timer_list.\\n * It should have been called from irqsafe timer with irq already off.\\n */\\nvoid kthread_delayed_work_timer_fn(struct timer_list *t)\\n{\\n\\tstruct kthread_delayed_work *dwork = from_timer(dwork, t, timer);\\n\\tstruct kthread_work *work = &dwork->work;\\n\\tstruct kthread_worker *worker = work->worker;\\n\\tunsigned long flags;\\n\\n\\t/*\\n\\t * This might happen when a pending work is reinitialized.\\n\\t * It means that it is used a wrong way.\\n\\t */\\n\\tif (WARN_ON_ONCE(!worker))\\n\\t\\treturn;\\n\\n\\traw_spin_lock_irqsave(&worker->lock, flags);\\n\\t/* Work must not be used with >1 worker, see kthread_queue_work(). */\\n\\tWARN_ON_ONCE(work->worker != worker);\\n\\n\\t/* Move the work from worker->delayed_work_list. */\\n\\tWARN_ON_ONCE(list_empty(&work->node));\\n\\tlist_del_init(&work->node);\\n\\tif (!work->canceling)\\n\\t\\tkthread_insert_work(worker, work, &worker->work_list);\\n\\n\\traw_spin_unlock_irqrestore(&worker->lock, flags);\\n}\\nEXPORT_SYMBOL(kthread_delayed_work_timer_fn);\\n\\nstatic void __kthread_queue_delayed_work(struct kthread_worker *worker,\\n\\t\\t\\t\\t\\t struct kthread_delayed_work *dwork,\\n\\t\\t\\t\\t\\t unsigned long delay)\\n{\\n\\tstruct timer_list *timer = &dwork->timer;\\n\\tstruct kthread_work *work = &dwork->work;\\n\\n\\tWARN_ON_ONCE(timer->function != kthread_delayed_work_timer_fn);\\n\\n\\t/*\\n\\t * If @delay is 0, queue @dwork->work immediately.  This is for\\n\\t * both optimization and correctness.  The earliest @timer can\\n\\t * expire is on the closest next tick and delayed_work users depend\\n\\t * on that there\\'s no such delay when @delay is 0.\\n\\t */\\n\\tif (!delay) {\\n\\t\\tkthread_insert_work(worker, work, &worker->work_list);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* Be paranoid and try to detect possible races already now. */\\n\\tkthread_insert_work_sanity_check(worker, work);\\n\\n\\tlist_add(&work->node, &worker->delayed_work_list);\\n\\twork->worker = worker;\\n\\ttimer->expires = jiffies + delay;\\n\\tadd_timer(timer);\\n}\\n\\n/**\\n * kthread_queue_delayed_work - queue the associated kthread work\\n *\\tafter a delay.\\n * @worker: target kthread_worker\\n * @dwork: kthread_delayed_work to queue\\n * @delay: number of jiffies to wait before queuing\\n *\\n * If the work has not been pending it starts a timer that will queue\\n * the work after the given @delay. If @delay is zero, it queues the\\n * work immediately.\\n *\\n * Return: %false if the @work has already been pending. It means that\\n * either the timer was running or the work was queued. It returns %true\\n * otherwise.\\n */\\nbool kthread_queue_delayed_work(struct kthread_worker *worker,\\n\\t\\t\\t\\tstruct kthread_delayed_work *dwork,\\n\\t\\t\\t\\tunsigned long delay)\\n{\\n\\tstruct kthread_work *work = &dwork->work;\\n\\tunsigned long flags;\\n\\tbool ret = false;\\n\\n\\traw_spin_lock_irqsave(&worker->lock, flags);\\n\\n\\tif (!queuing_blocked(worker, work)) {\\n\\t\\t__kthread_queue_delayed_work(worker, dwork, delay);\\n\\t\\tret = true;\\n\\t}\\n\\n\\traw_spin_unlock_irqrestore(&worker->lock, flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(kthread_queue_delayed_work);\\n\\nstruct kthread_flush_work {\\n\\tstruct kthread_work\\twork;\\n\\tstruct completion\\tdone;\\n};\\n\\nstatic void kthread_flush_work_fn(struct kthread_work *work)\\n{\\n\\tstruct kthread_flush_work *fwork =\\n\\t\\tcontainer_of(work, struct kthread_flush_work, work);\\n\\tcomplete(&fwork->done);\\n}\\n\\n/**\\n * kthread_flush_work - flush a kthread_work\\n * @work: work to flush\\n *\\n * If @work is queued or executing, wait for it to finish execution.\\n */\\nvoid kthread_flush_work(struct kthread_work *work)\\n{\\n\\tstruct kthread_flush_work fwork = {\\n\\t\\tKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\\n\\t\\tCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\\n\\t};\\n\\tstruct kthread_worker *worker;\\n\\tbool noop = false;\\n\\n\\tworker = work->worker;\\n\\tif (!worker)\\n\\t\\treturn;\\n\\n\\traw_spin_lock_irq(&worker->lock);\\n\\t/* Work must not be used with >1 worker, see kthread_queue_work(). */\\n\\tWARN_ON_ONCE(work->worker != worker);\\n\\n\\tif (!list_empty(&work->node))\\n\\t\\tkthread_insert_work(worker, &fwork.work, work->node.next);\\n\\telse if (worker->current_work == work)\\n\\t\\tkthread_insert_work(worker, &fwork.work,\\n\\t\\t\\t\\t    worker->work_list.next);\\n\\telse\\n\\t\\tnoop = true;\\n\\n\\traw_spin_unlock_irq(&worker->lock);\\n\\n\\tif (!noop)\\n\\t\\twait_for_completion(&fwork.done);\\n}\\nEXPORT_SYMBOL_GPL(kthread_flush_work);\\n\\n/*\\n * Make sure that the timer is neither set nor running and could\\n * not manipulate the work list_head any longer.\\n *\\n * The function is called under worker->lock. The lock is temporary\\n * released but the timer can\\'t be set again in the meantime.\\n */\\nstatic void kthread_cancel_delayed_work_timer(struct kthread_work *work,\\n\\t\\t\\t\\t\\t      unsigned long *flags)\\n{\\n\\tstruct kthread_delayed_work *dwork =\\n\\t\\tcontainer_of(work, struct kthread_delayed_work, work);\\n\\tstruct kthread_worker *worker = work->worker;\\n\\n\\t/*\\n\\t * del_timer_sync() must be called to make sure that the timer\\n\\t * callback is not running. The lock must be temporary released\\n\\t * to avoid a deadlock with the callback. In the meantime,\\n\\t * any queuing is blocked by setting the canceling counter.\\n\\t */\\n\\twork->canceling++;\\n\\traw_spin_unlock_irqrestore(&worker->lock, *flags);\\n\\tdel_timer_sync(&dwork->timer);\\n\\traw_spin_lock_irqsave(&worker->lock, *flags);\\n\\twork->canceling--;\\n}\\n\\n/*\\n * This function removes the work from the worker queue.\\n *\\n * It is called under worker->lock. The caller must make sure that\\n * the timer used by delayed work is not running, e.g. by calling\\n * kthread_cancel_delayed_work_timer().\\n *\\n * The work might still be in use when this function finishes. See the\\n * current_work proceed by the worker.\\n *\\n * Return: %true if @work was pending and successfully canceled,\\n *\\t%false if @work was not pending\\n */\\nstatic bool __kthread_cancel_work(struct kthread_work *work)\\n{\\n\\t/*\\n\\t * Try to remove the work from a worker list. It might either\\n\\t * be from worker->work_list or from worker->delayed_work_list.\\n\\t */\\n\\tif (!list_empty(&work->node)) {\\n\\t\\tlist_del_init(&work->node);\\n\\t\\treturn true;\\n\\t}\\n\\n\\treturn false;\\n}\\n\\n/**\\n * kthread_mod_delayed_work - modify delay of or queue a kthread delayed work\\n * @worker: kthread worker to use\\n * @dwork: kthread delayed work to queue\\n * @delay: number of jiffies to wait before queuing\\n *\\n * If @dwork is idle, equivalent to kthread_queue_delayed_work(). Otherwise,\\n * modify @dwork\\'s timer so that it expires after @delay. If @delay is zero,\\n * @work is guaranteed to be queued immediately.\\n *\\n * Return: %false if @dwork was idle and queued, %true otherwise.\\n *\\n * A special case is when the work is being canceled in parallel.\\n * It might be caused either by the real kthread_cancel_delayed_work_sync()\\n * or yet another kthread_mod_delayed_work() call. We let the other command\\n * win and return %true here. The return value can be used for reference\\n * counting and the number of queued works stays the same. Anyway, the caller\\n * is supposed to synchronize these operations a reasonable way.\\n *\\n * This function is safe to call from any context including IRQ handler.\\n * See __kthread_cancel_work() and kthread_delayed_work_timer_fn()\\n * for details.\\n */\\nbool kthread_mod_delayed_work(struct kthread_worker *worker,\\n\\t\\t\\t      struct kthread_delayed_work *dwork,\\n\\t\\t\\t      unsigned long delay)\\n{\\n\\tstruct kthread_work *work = &dwork->work;\\n\\tunsigned long flags;\\n\\tint ret;\\n\\n\\traw_spin_lock_irqsave(&worker->lock, flags);\\n\\n\\t/* Do not bother with canceling when never queued. */\\n\\tif (!work->worker) {\\n\\t\\tret = false;\\n\\t\\tgoto fast_queue;\\n\\t}\\n\\n\\t/* Work must not be used with >1 worker, see kthread_queue_work() */\\n\\tWARN_ON_ONCE(work->worker != worker);\\n\\n\\t/*\\n\\t * Temporary cancel the work but do not fight with another command\\n\\t * that is canceling the work as well.\\n\\t *\\n\\t * It is a bit tricky because of possible races with another\\n\\t * mod_delayed_work() and cancel_delayed_work() callers.\\n\\t *\\n\\t * The timer must be canceled first because worker->lock is released\\n\\t * when doing so. But the work can be removed from the queue (list)\\n\\t * only when it can be queued again so that the return value can\\n\\t * be used for reference counting.\\n\\t */\\n\\tkthread_cancel_delayed_work_timer(work, &flags);\\n\\tif (work->canceling) {\\n\\t\\t/* The number of works in the queue does not change. */\\n\\t\\tret = true;\\n\\t\\tgoto out;\\n\\t}\\n\\tret = __kthread_cancel_work(work);\\n\\nfast_queue:\\n\\t__kthread_queue_delayed_work(worker, dwork, delay);\\nout:\\n\\traw_spin_unlock_irqrestore(&worker->lock, flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(kthread_mod_delayed_work);\\n\\nstatic bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)\\n{\\n\\tstruct kthread_worker *worker = work->worker;\\n\\tunsigned long flags;\\n\\tint ret = false;\\n\\n\\tif (!worker)\\n\\t\\tgoto out;\\n\\n\\traw_spin_lock_irqsave(&worker->lock, flags);\\n\\t/* Work must not be used with >1 worker, see kthread_queue_work(). */\\n\\tWARN_ON_ONCE(work->worker != worker);\\n\\n\\tif (is_dwork)\\n\\t\\tkthread_cancel_delayed_work_timer(work, &flags);\\n\\n\\tret = __kthread_cancel_work(work);\\n\\n\\tif (worker->current_work != work)\\n\\t\\tgoto out_fast;\\n\\n\\t/*\\n\\t * The work is in progress and we need to wait with the lock released.\\n\\t * In the meantime, block any queuing by setting the canceling counter.\\n\\t */\\n\\twork->canceling++;\\n\\traw_spin_unlock_irqrestore(&worker->lock, flags);\\n\\tkthread_flush_work(work);\\n\\traw_spin_lock_irqsave(&worker->lock, flags);\\n\\twork->canceling--;\\n\\nout_fast:\\n\\traw_spin_unlock_irqrestore(&worker->lock, flags);\\nout:\\n\\treturn ret;\\n}\\n\\n/**\\n * kthread_cancel_work_sync - cancel a kthread work and wait for it to finish\\n * @work: the kthread work to cancel\\n *\\n * Cancel @work and wait for its execution to finish.  This function\\n * can be used even if the work re-queues itself. On return from this\\n * function, @work is guaranteed to be not pending or executing on any CPU.\\n *\\n * kthread_cancel_work_sync(&delayed_work->work) must not be used for\\n * delayed_work\\'s. Use kthread_cancel_delayed_work_sync() instead.\\n *\\n * The caller must ensure that the worker on which @work was last\\n * queued can\\'t be destroyed before this function returns.\\n *\\n * Return: %true if @work was pending, %false otherwise.\\n */\\nbool kthread_cancel_work_sync(struct kthread_work *work)\\n{\\n\\treturn __kthread_cancel_work_sync(work, false);\\n}\\nEXPORT_SYMBOL_GPL(kthread_cancel_work_sync);\\n\\n/**\\n * kthread_cancel_delayed_work_sync - cancel a kthread delayed work and\\n *\\twait for it to finish.\\n * @dwork: the kthread delayed work to cancel\\n *\\n * This is kthread_cancel_work_sync() for delayed works.\\n *\\n * Return: %true if @dwork was pending, %false otherwise.\\n */\\nbool kthread_cancel_delayed_work_sync(struct kthread_delayed_work *dwork)\\n{\\n\\treturn __kthread_cancel_work_sync(&dwork->work, true);\\n}\\nEXPORT_SYMBOL_GPL(kthread_cancel_delayed_work_sync);\\n\\n/**\\n * kthread_flush_worker - flush all current works on a kthread_worker\\n * @worker: worker to flush\\n *\\n * Wait until all currently executing or pending works on @worker are\\n * finished.\\n */\\nvoid kthread_flush_worker(struct kthread_worker *worker)\\n{\\n\\tstruct kthread_flush_work fwork = {\\n\\t\\tKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\\n\\t\\tCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\\n\\t};\\n\\n\\tkthread_queue_work(worker, &fwork.work);\\n\\twait_for_completion(&fwork.done);\\n}\\nEXPORT_SYMBOL_GPL(kthread_flush_worker);\\n\\n/**\\n * kthread_destroy_worker - destroy a kthread worker\\n * @worker: worker to be destroyed\\n *\\n * Flush and destroy @worker.  The simple flush is enough because the kthread\\n * worker API is used only in trivial scenarios.  There are no multi-step state\\n * machines needed.\\n *\\n * Note that this function is not responsible for handling delayed work, so\\n * caller should be responsible for queuing or canceling all delayed work items\\n * before invoke this function.\\n */\\nvoid kthread_destroy_worker(struct kthread_worker *worker)\\n{\\n\\tstruct task_struct *task;\\n\\n\\ttask = worker->task;\\n\\tif (WARN_ON(!task))\\n\\t\\treturn;\\n\\n\\tkthread_flush_worker(worker);\\n\\tkthread_stop(task);\\n\\tWARN_ON(!list_empty(&worker->delayed_work_list));\\n\\tWARN_ON(!list_empty(&worker->work_list));\\n\\tkfree(worker);\\n}\\nEXPORT_SYMBOL(kthread_destroy_worker);\\n\\n/**\\n * kthread_use_mm - make the calling kthread operate on an address space\\n * @mm: address space to operate on\\n */\\nvoid kthread_use_mm(struct mm_struct *mm)\\n{\\n\\tstruct mm_struct *active_mm;\\n\\tstruct task_struct *tsk = current;\\n\\n\\tWARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));\\n\\tWARN_ON_ONCE(tsk->mm);\\n\\n\\t/*\\n\\t * It is possible for mm to be the same as tsk->active_mm, but\\n\\t * we must still mmgrab(mm) and mmdrop_lazy_tlb(active_mm),\\n\\t * because these references are not equivalent.\\n\\t */\\n\\tmmgrab(mm);\\n\\n\\ttask_lock(tsk);\\n\\t/* Hold off tlb flush IPIs while switching mm\\'s */\\n\\tlocal_irq_disable();\\n\\tactive_mm = tsk->active_mm;\\n\\ttsk->active_mm = mm;\\n\\ttsk->mm = mm;\\n\\tmembarrier_update_current_mm(mm);\\n\\tswitch_mm_irqs_off(active_mm, mm, tsk);\\n\\tlocal_irq_enable();\\n\\ttask_unlock(tsk);\\n#ifdef finish_arch_post_lock_switch\\n\\tfinish_arch_post_lock_switch();\\n#endif\\n\\n\\t/*\\n\\t * When a kthread starts operating on an address space, the loop\\n\\t * in membarrier_{private,global}_expedited() may not observe\\n\\t * that tsk->mm, and not issue an IPI. Membarrier requires a\\n\\t * memory barrier after storing to tsk->mm, before accessing\\n\\t * user-space memory. A full memory barrier for membarrier\\n\\t * {PRIVATE,GLOBAL}_EXPEDITED is implicitly provided by\\n\\t * mmdrop_lazy_tlb().\\n\\t */\\n\\tmmdrop_lazy_tlb(active_mm);\\n}\\nEXPORT_SYMBOL_GPL(kthread_use_mm);\\n\\n/**\\n * kthread_unuse_mm - reverse the effect of kthread_use_mm()\\n * @mm: address space to operate on\\n */\\nvoid kthread_unuse_mm(struct mm_struct *mm)\\n{\\n\\tstruct task_struct *tsk = current;\\n\\n\\tWARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));\\n\\tWARN_ON_ONCE(!tsk->mm);\\n\\n\\ttask_lock(tsk);\\n\\t/*\\n\\t * When a kthread stops operating on an address space, the loop\\n\\t * in membarrier_{private,global}_expedited() may not observe\\n\\t * that tsk->mm, and not issue an IPI. Membarrier requires a\\n\\t * memory barrier after accessing user-space memory, before\\n\\t * clearing tsk->mm.\\n\\t */\\n\\tsmp_mb__after_spinlock();\\n\\tlocal_irq_disable();\\n\\ttsk->mm = NULL;\\n\\tmembarrier_update_current_mm(NULL);\\n\\tmmgrab_lazy_tlb(mm);\\n\\t/* active_mm is still \\'mm\\' */\\n\\tenter_lazy_tlb(mm, tsk);\\n\\tlocal_irq_enable();\\n\\ttask_unlock(tsk);\\n\\n\\tmmdrop(mm);\\n}\\nEXPORT_SYMBOL_GPL(kthread_unuse_mm);\\n\\n#ifdef CONFIG_BLK_CGROUP\\n/**\\n * kthread_associate_blkcg - associate blkcg to current kthread\\n * @css: the cgroup info\\n *\\n * Current thread must be a kthread. The thread is running jobs on behalf of\\n * other threads. In some cases, we expect the jobs attach cgroup info of\\n * original threads instead of that of current thread. This function stores\\n * original thread\\'s cgroup info in current kthread context for later\\n * retrieval.\\n */\\nvoid kthread_associate_blkcg(struct cgroup_subsys_state *css)\\n{\\n\\tstruct kthread *kthread;\\n\\n\\tif (!(current->flags & PF_KTHREAD))\\n\\t\\treturn;\\n\\tkthread = to_kthread(current);\\n\\tif (!kthread)\\n\\t\\treturn;\\n\\n\\tif (kthread->blkcg_css) {\\n\\t\\tcss_put(kthread->blkcg_css);\\n\\t\\tkthread->blkcg_css = NULL;\\n\\t}\\n\\tif (css) {\\n\\t\\tcss_get(css);\\n\\t\\tkthread->blkcg_css = css;\\n\\t}\\n}\\nEXPORT_SYMBOL(kthread_associate_blkcg);\\n\\n/**\\n * kthread_blkcg - get associated blkcg css of current kthread\\n *\\n * Current thread must be a kthread.\\n */\\nstruct cgroup_subsys_state *kthread_blkcg(void)\\n{\\n\\tstruct kthread *kthread;\\n\\n\\tif (current->flags & PF_KTHREAD) {\\n\\t\\tkthread = to_kthread(current);\\n\\t\\tif (kthread)\\n\\t\\t\\treturn kthread->blkcg_css;\\n\\t}\\n\\treturn NULL;\\n}\\n#endif\\n\\n// SPDX-License-Identifier: GPL-2.0+\\n//\\n// Torture test for smp_call_function() and friends.\\n//\\n// Copyright (C) Facebook, 2020.\\n//\\n// Author: Paul E. McKenney <paulmck@kernel.org>\\n\\n#define pr_fmt(fmt) fmt\\n\\n#include <linux/atomic.h>\\n#include <linux/bitops.h>\\n#include <linux/completion.h>\\n#include <linux/cpu.h>\\n#include <linux/delay.h>\\n#include <linux/err.h>\\n#include <linux/init.h>\\n#include <linux/interrupt.h>\\n#include <linux/kthread.h>\\n#include <linux/kernel.h>\\n#include <linux/mm.h>\\n#include <linux/module.h>\\n#include <linux/moduleparam.h>\\n#include <linux/notifier.h>\\n#include <linux/percpu.h>\\n#include <linux/rcupdate.h>\\n#include <linux/rcupdate_trace.h>\\n#include <linux/reboot.h>\\n#include <linux/sched.h>\\n#include <linux/spinlock.h>\\n#include <linux/smp.h>\\n#include <linux/stat.h>\\n#include <linux/srcu.h>\\n#include <linux/slab.h>\\n#include <linux/torture.h>\\n#include <linux/types.h>\\n\\n#define SCFTORT_STRING \"scftorture\"\\n#define SCFTORT_FLAG SCFTORT_STRING \": \"\\n\\n#define VERBOSE_SCFTORTOUT(s, x...) \\\\\\n\\tdo { if (verbose) pr_alert(SCFTORT_FLAG s \"\\\\n\", ## x); } while (0)\\n\\n#define SCFTORTOUT_ERRSTRING(s, x...) pr_alert(SCFTORT_FLAG \"!!! \" s \"\\\\n\", ## x)\\n\\nMODULE_DESCRIPTION(\"Torture tests on the smp_call_function() family of primitives\");\\nMODULE_LICENSE(\"GPL\");\\nMODULE_AUTHOR(\"Paul E. McKenney <paulmck@kernel.org>\");\\n\\n// Wait until there are multiple CPUs before starting test.\\ntorture_param(int, holdoff, IS_BUILTIN(CONFIG_SCF_TORTURE_TEST) ? 10 : 0,\\n\\t      \"Holdoff time before test start (s)\");\\ntorture_param(int, longwait, 0, \"Include ridiculously long waits? (seconds)\");\\ntorture_param(int, nthreads, -1, \"# threads, defaults to -1 for all CPUs.\");\\ntorture_param(int, onoff_holdoff, 0, \"Time after boot before CPU hotplugs (s)\");\\ntorture_param(int, onoff_interval, 0, \"Time between CPU hotplugs (s), 0=disable\");\\ntorture_param(int, shutdown_secs, 0, \"Shutdown time (ms), <= zero to disable.\");\\ntorture_param(int, stat_interval, 60, \"Number of seconds between stats printk()s.\");\\ntorture_param(int, stutter, 5, \"Number of jiffies to run/halt test, 0=disable\");\\ntorture_param(bool, use_cpus_read_lock, 0, \"Use cpus_read_lock() to exclude CPU hotplug.\");\\ntorture_param(int, verbose, 0, \"Enable verbose debugging printk()s\");\\ntorture_param(int, weight_resched, -1, \"Testing weight for resched_cpu() operations.\");\\ntorture_param(int, weight_single, -1, \"Testing weight for single-CPU no-wait operations.\");\\ntorture_param(int, weight_single_rpc, -1, \"Testing weight for single-CPU RPC operations.\");\\ntorture_param(int, weight_single_wait, -1, \"Testing weight for single-CPU operations.\");\\ntorture_param(int, weight_many, -1, \"Testing weight for multi-CPU no-wait operations.\");\\ntorture_param(int, weight_many_wait, -1, \"Testing weight for multi-CPU operations.\");\\ntorture_param(int, weight_all, -1, \"Testing weight for all-CPU no-wait operations.\");\\ntorture_param(int, weight_all_wait, -1, \"Testing weight for all-CPU operations.\");\\n\\nstatic char *torture_type = \"\";\\n\\n#ifdef MODULE\\n# define SCFTORT_SHUTDOWN 0\\n#else\\n# define SCFTORT_SHUTDOWN 1\\n#endif\\n\\ntorture_param(bool, shutdown, SCFTORT_SHUTDOWN, \"Shutdown at end of torture test.\");\\n\\nstruct scf_statistics {\\n\\tstruct task_struct *task;\\n\\tint cpu;\\n\\tlong long n_resched;\\n\\tlong long n_single;\\n\\tlong long n_single_ofl;\\n\\tlong long n_single_rpc;\\n\\tlong long n_single_rpc_ofl;\\n\\tlong long n_single_wait;\\n\\tlong long n_single_wait_ofl;\\n\\tlong long n_many;\\n\\tlong long n_many_wait;\\n\\tlong long n_all;\\n\\tlong long n_all_wait;\\n};\\n\\nstatic struct scf_statistics *scf_stats_p;\\nstatic struct task_struct *scf_torture_stats_task;\\nstatic DEFINE_PER_CPU(long long, scf_invoked_count);\\nstatic DEFINE_PER_CPU(struct llist_head, scf_free_pool);\\n\\n// Data for random primitive selection\\n#define SCF_PRIM_RESCHED\\t0\\n#define SCF_PRIM_SINGLE\\t\\t1\\n#define SCF_PRIM_SINGLE_RPC\\t2\\n#define SCF_PRIM_MANY\\t\\t3\\n#define SCF_PRIM_ALL\\t\\t4\\n#define SCF_NPRIMS\\t\\t8 // Need wait and no-wait versions of each,\\n\\t\\t\\t\\t  //  except for SCF_PRIM_RESCHED and\\n\\t\\t\\t\\t  //  SCF_PRIM_SINGLE_RPC.\\n\\nstatic char *scf_prim_name[] = {\\n\\t\"resched_cpu\",\\n\\t\"smp_call_function_single\",\\n\\t\"smp_call_function_single_rpc\",\\n\\t\"smp_call_function_many\",\\n\\t\"smp_call_function\",\\n};\\n\\nstruct scf_selector {\\n\\tunsigned long scfs_weight;\\n\\tint scfs_prim;\\n\\tbool scfs_wait;\\n};\\nstatic struct scf_selector scf_sel_array[SCF_NPRIMS];\\nstatic int scf_sel_array_len;\\nstatic unsigned long scf_sel_totweight;\\n\\n// Communicate between caller and handler.\\nstruct scf_check {\\n\\tbool scfc_in;\\n\\tbool scfc_out;\\n\\tint scfc_cpu; // -1 for not _single().\\n\\tbool scfc_wait;\\n\\tbool scfc_rpc;\\n\\tstruct completion scfc_completion;\\n\\tstruct llist_node scf_node;\\n};\\n\\n// Use to wait for all threads to start.\\nstatic atomic_t n_started;\\nstatic atomic_t n_errs;\\nstatic atomic_t n_mb_in_errs;\\nstatic atomic_t n_mb_out_errs;\\nstatic atomic_t n_alloc_errs;\\nstatic bool scfdone;\\nstatic char *bangstr = \"\";\\n\\nstatic DEFINE_TORTURE_RANDOM_PERCPU(scf_torture_rand);\\n\\nextern void resched_cpu(int cpu); // An alternative IPI vector.\\n\\nstatic void scf_add_to_free_list(struct scf_check *scfcp)\\n{\\n\\tstruct llist_head *pool;\\n\\tunsigned int cpu;\\n\\n\\tif (!scfcp)\\n\\t\\treturn;\\n\\tcpu = raw_smp_processor_id() % nthreads;\\n\\tpool = &per_cpu(scf_free_pool, cpu);\\n\\tllist_add(&scfcp->scf_node, pool);\\n}\\n\\nstatic void scf_cleanup_free_list(unsigned int cpu)\\n{\\n\\tstruct llist_head *pool;\\n\\tstruct llist_node *node;\\n\\tstruct scf_check *scfcp;\\n\\n\\tpool = &per_cpu(scf_free_pool, cpu);\\n\\tnode = llist_del_all(pool);\\n\\twhile (node) {\\n\\t\\tscfcp = llist_entry(node, struct scf_check, scf_node);\\n\\t\\tnode = node->next;\\n\\t\\tkfree(scfcp);\\n\\t}\\n}\\n\\n// Print torture statistics.  Caller must ensure serialization.\\nstatic void scf_torture_stats_print(void)\\n{\\n\\tint cpu;\\n\\tint i;\\n\\tlong long invoked_count = 0;\\n\\tbool isdone = READ_ONCE(scfdone);\\n\\tstruct scf_statistics scfs = {};\\n\\n\\tfor_each_possible_cpu(cpu)\\n\\t\\tinvoked_count += data_race(per_cpu(scf_invoked_count, cpu));\\n\\tfor (i = 0; i < nthreads; i++) {\\n\\t\\tscfs.n_resched += scf_stats_p[i].n_resched;\\n\\t\\tscfs.n_single += scf_stats_p[i].n_single;\\n\\t\\tscfs.n_single_ofl += scf_stats_p[i].n_single_ofl;\\n\\t\\tscfs.n_single_rpc += scf_stats_p[i].n_single_rpc;\\n\\t\\tscfs.n_single_wait += scf_stats_p[i].n_single_wait;\\n\\t\\tscfs.n_single_wait_ofl += scf_stats_p[i].n_single_wait_ofl;\\n\\t\\tscfs.n_many += scf_stats_p[i].n_many;\\n\\t\\tscfs.n_many_wait += scf_stats_p[i].n_many_wait;\\n\\t\\tscfs.n_all += scf_stats_p[i].n_all;\\n\\t\\tscfs.n_all_wait += scf_stats_p[i].n_all_wait;\\n\\t}\\n\\tif (atomic_read(&n_errs) || atomic_read(&n_mb_in_errs) ||\\n\\t    atomic_read(&n_mb_out_errs) ||\\n\\t    (!IS_ENABLED(CONFIG_KASAN) && atomic_read(&n_alloc_errs)))\\n\\t\\tbangstr = \"!!! \";\\n\\tpr_alert(\"%s %sscf_invoked_count %s: %lld resched: %lld single: %lld/%lld single_ofl: %lld/%lld single_rpc: %lld single_rpc_ofl: %lld many: %lld/%lld all: %lld/%lld \",\\n\\t\\t SCFTORT_FLAG, bangstr, isdone ? \"VER\" : \"ver\", invoked_count, scfs.n_resched,\\n\\t\\t scfs.n_single, scfs.n_single_wait, scfs.n_single_ofl, scfs.n_single_wait_ofl,\\n\\t\\t scfs.n_single_rpc, scfs.n_single_rpc_ofl,\\n\\t\\t scfs.n_many, scfs.n_many_wait, scfs.n_all, scfs.n_all_wait);\\n\\ttorture_onoff_stats();\\n\\tpr_cont(\"ste: %d stnmie: %d stnmoe: %d staf: %d\\\\n\", atomic_read(&n_errs),\\n\\t\\tatomic_read(&n_mb_in_errs), atomic_read(&n_mb_out_errs),\\n\\t\\tatomic_read(&n_alloc_errs));\\n}\\n\\n// Periodically prints torture statistics, if periodic statistics printing\\n// was specified via the stat_interval module parameter.\\nstatic int\\nscf_torture_stats(void *arg)\\n{\\n\\tVERBOSE_TOROUT_STRING(\"scf_torture_stats task started\");\\n\\tdo {\\n\\t\\tschedule_timeout_interruptible(stat_interval * HZ);\\n\\t\\tscf_torture_stats_print();\\n\\t\\ttorture_shutdown_absorb(\"scf_torture_stats\");\\n\\t} while (!torture_must_stop());\\n\\ttorture_kthread_stopping(\"scf_torture_stats\");\\n\\treturn 0;\\n}\\n\\n// Add a primitive to the scf_sel_array[].\\nstatic void scf_sel_add(unsigned long weight, int prim, bool wait)\\n{\\n\\tstruct scf_selector *scfsp = &scf_sel_array[scf_sel_array_len];\\n\\n\\t// If no weight, if array would overflow, if computing three-place\\n\\t// percentages would overflow, or if the scf_prim_name[] array would\\n\\t// overflow, don\\'t bother.  In the last three two cases, complain.\\n\\tif (!weight ||\\n\\t    WARN_ON_ONCE(scf_sel_array_len >= ARRAY_SIZE(scf_sel_array)) ||\\n\\t    WARN_ON_ONCE(0 - 100000 * weight <= 100000 * scf_sel_totweight) ||\\n\\t    WARN_ON_ONCE(prim >= ARRAY_SIZE(scf_prim_name)))\\n\\t\\treturn;\\n\\tscf_sel_totweight += weight;\\n\\tscfsp->scfs_weight = scf_sel_totweight;\\n\\tscfsp->scfs_prim = prim;\\n\\tscfsp->scfs_wait = wait;\\n\\tscf_sel_array_len++;\\n}\\n\\n// Dump out weighting percentages for scf_prim_name[] array.\\nstatic void scf_sel_dump(void)\\n{\\n\\tint i;\\n\\tunsigned long oldw = 0;\\n\\tstruct scf_selector *scfsp;\\n\\tunsigned long w;\\n\\n\\tfor (i = 0; i < scf_sel_array_len; i++) {\\n\\t\\tscfsp = &scf_sel_array[i];\\n\\t\\tw = (scfsp->scfs_weight - oldw) * 100000 / scf_sel_totweight;\\n\\t\\tpr_info(\"%s: %3lu.%03lu %s(%s)\\\\n\", __func__, w / 1000, w % 1000,\\n\\t\\t\\tscf_prim_name[scfsp->scfs_prim],\\n\\t\\t\\tscfsp->scfs_wait ? \"wait\" : \"nowait\");\\n\\t\\toldw = scfsp->scfs_weight;\\n\\t}\\n}\\n\\n// Randomly pick a primitive and wait/nowait, based on weightings.\\nstatic struct scf_selector *scf_sel_rand(struct torture_random_state *trsp)\\n{\\n\\tint i;\\n\\tunsigned long w = torture_random(trsp) % (scf_sel_totweight + 1);\\n\\n\\tfor (i = 0; i < scf_sel_array_len; i++)\\n\\t\\tif (scf_sel_array[i].scfs_weight >= w)\\n\\t\\t\\treturn &scf_sel_array[i];\\n\\tWARN_ON_ONCE(1);\\n\\treturn &scf_sel_array[0];\\n}\\n\\n// Update statistics and occasionally burn up mass quantities of CPU time,\\n// if told to do so via scftorture.longwait.  Otherwise, occasionally burn\\n// a little bit.\\nstatic void scf_handler(void *scfc_in)\\n{\\n\\tint i;\\n\\tint j;\\n\\tunsigned long r = torture_random(this_cpu_ptr(&scf_torture_rand));\\n\\tstruct scf_check *scfcp = scfc_in;\\n\\n\\tif (likely(scfcp)) {\\n\\t\\tWRITE_ONCE(scfcp->scfc_out, false); // For multiple receivers.\\n\\t\\tif (WARN_ON_ONCE(unlikely(!READ_ONCE(scfcp->scfc_in))))\\n\\t\\t\\tatomic_inc(&n_mb_in_errs);\\n\\t}\\n\\tthis_cpu_inc(scf_invoked_count);\\n\\tif (longwait <= 0) {\\n\\t\\tif (!(r & 0xffc0)) {\\n\\t\\t\\tudelay(r & 0x3f);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\tif (r & 0xfff)\\n\\t\\tgoto out;\\n\\tr = (r >> 12);\\n\\tif (longwait <= 0) {\\n\\t\\tudelay((r & 0xff) + 1);\\n\\t\\tgoto out;\\n\\t}\\n\\tr = r % longwait + 1;\\n\\tfor (i = 0; i < r; i++) {\\n\\t\\tfor (j = 0; j < 1000; j++) {\\n\\t\\t\\tudelay(1000);\\n\\t\\t\\tcpu_relax();\\n\\t\\t}\\n\\t}\\nout:\\n\\tif (unlikely(!scfcp))\\n\\t\\treturn;\\n\\tif (scfcp->scfc_wait) {\\n\\t\\tWRITE_ONCE(scfcp->scfc_out, true);\\n\\t\\tif (scfcp->scfc_rpc)\\n\\t\\t\\tcomplete(&scfcp->scfc_completion);\\n\\t} else {\\n\\t\\tscf_add_to_free_list(scfcp);\\n\\t}\\n}\\n\\n// As above, but check for correct CPU.\\nstatic void scf_handler_1(void *scfc_in)\\n{\\n\\tstruct scf_check *scfcp = scfc_in;\\n\\n\\tif (likely(scfcp) && WARN_ONCE(smp_processor_id() != scfcp->scfc_cpu, \"%s: Wanted CPU %d got CPU %d\\\\n\", __func__, scfcp->scfc_cpu, smp_processor_id())) {\\n\\t\\tatomic_inc(&n_errs);\\n\\t}\\n\\tscf_handler(scfcp);\\n}\\n\\n// Randomly do an smp_call_function*() invocation.\\nstatic void scftorture_invoke_one(struct scf_statistics *scfp, struct torture_random_state *trsp)\\n{\\n\\tbool allocfail = false;\\n\\tuintptr_t cpu;\\n\\tint ret = 0;\\n\\tstruct scf_check *scfcp = NULL;\\n\\tstruct scf_selector *scfsp = scf_sel_rand(trsp);\\n\\n\\tif (scfsp->scfs_prim == SCF_PRIM_SINGLE || scfsp->scfs_wait) {\\n\\t\\tscfcp = kmalloc(sizeof(*scfcp), GFP_ATOMIC);\\n\\t\\tif (!scfcp) {\\n\\t\\t\\tWARN_ON_ONCE(!IS_ENABLED(CONFIG_KASAN));\\n\\t\\t\\tatomic_inc(&n_alloc_errs);\\n\\t\\t\\tallocfail = true;\\n\\t\\t} else {\\n\\t\\t\\tscfcp->scfc_cpu = -1;\\n\\t\\t\\tscfcp->scfc_wait = scfsp->scfs_wait;\\n\\t\\t\\tscfcp->scfc_out = false;\\n\\t\\t\\tscfcp->scfc_rpc = false;\\n\\t\\t}\\n\\t}\\n\\tif (use_cpus_read_lock)\\n\\t\\tcpus_read_lock();\\n\\telse\\n\\t\\tpreempt_disable();\\n\\tswitch (scfsp->scfs_prim) {\\n\\tcase SCF_PRIM_RESCHED:\\n\\t\\tif (IS_BUILTIN(CONFIG_SCF_TORTURE_TEST)) {\\n\\t\\t\\tcpu = torture_random(trsp) % nr_cpu_ids;\\n\\t\\t\\tscfp->n_resched++;\\n\\t\\t\\tresched_cpu(cpu);\\n\\t\\t\\tthis_cpu_inc(scf_invoked_count);\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase SCF_PRIM_SINGLE:\\n\\t\\tcpu = torture_random(trsp) % nr_cpu_ids;\\n\\t\\tif (scfsp->scfs_wait)\\n\\t\\t\\tscfp->n_single_wait++;\\n\\t\\telse\\n\\t\\t\\tscfp->n_single++;\\n\\t\\tif (scfcp) {\\n\\t\\t\\tscfcp->scfc_cpu = cpu;\\n\\t\\t\\tbarrier(); // Prevent race-reduction compiler optimizations.\\n\\t\\t\\tscfcp->scfc_in = true;\\n\\t\\t}\\n\\t\\tret = smp_call_function_single(cpu, scf_handler_1, (void *)scfcp, scfsp->scfs_wait);\\n\\t\\tif (ret) {\\n\\t\\t\\tif (scfsp->scfs_wait)\\n\\t\\t\\t\\tscfp->n_single_wait_ofl++;\\n\\t\\t\\telse\\n\\t\\t\\t\\tscfp->n_single_ofl++;\\n\\t\\t\\tscf_add_to_free_list(scfcp);\\n\\t\\t\\tscfcp = NULL;\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase SCF_PRIM_SINGLE_RPC:\\n\\t\\tif (!scfcp)\\n\\t\\t\\tbreak;\\n\\t\\tcpu = torture_random(trsp) % nr_cpu_ids;\\n\\t\\tscfp->n_single_rpc++;\\n\\t\\tscfcp->scfc_cpu = cpu;\\n\\t\\tscfcp->scfc_wait = true;\\n\\t\\tinit_completion(&scfcp->scfc_completion);\\n\\t\\tscfcp->scfc_rpc = true;\\n\\t\\tbarrier(); // Prevent race-reduction compiler optimizations.\\n\\t\\tscfcp->scfc_in = true;\\n\\t\\tret = smp_call_function_single(cpu, scf_handler_1, (void *)scfcp, 0);\\n\\t\\tif (!ret) {\\n\\t\\t\\tif (use_cpus_read_lock)\\n\\t\\t\\t\\tcpus_read_unlock();\\n\\t\\t\\telse\\n\\t\\t\\t\\tpreempt_enable();\\n\\t\\t\\twait_for_completion(&scfcp->scfc_completion);\\n\\t\\t\\tif (use_cpus_read_lock)\\n\\t\\t\\t\\tcpus_read_lock();\\n\\t\\t\\telse\\n\\t\\t\\t\\tpreempt_disable();\\n\\t\\t} else {\\n\\t\\t\\tscfp->n_single_rpc_ofl++;\\n\\t\\t\\tscf_add_to_free_list(scfcp);\\n\\t\\t\\tscfcp = NULL;\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase SCF_PRIM_MANY:\\n\\t\\tif (scfsp->scfs_wait)\\n\\t\\t\\tscfp->n_many_wait++;\\n\\t\\telse\\n\\t\\t\\tscfp->n_many++;\\n\\t\\tif (scfcp) {\\n\\t\\t\\tbarrier(); // Prevent race-reduction compiler optimizations.\\n\\t\\t\\tscfcp->scfc_in = true;\\n\\t\\t}\\n\\t\\tsmp_call_function_many(cpu_online_mask, scf_handler, scfcp, scfsp->scfs_wait);\\n\\t\\tbreak;\\n\\tcase SCF_PRIM_ALL:\\n\\t\\tif (scfsp->scfs_wait)\\n\\t\\t\\tscfp->n_all_wait++;\\n\\t\\telse\\n\\t\\t\\tscfp->n_all++;\\n\\t\\tif (scfcp) {\\n\\t\\t\\tbarrier(); // Prevent race-reduction compiler optimizations.\\n\\t\\t\\tscfcp->scfc_in = true;\\n\\t\\t}\\n\\t\\tsmp_call_function(scf_handler, scfcp, scfsp->scfs_wait);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tWARN_ON_ONCE(1);\\n\\t\\tif (scfcp)\\n\\t\\t\\tscfcp->scfc_out = true;\\n\\t}\\n\\tif (scfcp && scfsp->scfs_wait) {\\n\\t\\tif (WARN_ON_ONCE((num_online_cpus() > 1 || scfsp->scfs_prim == SCF_PRIM_SINGLE) &&\\n\\t\\t\\t\\t !scfcp->scfc_out)) {\\n\\t\\t\\tpr_warn(\"%s: Memory-ordering failure, scfs_prim: %d.\\\\n\", __func__, scfsp->scfs_prim);\\n\\t\\t\\tatomic_inc(&n_mb_out_errs); // Leak rather than trash!\\n\\t\\t} else {\\n\\t\\t\\tscf_add_to_free_list(scfcp);\\n\\t\\t}\\n\\t\\tbarrier(); // Prevent race-reduction compiler optimizations.\\n\\t}\\n\\tif (use_cpus_read_lock)\\n\\t\\tcpus_read_unlock();\\n\\telse\\n\\t\\tpreempt_enable();\\n\\tif (allocfail)\\n\\t\\tschedule_timeout_idle((1 + longwait) * HZ);  // Let no-wait handlers complete.\\n\\telse if (!(torture_random(trsp) & 0xfff))\\n\\t\\tschedule_timeout_uninterruptible(1);\\n}\\n\\n// SCF test kthread.  Repeatedly does calls to members of the\\n// smp_call_function() family of functions.\\nstatic int scftorture_invoker(void *arg)\\n{\\n\\tint cpu;\\n\\tint curcpu;\\n\\tDEFINE_TORTURE_RANDOM(rand);\\n\\tstruct scf_statistics *scfp = (struct scf_statistics *)arg;\\n\\tbool was_offline = false;\\n\\n\\tVERBOSE_SCFTORTOUT(\"scftorture_invoker %d: task started\", scfp->cpu);\\n\\tcpu = scfp->cpu % nr_cpu_ids;\\n\\tWARN_ON_ONCE(set_cpus_allowed_ptr(current, cpumask_of(cpu)));\\n\\tset_user_nice(current, MAX_NICE);\\n\\tif (holdoff)\\n\\t\\tschedule_timeout_interruptible(holdoff * HZ);\\n\\n\\tVERBOSE_SCFTORTOUT(\"scftorture_invoker %d: Waiting for all SCF torturers from cpu %d\", scfp->cpu, raw_smp_processor_id());\\n\\n\\t// Make sure that the CPU is affinitized appropriately during testing.\\n\\tcurcpu = raw_smp_processor_id();\\n\\tWARN_ONCE(curcpu != cpu,\\n\\t\\t  \"%s: Wanted CPU %d, running on %d, nr_cpu_ids = %d\\\\n\",\\n\\t\\t  __func__, scfp->cpu, curcpu, nr_cpu_ids);\\n\\n\\tif (!atomic_dec_return(&n_started))\\n\\t\\twhile (atomic_read_acquire(&n_started)) {\\n\\t\\t\\tif (torture_must_stop()) {\\n\\t\\t\\t\\tVERBOSE_SCFTORTOUT(\"scftorture_invoker %d ended before starting\", scfp->cpu);\\n\\t\\t\\t\\tgoto end;\\n\\t\\t\\t}\\n\\t\\t\\tschedule_timeout_uninterruptible(1);\\n\\t\\t}\\n\\n\\tVERBOSE_SCFTORTOUT(\"scftorture_invoker %d started\", scfp->cpu);\\n\\n\\tdo {\\n\\t\\tscf_cleanup_free_list(cpu);\\n\\n\\t\\tscftorture_invoke_one(scfp, &rand);\\n\\t\\twhile (cpu_is_offline(cpu) && !torture_must_stop()) {\\n\\t\\t\\tschedule_timeout_interruptible(HZ / 5);\\n\\t\\t\\twas_offline = true;\\n\\t\\t}\\n\\t\\tif (was_offline) {\\n\\t\\t\\tset_cpus_allowed_ptr(current, cpumask_of(cpu));\\n\\t\\t\\twas_offline = false;\\n\\t\\t}\\n\\t\\tcond_resched();\\n\\t\\tstutter_wait(\"scftorture_invoker\");\\n\\t} while (!torture_must_stop());\\n\\n\\tVERBOSE_SCFTORTOUT(\"scftorture_invoker %d ended\", scfp->cpu);\\nend:\\n\\ttorture_kthread_stopping(\"scftorture_invoker\");\\n\\treturn 0;\\n}\\n\\nstatic void\\nscftorture_print_module_parms(const char *tag)\\n{\\n\\tpr_alert(SCFTORT_FLAG\\n\\t\\t \"--- %s:  verbose=%d holdoff=%d longwait=%d nthreads=%d onoff_holdoff=%d onoff_interval=%d shutdown_secs=%d stat_interval=%d stutter=%d use_cpus_read_lock=%d, weight_resched=%d, weight_single=%d, weight_single_rpc=%d, weight_single_wait=%d, weight_many=%d, weight_many_wait=%d, weight_all=%d, weight_all_wait=%d\\\\n\", tag,\\n\\t\\t verbose, holdoff, longwait, nthreads, onoff_holdoff, onoff_interval, shutdown, stat_interval, stutter, use_cpus_read_lock, weight_resched, weight_single, weight_single_rpc, weight_single_wait, weight_many, weight_many_wait, weight_all, weight_all_wait);\\n}\\n\\nstatic void scf_cleanup_handler(void *unused)\\n{\\n}\\n\\nstatic void scf_torture_cleanup(void)\\n{\\n\\tint i;\\n\\n\\tif (torture_cleanup_begin())\\n\\t\\treturn;\\n\\n\\tWRITE_ONCE(scfdone, true);\\n\\tif (nthreads && scf_stats_p)\\n\\t\\tfor (i = 0; i < nthreads; i++)\\n\\t\\t\\ttorture_stop_kthread(\"scftorture_invoker\", scf_stats_p[i].task);\\n\\telse\\n\\t\\tgoto end;\\n\\tsmp_call_function(scf_cleanup_handler, NULL, 1);\\n\\ttorture_stop_kthread(scf_torture_stats, scf_torture_stats_task);\\n\\tscf_torture_stats_print();  // -After- the stats thread is stopped!\\n\\tkfree(scf_stats_p);  // -After- the last stats print has completed!\\n\\tscf_stats_p = NULL;\\n\\n\\tfor (i = 0; i < nr_cpu_ids; i++)\\n\\t\\tscf_cleanup_free_list(i);\\n\\n\\tif (atomic_read(&n_errs) || atomic_read(&n_mb_in_errs) || atomic_read(&n_mb_out_errs))\\n\\t\\tscftorture_print_module_parms(\"End of test: FAILURE\");\\n\\telse if (torture_onoff_failures())\\n\\t\\tscftorture_print_module_parms(\"End of test: LOCK_HOTPLUG\");\\n\\telse\\n\\t\\tscftorture_print_module_parms(\"End of test: SUCCESS\");\\n\\nend:\\n\\ttorture_cleanup_end();\\n}\\n\\nstatic int __init scf_torture_init(void)\\n{\\n\\tlong i;\\n\\tint firsterr = 0;\\n\\tunsigned long weight_resched1 = weight_resched;\\n\\tunsigned long weight_single1 = weight_single;\\n\\tunsigned long weight_single_rpc1 = weight_single_rpc;\\n\\tunsigned long weight_single_wait1 = weight_single_wait;\\n\\tunsigned long weight_many1 = weight_many;\\n\\tunsigned long weight_many_wait1 = weight_many_wait;\\n\\tunsigned long weight_all1 = weight_all;\\n\\tunsigned long weight_all_wait1 = weight_all_wait;\\n\\n\\tif (!torture_init_begin(SCFTORT_STRING, verbose))\\n\\t\\treturn -EBUSY;\\n\\n\\tscftorture_print_module_parms(\"Start of test\");\\n\\n\\tif (weight_resched <= 0 &&\\n\\t    weight_single <= 0 && weight_single_rpc <= 0 && weight_single_wait <= 0 &&\\n\\t    weight_many <= 0 && weight_many_wait <= 0 &&\\n\\t    weight_all <= 0 && weight_all_wait <= 0) {\\n\\t\\tweight_resched1 = weight_resched == 0 ? 0 : 2 * nr_cpu_ids;\\n\\t\\tweight_single1 = weight_single == 0 ? 0 : 2 * nr_cpu_ids;\\n\\t\\tweight_single_rpc1 = weight_single_rpc == 0 ? 0 : 2 * nr_cpu_ids;\\n\\t\\tweight_single_wait1 = weight_single_wait == 0 ? 0 : 2 * nr_cpu_ids;\\n\\t\\tweight_many1 = weight_many == 0 ? 0 : 2;\\n\\t\\tweight_many_wait1 = weight_many_wait == 0 ? 0 : 2;\\n\\t\\tweight_all1 = weight_all == 0 ? 0 : 1;\\n\\t\\tweight_all_wait1 = weight_all_wait == 0 ? 0 : 1;\\n\\t} else {\\n\\t\\tif (weight_resched == -1)\\n\\t\\t\\tweight_resched1 = 0;\\n\\t\\tif (weight_single == -1)\\n\\t\\t\\tweight_single1 = 0;\\n\\t\\tif (weight_single_rpc == -1)\\n\\t\\t\\tweight_single_rpc1 = 0;\\n\\t\\tif (weight_single_wait == -1)\\n\\t\\t\\tweight_single_wait1 = 0;\\n\\t\\tif (weight_many == -1)\\n\\t\\t\\tweight_many1 = 0;\\n\\t\\tif (weight_many_wait == -1)\\n\\t\\t\\tweight_many_wait1 = 0;\\n\\t\\tif (weight_all == -1)\\n\\t\\t\\tweight_all1 = 0;\\n\\t\\tif (weight_all_wait == -1)\\n\\t\\t\\tweight_all_wait1 = 0;\\n\\t}\\n\\tif (weight_resched1 == 0 && weight_single1 == 0 && weight_single_rpc1 == 0 &&\\n\\t    weight_single_wait1 == 0 && weight_many1 == 0 && weight_many_wait1 == 0 &&\\n\\t    weight_all1 == 0 && weight_all_wait1 == 0) {\\n\\t\\tSCFTORTOUT_ERRSTRING(\"all zero weights makes no sense\");\\n\\t\\tfirsterr = -EINVAL;\\n\\t\\tgoto unwind;\\n\\t}\\n\\tif (IS_BUILTIN(CONFIG_SCF_TORTURE_TEST))\\n\\t\\tscf_sel_add(weight_resched1, SCF_PRIM_RESCHED, false);\\n\\telse if (weight_resched1)\\n\\t\\tSCFTORTOUT_ERRSTRING(\"built as module, weight_resched ignored\");\\n\\tscf_sel_add(weight_single1, SCF_PRIM_SINGLE, false);\\n\\tscf_sel_add(weight_single_rpc1, SCF_PRIM_SINGLE_RPC, true);\\n\\tscf_sel_add(weight_single_wait1, SCF_PRIM_SINGLE, true);\\n\\tscf_sel_add(weight_many1, SCF_PRIM_MANY, false);\\n\\tscf_sel_add(weight_many_wait1, SCF_PRIM_MANY, true);\\n\\tscf_sel_add(weight_all1, SCF_PRIM_ALL, false);\\n\\tscf_sel_add(weight_all_wait1, SCF_PRIM_ALL, true);\\n\\tscf_sel_dump();\\n\\n\\tif (onoff_interval > 0) {\\n\\t\\tfirsterr = torture_onoff_init(onoff_holdoff * HZ, onoff_interval, NULL);\\n\\t\\tif (torture_init_error(firsterr))\\n\\t\\t\\tgoto unwind;\\n\\t}\\n\\tif (shutdown_secs > 0) {\\n\\t\\tfirsterr = torture_shutdown_init(shutdown_secs, scf_torture_cleanup);\\n\\t\\tif (torture_init_error(firsterr))\\n\\t\\t\\tgoto unwind;\\n\\t}\\n\\tif (stutter > 0) {\\n\\t\\tfirsterr = torture_stutter_init(stutter, stutter);\\n\\t\\tif (torture_init_error(firsterr))\\n\\t\\t\\tgoto unwind;\\n\\t}\\n\\n\\t// Worker tasks invoking smp_call_function().\\n\\tif (nthreads < 0)\\n\\t\\tnthreads = num_online_cpus();\\n\\tscf_stats_p = kcalloc(nthreads, sizeof(scf_stats_p[0]), GFP_KERNEL);\\n\\tif (!scf_stats_p) {\\n\\t\\tSCFTORTOUT_ERRSTRING(\"out of memory\");\\n\\t\\tfirsterr = -ENOMEM;\\n\\t\\tgoto unwind;\\n\\t}\\n\\n\\tVERBOSE_SCFTORTOUT(\"Starting %d smp_call_function() threads\", nthreads);\\n\\n\\tatomic_set(&n_started, nthreads);\\n\\tfor (i = 0; i < nthreads; i++) {\\n\\t\\tscf_stats_p[i].cpu = i;\\n\\t\\tfirsterr = torture_create_kthread(scftorture_invoker, (void *)&scf_stats_p[i],\\n\\t\\t\\t\\t\\t\\t  scf_stats_p[i].task);\\n\\t\\tif (torture_init_error(firsterr))\\n\\t\\t\\tgoto unwind;\\n\\t}\\n\\tif (stat_interval > 0) {\\n\\t\\tfirsterr = torture_create_kthread(scf_torture_stats, NULL, scf_torture_stats_task);\\n\\t\\tif (torture_init_error(firsterr))\\n\\t\\t\\tgoto unwind;\\n\\t}\\n\\n\\ttorture_init_end();\\n\\treturn 0;\\n\\nunwind:\\n\\ttorture_init_end();\\n\\tscf_torture_cleanup();\\n\\tif (shutdown_secs) {\\n\\t\\tWARN_ON(!IS_MODULE(CONFIG_SCF_TORTURE_TEST));\\n\\t\\tkernel_power_off();\\n\\t}\\n\\treturn firsterr;\\n}\\n\\nmodule_init(scf_torture_init);\\nmodule_exit(scf_torture_cleanup);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * linux/kernel/ptrace.c\\n *\\n * (C) Copyright 1999 Linus Torvalds\\n *\\n * Common interfaces for \"ptrace()\" which we do not want\\n * to continually duplicate across every architecture.\\n */\\n\\n#include <linux/capability.h>\\n#include <linux/export.h>\\n#include <linux/sched.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/coredump.h>\\n#include <linux/sched/task.h>\\n#include <linux/errno.h>\\n#include <linux/mm.h>\\n#include <linux/highmem.h>\\n#include <linux/pagemap.h>\\n#include <linux/ptrace.h>\\n#include <linux/security.h>\\n#include <linux/signal.h>\\n#include <linux/uio.h>\\n#include <linux/audit.h>\\n#include <linux/pid_namespace.h>\\n#include <linux/syscalls.h>\\n#include <linux/uaccess.h>\\n#include <linux/regset.h>\\n#include <linux/hw_breakpoint.h>\\n#include <linux/cn_proc.h>\\n#include <linux/compat.h>\\n#include <linux/sched/signal.h>\\n#include <linux/minmax.h>\\n#include <linux/syscall_user_dispatch.h>\\n\\n#include <asm/syscall.h>\\t/* for syscall_get_* */\\n\\n/*\\n * Access another process\\' address space via ptrace.\\n * Source/target buffer must be kernel space,\\n * Do not walk the page table directly, use get_user_pages\\n */\\nint ptrace_access_vm(struct task_struct *tsk, unsigned long addr,\\n\\t\\t     void *buf, int len, unsigned int gup_flags)\\n{\\n\\tstruct mm_struct *mm;\\n\\tint ret;\\n\\n\\tmm = get_task_mm(tsk);\\n\\tif (!mm)\\n\\t\\treturn 0;\\n\\n\\tif (!tsk->ptrace ||\\n\\t    (current != tsk->parent) ||\\n\\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\\n\\t     !ptracer_capable(tsk, mm->user_ns))) {\\n\\t\\tmmput(mm);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tret = access_remote_vm(mm, addr, buf, len, gup_flags);\\n\\tmmput(mm);\\n\\n\\treturn ret;\\n}\\n\\n\\nvoid __ptrace_link(struct task_struct *child, struct task_struct *new_parent,\\n\\t\\t   const struct cred *ptracer_cred)\\n{\\n\\tBUG_ON(!list_empty(&child->ptrace_entry));\\n\\tlist_add(&child->ptrace_entry, &new_parent->ptraced);\\n\\tchild->parent = new_parent;\\n\\tchild->ptracer_cred = get_cred(ptracer_cred);\\n}\\n\\n/*\\n * ptrace a task: make the debugger its new parent and\\n * move it to the ptrace list.\\n *\\n * Must be called with the tasklist lock write-held.\\n */\\nstatic void ptrace_link(struct task_struct *child, struct task_struct *new_parent)\\n{\\n\\t__ptrace_link(child, new_parent, current_cred());\\n}\\n\\n/**\\n * __ptrace_unlink - unlink ptracee and restore its execution state\\n * @child: ptracee to be unlinked\\n *\\n * Remove @child from the ptrace list, move it back to the original parent,\\n * and restore the execution state so that it conforms to the group stop\\n * state.\\n *\\n * Unlinking can happen via two paths - explicit PTRACE_DETACH or ptracer\\n * exiting.  For PTRACE_DETACH, unless the ptracee has been killed between\\n * ptrace_check_attach() and here, it\\'s guaranteed to be in TASK_TRACED.\\n * If the ptracer is exiting, the ptracee can be in any state.\\n *\\n * After detach, the ptracee should be in a state which conforms to the\\n * group stop.  If the group is stopped or in the process of stopping, the\\n * ptracee should be put into TASK_STOPPED; otherwise, it should be woken\\n * up from TASK_TRACED.\\n *\\n * If the ptracee is in TASK_TRACED and needs to be moved to TASK_STOPPED,\\n * it goes through TRACED -> RUNNING -> STOPPED transition which is similar\\n * to but in the opposite direction of what happens while attaching to a\\n * stopped task.  However, in this direction, the intermediate RUNNING\\n * state is not hidden even from the current ptracer and if it immediately\\n * re-attaches and performs a WNOHANG wait(2), it may fail.\\n *\\n * CONTEXT:\\n * write_lock_irq(tasklist_lock)\\n */\\nvoid __ptrace_unlink(struct task_struct *child)\\n{\\n\\tconst struct cred *old_cred;\\n\\tBUG_ON(!child->ptrace);\\n\\n\\tclear_task_syscall_work(child, SYSCALL_TRACE);\\n#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)\\n\\tclear_task_syscall_work(child, SYSCALL_EMU);\\n#endif\\n\\n\\tchild->parent = child->real_parent;\\n\\tlist_del_init(&child->ptrace_entry);\\n\\told_cred = child->ptracer_cred;\\n\\tchild->ptracer_cred = NULL;\\n\\tput_cred(old_cred);\\n\\n\\tspin_lock(&child->sighand->siglock);\\n\\tchild->ptrace = 0;\\n\\t/*\\n\\t * Clear all pending traps and TRAPPING.  TRAPPING should be\\n\\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\\n\\t */\\n\\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\\n\\ttask_clear_jobctl_trapping(child);\\n\\n\\t/*\\n\\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\\n\\t * @child isn\\'t dead.\\n\\t */\\n\\tif (!(child->flags & PF_EXITING) &&\\n\\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\\n\\t     child->signal->group_stop_count))\\n\\t\\tchild->jobctl |= JOBCTL_STOP_PENDING;\\n\\n\\t/*\\n\\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\\n\\t * @child in the butt.  Note that @resume should be used iff @child\\n\\t * is in TASK_TRACED; otherwise, we might unduly disrupt\\n\\t * TASK_KILLABLE sleeps.\\n\\t */\\n\\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\\n\\t\\tptrace_signal_wake_up(child, true);\\n\\n\\tspin_unlock(&child->sighand->siglock);\\n}\\n\\nstatic bool looks_like_a_spurious_pid(struct task_struct *task)\\n{\\n\\tif (task->exit_code != ((PTRACE_EVENT_EXEC << 8) | SIGTRAP))\\n\\t\\treturn false;\\n\\n\\tif (task_pid_vnr(task) == task->ptrace_message)\\n\\t\\treturn false;\\n\\t/*\\n\\t * The tracee changed its pid but the PTRACE_EVENT_EXEC event\\n\\t * was not wait()\\'ed, most probably debugger targets the old\\n\\t * leader which was destroyed in de_thread().\\n\\t */\\n\\treturn true;\\n}\\n\\n/*\\n * Ensure that nothing can wake it up, even SIGKILL\\n *\\n * A task is switched to this state while a ptrace operation is in progress;\\n * such that the ptrace operation is uninterruptible.\\n */\\nstatic bool ptrace_freeze_traced(struct task_struct *task)\\n{\\n\\tbool ret = false;\\n\\n\\t/* Lockless, nobody but us can set this flag */\\n\\tif (task->jobctl & JOBCTL_LISTENING)\\n\\t\\treturn ret;\\n\\n\\tspin_lock_irq(&task->sighand->siglock);\\n\\tif (task_is_traced(task) && !looks_like_a_spurious_pid(task) &&\\n\\t    !__fatal_signal_pending(task)) {\\n\\t\\ttask->jobctl |= JOBCTL_PTRACE_FROZEN;\\n\\t\\tret = true;\\n\\t}\\n\\tspin_unlock_irq(&task->sighand->siglock);\\n\\n\\treturn ret;\\n}\\n\\nstatic void ptrace_unfreeze_traced(struct task_struct *task)\\n{\\n\\tunsigned long flags;\\n\\n\\t/*\\n\\t * The child may be awake and may have cleared\\n\\t * JOBCTL_PTRACE_FROZEN (see ptrace_resume).  The child will\\n\\t * not set JOBCTL_PTRACE_FROZEN or enter __TASK_TRACED anew.\\n\\t */\\n\\tif (lock_task_sighand(task, &flags)) {\\n\\t\\ttask->jobctl &= ~JOBCTL_PTRACE_FROZEN;\\n\\t\\tif (__fatal_signal_pending(task)) {\\n\\t\\t\\ttask->jobctl &= ~JOBCTL_TRACED;\\n\\t\\t\\twake_up_state(task, __TASK_TRACED);\\n\\t\\t}\\n\\t\\tunlock_task_sighand(task, &flags);\\n\\t}\\n}\\n\\n/**\\n * ptrace_check_attach - check whether ptracee is ready for ptrace operation\\n * @child: ptracee to check for\\n * @ignore_state: don\\'t check whether @child is currently %TASK_TRACED\\n *\\n * Check whether @child is being ptraced by %current and ready for further\\n * ptrace operations.  If @ignore_state is %false, @child also should be in\\n * %TASK_TRACED state and on return the child is guaranteed to be traced\\n * and not executing.  If @ignore_state is %true, @child can be in any\\n * state.\\n *\\n * CONTEXT:\\n * Grabs and releases tasklist_lock and @child->sighand->siglock.\\n *\\n * RETURNS:\\n * 0 on success, -ESRCH if %child is not ready.\\n */\\nstatic int ptrace_check_attach(struct task_struct *child, bool ignore_state)\\n{\\n\\tint ret = -ESRCH;\\n\\n\\t/*\\n\\t * We take the read lock around doing both checks to close a\\n\\t * possible race where someone else was tracing our child and\\n\\t * detached between these two checks.  After this locked check,\\n\\t * we are sure that this is our traced child and that can only\\n\\t * be changed by us so it\\'s not changing right after this.\\n\\t */\\n\\tread_lock(&tasklist_lock);\\n\\tif (child->ptrace && child->parent == current) {\\n\\t\\t/*\\n\\t\\t * child->sighand can\\'t be NULL, release_task()\\n\\t\\t * does ptrace_unlink() before __exit_signal().\\n\\t\\t */\\n\\t\\tif (ignore_state || ptrace_freeze_traced(child))\\n\\t\\t\\tret = 0;\\n\\t}\\n\\tread_unlock(&tasklist_lock);\\n\\n\\tif (!ret && !ignore_state &&\\n\\t    WARN_ON_ONCE(!wait_task_inactive(child, __TASK_TRACED|TASK_FROZEN)))\\n\\t\\tret = -ESRCH;\\n\\n\\treturn ret;\\n}\\n\\nstatic bool ptrace_has_cap(struct user_namespace *ns, unsigned int mode)\\n{\\n\\tif (mode & PTRACE_MODE_NOAUDIT)\\n\\t\\treturn ns_capable_noaudit(ns, CAP_SYS_PTRACE);\\n\\treturn ns_capable(ns, CAP_SYS_PTRACE);\\n}\\n\\n/* Returns 0 on success, -errno on denial. */\\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\\n{\\n\\tconst struct cred *cred = current_cred(), *tcred;\\n\\tstruct mm_struct *mm;\\n\\tkuid_t caller_uid;\\n\\tkgid_t caller_gid;\\n\\n\\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\\n\\t\\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\\\n\");\\n\\t\\treturn -EPERM;\\n\\t}\\n\\n\\t/* May we inspect the given task?\\n\\t * This check is used both for attaching with ptrace\\n\\t * and for allowing access to sensitive information in /proc.\\n\\t *\\n\\t * ptrace_attach denies several cases that /proc allows\\n\\t * because setting up the necessary parent/child relationship\\n\\t * or halting the specified task is impossible.\\n\\t */\\n\\n\\t/* Don\\'t let security modules deny introspection */\\n\\tif (same_thread_group(task, current))\\n\\t\\treturn 0;\\n\\trcu_read_lock();\\n\\tif (mode & PTRACE_MODE_FSCREDS) {\\n\\t\\tcaller_uid = cred->fsuid;\\n\\t\\tcaller_gid = cred->fsgid;\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * Using the euid would make more sense here, but something\\n\\t\\t * in userland might rely on the old behavior, and this\\n\\t\\t * shouldn\\'t be a security problem since\\n\\t\\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\\n\\t\\t * used a syscall that requests access to another process\\n\\t\\t * (and not a filesystem syscall to procfs).\\n\\t\\t */\\n\\t\\tcaller_uid = cred->uid;\\n\\t\\tcaller_gid = cred->gid;\\n\\t}\\n\\ttcred = __task_cred(task);\\n\\tif (uid_eq(caller_uid, tcred->euid) &&\\n\\t    uid_eq(caller_uid, tcred->suid) &&\\n\\t    uid_eq(caller_uid, tcred->uid)  &&\\n\\t    gid_eq(caller_gid, tcred->egid) &&\\n\\t    gid_eq(caller_gid, tcred->sgid) &&\\n\\t    gid_eq(caller_gid, tcred->gid))\\n\\t\\tgoto ok;\\n\\tif (ptrace_has_cap(tcred->user_ns, mode))\\n\\t\\tgoto ok;\\n\\trcu_read_unlock();\\n\\treturn -EPERM;\\nok:\\n\\trcu_read_unlock();\\n\\t/*\\n\\t * If a task drops privileges and becomes nondumpable (through a syscall\\n\\t * like setresuid()) while we are trying to access it, we must ensure\\n\\t * that the dumpability is read after the credentials; otherwise,\\n\\t * we may be able to attach to a task that we shouldn\\'t be able to\\n\\t * attach to (as if the task had dropped privileges without becoming\\n\\t * nondumpable).\\n\\t * Pairs with a write barrier in commit_creds().\\n\\t */\\n\\tsmp_rmb();\\n\\tmm = task->mm;\\n\\tif (mm &&\\n\\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\\n\\t     !ptrace_has_cap(mm->user_ns, mode)))\\n\\t    return -EPERM;\\n\\n\\treturn security_ptrace_access_check(task, mode);\\n}\\n\\nbool ptrace_may_access(struct task_struct *task, unsigned int mode)\\n{\\n\\tint err;\\n\\ttask_lock(task);\\n\\terr = __ptrace_may_access(task, mode);\\n\\ttask_unlock(task);\\n\\treturn !err;\\n}\\n\\nstatic int check_ptrace_options(unsigned long data)\\n{\\n\\tif (data & ~(unsigned long)PTRACE_O_MASK)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (unlikely(data & PTRACE_O_SUSPEND_SECCOMP)) {\\n\\t\\tif (!IS_ENABLED(CONFIG_CHECKPOINT_RESTORE) ||\\n\\t\\t    !IS_ENABLED(CONFIG_SECCOMP))\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tif (!capable(CAP_SYS_ADMIN))\\n\\t\\t\\treturn -EPERM;\\n\\n\\t\\tif (seccomp_mode(&current->seccomp) != SECCOMP_MODE_DISABLED ||\\n\\t\\t    current->ptrace & PT_SUSPEND_SECCOMP)\\n\\t\\t\\treturn -EPERM;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic inline void ptrace_set_stopped(struct task_struct *task, bool seize)\\n{\\n\\tguard(spinlock)(&task->sighand->siglock);\\n\\n\\t/* SEIZE doesn\\'t trap tracee on attach */\\n\\tif (!seize)\\n\\t\\tsend_signal_locked(SIGSTOP, SEND_SIG_PRIV, task, PIDTYPE_PID);\\n\\t/*\\n\\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\\n\\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\\n\\t * will be cleared if the child completes the transition or any\\n\\t * event which clears the group stop states happens.  We\\'ll wait\\n\\t * for the transition to complete before returning from this\\n\\t * function.\\n\\t *\\n\\t * This hides STOPPED -> RUNNING -> TRACED transition from the\\n\\t * attaching thread but a different thread in the same group can\\n\\t * still observe the transient RUNNING state.  IOW, if another\\n\\t * thread\\'s WNOHANG wait(2) on the stopped tracee races against\\n\\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\\n\\t *\\n\\t * The following task_is_stopped() test is safe as both transitions\\n\\t * in and out of STOPPED are protected by siglock.\\n\\t */\\n\\tif (task_is_stopped(task) &&\\n\\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING)) {\\n\\t\\ttask->jobctl &= ~JOBCTL_STOPPED;\\n\\t\\tsignal_wake_up_state(task, __TASK_STOPPED);\\n\\t}\\n}\\n\\nstatic int ptrace_attach(struct task_struct *task, long request,\\n\\t\\t\\t unsigned long addr,\\n\\t\\t\\t unsigned long flags)\\n{\\n\\tbool seize = (request == PTRACE_SEIZE);\\n\\tint retval;\\n\\n\\tif (seize) {\\n\\t\\tif (addr != 0)\\n\\t\\t\\treturn -EIO;\\n\\t\\t/*\\n\\t\\t * This duplicates the check in check_ptrace_options() because\\n\\t\\t * ptrace_attach() and ptrace_setoptions() have historically\\n\\t\\t * used different error codes for unknown ptrace options.\\n\\t\\t */\\n\\t\\tif (flags & ~(unsigned long)PTRACE_O_MASK)\\n\\t\\t\\treturn -EIO;\\n\\n\\t\\tretval = check_ptrace_options(flags);\\n\\t\\tif (retval)\\n\\t\\t\\treturn retval;\\n\\t\\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\\n\\t} else {\\n\\t\\tflags = PT_PTRACED;\\n\\t}\\n\\n\\taudit_ptrace(task);\\n\\n\\tif (unlikely(task->flags & PF_KTHREAD))\\n\\t\\treturn -EPERM;\\n\\tif (same_thread_group(task, current))\\n\\t\\treturn -EPERM;\\n\\n\\t/*\\n\\t * Protect exec\\'s credential calculations against our interference;\\n\\t * SUID, SGID and LSM creds get determined differently\\n\\t * under ptrace.\\n\\t */\\n\\tscoped_cond_guard (mutex_intr, return -ERESTARTNOINTR,\\n\\t\\t\\t   &task->signal->cred_guard_mutex) {\\n\\n\\t\\tscoped_guard (task_lock, task) {\\n\\t\\t\\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\\n\\t\\t\\tif (retval)\\n\\t\\t\\t\\treturn retval;\\n\\t\\t}\\n\\n\\t\\tscoped_guard (write_lock_irq, &tasklist_lock) {\\n\\t\\t\\tif (unlikely(task->exit_state))\\n\\t\\t\\t\\treturn -EPERM;\\n\\t\\t\\tif (task->ptrace)\\n\\t\\t\\t\\treturn -EPERM;\\n\\n\\t\\t\\ttask->ptrace = flags;\\n\\t\\t\\tptrace_link(task, current);\\n\\t\\t\\tptrace_set_stopped(task, seize);\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * We do not bother to change retval or clear JOBCTL_TRAPPING\\n\\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\\n\\t * not return to user-mode, it will exit and clear this bit in\\n\\t * __ptrace_unlink() if it wasn\\'t already cleared by the tracee;\\n\\t * and until then nobody can ptrace this task.\\n\\t */\\n\\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\\n\\tproc_ptrace_connector(task, PTRACE_ATTACH);\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * ptrace_traceme  --  helper for PTRACE_TRACEME\\n *\\n * Performs checks and sets PT_PTRACED.\\n * Should be used by all ptrace implementations for PTRACE_TRACEME.\\n */\\nstatic int ptrace_traceme(void)\\n{\\n\\tint ret = -EPERM;\\n\\n\\twrite_lock_irq(&tasklist_lock);\\n\\t/* Are we already being traced? */\\n\\tif (!current->ptrace) {\\n\\t\\tret = security_ptrace_traceme(current->parent);\\n\\t\\t/*\\n\\t\\t * Check PF_EXITING to ensure ->real_parent has not passed\\n\\t\\t * exit_ptrace(). Otherwise we don\\'t report the error but\\n\\t\\t * pretend ->real_parent untraces us right after return.\\n\\t\\t */\\n\\t\\tif (!ret && !(current->real_parent->flags & PF_EXITING)) {\\n\\t\\t\\tcurrent->ptrace = PT_PTRACED;\\n\\t\\t\\tptrace_link(current, current->real_parent);\\n\\t\\t}\\n\\t}\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * Called with irqs disabled, returns true if childs should reap themselves.\\n */\\nstatic int ignoring_children(struct sighand_struct *sigh)\\n{\\n\\tint ret;\\n\\tspin_lock(&sigh->siglock);\\n\\tret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||\\n\\t      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);\\n\\tspin_unlock(&sigh->siglock);\\n\\treturn ret;\\n}\\n\\n/*\\n * Called with tasklist_lock held for writing.\\n * Unlink a traced task, and clean it up if it was a traced zombie.\\n * Return true if it needs to be reaped with release_task().\\n * (We can\\'t call release_task() here because we already hold tasklist_lock.)\\n *\\n * If it\\'s a zombie, our attachedness prevented normal parent notification\\n * or self-reaping.  Do notification now if it would have happened earlier.\\n * If it should reap itself, return true.\\n *\\n * If it\\'s our own child, there is no notification to do. But if our normal\\n * children self-reap, then this child was prevented by ptrace and we must\\n * reap it now, in that case we must also wake up sub-threads sleeping in\\n * do_wait().\\n */\\nstatic bool __ptrace_detach(struct task_struct *tracer, struct task_struct *p)\\n{\\n\\tbool dead;\\n\\n\\t__ptrace_unlink(p);\\n\\n\\tif (p->exit_state != EXIT_ZOMBIE)\\n\\t\\treturn false;\\n\\n\\tdead = !thread_group_leader(p);\\n\\n\\tif (!dead && thread_group_empty(p)) {\\n\\t\\tif (!same_thread_group(p->real_parent, tracer))\\n\\t\\t\\tdead = do_notify_parent(p, p->exit_signal);\\n\\t\\telse if (ignoring_children(tracer->sighand)) {\\n\\t\\t\\t__wake_up_parent(p, tracer);\\n\\t\\t\\tdead = true;\\n\\t\\t}\\n\\t}\\n\\t/* Mark it as in the process of being reaped. */\\n\\tif (dead)\\n\\t\\tp->exit_state = EXIT_DEAD;\\n\\treturn dead;\\n}\\n\\nstatic int ptrace_detach(struct task_struct *child, unsigned int data)\\n{\\n\\tif (!valid_signal(data))\\n\\t\\treturn -EIO;\\n\\n\\t/* Architecture-specific hardware disable .. */\\n\\tptrace_disable(child);\\n\\n\\twrite_lock_irq(&tasklist_lock);\\n\\t/*\\n\\t * We rely on ptrace_freeze_traced(). It can\\'t be killed and\\n\\t * untraced by another thread, it can\\'t be a zombie.\\n\\t */\\n\\tWARN_ON(!child->ptrace || child->exit_state);\\n\\t/*\\n\\t * tasklist_lock avoids the race with wait_task_stopped(), see\\n\\t * the comment in ptrace_resume().\\n\\t */\\n\\tchild->exit_code = data;\\n\\t__ptrace_detach(current, child);\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\n\\tproc_ptrace_connector(child, PTRACE_DETACH);\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * Detach all tasks we were using ptrace on. Called with tasklist held\\n * for writing.\\n */\\nvoid exit_ptrace(struct task_struct *tracer, struct list_head *dead)\\n{\\n\\tstruct task_struct *p, *n;\\n\\n\\tlist_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {\\n\\t\\tif (unlikely(p->ptrace & PT_EXITKILL))\\n\\t\\t\\tsend_sig_info(SIGKILL, SEND_SIG_PRIV, p);\\n\\n\\t\\tif (__ptrace_detach(tracer, p))\\n\\t\\t\\tlist_add(&p->ptrace_entry, dead);\\n\\t}\\n}\\n\\nint ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len)\\n{\\n\\tint copied = 0;\\n\\n\\twhile (len > 0) {\\n\\t\\tchar buf[128];\\n\\t\\tint this_len, retval;\\n\\n\\t\\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\\n\\t\\tretval = ptrace_access_vm(tsk, src, buf, this_len, FOLL_FORCE);\\n\\n\\t\\tif (!retval) {\\n\\t\\t\\tif (copied)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\treturn -EIO;\\n\\t\\t}\\n\\t\\tif (copy_to_user(dst, buf, retval))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tcopied += retval;\\n\\t\\tsrc += retval;\\n\\t\\tdst += retval;\\n\\t\\tlen -= retval;\\n\\t}\\n\\treturn copied;\\n}\\n\\nint ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len)\\n{\\n\\tint copied = 0;\\n\\n\\twhile (len > 0) {\\n\\t\\tchar buf[128];\\n\\t\\tint this_len, retval;\\n\\n\\t\\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\\n\\t\\tif (copy_from_user(buf, src, this_len))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tretval = ptrace_access_vm(tsk, dst, buf, this_len,\\n\\t\\t\\t\\tFOLL_FORCE | FOLL_WRITE);\\n\\t\\tif (!retval) {\\n\\t\\t\\tif (copied)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\treturn -EIO;\\n\\t\\t}\\n\\t\\tcopied += retval;\\n\\t\\tsrc += retval;\\n\\t\\tdst += retval;\\n\\t\\tlen -= retval;\\n\\t}\\n\\treturn copied;\\n}\\n\\nstatic int ptrace_setoptions(struct task_struct *child, unsigned long data)\\n{\\n\\tunsigned flags;\\n\\tint ret;\\n\\n\\tret = check_ptrace_options(data);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/* Avoid intermediate state when all opts are cleared */\\n\\tflags = child->ptrace;\\n\\tflags &= ~(PTRACE_O_MASK << PT_OPT_FLAG_SHIFT);\\n\\tflags |= (data << PT_OPT_FLAG_SHIFT);\\n\\tchild->ptrace = flags;\\n\\n\\treturn 0;\\n}\\n\\nstatic int ptrace_getsiginfo(struct task_struct *child, kernel_siginfo_t *info)\\n{\\n\\tunsigned long flags;\\n\\tint error = -ESRCH;\\n\\n\\tif (lock_task_sighand(child, &flags)) {\\n\\t\\terror = -EINVAL;\\n\\t\\tif (likely(child->last_siginfo != NULL)) {\\n\\t\\t\\tcopy_siginfo(info, child->last_siginfo);\\n\\t\\t\\terror = 0;\\n\\t\\t}\\n\\t\\tunlock_task_sighand(child, &flags);\\n\\t}\\n\\treturn error;\\n}\\n\\nstatic int ptrace_setsiginfo(struct task_struct *child, const kernel_siginfo_t *info)\\n{\\n\\tunsigned long flags;\\n\\tint error = -ESRCH;\\n\\n\\tif (lock_task_sighand(child, &flags)) {\\n\\t\\terror = -EINVAL;\\n\\t\\tif (likely(child->last_siginfo != NULL)) {\\n\\t\\t\\tcopy_siginfo(child->last_siginfo, info);\\n\\t\\t\\terror = 0;\\n\\t\\t}\\n\\t\\tunlock_task_sighand(child, &flags);\\n\\t}\\n\\treturn error;\\n}\\n\\nstatic int ptrace_peek_siginfo(struct task_struct *child,\\n\\t\\t\\t\\tunsigned long addr,\\n\\t\\t\\t\\tunsigned long data)\\n{\\n\\tstruct ptrace_peeksiginfo_args arg;\\n\\tstruct sigpending *pending;\\n\\tstruct sigqueue *q;\\n\\tint ret, i;\\n\\n\\tret = copy_from_user(&arg, (void __user *) addr,\\n\\t\\t\\t\\tsizeof(struct ptrace_peeksiginfo_args));\\n\\tif (ret)\\n\\t\\treturn -EFAULT;\\n\\n\\tif (arg.flags & ~PTRACE_PEEKSIGINFO_SHARED)\\n\\t\\treturn -EINVAL; /* unknown flags */\\n\\n\\tif (arg.nr < 0)\\n\\t\\treturn -EINVAL;\\n\\n\\t/* Ensure arg.off fits in an unsigned long */\\n\\tif (arg.off > ULONG_MAX)\\n\\t\\treturn 0;\\n\\n\\tif (arg.flags & PTRACE_PEEKSIGINFO_SHARED)\\n\\t\\tpending = &child->signal->shared_pending;\\n\\telse\\n\\t\\tpending = &child->pending;\\n\\n\\tfor (i = 0; i < arg.nr; ) {\\n\\t\\tkernel_siginfo_t info;\\n\\t\\tunsigned long off = arg.off + i;\\n\\t\\tbool found = false;\\n\\n\\t\\tspin_lock_irq(&child->sighand->siglock);\\n\\t\\tlist_for_each_entry(q, &pending->list, list) {\\n\\t\\t\\tif (!off--) {\\n\\t\\t\\t\\tfound = true;\\n\\t\\t\\t\\tcopy_siginfo(&info, &q->info);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tspin_unlock_irq(&child->sighand->siglock);\\n\\n\\t\\tif (!found) /* beyond the end of the list */\\n\\t\\t\\tbreak;\\n\\n#ifdef CONFIG_COMPAT\\n\\t\\tif (unlikely(in_compat_syscall())) {\\n\\t\\t\\tcompat_siginfo_t __user *uinfo = compat_ptr(data);\\n\\n\\t\\t\\tif (copy_siginfo_to_user32(uinfo, &info)) {\\n\\t\\t\\t\\tret = -EFAULT;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t} else\\n#endif\\n\\t\\t{\\n\\t\\t\\tsiginfo_t __user *uinfo = (siginfo_t __user *) data;\\n\\n\\t\\t\\tif (copy_siginfo_to_user(uinfo, &info)) {\\n\\t\\t\\t\\tret = -EFAULT;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\tdata += sizeof(siginfo_t);\\n\\t\\ti++;\\n\\n\\t\\tif (signal_pending(current))\\n\\t\\t\\tbreak;\\n\\n\\t\\tcond_resched();\\n\\t}\\n\\n\\tif (i > 0)\\n\\t\\treturn i;\\n\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_RSEQ\\nstatic long ptrace_get_rseq_configuration(struct task_struct *task,\\n\\t\\t\\t\\t\\t  unsigned long size, void __user *data)\\n{\\n\\tstruct ptrace_rseq_configuration conf = {\\n\\t\\t.rseq_abi_pointer = (u64)(uintptr_t)task->rseq,\\n\\t\\t.rseq_abi_size = task->rseq_len,\\n\\t\\t.signature = task->rseq_sig,\\n\\t\\t.flags = 0,\\n\\t};\\n\\n\\tsize = min_t(unsigned long, size, sizeof(conf));\\n\\tif (copy_to_user(data, &conf, size))\\n\\t\\treturn -EFAULT;\\n\\treturn sizeof(conf);\\n}\\n#endif\\n\\n#define is_singlestep(request)\\t\\t((request) == PTRACE_SINGLESTEP)\\n\\n#ifdef PTRACE_SINGLEBLOCK\\n#define is_singleblock(request)\\t\\t((request) == PTRACE_SINGLEBLOCK)\\n#else\\n#define is_singleblock(request)\\t\\t0\\n#endif\\n\\n#ifdef PTRACE_SYSEMU\\n#define is_sysemu_singlestep(request)\\t((request) == PTRACE_SYSEMU_SINGLESTEP)\\n#else\\n#define is_sysemu_singlestep(request)\\t0\\n#endif\\n\\nstatic int ptrace_resume(struct task_struct *child, long request,\\n\\t\\t\\t unsigned long data)\\n{\\n\\tif (!valid_signal(data))\\n\\t\\treturn -EIO;\\n\\n\\tif (request == PTRACE_SYSCALL)\\n\\t\\tset_task_syscall_work(child, SYSCALL_TRACE);\\n\\telse\\n\\t\\tclear_task_syscall_work(child, SYSCALL_TRACE);\\n\\n#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)\\n\\tif (request == PTRACE_SYSEMU || request == PTRACE_SYSEMU_SINGLESTEP)\\n\\t\\tset_task_syscall_work(child, SYSCALL_EMU);\\n\\telse\\n\\t\\tclear_task_syscall_work(child, SYSCALL_EMU);\\n#endif\\n\\n\\tif (is_singleblock(request)) {\\n\\t\\tif (unlikely(!arch_has_block_step()))\\n\\t\\t\\treturn -EIO;\\n\\t\\tuser_enable_block_step(child);\\n\\t} else if (is_singlestep(request) || is_sysemu_singlestep(request)) {\\n\\t\\tif (unlikely(!arch_has_single_step()))\\n\\t\\t\\treturn -EIO;\\n\\t\\tuser_enable_single_step(child);\\n\\t} else {\\n\\t\\tuser_disable_single_step(child);\\n\\t}\\n\\n\\t/*\\n\\t * Change ->exit_code and ->state under siglock to avoid the race\\n\\t * with wait_task_stopped() in between; a non-zero ->exit_code will\\n\\t * wrongly look like another report from tracee.\\n\\t *\\n\\t * Note that we need siglock even if ->exit_code == data and/or this\\n\\t * status was not reported yet, the new status must not be cleared by\\n\\t * wait_task_stopped() after resume.\\n\\t */\\n\\tspin_lock_irq(&child->sighand->siglock);\\n\\tchild->exit_code = data;\\n\\tchild->jobctl &= ~JOBCTL_TRACED;\\n\\twake_up_state(child, __TASK_TRACED);\\n\\tspin_unlock_irq(&child->sighand->siglock);\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\\n\\nstatic const struct user_regset *\\nfind_regset(const struct user_regset_view *view, unsigned int type)\\n{\\n\\tconst struct user_regset *regset;\\n\\tint n;\\n\\n\\tfor (n = 0; n < view->n; ++n) {\\n\\t\\tregset = view->regsets + n;\\n\\t\\tif (regset->core_note_type == type)\\n\\t\\t\\treturn regset;\\n\\t}\\n\\n\\treturn NULL;\\n}\\n\\nstatic int ptrace_regset(struct task_struct *task, int req, unsigned int type,\\n\\t\\t\\t struct iovec *kiov)\\n{\\n\\tconst struct user_regset_view *view = task_user_regset_view(task);\\n\\tconst struct user_regset *regset = find_regset(view, type);\\n\\tint regset_no;\\n\\n\\tif (!regset || (kiov->iov_len % regset->size) != 0)\\n\\t\\treturn -EINVAL;\\n\\n\\tregset_no = regset - view->regsets;\\n\\tkiov->iov_len = min(kiov->iov_len,\\n\\t\\t\\t    (__kernel_size_t) (regset->n * regset->size));\\n\\n\\tif (req == PTRACE_GETREGSET)\\n\\t\\treturn copy_regset_to_user(task, view, regset_no, 0,\\n\\t\\t\\t\\t\\t   kiov->iov_len, kiov->iov_base);\\n\\telse\\n\\t\\treturn copy_regset_from_user(task, view, regset_no, 0,\\n\\t\\t\\t\\t\\t     kiov->iov_len, kiov->iov_base);\\n}\\n\\n/*\\n * This is declared in linux/regset.h and defined in machine-dependent\\n * code.  We put the export here, near the primary machine-neutral use,\\n * to ensure no machine forgets it.\\n */\\nEXPORT_SYMBOL_GPL(task_user_regset_view);\\n\\nstatic unsigned long\\nptrace_get_syscall_info_entry(struct task_struct *child, struct pt_regs *regs,\\n\\t\\t\\t      struct ptrace_syscall_info *info)\\n{\\n\\tunsigned long args[ARRAY_SIZE(info->entry.args)];\\n\\tint i;\\n\\n\\tinfo->op = PTRACE_SYSCALL_INFO_ENTRY;\\n\\tinfo->entry.nr = syscall_get_nr(child, regs);\\n\\tsyscall_get_arguments(child, regs, args);\\n\\tfor (i = 0; i < ARRAY_SIZE(args); i++)\\n\\t\\tinfo->entry.args[i] = args[i];\\n\\n\\t/* args is the last field in struct ptrace_syscall_info.entry */\\n\\treturn offsetofend(struct ptrace_syscall_info, entry.args);\\n}\\n\\nstatic unsigned long\\nptrace_get_syscall_info_seccomp(struct task_struct *child, struct pt_regs *regs,\\n\\t\\t\\t\\tstruct ptrace_syscall_info *info)\\n{\\n\\t/*\\n\\t * As struct ptrace_syscall_info.entry is currently a subset\\n\\t * of struct ptrace_syscall_info.seccomp, it makes sense to\\n\\t * initialize that subset using ptrace_get_syscall_info_entry().\\n\\t * This can be reconsidered in the future if these structures\\n\\t * diverge significantly enough.\\n\\t */\\n\\tptrace_get_syscall_info_entry(child, regs, info);\\n\\tinfo->op = PTRACE_SYSCALL_INFO_SECCOMP;\\n\\tinfo->seccomp.ret_data = child->ptrace_message;\\n\\n\\t/* ret_data is the last field in struct ptrace_syscall_info.seccomp */\\n\\treturn offsetofend(struct ptrace_syscall_info, seccomp.ret_data);\\n}\\n\\nstatic unsigned long\\nptrace_get_syscall_info_exit(struct task_struct *child, struct pt_regs *regs,\\n\\t\\t\\t     struct ptrace_syscall_info *info)\\n{\\n\\tinfo->op = PTRACE_SYSCALL_INFO_EXIT;\\n\\tinfo->exit.rval = syscall_get_error(child, regs);\\n\\tinfo->exit.is_error = !!info->exit.rval;\\n\\tif (!info->exit.is_error)\\n\\t\\tinfo->exit.rval = syscall_get_return_value(child, regs);\\n\\n\\t/* is_error is the last field in struct ptrace_syscall_info.exit */\\n\\treturn offsetofend(struct ptrace_syscall_info, exit.is_error);\\n}\\n\\nstatic int\\nptrace_get_syscall_info(struct task_struct *child, unsigned long user_size,\\n\\t\\t\\tvoid __user *datavp)\\n{\\n\\tstruct pt_regs *regs = task_pt_regs(child);\\n\\tstruct ptrace_syscall_info info = {\\n\\t\\t.op = PTRACE_SYSCALL_INFO_NONE,\\n\\t\\t.arch = syscall_get_arch(child),\\n\\t\\t.instruction_pointer = instruction_pointer(regs),\\n\\t\\t.stack_pointer = user_stack_pointer(regs),\\n\\t};\\n\\tunsigned long actual_size = offsetof(struct ptrace_syscall_info, entry);\\n\\tunsigned long write_size;\\n\\n\\t/*\\n\\t * This does not need lock_task_sighand() to access\\n\\t * child->last_siginfo because ptrace_freeze_traced()\\n\\t * called earlier by ptrace_check_attach() ensures that\\n\\t * the tracee cannot go away and clear its last_siginfo.\\n\\t */\\n\\tswitch (child->last_siginfo ? child->last_siginfo->si_code : 0) {\\n\\tcase SIGTRAP | 0x80:\\n\\t\\tswitch (child->ptrace_message) {\\n\\t\\tcase PTRACE_EVENTMSG_SYSCALL_ENTRY:\\n\\t\\t\\tactual_size = ptrace_get_syscall_info_entry(child, regs,\\n\\t\\t\\t\\t\\t\\t\\t\\t    &info);\\n\\t\\t\\tbreak;\\n\\t\\tcase PTRACE_EVENTMSG_SYSCALL_EXIT:\\n\\t\\t\\tactual_size = ptrace_get_syscall_info_exit(child, regs,\\n\\t\\t\\t\\t\\t\\t\\t\\t   &info);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase SIGTRAP | (PTRACE_EVENT_SECCOMP << 8):\\n\\t\\tactual_size = ptrace_get_syscall_info_seccomp(child, regs,\\n\\t\\t\\t\\t\\t\\t\\t      &info);\\n\\t\\tbreak;\\n\\t}\\n\\n\\twrite_size = min(actual_size, user_size);\\n\\treturn copy_to_user(datavp, &info, write_size) ? -EFAULT : actual_size;\\n}\\n#endif /* CONFIG_HAVE_ARCH_TRACEHOOK */\\n\\nint ptrace_request(struct task_struct *child, long request,\\n\\t\\t   unsigned long addr, unsigned long data)\\n{\\n\\tbool seized = child->ptrace & PT_SEIZED;\\n\\tint ret = -EIO;\\n\\tkernel_siginfo_t siginfo, *si;\\n\\tvoid __user *datavp = (void __user *) data;\\n\\tunsigned long __user *datalp = datavp;\\n\\tunsigned long flags;\\n\\n\\tswitch (request) {\\n\\tcase PTRACE_PEEKTEXT:\\n\\tcase PTRACE_PEEKDATA:\\n\\t\\treturn generic_ptrace_peekdata(child, addr, data);\\n\\tcase PTRACE_POKETEXT:\\n\\tcase PTRACE_POKEDATA:\\n\\t\\treturn generic_ptrace_pokedata(child, addr, data);\\n\\n#ifdef PTRACE_OLDSETOPTIONS\\n\\tcase PTRACE_OLDSETOPTIONS:\\n#endif\\n\\tcase PTRACE_SETOPTIONS:\\n\\t\\tret = ptrace_setoptions(child, data);\\n\\t\\tbreak;\\n\\tcase PTRACE_GETEVENTMSG:\\n\\t\\tret = put_user(child->ptrace_message, datalp);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_PEEKSIGINFO:\\n\\t\\tret = ptrace_peek_siginfo(child, addr, data);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_GETSIGINFO:\\n\\t\\tret = ptrace_getsiginfo(child, &siginfo);\\n\\t\\tif (!ret)\\n\\t\\t\\tret = copy_siginfo_to_user(datavp, &siginfo);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_SETSIGINFO:\\n\\t\\tret = copy_siginfo_from_user(&siginfo, datavp);\\n\\t\\tif (!ret)\\n\\t\\t\\tret = ptrace_setsiginfo(child, &siginfo);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_GETSIGMASK: {\\n\\t\\tsigset_t *mask;\\n\\n\\t\\tif (addr != sizeof(sigset_t)) {\\n\\t\\t\\tret = -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tif (test_tsk_restore_sigmask(child))\\n\\t\\t\\tmask = &child->saved_sigmask;\\n\\t\\telse\\n\\t\\t\\tmask = &child->blocked;\\n\\n\\t\\tif (copy_to_user(datavp, mask, sizeof(sigset_t)))\\n\\t\\t\\tret = -EFAULT;\\n\\t\\telse\\n\\t\\t\\tret = 0;\\n\\n\\t\\tbreak;\\n\\t}\\n\\n\\tcase PTRACE_SETSIGMASK: {\\n\\t\\tsigset_t new_set;\\n\\n\\t\\tif (addr != sizeof(sigset_t)) {\\n\\t\\t\\tret = -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tif (copy_from_user(&new_set, datavp, sizeof(sigset_t))) {\\n\\t\\t\\tret = -EFAULT;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\\n\\n\\t\\t/*\\n\\t\\t * Every thread does recalc_sigpending() after resume, so\\n\\t\\t * retarget_shared_pending() and recalc_sigpending() are not\\n\\t\\t * called here.\\n\\t\\t */\\n\\t\\tspin_lock_irq(&child->sighand->siglock);\\n\\t\\tchild->blocked = new_set;\\n\\t\\tspin_unlock_irq(&child->sighand->siglock);\\n\\n\\t\\tclear_tsk_restore_sigmask(child);\\n\\n\\t\\tret = 0;\\n\\t\\tbreak;\\n\\t}\\n\\n\\tcase PTRACE_INTERRUPT:\\n\\t\\t/*\\n\\t\\t * Stop tracee without any side-effect on signal or job\\n\\t\\t * control.  At least one trap is guaranteed to happen\\n\\t\\t * after this request.  If @child is already trapped, the\\n\\t\\t * current trap is not disturbed and another trap will\\n\\t\\t * happen after the current trap is ended with PTRACE_CONT.\\n\\t\\t *\\n\\t\\t * The actual trap might not be PTRACE_EVENT_STOP trap but\\n\\t\\t * the pending condition is cleared regardless.\\n\\t\\t */\\n\\t\\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\\n\\t\\t\\tbreak;\\n\\n\\t\\t/*\\n\\t\\t * INTERRUPT doesn\\'t disturb existing trap sans one\\n\\t\\t * exception.  If ptracer issued LISTEN for the current\\n\\t\\t * STOP, this INTERRUPT should clear LISTEN and re-trap\\n\\t\\t * tracee into STOP.\\n\\t\\t */\\n\\t\\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\\n\\t\\t\\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\\n\\n\\t\\tunlock_task_sighand(child, &flags);\\n\\t\\tret = 0;\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_LISTEN:\\n\\t\\t/*\\n\\t\\t * Listen for events.  Tracee must be in STOP.  It\\'s not\\n\\t\\t * resumed per-se but is not considered to be in TRACED by\\n\\t\\t * wait(2) or ptrace(2).  If an async event (e.g. group\\n\\t\\t * stop state change) happens, tracee will enter STOP trap\\n\\t\\t * again.  Alternatively, ptracer can issue INTERRUPT to\\n\\t\\t * finish listening and re-trap tracee into STOP.\\n\\t\\t */\\n\\t\\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\\n\\t\\t\\tbreak;\\n\\n\\t\\tsi = child->last_siginfo;\\n\\t\\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\\n\\t\\t\\tchild->jobctl |= JOBCTL_LISTENING;\\n\\t\\t\\t/*\\n\\t\\t\\t * If NOTIFY is set, it means event happened between\\n\\t\\t\\t * start of this trap and now.  Trigger re-trap.\\n\\t\\t\\t */\\n\\t\\t\\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\\n\\t\\t\\t\\tptrace_signal_wake_up(child, true);\\n\\t\\t\\tret = 0;\\n\\t\\t}\\n\\t\\tunlock_task_sighand(child, &flags);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_DETACH:\\t /* detach a process that was attached. */\\n\\t\\tret = ptrace_detach(child, data);\\n\\t\\tbreak;\\n\\n#ifdef CONFIG_BINFMT_ELF_FDPIC\\n\\tcase PTRACE_GETFDPIC: {\\n\\t\\tstruct mm_struct *mm = get_task_mm(child);\\n\\t\\tunsigned long tmp = 0;\\n\\n\\t\\tret = -ESRCH;\\n\\t\\tif (!mm)\\n\\t\\t\\tbreak;\\n\\n\\t\\tswitch (addr) {\\n\\t\\tcase PTRACE_GETFDPIC_EXEC:\\n\\t\\t\\ttmp = mm->context.exec_fdpic_loadmap;\\n\\t\\t\\tbreak;\\n\\t\\tcase PTRACE_GETFDPIC_INTERP:\\n\\t\\t\\ttmp = mm->context.interp_fdpic_loadmap;\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tmmput(mm);\\n\\n\\t\\tret = put_user(tmp, datalp);\\n\\t\\tbreak;\\n\\t}\\n#endif\\n\\n\\tcase PTRACE_SINGLESTEP:\\n#ifdef PTRACE_SINGLEBLOCK\\n\\tcase PTRACE_SINGLEBLOCK:\\n#endif\\n#ifdef PTRACE_SYSEMU\\n\\tcase PTRACE_SYSEMU:\\n\\tcase PTRACE_SYSEMU_SINGLESTEP:\\n#endif\\n\\tcase PTRACE_SYSCALL:\\n\\tcase PTRACE_CONT:\\n\\t\\treturn ptrace_resume(child, request, data);\\n\\n\\tcase PTRACE_KILL:\\n\\t\\tsend_sig_info(SIGKILL, SEND_SIG_NOINFO, child);\\n\\t\\treturn 0;\\n\\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\\n\\tcase PTRACE_GETREGSET:\\n\\tcase PTRACE_SETREGSET: {\\n\\t\\tstruct iovec kiov;\\n\\t\\tstruct iovec __user *uiov = datavp;\\n\\n\\t\\tif (!access_ok(uiov, sizeof(*uiov)))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\\n\\t\\t    __get_user(kiov.iov_len, &uiov->iov_len))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\tret = ptrace_regset(child, request, addr, &kiov);\\n\\t\\tif (!ret)\\n\\t\\t\\tret = __put_user(kiov.iov_len, &uiov->iov_len);\\n\\t\\tbreak;\\n\\t}\\n\\n\\tcase PTRACE_GET_SYSCALL_INFO:\\n\\t\\tret = ptrace_get_syscall_info(child, addr, datavp);\\n\\t\\tbreak;\\n#endif\\n\\n\\tcase PTRACE_SECCOMP_GET_FILTER:\\n\\t\\tret = seccomp_get_filter(child, addr, datavp);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_SECCOMP_GET_METADATA:\\n\\t\\tret = seccomp_get_metadata(child, addr, datavp);\\n\\t\\tbreak;\\n\\n#ifdef CONFIG_RSEQ\\n\\tcase PTRACE_GET_RSEQ_CONFIGURATION:\\n\\t\\tret = ptrace_get_rseq_configuration(child, addr, datavp);\\n\\t\\tbreak;\\n#endif\\n\\n\\tcase PTRACE_SET_SYSCALL_USER_DISPATCH_CONFIG:\\n\\t\\tret = syscall_user_dispatch_set_config(child, addr, datavp);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_GET_SYSCALL_USER_DISPATCH_CONFIG:\\n\\t\\tret = syscall_user_dispatch_get_config(child, addr, datavp);\\n\\t\\tbreak;\\n\\n\\tdefault:\\n\\t\\tbreak;\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,\\n\\t\\tunsigned long, data)\\n{\\n\\tstruct task_struct *child;\\n\\tlong ret;\\n\\n\\tif (request == PTRACE_TRACEME) {\\n\\t\\tret = ptrace_traceme();\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tchild = find_get_task_by_vpid(pid);\\n\\tif (!child) {\\n\\t\\tret = -ESRCH;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\\n\\t\\tret = ptrace_attach(child, request, addr, data);\\n\\t\\tgoto out_put_task_struct;\\n\\t}\\n\\n\\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\\n\\t\\t\\t\\t  request == PTRACE_INTERRUPT);\\n\\tif (ret < 0)\\n\\t\\tgoto out_put_task_struct;\\n\\n\\tret = arch_ptrace(child, request, addr, data);\\n\\tif (ret || request != PTRACE_DETACH)\\n\\t\\tptrace_unfreeze_traced(child);\\n\\n out_put_task_struct:\\n\\tput_task_struct(child);\\n out:\\n\\treturn ret;\\n}\\n\\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\\n\\t\\t\\t    unsigned long data)\\n{\\n\\tunsigned long tmp;\\n\\tint copied;\\n\\n\\tcopied = ptrace_access_vm(tsk, addr, &tmp, sizeof(tmp), FOLL_FORCE);\\n\\tif (copied != sizeof(tmp))\\n\\t\\treturn -EIO;\\n\\treturn put_user(tmp, (unsigned long __user *)data);\\n}\\n\\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\\n\\t\\t\\t    unsigned long data)\\n{\\n\\tint copied;\\n\\n\\tcopied = ptrace_access_vm(tsk, addr, &data, sizeof(data),\\n\\t\\t\\tFOLL_FORCE | FOLL_WRITE);\\n\\treturn (copied == sizeof(data)) ? 0 : -EIO;\\n}\\n\\n#if defined CONFIG_COMPAT\\n\\nint compat_ptrace_request(struct task_struct *child, compat_long_t request,\\n\\t\\t\\t  compat_ulong_t addr, compat_ulong_t data)\\n{\\n\\tcompat_ulong_t __user *datap = compat_ptr(data);\\n\\tcompat_ulong_t word;\\n\\tkernel_siginfo_t siginfo;\\n\\tint ret;\\n\\n\\tswitch (request) {\\n\\tcase PTRACE_PEEKTEXT:\\n\\tcase PTRACE_PEEKDATA:\\n\\t\\tret = ptrace_access_vm(child, addr, &word, sizeof(word),\\n\\t\\t\\t\\tFOLL_FORCE);\\n\\t\\tif (ret != sizeof(word))\\n\\t\\t\\tret = -EIO;\\n\\t\\telse\\n\\t\\t\\tret = put_user(word, datap);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_POKETEXT:\\n\\tcase PTRACE_POKEDATA:\\n\\t\\tret = ptrace_access_vm(child, addr, &data, sizeof(data),\\n\\t\\t\\t\\tFOLL_FORCE | FOLL_WRITE);\\n\\t\\tret = (ret != sizeof(data) ? -EIO : 0);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_GETEVENTMSG:\\n\\t\\tret = put_user((compat_ulong_t) child->ptrace_message, datap);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_GETSIGINFO:\\n\\t\\tret = ptrace_getsiginfo(child, &siginfo);\\n\\t\\tif (!ret)\\n\\t\\t\\tret = copy_siginfo_to_user32(\\n\\t\\t\\t\\t(struct compat_siginfo __user *) datap,\\n\\t\\t\\t\\t&siginfo);\\n\\t\\tbreak;\\n\\n\\tcase PTRACE_SETSIGINFO:\\n\\t\\tret = copy_siginfo_from_user32(\\n\\t\\t\\t&siginfo, (struct compat_siginfo __user *) datap);\\n\\t\\tif (!ret)\\n\\t\\t\\tret = ptrace_setsiginfo(child, &siginfo);\\n\\t\\tbreak;\\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\\n\\tcase PTRACE_GETREGSET:\\n\\tcase PTRACE_SETREGSET:\\n\\t{\\n\\t\\tstruct iovec kiov;\\n\\t\\tstruct compat_iovec __user *uiov =\\n\\t\\t\\t(struct compat_iovec __user *) datap;\\n\\t\\tcompat_uptr_t ptr;\\n\\t\\tcompat_size_t len;\\n\\n\\t\\tif (!access_ok(uiov, sizeof(*uiov)))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\tif (__get_user(ptr, &uiov->iov_base) ||\\n\\t\\t    __get_user(len, &uiov->iov_len))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\tkiov.iov_base = compat_ptr(ptr);\\n\\t\\tkiov.iov_len = len;\\n\\n\\t\\tret = ptrace_regset(child, request, addr, &kiov);\\n\\t\\tif (!ret)\\n\\t\\t\\tret = __put_user(kiov.iov_len, &uiov->iov_len);\\n\\t\\tbreak;\\n\\t}\\n#endif\\n\\n\\tdefault:\\n\\t\\tret = ptrace_request(child, request, addr, data);\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nCOMPAT_SYSCALL_DEFINE4(ptrace, compat_long_t, request, compat_long_t, pid,\\n\\t\\t       compat_long_t, addr, compat_long_t, data)\\n{\\n\\tstruct task_struct *child;\\n\\tlong ret;\\n\\n\\tif (request == PTRACE_TRACEME) {\\n\\t\\tret = ptrace_traceme();\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tchild = find_get_task_by_vpid(pid);\\n\\tif (!child) {\\n\\t\\tret = -ESRCH;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\\n\\t\\tret = ptrace_attach(child, request, addr, data);\\n\\t\\tgoto out_put_task_struct;\\n\\t}\\n\\n\\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\\n\\t\\t\\t\\t  request == PTRACE_INTERRUPT);\\n\\tif (!ret) {\\n\\t\\tret = compat_arch_ptrace(child, request, addr, data);\\n\\t\\tif (ret || request != PTRACE_DETACH)\\n\\t\\t\\tptrace_unfreeze_traced(child);\\n\\t}\\n\\n out_put_task_struct:\\n\\tput_task_struct(child);\\n out:\\n\\treturn ret;\\n}\\n#endif\\t/* CONFIG_COMPAT */\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/*\\n * Copyright (C) 2008-2014 Mathieu Desnoyers\\n */\\n#include <linux/module.h>\\n#include <linux/mutex.h>\\n#include <linux/types.h>\\n#include <linux/jhash.h>\\n#include <linux/list.h>\\n#include <linux/rcupdate.h>\\n#include <linux/tracepoint.h>\\n#include <linux/err.h>\\n#include <linux/slab.h>\\n#include <linux/sched/signal.h>\\n#include <linux/sched/task.h>\\n#include <linux/static_key.h>\\n\\nenum tp_func_state {\\n\\tTP_FUNC_0,\\n\\tTP_FUNC_1,\\n\\tTP_FUNC_2,\\n\\tTP_FUNC_N,\\n};\\n\\nextern tracepoint_ptr_t __start___tracepoints_ptrs[];\\nextern tracepoint_ptr_t __stop___tracepoints_ptrs[];\\n\\nenum tp_transition_sync {\\n\\tTP_TRANSITION_SYNC_1_0_1,\\n\\tTP_TRANSITION_SYNC_N_2_1,\\n\\n\\t_NR_TP_TRANSITION_SYNC,\\n};\\n\\nstruct tp_transition_snapshot {\\n\\tunsigned long rcu;\\n\\tbool ongoing;\\n};\\n\\n/* Protected by tracepoints_mutex */\\nstatic struct tp_transition_snapshot tp_transition_snapshot[_NR_TP_TRANSITION_SYNC];\\n\\nstatic void tp_rcu_get_state(enum tp_transition_sync sync)\\n{\\n\\tstruct tp_transition_snapshot *snapshot = &tp_transition_snapshot[sync];\\n\\n\\t/* Keep the latest get_state snapshot. */\\n\\tsnapshot->rcu = get_state_synchronize_rcu();\\n\\tsnapshot->ongoing = true;\\n}\\n\\nstatic void tp_rcu_cond_sync(enum tp_transition_sync sync)\\n{\\n\\tstruct tp_transition_snapshot *snapshot = &tp_transition_snapshot[sync];\\n\\n\\tif (!snapshot->ongoing)\\n\\t\\treturn;\\n\\tcond_synchronize_rcu(snapshot->rcu);\\n\\tsnapshot->ongoing = false;\\n}\\n\\n/* Set to 1 to enable tracepoint debug output */\\nstatic const int tracepoint_debug;\\n\\n#ifdef CONFIG_MODULES\\n/*\\n * Tracepoint module list mutex protects the local module list.\\n */\\nstatic DEFINE_MUTEX(tracepoint_module_list_mutex);\\n\\n/* Local list of struct tp_module */\\nstatic LIST_HEAD(tracepoint_module_list);\\n#endif /* CONFIG_MODULES */\\n\\n/*\\n * tracepoints_mutex protects the builtin and module tracepoints.\\n * tracepoints_mutex nests inside tracepoint_module_list_mutex.\\n */\\nstatic DEFINE_MUTEX(tracepoints_mutex);\\n\\n/*\\n * Note about RCU :\\n * It is used to delay the free of multiple probes array until a quiescent\\n * state is reached.\\n */\\nstruct tp_probes {\\n\\tstruct rcu_head rcu;\\n\\tstruct tracepoint_func probes[];\\n};\\n\\n/* Called in removal of a func but failed to allocate a new tp_funcs */\\nstatic void tp_stub_func(void)\\n{\\n\\treturn;\\n}\\n\\nstatic inline void *allocate_probes(int count)\\n{\\n\\tstruct tp_probes *p  = kmalloc(struct_size(p, probes, count),\\n\\t\\t\\t\\t       GFP_KERNEL);\\n\\treturn p == NULL ? NULL : p->probes;\\n}\\n\\nstatic void rcu_free_old_probes(struct rcu_head *head)\\n{\\n\\tkfree(container_of(head, struct tp_probes, rcu));\\n}\\n\\nstatic inline void release_probes(struct tracepoint *tp, struct tracepoint_func *old)\\n{\\n\\tif (old) {\\n\\t\\tstruct tp_probes *tp_probes = container_of(old,\\n\\t\\t\\tstruct tp_probes, probes[0]);\\n\\n\\t\\tif (tracepoint_is_faultable(tp))\\n\\t\\t\\tcall_rcu_tasks_trace(&tp_probes->rcu, rcu_free_old_probes);\\n\\t\\telse\\n\\t\\t\\tcall_rcu(&tp_probes->rcu, rcu_free_old_probes);\\n\\t}\\n}\\n\\nstatic void debug_print_probes(struct tracepoint_func *funcs)\\n{\\n\\tint i;\\n\\n\\tif (!tracepoint_debug || !funcs)\\n\\t\\treturn;\\n\\n\\tfor (i = 0; funcs[i].func; i++)\\n\\t\\tprintk(KERN_DEBUG \"Probe %d : %p\\\\n\", i, funcs[i].func);\\n}\\n\\nstatic struct tracepoint_func *\\nfunc_add(struct tracepoint_func **funcs, struct tracepoint_func *tp_func,\\n\\t int prio)\\n{\\n\\tstruct tracepoint_func *old, *new;\\n\\tint iter_probes;\\t/* Iterate over old probe array. */\\n\\tint nr_probes = 0;\\t/* Counter for probes */\\n\\tint pos = -1;\\t\\t/* Insertion position into new array */\\n\\n\\tif (WARN_ON(!tp_func->func))\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\tdebug_print_probes(*funcs);\\n\\told = *funcs;\\n\\tif (old) {\\n\\t\\t/* (N -> N+1), (N != 0, 1) probes */\\n\\t\\tfor (iter_probes = 0; old[iter_probes].func; iter_probes++) {\\n\\t\\t\\tif (old[iter_probes].func == tp_stub_func)\\n\\t\\t\\t\\tcontinue;\\t/* Skip stub functions. */\\n\\t\\t\\tif (old[iter_probes].func == tp_func->func &&\\n\\t\\t\\t    old[iter_probes].data == tp_func->data)\\n\\t\\t\\t\\treturn ERR_PTR(-EEXIST);\\n\\t\\t\\tnr_probes++;\\n\\t\\t}\\n\\t}\\n\\t/* + 2 : one for new probe, one for NULL func */\\n\\tnew = allocate_probes(nr_probes + 2);\\n\\tif (new == NULL)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\tif (old) {\\n\\t\\tnr_probes = 0;\\n\\t\\tfor (iter_probes = 0; old[iter_probes].func; iter_probes++) {\\n\\t\\t\\tif (old[iter_probes].func == tp_stub_func)\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t/* Insert before probes of lower priority */\\n\\t\\t\\tif (pos < 0 && old[iter_probes].prio < prio)\\n\\t\\t\\t\\tpos = nr_probes++;\\n\\t\\t\\tnew[nr_probes++] = old[iter_probes];\\n\\t\\t}\\n\\t\\tif (pos < 0)\\n\\t\\t\\tpos = nr_probes++;\\n\\t\\t/* nr_probes now points to the end of the new array */\\n\\t} else {\\n\\t\\tpos = 0;\\n\\t\\tnr_probes = 1; /* must point at end of array */\\n\\t}\\n\\tnew[pos] = *tp_func;\\n\\tnew[nr_probes].func = NULL;\\n\\t*funcs = new;\\n\\tdebug_print_probes(*funcs);\\n\\treturn old;\\n}\\n\\nstatic void *func_remove(struct tracepoint_func **funcs,\\n\\t\\tstruct tracepoint_func *tp_func)\\n{\\n\\tint nr_probes = 0, nr_del = 0, i;\\n\\tstruct tracepoint_func *old, *new;\\n\\n\\told = *funcs;\\n\\n\\tif (!old)\\n\\t\\treturn ERR_PTR(-ENOENT);\\n\\n\\tdebug_print_probes(*funcs);\\n\\t/* (N -> M), (N > 1, M >= 0) probes */\\n\\tif (tp_func->func) {\\n\\t\\tfor (nr_probes = 0; old[nr_probes].func; nr_probes++) {\\n\\t\\t\\tif ((old[nr_probes].func == tp_func->func &&\\n\\t\\t\\t     old[nr_probes].data == tp_func->data) ||\\n\\t\\t\\t    old[nr_probes].func == tp_stub_func)\\n\\t\\t\\t\\tnr_del++;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * If probe is NULL, then nr_probes = nr_del = 0, and then the\\n\\t * entire entry will be removed.\\n\\t */\\n\\tif (nr_probes - nr_del == 0) {\\n\\t\\t/* N -> 0, (N > 1) */\\n\\t\\t*funcs = NULL;\\n\\t\\tdebug_print_probes(*funcs);\\n\\t\\treturn old;\\n\\t} else {\\n\\t\\tint j = 0;\\n\\t\\t/* N -> M, (N > 1, M > 0) */\\n\\t\\t/* + 1 for NULL */\\n\\t\\tnew = allocate_probes(nr_probes - nr_del + 1);\\n\\t\\tif (new) {\\n\\t\\t\\tfor (i = 0; old[i].func; i++) {\\n\\t\\t\\t\\tif ((old[i].func != tp_func->func ||\\n\\t\\t\\t\\t     old[i].data != tp_func->data) &&\\n\\t\\t\\t\\t    old[i].func != tp_stub_func)\\n\\t\\t\\t\\t\\tnew[j++] = old[i];\\n\\t\\t\\t}\\n\\t\\t\\tnew[nr_probes - nr_del].func = NULL;\\n\\t\\t\\t*funcs = new;\\n\\t\\t} else {\\n\\t\\t\\t/*\\n\\t\\t\\t * Failed to allocate, replace the old function\\n\\t\\t\\t * with calls to tp_stub_func.\\n\\t\\t\\t */\\n\\t\\t\\tfor (i = 0; old[i].func; i++) {\\n\\t\\t\\t\\tif (old[i].func == tp_func->func &&\\n\\t\\t\\t\\t    old[i].data == tp_func->data)\\n\\t\\t\\t\\t\\tWRITE_ONCE(old[i].func, tp_stub_func);\\n\\t\\t\\t}\\n\\t\\t\\t*funcs = old;\\n\\t\\t}\\n\\t}\\n\\tdebug_print_probes(*funcs);\\n\\treturn old;\\n}\\n\\n/*\\n * Count the number of functions (enum tp_func_state) in a tp_funcs array.\\n */\\nstatic enum tp_func_state nr_func_state(const struct tracepoint_func *tp_funcs)\\n{\\n\\tif (!tp_funcs)\\n\\t\\treturn TP_FUNC_0;\\n\\tif (!tp_funcs[1].func)\\n\\t\\treturn TP_FUNC_1;\\n\\tif (!tp_funcs[2].func)\\n\\t\\treturn TP_FUNC_2;\\n\\treturn TP_FUNC_N;\\t/* 3 or more */\\n}\\n\\nstatic void tracepoint_update_call(struct tracepoint *tp, struct tracepoint_func *tp_funcs)\\n{\\n\\tvoid *func = tp->iterator;\\n\\n\\t/* Synthetic events do not have static call sites */\\n\\tif (!tp->static_call_key)\\n\\t\\treturn;\\n\\tif (nr_func_state(tp_funcs) == TP_FUNC_1)\\n\\t\\tfunc = tp_funcs[0].func;\\n\\t__static_call_update(tp->static_call_key, tp->static_call_tramp, func);\\n}\\n\\n/*\\n * Add the probe function to a tracepoint.\\n */\\nstatic int tracepoint_add_func(struct tracepoint *tp,\\n\\t\\t\\t       struct tracepoint_func *func, int prio,\\n\\t\\t\\t       bool warn)\\n{\\n\\tstruct tracepoint_func *old, *tp_funcs;\\n\\tint ret;\\n\\n\\tif (tp->ext && tp->ext->regfunc && !static_key_enabled(&tp->key)) {\\n\\t\\tret = tp->ext->regfunc();\\n\\t\\tif (ret < 0)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\n\\ttp_funcs = rcu_dereference_protected(tp->funcs,\\n\\t\\t\\tlockdep_is_held(&tracepoints_mutex));\\n\\told = func_add(&tp_funcs, func, prio);\\n\\tif (IS_ERR(old)) {\\n\\t\\tWARN_ON_ONCE(warn && PTR_ERR(old) != -ENOMEM);\\n\\t\\treturn PTR_ERR(old);\\n\\t}\\n\\n\\t/*\\n\\t * rcu_assign_pointer has as smp_store_release() which makes sure\\n\\t * that the new probe callbacks array is consistent before setting\\n\\t * a pointer to it.  This array is referenced by __DO_TRACE from\\n\\t * include/linux/tracepoint.h using rcu_dereference_sched().\\n\\t */\\n\\tswitch (nr_func_state(tp_funcs)) {\\n\\tcase TP_FUNC_1:\\t\\t/* 0->1 */\\n\\t\\t/*\\n\\t\\t * Make sure new static func never uses old data after a\\n\\t\\t * 1->0->1 transition sequence.\\n\\t\\t */\\n\\t\\ttp_rcu_cond_sync(TP_TRANSITION_SYNC_1_0_1);\\n\\t\\t/* Set static call to first function */\\n\\t\\ttracepoint_update_call(tp, tp_funcs);\\n\\t\\t/* Both iterator and static call handle NULL tp->funcs */\\n\\t\\trcu_assign_pointer(tp->funcs, tp_funcs);\\n\\t\\tstatic_branch_enable(&tp->key);\\n\\t\\tbreak;\\n\\tcase TP_FUNC_2:\\t\\t/* 1->2 */\\n\\t\\t/* Set iterator static call */\\n\\t\\ttracepoint_update_call(tp, tp_funcs);\\n\\t\\t/*\\n\\t\\t * Iterator callback installed before updating tp->funcs.\\n\\t\\t * Requires ordering between RCU assign/dereference and\\n\\t\\t * static call update/call.\\n\\t\\t */\\n\\t\\tfallthrough;\\n\\tcase TP_FUNC_N:\\t\\t/* N->N+1 (N>1) */\\n\\t\\trcu_assign_pointer(tp->funcs, tp_funcs);\\n\\t\\t/*\\n\\t\\t * Make sure static func never uses incorrect data after a\\n\\t\\t * N->...->2->1 (N>1) transition sequence.\\n\\t\\t */\\n\\t\\tif (tp_funcs[0].data != old[0].data)\\n\\t\\t\\ttp_rcu_get_state(TP_TRANSITION_SYNC_N_2_1);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tWARN_ON_ONCE(1);\\n\\t\\tbreak;\\n\\t}\\n\\n\\trelease_probes(tp, old);\\n\\treturn 0;\\n}\\n\\n/*\\n * Remove a probe function from a tracepoint.\\n * Note: only waiting an RCU period after setting elem->call to the empty\\n * function insures that the original callback is not used anymore. This insured\\n * by preempt_disable around the call site.\\n */\\nstatic int tracepoint_remove_func(struct tracepoint *tp,\\n\\t\\tstruct tracepoint_func *func)\\n{\\n\\tstruct tracepoint_func *old, *tp_funcs;\\n\\n\\ttp_funcs = rcu_dereference_protected(tp->funcs,\\n\\t\\t\\tlockdep_is_held(&tracepoints_mutex));\\n\\told = func_remove(&tp_funcs, func);\\n\\tif (WARN_ON_ONCE(IS_ERR(old)))\\n\\t\\treturn PTR_ERR(old);\\n\\n\\tif (tp_funcs == old)\\n\\t\\t/* Failed allocating new tp_funcs, replaced func with stub */\\n\\t\\treturn 0;\\n\\n\\tswitch (nr_func_state(tp_funcs)) {\\n\\tcase TP_FUNC_0:\\t\\t/* 1->0 */\\n\\t\\t/* Removed last function */\\n\\t\\tif (tp->ext && tp->ext->unregfunc && static_key_enabled(&tp->key))\\n\\t\\t\\ttp->ext->unregfunc();\\n\\t\\tstatic_branch_disable(&tp->key);\\n\\t\\t/* Set iterator static call */\\n\\t\\ttracepoint_update_call(tp, tp_funcs);\\n\\t\\t/* Both iterator and static call handle NULL tp->funcs */\\n\\t\\trcu_assign_pointer(tp->funcs, NULL);\\n\\t\\t/*\\n\\t\\t * Make sure new static func never uses old data after a\\n\\t\\t * 1->0->1 transition sequence.\\n\\t\\t */\\n\\t\\ttp_rcu_get_state(TP_TRANSITION_SYNC_1_0_1);\\n\\t\\tbreak;\\n\\tcase TP_FUNC_1:\\t\\t/* 2->1 */\\n\\t\\trcu_assign_pointer(tp->funcs, tp_funcs);\\n\\t\\t/*\\n\\t\\t * Make sure static func never uses incorrect data after a\\n\\t\\t * N->...->2->1 (N>2) transition sequence. If the first\\n\\t\\t * element\\'s data has changed, then force the synchronization\\n\\t\\t * to prevent current readers that have loaded the old data\\n\\t\\t * from calling the new function.\\n\\t\\t */\\n\\t\\tif (tp_funcs[0].data != old[0].data)\\n\\t\\t\\ttp_rcu_get_state(TP_TRANSITION_SYNC_N_2_1);\\n\\t\\ttp_rcu_cond_sync(TP_TRANSITION_SYNC_N_2_1);\\n\\t\\t/* Set static call to first function */\\n\\t\\ttracepoint_update_call(tp, tp_funcs);\\n\\t\\tbreak;\\n\\tcase TP_FUNC_2:\\t\\t/* N->N-1 (N>2) */\\n\\t\\tfallthrough;\\n\\tcase TP_FUNC_N:\\n\\t\\trcu_assign_pointer(tp->funcs, tp_funcs);\\n\\t\\t/*\\n\\t\\t * Make sure static func never uses incorrect data after a\\n\\t\\t * N->...->2->1 (N>2) transition sequence.\\n\\t\\t */\\n\\t\\tif (tp_funcs[0].data != old[0].data)\\n\\t\\t\\ttp_rcu_get_state(TP_TRANSITION_SYNC_N_2_1);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tWARN_ON_ONCE(1);\\n\\t\\tbreak;\\n\\t}\\n\\trelease_probes(tp, old);\\n\\treturn 0;\\n}\\n\\n/**\\n * tracepoint_probe_register_prio_may_exist -  Connect a probe to a tracepoint with priority\\n * @tp: tracepoint\\n * @probe: probe handler\\n * @data: tracepoint data\\n * @prio: priority of this function over other registered functions\\n *\\n * Same as tracepoint_probe_register_prio() except that it will not warn\\n * if the tracepoint is already registered.\\n */\\nint tracepoint_probe_register_prio_may_exist(struct tracepoint *tp, void *probe,\\n\\t\\t\\t\\t\\t     void *data, int prio)\\n{\\n\\tstruct tracepoint_func tp_func;\\n\\tint ret;\\n\\n\\tmutex_lock(&tracepoints_mutex);\\n\\ttp_func.func = probe;\\n\\ttp_func.data = data;\\n\\ttp_func.prio = prio;\\n\\tret = tracepoint_add_func(tp, &tp_func, prio, false);\\n\\tmutex_unlock(&tracepoints_mutex);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(tracepoint_probe_register_prio_may_exist);\\n\\n/**\\n * tracepoint_probe_register_prio -  Connect a probe to a tracepoint with priority\\n * @tp: tracepoint\\n * @probe: probe handler\\n * @data: tracepoint data\\n * @prio: priority of this function over other registered functions\\n *\\n * Returns 0 if ok, error value on error.\\n * Note: if @tp is within a module, the caller is responsible for\\n * unregistering the probe before the module is gone. This can be\\n * performed either with a tracepoint module going notifier, or from\\n * within module exit functions.\\n */\\nint tracepoint_probe_register_prio(struct tracepoint *tp, void *probe,\\n\\t\\t\\t\\t   void *data, int prio)\\n{\\n\\tstruct tracepoint_func tp_func;\\n\\tint ret;\\n\\n\\tmutex_lock(&tracepoints_mutex);\\n\\ttp_func.func = probe;\\n\\ttp_func.data = data;\\n\\ttp_func.prio = prio;\\n\\tret = tracepoint_add_func(tp, &tp_func, prio, true);\\n\\tmutex_unlock(&tracepoints_mutex);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(tracepoint_probe_register_prio);\\n\\n/**\\n * tracepoint_probe_register -  Connect a probe to a tracepoint\\n * @tp: tracepoint\\n * @probe: probe handler\\n * @data: tracepoint data\\n *\\n * Returns 0 if ok, error value on error.\\n * Note: if @tp is within a module, the caller is responsible for\\n * unregistering the probe before the module is gone. This can be\\n * performed either with a tracepoint module going notifier, or from\\n * within module exit functions.\\n */\\nint tracepoint_probe_register(struct tracepoint *tp, void *probe, void *data)\\n{\\n\\treturn tracepoint_probe_register_prio(tp, probe, data, TRACEPOINT_DEFAULT_PRIO);\\n}\\nEXPORT_SYMBOL_GPL(tracepoint_probe_register);\\n\\n/**\\n * tracepoint_probe_unregister -  Disconnect a probe from a tracepoint\\n * @tp: tracepoint\\n * @probe: probe function pointer\\n * @data: tracepoint data\\n *\\n * Returns 0 if ok, error value on error.\\n */\\nint tracepoint_probe_unregister(struct tracepoint *tp, void *probe, void *data)\\n{\\n\\tstruct tracepoint_func tp_func;\\n\\tint ret;\\n\\n\\tmutex_lock(&tracepoints_mutex);\\n\\ttp_func.func = probe;\\n\\ttp_func.data = data;\\n\\tret = tracepoint_remove_func(tp, &tp_func);\\n\\tmutex_unlock(&tracepoints_mutex);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(tracepoint_probe_unregister);\\n\\nstatic void for_each_tracepoint_range(\\n\\t\\ttracepoint_ptr_t *begin, tracepoint_ptr_t *end,\\n\\t\\tvoid (*fct)(struct tracepoint *tp, void *priv),\\n\\t\\tvoid *priv)\\n{\\n\\ttracepoint_ptr_t *iter;\\n\\n\\tif (!begin)\\n\\t\\treturn;\\n\\tfor (iter = begin; iter < end; iter++)\\n\\t\\tfct(tracepoint_ptr_deref(iter), priv);\\n}\\n\\n#ifdef CONFIG_MODULES\\nbool trace_module_has_bad_taint(struct module *mod)\\n{\\n\\treturn mod->taints & ~((1 << TAINT_OOT_MODULE) | (1 << TAINT_CRAP) |\\n\\t\\t\\t\\t(1 << TAINT_UNSIGNED_MODULE) | (1 << TAINT_TEST) |\\n\\t\\t\\t\\t(1 << TAINT_LIVEPATCH));\\n}\\n\\nstatic BLOCKING_NOTIFIER_HEAD(tracepoint_notify_list);\\n\\n/**\\n * register_tracepoint_module_notifier - register tracepoint coming/going notifier\\n * @nb: notifier block\\n *\\n * Notifiers registered with this function are called on module\\n * coming/going with the tracepoint_module_list_mutex held.\\n * The notifier block callback should expect a \"struct tp_module\" data\\n * pointer.\\n */\\nint register_tracepoint_module_notifier(struct notifier_block *nb)\\n{\\n\\tstruct tp_module *tp_mod;\\n\\tint ret;\\n\\n\\tmutex_lock(&tracepoint_module_list_mutex);\\n\\tret = blocking_notifier_chain_register(&tracepoint_notify_list, nb);\\n\\tif (ret)\\n\\t\\tgoto end;\\n\\tlist_for_each_entry(tp_mod, &tracepoint_module_list, list)\\n\\t\\t(void) nb->notifier_call(nb, MODULE_STATE_COMING, tp_mod);\\nend:\\n\\tmutex_unlock(&tracepoint_module_list_mutex);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(register_tracepoint_module_notifier);\\n\\n/**\\n * unregister_tracepoint_module_notifier - unregister tracepoint coming/going notifier\\n * @nb: notifier block\\n *\\n * The notifier block callback should expect a \"struct tp_module\" data\\n * pointer.\\n */\\nint unregister_tracepoint_module_notifier(struct notifier_block *nb)\\n{\\n\\tstruct tp_module *tp_mod;\\n\\tint ret;\\n\\n\\tmutex_lock(&tracepoint_module_list_mutex);\\n\\tret = blocking_notifier_chain_unregister(&tracepoint_notify_list, nb);\\n\\tif (ret)\\n\\t\\tgoto end;\\n\\tlist_for_each_entry(tp_mod, &tracepoint_module_list, list)\\n\\t\\t(void) nb->notifier_call(nb, MODULE_STATE_GOING, tp_mod);\\nend:\\n\\tmutex_unlock(&tracepoint_module_list_mutex);\\n\\treturn ret;\\n\\n}\\nEXPORT_SYMBOL_GPL(unregister_tracepoint_module_notifier);\\n\\n/*\\n * Ensure the tracer unregistered the module\\'s probes before the module\\n * teardown is performed. Prevents leaks of probe and data pointers.\\n */\\nstatic void tp_module_going_check_quiescent(struct tracepoint *tp, void *priv)\\n{\\n\\tWARN_ON_ONCE(tp->funcs);\\n}\\n\\nstatic int tracepoint_module_coming(struct module *mod)\\n{\\n\\tstruct tp_module *tp_mod;\\n\\n\\tif (!mod->num_tracepoints)\\n\\t\\treturn 0;\\n\\n\\t/*\\n\\t * We skip modules that taint the kernel, especially those with different\\n\\t * module headers (for forced load), to make sure we don\\'t cause a crash.\\n\\t * Staging, out-of-tree, unsigned GPL, and test modules are fine.\\n\\t */\\n\\tif (trace_module_has_bad_taint(mod))\\n\\t\\treturn 0;\\n\\n\\ttp_mod = kmalloc(sizeof(struct tp_module), GFP_KERNEL);\\n\\tif (!tp_mod)\\n\\t\\treturn -ENOMEM;\\n\\ttp_mod->mod = mod;\\n\\n\\tmutex_lock(&tracepoint_module_list_mutex);\\n\\tlist_add_tail(&tp_mod->list, &tracepoint_module_list);\\n\\tblocking_notifier_call_chain(&tracepoint_notify_list,\\n\\t\\t\\tMODULE_STATE_COMING, tp_mod);\\n\\tmutex_unlock(&tracepoint_module_list_mutex);\\n\\treturn 0;\\n}\\n\\nstatic void tracepoint_module_going(struct module *mod)\\n{\\n\\tstruct tp_module *tp_mod;\\n\\n\\tif (!mod->num_tracepoints)\\n\\t\\treturn;\\n\\n\\tmutex_lock(&tracepoint_module_list_mutex);\\n\\tlist_for_each_entry(tp_mod, &tracepoint_module_list, list) {\\n\\t\\tif (tp_mod->mod == mod) {\\n\\t\\t\\tblocking_notifier_call_chain(&tracepoint_notify_list,\\n\\t\\t\\t\\t\\tMODULE_STATE_GOING, tp_mod);\\n\\t\\t\\tlist_del(&tp_mod->list);\\n\\t\\t\\tkfree(tp_mod);\\n\\t\\t\\t/*\\n\\t\\t\\t * Called the going notifier before checking for\\n\\t\\t\\t * quiescence.\\n\\t\\t\\t */\\n\\t\\t\\tfor_each_tracepoint_range(mod->tracepoints_ptrs,\\n\\t\\t\\t\\tmod->tracepoints_ptrs + mod->num_tracepoints,\\n\\t\\t\\t\\ttp_module_going_check_quiescent, NULL);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\t/*\\n\\t * In the case of modules that were tainted at \"coming\", we\\'ll simply\\n\\t * walk through the list without finding it. We cannot use the \"tainted\"\\n\\t * flag on \"going\", in case a module taints the kernel only after being\\n\\t * loaded.\\n\\t */\\n\\tmutex_unlock(&tracepoint_module_list_mutex);\\n}\\n\\nstatic int tracepoint_module_notify(struct notifier_block *self,\\n\\t\\tunsigned long val, void *data)\\n{\\n\\tstruct module *mod = data;\\n\\tint ret = 0;\\n\\n\\tswitch (val) {\\n\\tcase MODULE_STATE_COMING:\\n\\t\\tret = tracepoint_module_coming(mod);\\n\\t\\tbreak;\\n\\tcase MODULE_STATE_LIVE:\\n\\t\\tbreak;\\n\\tcase MODULE_STATE_GOING:\\n\\t\\ttracepoint_module_going(mod);\\n\\t\\tbreak;\\n\\tcase MODULE_STATE_UNFORMED:\\n\\t\\tbreak;\\n\\t}\\n\\treturn notifier_from_errno(ret);\\n}\\n\\nstatic struct notifier_block tracepoint_module_nb = {\\n\\t.notifier_call = tracepoint_module_notify,\\n\\t.priority = 0,\\n};\\n\\nstatic __init int init_tracepoints(void)\\n{\\n\\tint ret;\\n\\n\\tret = register_module_notifier(&tracepoint_module_nb);\\n\\tif (ret)\\n\\t\\tpr_warn(\"Failed to register tracepoint module enter notifier\\\\n\");\\n\\n\\treturn ret;\\n}\\n__initcall(init_tracepoints);\\n\\n/**\\n * for_each_tracepoint_in_module - iteration on all tracepoints in a module\\n * @mod: module\\n * @fct: callback\\n * @priv: private data\\n */\\nvoid for_each_tracepoint_in_module(struct module *mod,\\n\\t\\t\\t\\t   void (*fct)(struct tracepoint *tp,\\n\\t\\t\\t\\t    struct module *mod, void *priv),\\n\\t\\t\\t\\t   void *priv)\\n{\\n\\ttracepoint_ptr_t *begin, *end, *iter;\\n\\n\\tlockdep_assert_held(&tracepoint_module_list_mutex);\\n\\n\\tif (!mod)\\n\\t\\treturn;\\n\\n\\tbegin = mod->tracepoints_ptrs;\\n\\tend = mod->tracepoints_ptrs + mod->num_tracepoints;\\n\\n\\tfor (iter = begin; iter < end; iter++)\\n\\t\\tfct(tracepoint_ptr_deref(iter), mod, priv);\\n}\\n\\n/**\\n * for_each_module_tracepoint - iteration on all tracepoints in all modules\\n * @fct: callback\\n * @priv: private data\\n */\\nvoid for_each_module_tracepoint(void (*fct)(struct tracepoint *tp,\\n\\t\\t\\t\\t struct module *mod, void *priv),\\n\\t\\t\\t\\tvoid *priv)\\n{\\n\\tstruct tp_module *tp_mod;\\n\\n\\tmutex_lock(&tracepoint_module_list_mutex);\\n\\tlist_for_each_entry(tp_mod, &tracepoint_module_list, list)\\n\\t\\tfor_each_tracepoint_in_module(tp_mod->mod, fct, priv);\\n\\tmutex_unlock(&tracepoint_module_list_mutex);\\n}\\n#endif /* CONFIG_MODULES */\\n\\n/**\\n * for_each_kernel_tracepoint - iteration on all kernel tracepoints\\n * @fct: callback\\n * @priv: private data\\n */\\nvoid for_each_kernel_tracepoint(void (*fct)(struct tracepoint *tp, void *priv),\\n\\t\\tvoid *priv)\\n{\\n\\tfor_each_tracepoint_range(__start___tracepoints_ptrs,\\n\\t\\t__stop___tracepoints_ptrs, fct, priv);\\n}\\nEXPORT_SYMBOL_GPL(for_each_kernel_tracepoint);\\n\\n#ifdef CONFIG_HAVE_SYSCALL_TRACEPOINTS\\n\\n/* NB: reg/unreg are called while guarded with the tracepoints_mutex */\\nstatic int sys_tracepoint_refcount;\\n\\nint syscall_regfunc(void)\\n{\\n\\tstruct task_struct *p, *t;\\n\\n\\tif (!sys_tracepoint_refcount) {\\n\\t\\tread_lock(&tasklist_lock);\\n\\t\\tfor_each_process_thread(p, t) {\\n\\t\\t\\tset_task_syscall_work(t, SYSCALL_TRACEPOINT);\\n\\t\\t}\\n\\t\\tread_unlock(&tasklist_lock);\\n\\t}\\n\\tsys_tracepoint_refcount++;\\n\\n\\treturn 0;\\n}\\n\\nvoid syscall_unregfunc(void)\\n{\\n\\tstruct task_struct *p, *t;\\n\\n\\tsys_tracepoint_refcount--;\\n\\tif (!sys_tracepoint_refcount) {\\n\\t\\tread_lock(&tasklist_lock);\\n\\t\\tfor_each_process_thread(p, t) {\\n\\t\\t\\tclear_task_syscall_work(t, SYSCALL_TRACEPOINT);\\n\\t\\t}\\n\\t\\tread_unlock(&tasklist_lock);\\n\\t}\\n}\\n#endif\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  linux/kernel/reboot.c\\n *\\n *  Copyright (C) 2013  Linus Torvalds\\n */\\n\\n#define pr_fmt(fmt)\\t\"reboot: \" fmt\\n\\n#include <linux/atomic.h>\\n#include <linux/ctype.h>\\n#include <linux/export.h>\\n#include <linux/kexec.h>\\n#include <linux/kmod.h>\\n#include <linux/kmsg_dump.h>\\n#include <linux/reboot.h>\\n#include <linux/suspend.h>\\n#include <linux/syscalls.h>\\n#include <linux/syscore_ops.h>\\n#include <linux/uaccess.h>\\n\\n/*\\n * this indicates whether you can reboot with ctrl-alt-del: the default is yes\\n */\\n\\nstatic int C_A_D = 1;\\nstruct pid *cad_pid;\\nEXPORT_SYMBOL(cad_pid);\\n\\n#if defined(CONFIG_ARM)\\n#define DEFAULT_REBOOT_MODE\\t\\t= REBOOT_HARD\\n#else\\n#define DEFAULT_REBOOT_MODE\\n#endif\\nenum reboot_mode reboot_mode DEFAULT_REBOOT_MODE;\\nEXPORT_SYMBOL_GPL(reboot_mode);\\nenum reboot_mode panic_reboot_mode = REBOOT_UNDEFINED;\\n\\n/*\\n * This variable is used privately to keep track of whether or not\\n * reboot_type is still set to its default value (i.e., reboot= hasn\\'t\\n * been set on the command line).  This is needed so that we can\\n * suppress DMI scanning for reboot quirks.  Without it, it\\'s\\n * impossible to override a faulty reboot quirk without recompiling.\\n */\\nint reboot_default = 1;\\nint reboot_cpu;\\nenum reboot_type reboot_type = BOOT_ACPI;\\nint reboot_force;\\n\\nstruct sys_off_handler {\\n\\tstruct notifier_block nb;\\n\\tint (*sys_off_cb)(struct sys_off_data *data);\\n\\tvoid *cb_data;\\n\\tenum sys_off_mode mode;\\n\\tbool blocking;\\n\\tvoid *list;\\n\\tstruct device *dev;\\n};\\n\\n/*\\n * This variable is used to indicate if a halt was initiated instead of a\\n * reboot when the reboot call was invoked with LINUX_REBOOT_CMD_POWER_OFF, but\\n * the system cannot be powered off. This allowes kernel_halt() to notify users\\n * of that.\\n */\\nstatic bool poweroff_fallback_to_halt;\\n\\n/*\\n * Temporary stub that prevents linkage failure while we\\'re in process\\n * of removing all uses of legacy pm_power_off() around the kernel.\\n */\\nvoid __weak (*pm_power_off)(void);\\n\\n/*\\n *\\tNotifier list for kernel code which wants to be called\\n *\\tat shutdown. This is used to stop any idling DMA operations\\n *\\tand the like.\\n */\\nstatic BLOCKING_NOTIFIER_HEAD(reboot_notifier_list);\\n\\n/**\\n *\\temergency_restart - reboot the system\\n *\\n *\\tWithout shutting down any hardware or taking any locks\\n *\\treboot the system.  This is called when we know we are in\\n *\\ttrouble so this is our best effort to reboot.  This is\\n *\\tsafe to call in interrupt context.\\n */\\nvoid emergency_restart(void)\\n{\\n\\tkmsg_dump(KMSG_DUMP_EMERG);\\n\\tsystem_state = SYSTEM_RESTART;\\n\\tmachine_emergency_restart();\\n}\\nEXPORT_SYMBOL_GPL(emergency_restart);\\n\\nvoid kernel_restart_prepare(char *cmd)\\n{\\n\\tblocking_notifier_call_chain(&reboot_notifier_list, SYS_RESTART, cmd);\\n\\tsystem_state = SYSTEM_RESTART;\\n\\tusermodehelper_disable();\\n\\tdevice_shutdown();\\n}\\n\\n/**\\n *\\tregister_reboot_notifier - Register function to be called at reboot time\\n *\\t@nb: Info about notifier function to be called\\n *\\n *\\tRegisters a function with the list of functions\\n *\\tto be called at reboot time.\\n *\\n *\\tCurrently always returns zero, as blocking_notifier_chain_register()\\n *\\talways returns zero.\\n */\\nint register_reboot_notifier(struct notifier_block *nb)\\n{\\n\\treturn blocking_notifier_chain_register(&reboot_notifier_list, nb);\\n}\\nEXPORT_SYMBOL(register_reboot_notifier);\\n\\n/**\\n *\\tunregister_reboot_notifier - Unregister previously registered reboot notifier\\n *\\t@nb: Hook to be unregistered\\n *\\n *\\tUnregisters a previously registered reboot\\n *\\tnotifier function.\\n *\\n *\\tReturns zero on success, or %-ENOENT on failure.\\n */\\nint unregister_reboot_notifier(struct notifier_block *nb)\\n{\\n\\treturn blocking_notifier_chain_unregister(&reboot_notifier_list, nb);\\n}\\nEXPORT_SYMBOL(unregister_reboot_notifier);\\n\\nstatic void devm_unregister_reboot_notifier(struct device *dev, void *res)\\n{\\n\\tWARN_ON(unregister_reboot_notifier(*(struct notifier_block **)res));\\n}\\n\\nint devm_register_reboot_notifier(struct device *dev, struct notifier_block *nb)\\n{\\n\\tstruct notifier_block **rcnb;\\n\\tint ret;\\n\\n\\trcnb = devres_alloc(devm_unregister_reboot_notifier,\\n\\t\\t\\t    sizeof(*rcnb), GFP_KERNEL);\\n\\tif (!rcnb)\\n\\t\\treturn -ENOMEM;\\n\\n\\tret = register_reboot_notifier(nb);\\n\\tif (!ret) {\\n\\t\\t*rcnb = nb;\\n\\t\\tdevres_add(dev, rcnb);\\n\\t} else {\\n\\t\\tdevres_free(rcnb);\\n\\t}\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(devm_register_reboot_notifier);\\n\\n/*\\n *\\tNotifier list for kernel code which wants to be called\\n *\\tto restart the system.\\n */\\nstatic ATOMIC_NOTIFIER_HEAD(restart_handler_list);\\n\\n/**\\n *\\tregister_restart_handler - Register function to be called to reset\\n *\\t\\t\\t\\t   the system\\n *\\t@nb: Info about handler function to be called\\n *\\t@nb->priority:\\tHandler priority. Handlers should follow the\\n *\\t\\t\\tfollowing guidelines for setting priorities.\\n *\\t\\t\\t0:\\tRestart handler of last resort,\\n *\\t\\t\\t\\twith limited restart capabilities\\n *\\t\\t\\t128:\\tDefault restart handler; use if no other\\n *\\t\\t\\t\\trestart handler is expected to be available,\\n *\\t\\t\\t\\tand/or if restart functionality is\\n *\\t\\t\\t\\tsufficient to restart the entire system\\n *\\t\\t\\t255:\\tHighest priority restart handler, will\\n *\\t\\t\\t\\tpreempt all other restart handlers\\n *\\n *\\tRegisters a function with code to be called to restart the\\n *\\tsystem.\\n *\\n *\\tRegistered functions will be called from machine_restart as last\\n *\\tstep of the restart sequence (if the architecture specific\\n *\\tmachine_restart function calls do_kernel_restart - see below\\n *\\tfor details).\\n *\\tRegistered functions are expected to restart the system immediately.\\n *\\tIf more than one function is registered, the restart handler priority\\n *\\tselects which function will be called first.\\n *\\n *\\tRestart handlers are expected to be registered from non-architecture\\n *\\tcode, typically from drivers. A typical use case would be a system\\n *\\twhere restart functionality is provided through a watchdog. Multiple\\n *\\trestart handlers may exist; for example, one restart handler might\\n *\\trestart the entire system, while another only restarts the CPU.\\n *\\tIn such cases, the restart handler which only restarts part of the\\n *\\thardware is expected to register with low priority to ensure that\\n *\\tit only runs if no other means to restart the system is available.\\n *\\n *\\tCurrently always returns zero, as atomic_notifier_chain_register()\\n *\\talways returns zero.\\n */\\nint register_restart_handler(struct notifier_block *nb)\\n{\\n\\treturn atomic_notifier_chain_register(&restart_handler_list, nb);\\n}\\nEXPORT_SYMBOL(register_restart_handler);\\n\\n/**\\n *\\tunregister_restart_handler - Unregister previously registered\\n *\\t\\t\\t\\t     restart handler\\n *\\t@nb: Hook to be unregistered\\n *\\n *\\tUnregisters a previously registered restart handler function.\\n *\\n *\\tReturns zero on success, or %-ENOENT on failure.\\n */\\nint unregister_restart_handler(struct notifier_block *nb)\\n{\\n\\treturn atomic_notifier_chain_unregister(&restart_handler_list, nb);\\n}\\nEXPORT_SYMBOL(unregister_restart_handler);\\n\\n/**\\n *\\tdo_kernel_restart - Execute kernel restart handler call chain\\n *\\n *\\tCalls functions registered with register_restart_handler.\\n *\\n *\\tExpected to be called from machine_restart as last step of the restart\\n *\\tsequence.\\n *\\n *\\tRestarts the system immediately if a restart handler function has been\\n *\\tregistered. Otherwise does nothing.\\n */\\nvoid do_kernel_restart(char *cmd)\\n{\\n\\tatomic_notifier_call_chain(&restart_handler_list, reboot_mode, cmd);\\n}\\n\\nvoid migrate_to_reboot_cpu(void)\\n{\\n\\t/* The boot cpu is always logical cpu 0 */\\n\\tint cpu = reboot_cpu;\\n\\n\\tcpu_hotplug_disable();\\n\\n\\t/* Make certain the cpu I\\'m about to reboot on is online */\\n\\tif (!cpu_online(cpu))\\n\\t\\tcpu = cpumask_first(cpu_online_mask);\\n\\n\\t/* Prevent races with other tasks migrating this task */\\n\\tcurrent->flags |= PF_NO_SETAFFINITY;\\n\\n\\t/* Make certain I only run on the appropriate processor */\\n\\tset_cpus_allowed_ptr(current, cpumask_of(cpu));\\n}\\n\\n/*\\n *\\tNotifier list for kernel code which wants to be called\\n *\\tto prepare system for restart.\\n */\\nstatic BLOCKING_NOTIFIER_HEAD(restart_prep_handler_list);\\n\\nstatic void do_kernel_restart_prepare(void)\\n{\\n\\tblocking_notifier_call_chain(&restart_prep_handler_list, 0, NULL);\\n}\\n\\n/**\\n *\\tkernel_restart - reboot the system\\n *\\t@cmd: pointer to buffer containing command to execute for restart\\n *\\t\\tor %NULL\\n *\\n *\\tShutdown everything and perform a clean reboot.\\n *\\tThis is not safe to call in interrupt context.\\n */\\nvoid kernel_restart(char *cmd)\\n{\\n\\tkernel_restart_prepare(cmd);\\n\\tdo_kernel_restart_prepare();\\n\\tmigrate_to_reboot_cpu();\\n\\tsyscore_shutdown();\\n\\tif (!cmd)\\n\\t\\tpr_emerg(\"Restarting system\\\\n\");\\n\\telse\\n\\t\\tpr_emerg(\"Restarting system with command \\'%s\\'\\\\n\", cmd);\\n\\tkmsg_dump(KMSG_DUMP_SHUTDOWN);\\n\\tmachine_restart(cmd);\\n}\\nEXPORT_SYMBOL_GPL(kernel_restart);\\n\\nstatic void kernel_shutdown_prepare(enum system_states state)\\n{\\n\\tblocking_notifier_call_chain(&reboot_notifier_list,\\n\\t\\t(state == SYSTEM_HALT) ? SYS_HALT : SYS_POWER_OFF, NULL);\\n\\tsystem_state = state;\\n\\tusermodehelper_disable();\\n\\tdevice_shutdown();\\n}\\n/**\\n *\\tkernel_halt - halt the system\\n *\\n *\\tShutdown everything and perform a clean system halt.\\n */\\nvoid kernel_halt(void)\\n{\\n\\tkernel_shutdown_prepare(SYSTEM_HALT);\\n\\tmigrate_to_reboot_cpu();\\n\\tsyscore_shutdown();\\n\\tif (poweroff_fallback_to_halt)\\n\\t\\tpr_emerg(\"Power off not available: System halted instead\\\\n\");\\n\\telse\\n\\t\\tpr_emerg(\"System halted\\\\n\");\\n\\tkmsg_dump(KMSG_DUMP_SHUTDOWN);\\n\\tmachine_halt();\\n}\\nEXPORT_SYMBOL_GPL(kernel_halt);\\n\\n/*\\n *\\tNotifier list for kernel code which wants to be called\\n *\\tto prepare system for power off.\\n */\\nstatic BLOCKING_NOTIFIER_HEAD(power_off_prep_handler_list);\\n\\n/*\\n *\\tNotifier list for kernel code which wants to be called\\n *\\tto power off system.\\n */\\nstatic ATOMIC_NOTIFIER_HEAD(power_off_handler_list);\\n\\nstatic int sys_off_notify(struct notifier_block *nb,\\n\\t\\t\\t  unsigned long mode, void *cmd)\\n{\\n\\tstruct sys_off_handler *handler;\\n\\tstruct sys_off_data data = {};\\n\\n\\thandler = container_of(nb, struct sys_off_handler, nb);\\n\\tdata.cb_data = handler->cb_data;\\n\\tdata.mode = mode;\\n\\tdata.cmd = cmd;\\n\\tdata.dev = handler->dev;\\n\\n\\treturn handler->sys_off_cb(&data);\\n}\\n\\nstatic struct sys_off_handler platform_sys_off_handler;\\n\\nstatic struct sys_off_handler *alloc_sys_off_handler(int priority)\\n{\\n\\tstruct sys_off_handler *handler;\\n\\tgfp_t flags;\\n\\n\\t/*\\n\\t * Platforms like m68k can\\'t allocate sys_off handler dynamically\\n\\t * at the early boot time because memory allocator isn\\'t available yet.\\n\\t */\\n\\tif (priority == SYS_OFF_PRIO_PLATFORM) {\\n\\t\\thandler = &platform_sys_off_handler;\\n\\t\\tif (handler->cb_data)\\n\\t\\t\\treturn ERR_PTR(-EBUSY);\\n\\t} else {\\n\\t\\tif (system_state > SYSTEM_RUNNING)\\n\\t\\t\\tflags = GFP_ATOMIC;\\n\\t\\telse\\n\\t\\t\\tflags = GFP_KERNEL;\\n\\n\\t\\thandler = kzalloc(sizeof(*handler), flags);\\n\\t\\tif (!handler)\\n\\t\\t\\treturn ERR_PTR(-ENOMEM);\\n\\t}\\n\\n\\treturn handler;\\n}\\n\\nstatic void free_sys_off_handler(struct sys_off_handler *handler)\\n{\\n\\tif (handler == &platform_sys_off_handler)\\n\\t\\tmemset(handler, 0, sizeof(*handler));\\n\\telse\\n\\t\\tkfree(handler);\\n}\\n\\n/**\\n *\\tregister_sys_off_handler - Register sys-off handler\\n *\\t@mode: Sys-off mode\\n *\\t@priority: Handler priority\\n *\\t@callback: Callback function\\n *\\t@cb_data: Callback argument\\n *\\n *\\tRegisters system power-off or restart handler that will be invoked\\n *\\tat the step corresponding to the given sys-off mode. Handler\\'s callback\\n *\\tshould return NOTIFY_DONE to permit execution of the next handler in\\n *\\tthe call chain or NOTIFY_STOP to break the chain (in error case for\\n *\\texample).\\n *\\n *\\tMultiple handlers can be registered at the default priority level.\\n *\\n *\\tOnly one handler can be registered at the non-default priority level,\\n *\\totherwise ERR_PTR(-EBUSY) is returned.\\n *\\n *\\tReturns a new instance of struct sys_off_handler on success, or\\n *\\tan ERR_PTR()-encoded error code otherwise.\\n */\\nstruct sys_off_handler *\\nregister_sys_off_handler(enum sys_off_mode mode,\\n\\t\\t\\t int priority,\\n\\t\\t\\t int (*callback)(struct sys_off_data *data),\\n\\t\\t\\t void *cb_data)\\n{\\n\\tstruct sys_off_handler *handler;\\n\\tint err;\\n\\n\\thandler = alloc_sys_off_handler(priority);\\n\\tif (IS_ERR(handler))\\n\\t\\treturn handler;\\n\\n\\tswitch (mode) {\\n\\tcase SYS_OFF_MODE_POWER_OFF_PREPARE:\\n\\t\\thandler->list = &power_off_prep_handler_list;\\n\\t\\thandler->blocking = true;\\n\\t\\tbreak;\\n\\n\\tcase SYS_OFF_MODE_POWER_OFF:\\n\\t\\thandler->list = &power_off_handler_list;\\n\\t\\tbreak;\\n\\n\\tcase SYS_OFF_MODE_RESTART_PREPARE:\\n\\t\\thandler->list = &restart_prep_handler_list;\\n\\t\\thandler->blocking = true;\\n\\t\\tbreak;\\n\\n\\tcase SYS_OFF_MODE_RESTART:\\n\\t\\thandler->list = &restart_handler_list;\\n\\t\\tbreak;\\n\\n\\tdefault:\\n\\t\\tfree_sys_off_handler(handler);\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\t}\\n\\n\\thandler->nb.notifier_call = sys_off_notify;\\n\\thandler->nb.priority = priority;\\n\\thandler->sys_off_cb = callback;\\n\\thandler->cb_data = cb_data;\\n\\thandler->mode = mode;\\n\\n\\tif (handler->blocking) {\\n\\t\\tif (priority == SYS_OFF_PRIO_DEFAULT)\\n\\t\\t\\terr = blocking_notifier_chain_register(handler->list,\\n\\t\\t\\t\\t\\t\\t\\t       &handler->nb);\\n\\t\\telse\\n\\t\\t\\terr = blocking_notifier_chain_register_unique_prio(handler->list,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   &handler->nb);\\n\\t} else {\\n\\t\\tif (priority == SYS_OFF_PRIO_DEFAULT)\\n\\t\\t\\terr = atomic_notifier_chain_register(handler->list,\\n\\t\\t\\t\\t\\t\\t\\t     &handler->nb);\\n\\t\\telse\\n\\t\\t\\terr = atomic_notifier_chain_register_unique_prio(handler->list,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t &handler->nb);\\n\\t}\\n\\n\\tif (err) {\\n\\t\\tfree_sys_off_handler(handler);\\n\\t\\treturn ERR_PTR(err);\\n\\t}\\n\\n\\treturn handler;\\n}\\nEXPORT_SYMBOL_GPL(register_sys_off_handler);\\n\\n/**\\n *\\tunregister_sys_off_handler - Unregister sys-off handler\\n *\\t@handler: Sys-off handler\\n *\\n *\\tUnregisters given sys-off handler.\\n */\\nvoid unregister_sys_off_handler(struct sys_off_handler *handler)\\n{\\n\\tint err;\\n\\n\\tif (IS_ERR_OR_NULL(handler))\\n\\t\\treturn;\\n\\n\\tif (handler->blocking)\\n\\t\\terr = blocking_notifier_chain_unregister(handler->list,\\n\\t\\t\\t\\t\\t\\t\\t &handler->nb);\\n\\telse\\n\\t\\terr = atomic_notifier_chain_unregister(handler->list,\\n\\t\\t\\t\\t\\t\\t       &handler->nb);\\n\\n\\t/* sanity check, shall never happen */\\n\\tWARN_ON(err);\\n\\n\\tfree_sys_off_handler(handler);\\n}\\nEXPORT_SYMBOL_GPL(unregister_sys_off_handler);\\n\\nstatic void devm_unregister_sys_off_handler(void *data)\\n{\\n\\tstruct sys_off_handler *handler = data;\\n\\n\\tunregister_sys_off_handler(handler);\\n}\\n\\n/**\\n *\\tdevm_register_sys_off_handler - Register sys-off handler\\n *\\t@dev: Device that registers handler\\n *\\t@mode: Sys-off mode\\n *\\t@priority: Handler priority\\n *\\t@callback: Callback function\\n *\\t@cb_data: Callback argument\\n *\\n *\\tRegisters resource-managed sys-off handler.\\n *\\n *\\tReturns zero on success, or error code on failure.\\n */\\nint devm_register_sys_off_handler(struct device *dev,\\n\\t\\t\\t\\t  enum sys_off_mode mode,\\n\\t\\t\\t\\t  int priority,\\n\\t\\t\\t\\t  int (*callback)(struct sys_off_data *data),\\n\\t\\t\\t\\t  void *cb_data)\\n{\\n\\tstruct sys_off_handler *handler;\\n\\n\\thandler = register_sys_off_handler(mode, priority, callback, cb_data);\\n\\tif (IS_ERR(handler))\\n\\t\\treturn PTR_ERR(handler);\\n\\thandler->dev = dev;\\n\\n\\treturn devm_add_action_or_reset(dev, devm_unregister_sys_off_handler,\\n\\t\\t\\t\\t\\thandler);\\n}\\nEXPORT_SYMBOL_GPL(devm_register_sys_off_handler);\\n\\n/**\\n *\\tdevm_register_power_off_handler - Register power-off handler\\n *\\t@dev: Device that registers callback\\n *\\t@callback: Callback function\\n *\\t@cb_data: Callback\\'s argument\\n *\\n *\\tRegisters resource-managed sys-off handler with a default priority\\n *\\tand using power-off mode.\\n *\\n *\\tReturns zero on success, or error code on failure.\\n */\\nint devm_register_power_off_handler(struct device *dev,\\n\\t\\t\\t\\t    int (*callback)(struct sys_off_data *data),\\n\\t\\t\\t\\t    void *cb_data)\\n{\\n\\treturn devm_register_sys_off_handler(dev,\\n\\t\\t\\t\\t\\t     SYS_OFF_MODE_POWER_OFF,\\n\\t\\t\\t\\t\\t     SYS_OFF_PRIO_DEFAULT,\\n\\t\\t\\t\\t\\t     callback, cb_data);\\n}\\nEXPORT_SYMBOL_GPL(devm_register_power_off_handler);\\n\\n/**\\n *\\tdevm_register_restart_handler - Register restart handler\\n *\\t@dev: Device that registers callback\\n *\\t@callback: Callback function\\n *\\t@cb_data: Callback\\'s argument\\n *\\n *\\tRegisters resource-managed sys-off handler with a default priority\\n *\\tand using restart mode.\\n *\\n *\\tReturns zero on success, or error code on failure.\\n */\\nint devm_register_restart_handler(struct device *dev,\\n\\t\\t\\t\\t  int (*callback)(struct sys_off_data *data),\\n\\t\\t\\t\\t  void *cb_data)\\n{\\n\\treturn devm_register_sys_off_handler(dev,\\n\\t\\t\\t\\t\\t     SYS_OFF_MODE_RESTART,\\n\\t\\t\\t\\t\\t     SYS_OFF_PRIO_DEFAULT,\\n\\t\\t\\t\\t\\t     callback, cb_data);\\n}\\nEXPORT_SYMBOL_GPL(devm_register_restart_handler);\\n\\nstatic struct sys_off_handler *platform_power_off_handler;\\n\\nstatic int platform_power_off_notify(struct sys_off_data *data)\\n{\\n\\tvoid (*platform_power_power_off_cb)(void) = data->cb_data;\\n\\n\\tplatform_power_power_off_cb();\\n\\n\\treturn NOTIFY_DONE;\\n}\\n\\n/**\\n *\\tregister_platform_power_off - Register platform-level power-off callback\\n *\\t@power_off: Power-off callback\\n *\\n *\\tRegisters power-off callback that will be called as last step\\n *\\tof the power-off sequence. This callback is expected to be invoked\\n *\\tfor the last resort. Only one platform power-off callback is allowed\\n *\\tto be registered at a time.\\n *\\n *\\tReturns zero on success, or error code on failure.\\n */\\nint register_platform_power_off(void (*power_off)(void))\\n{\\n\\tstruct sys_off_handler *handler;\\n\\n\\thandler = register_sys_off_handler(SYS_OFF_MODE_POWER_OFF,\\n\\t\\t\\t\\t\\t   SYS_OFF_PRIO_PLATFORM,\\n\\t\\t\\t\\t\\t   platform_power_off_notify,\\n\\t\\t\\t\\t\\t   power_off);\\n\\tif (IS_ERR(handler))\\n\\t\\treturn PTR_ERR(handler);\\n\\n\\tplatform_power_off_handler = handler;\\n\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(register_platform_power_off);\\n\\n/**\\n *\\tunregister_platform_power_off - Unregister platform-level power-off callback\\n *\\t@power_off: Power-off callback\\n *\\n *\\tUnregisters previously registered platform power-off callback.\\n */\\nvoid unregister_platform_power_off(void (*power_off)(void))\\n{\\n\\tif (platform_power_off_handler &&\\n\\t    platform_power_off_handler->cb_data == power_off) {\\n\\t\\tunregister_sys_off_handler(platform_power_off_handler);\\n\\t\\tplatform_power_off_handler = NULL;\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(unregister_platform_power_off);\\n\\nstatic int legacy_pm_power_off(struct sys_off_data *data)\\n{\\n\\tif (pm_power_off)\\n\\t\\tpm_power_off();\\n\\n\\treturn NOTIFY_DONE;\\n}\\n\\nstatic void do_kernel_power_off_prepare(void)\\n{\\n\\tblocking_notifier_call_chain(&power_off_prep_handler_list, 0, NULL);\\n}\\n\\n/**\\n *\\tdo_kernel_power_off - Execute kernel power-off handler call chain\\n *\\n *\\tExpected to be called as last step of the power-off sequence.\\n *\\n *\\tPowers off the system immediately if a power-off handler function has\\n *\\tbeen registered. Otherwise does nothing.\\n */\\nvoid do_kernel_power_off(void)\\n{\\n\\tstruct sys_off_handler *sys_off = NULL;\\n\\n\\t/*\\n\\t * Register sys-off handlers for legacy PM callback. This allows\\n\\t * legacy PM callbacks temporary co-exist with the new sys-off API.\\n\\t *\\n\\t * TODO: Remove legacy handlers once all legacy PM users will be\\n\\t *       switched to the sys-off based APIs.\\n\\t */\\n\\tif (pm_power_off)\\n\\t\\tsys_off = register_sys_off_handler(SYS_OFF_MODE_POWER_OFF,\\n\\t\\t\\t\\t\\t\\t   SYS_OFF_PRIO_DEFAULT,\\n\\t\\t\\t\\t\\t\\t   legacy_pm_power_off, NULL);\\n\\n\\tatomic_notifier_call_chain(&power_off_handler_list, 0, NULL);\\n\\n\\tunregister_sys_off_handler(sys_off);\\n}\\n\\n/**\\n *\\tkernel_can_power_off - check whether system can be powered off\\n *\\n *\\tReturns true if power-off handler is registered and system can be\\n *\\tpowered off, false otherwise.\\n */\\nbool kernel_can_power_off(void)\\n{\\n\\treturn !atomic_notifier_call_chain_is_empty(&power_off_handler_list) ||\\n\\t\\tpm_power_off;\\n}\\nEXPORT_SYMBOL_GPL(kernel_can_power_off);\\n\\n/**\\n *\\tkernel_power_off - power_off the system\\n *\\n *\\tShutdown everything and perform a clean system power_off.\\n */\\nvoid kernel_power_off(void)\\n{\\n\\tkernel_shutdown_prepare(SYSTEM_POWER_OFF);\\n\\tdo_kernel_power_off_prepare();\\n\\tmigrate_to_reboot_cpu();\\n\\tsyscore_shutdown();\\n\\tpr_emerg(\"Power down\\\\n\");\\n\\tkmsg_dump(KMSG_DUMP_SHUTDOWN);\\n\\tmachine_power_off();\\n}\\nEXPORT_SYMBOL_GPL(kernel_power_off);\\n\\nDEFINE_MUTEX(system_transition_mutex);\\n\\n/*\\n * Reboot system call: for obvious reasons only root may call it,\\n * and even root needs to set up some magic numbers in the registers\\n * so that some mistake won\\'t make this reboot the whole machine.\\n * You can also set the meaning of the ctrl-alt-del-key here.\\n *\\n * reboot doesn\\'t sync: do that yourself before calling this.\\n */\\nSYSCALL_DEFINE4(reboot, int, magic1, int, magic2, unsigned int, cmd,\\n\\t\\tvoid __user *, arg)\\n{\\n\\tstruct pid_namespace *pid_ns = task_active_pid_ns(current);\\n\\tchar buffer[256];\\n\\tint ret = 0;\\n\\n\\t/* We only trust the superuser with rebooting the system. */\\n\\tif (!ns_capable(pid_ns->user_ns, CAP_SYS_BOOT))\\n\\t\\treturn -EPERM;\\n\\n\\t/* For safety, we require \"magic\" arguments. */\\n\\tif (magic1 != LINUX_REBOOT_MAGIC1 ||\\n\\t\\t\\t(magic2 != LINUX_REBOOT_MAGIC2 &&\\n\\t\\t\\tmagic2 != LINUX_REBOOT_MAGIC2A &&\\n\\t\\t\\tmagic2 != LINUX_REBOOT_MAGIC2B &&\\n\\t\\t\\tmagic2 != LINUX_REBOOT_MAGIC2C))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * If pid namespaces are enabled and the current task is in a child\\n\\t * pid_namespace, the command is handled by reboot_pid_ns() which will\\n\\t * call do_exit().\\n\\t */\\n\\tret = reboot_pid_ns(pid_ns, cmd);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/* Instead of trying to make the power_off code look like\\n\\t * halt when pm_power_off is not set do it the easy way.\\n\\t */\\n\\tif ((cmd == LINUX_REBOOT_CMD_POWER_OFF) && !kernel_can_power_off()) {\\n\\t\\tpoweroff_fallback_to_halt = true;\\n\\t\\tcmd = LINUX_REBOOT_CMD_HALT;\\n\\t}\\n\\n\\tmutex_lock(&system_transition_mutex);\\n\\tswitch (cmd) {\\n\\tcase LINUX_REBOOT_CMD_RESTART:\\n\\t\\tkernel_restart(NULL);\\n\\t\\tbreak;\\n\\n\\tcase LINUX_REBOOT_CMD_CAD_ON:\\n\\t\\tC_A_D = 1;\\n\\t\\tbreak;\\n\\n\\tcase LINUX_REBOOT_CMD_CAD_OFF:\\n\\t\\tC_A_D = 0;\\n\\t\\tbreak;\\n\\n\\tcase LINUX_REBOOT_CMD_HALT:\\n\\t\\tkernel_halt();\\n\\t\\tdo_exit(0);\\n\\n\\tcase LINUX_REBOOT_CMD_POWER_OFF:\\n\\t\\tkernel_power_off();\\n\\t\\tdo_exit(0);\\n\\t\\tbreak;\\n\\n\\tcase LINUX_REBOOT_CMD_RESTART2:\\n\\t\\tret = strncpy_from_user(&buffer[0], arg, sizeof(buffer) - 1);\\n\\t\\tif (ret < 0) {\\n\\t\\t\\tret = -EFAULT;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tbuffer[sizeof(buffer) - 1] = \\'\\\\0\\';\\n\\n\\t\\tkernel_restart(buffer);\\n\\t\\tbreak;\\n\\n#ifdef CONFIG_KEXEC_CORE\\n\\tcase LINUX_REBOOT_CMD_KEXEC:\\n\\t\\tret = kernel_kexec();\\n\\t\\tbreak;\\n#endif\\n\\n#ifdef CONFIG_HIBERNATION\\n\\tcase LINUX_REBOOT_CMD_SW_SUSPEND:\\n\\t\\tret = hibernate();\\n\\t\\tbreak;\\n#endif\\n\\n\\tdefault:\\n\\t\\tret = -EINVAL;\\n\\t\\tbreak;\\n\\t}\\n\\tmutex_unlock(&system_transition_mutex);\\n\\treturn ret;\\n}\\n\\nstatic void deferred_cad(struct work_struct *dummy)\\n{\\n\\tkernel_restart(NULL);\\n}\\n\\n/*\\n * This function gets called by ctrl-alt-del - ie the keyboard interrupt.\\n * As it\\'s called within an interrupt, it may NOT sync: the only choice\\n * is whether to reboot at once, or just ignore the ctrl-alt-del.\\n */\\nvoid ctrl_alt_del(void)\\n{\\n\\tstatic DECLARE_WORK(cad_work, deferred_cad);\\n\\n\\tif (C_A_D)\\n\\t\\tschedule_work(&cad_work);\\n\\telse\\n\\t\\tkill_cad_pid(SIGINT, 1);\\n}\\n\\n#define POWEROFF_CMD_PATH_LEN  256\\nstatic char poweroff_cmd[POWEROFF_CMD_PATH_LEN] = \"/sbin/poweroff\";\\nstatic const char reboot_cmd[] = \"/sbin/reboot\";\\n\\nstatic int run_cmd(const char *cmd)\\n{\\n\\tchar **argv;\\n\\tstatic char *envp[] = {\\n\\t\\t\"HOME=/\",\\n\\t\\t\"PATH=/sbin:/bin:/usr/sbin:/usr/bin\",\\n\\t\\tNULL\\n\\t};\\n\\tint ret;\\n\\targv = argv_split(GFP_KERNEL, cmd, NULL);\\n\\tif (argv) {\\n\\t\\tret = call_usermodehelper(argv[0], argv, envp, UMH_WAIT_EXEC);\\n\\t\\targv_free(argv);\\n\\t} else {\\n\\t\\tret = -ENOMEM;\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic int __orderly_reboot(void)\\n{\\n\\tint ret;\\n\\n\\tret = run_cmd(reboot_cmd);\\n\\n\\tif (ret) {\\n\\t\\tpr_warn(\"Failed to start orderly reboot: forcing the issue\\\\n\");\\n\\t\\temergency_sync();\\n\\t\\tkernel_restart(NULL);\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic int __orderly_poweroff(bool force)\\n{\\n\\tint ret;\\n\\n\\tret = run_cmd(poweroff_cmd);\\n\\n\\tif (ret && force) {\\n\\t\\tpr_warn(\"Failed to start orderly shutdown: forcing the issue\\\\n\");\\n\\n\\t\\t/*\\n\\t\\t * I guess this should try to kick off some daemon to sync and\\n\\t\\t * poweroff asap.  Or not even bother syncing if we\\'re doing an\\n\\t\\t * emergency shutdown?\\n\\t\\t */\\n\\t\\temergency_sync();\\n\\t\\tkernel_power_off();\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic bool poweroff_force;\\n\\nstatic void poweroff_work_func(struct work_struct *work)\\n{\\n\\t__orderly_poweroff(poweroff_force);\\n}\\n\\nstatic DECLARE_WORK(poweroff_work, poweroff_work_func);\\n\\n/**\\n * orderly_poweroff - Trigger an orderly system poweroff\\n * @force: force poweroff if command execution fails\\n *\\n * This may be called from any context to trigger a system shutdown.\\n * If the orderly shutdown fails, it will force an immediate shutdown.\\n */\\nvoid orderly_poweroff(bool force)\\n{\\n\\tif (force) /* do not override the pending \"true\" */\\n\\t\\tpoweroff_force = true;\\n\\tschedule_work(&poweroff_work);\\n}\\nEXPORT_SYMBOL_GPL(orderly_poweroff);\\n\\nstatic void reboot_work_func(struct work_struct *work)\\n{\\n\\t__orderly_reboot();\\n}\\n\\nstatic DECLARE_WORK(reboot_work, reboot_work_func);\\n\\n/**\\n * orderly_reboot - Trigger an orderly system reboot\\n *\\n * This may be called from any context to trigger a system reboot.\\n * If the orderly reboot fails, it will force an immediate reboot.\\n */\\nvoid orderly_reboot(void)\\n{\\n\\tschedule_work(&reboot_work);\\n}\\nEXPORT_SYMBOL_GPL(orderly_reboot);\\n\\n/**\\n * hw_failure_emergency_poweroff_func - emergency poweroff work after a known delay\\n * @work: work_struct associated with the emergency poweroff function\\n *\\n * This function is called in very critical situations to force\\n * a kernel poweroff after a configurable timeout value.\\n */\\nstatic void hw_failure_emergency_poweroff_func(struct work_struct *work)\\n{\\n\\t/*\\n\\t * We have reached here after the emergency shutdown waiting period has\\n\\t * expired. This means orderly_poweroff has not been able to shut off\\n\\t * the system for some reason.\\n\\t *\\n\\t * Try to shut down the system immediately using kernel_power_off\\n\\t * if populated\\n\\t */\\n\\tpr_emerg(\"Hardware protection timed-out. Trying forced poweroff\\\\n\");\\n\\tkernel_power_off();\\n\\n\\t/*\\n\\t * Worst of the worst case trigger emergency restart\\n\\t */\\n\\tpr_emerg(\"Hardware protection shutdown failed. Trying emergency restart\\\\n\");\\n\\temergency_restart();\\n}\\n\\nstatic DECLARE_DELAYED_WORK(hw_failure_emergency_poweroff_work,\\n\\t\\t\\t    hw_failure_emergency_poweroff_func);\\n\\n/**\\n * hw_failure_emergency_poweroff - Trigger an emergency system poweroff\\n *\\n * This may be called from any critical situation to trigger a system shutdown\\n * after a given period of time. If time is negative this is not scheduled.\\n */\\nstatic void hw_failure_emergency_poweroff(int poweroff_delay_ms)\\n{\\n\\tif (poweroff_delay_ms <= 0)\\n\\t\\treturn;\\n\\tschedule_delayed_work(&hw_failure_emergency_poweroff_work,\\n\\t\\t\\t      msecs_to_jiffies(poweroff_delay_ms));\\n}\\n\\n/**\\n * __hw_protection_shutdown - Trigger an emergency system shutdown or reboot\\n *\\n * @reason:\\t\\tReason of emergency shutdown or reboot to be printed.\\n * @ms_until_forced:\\tTime to wait for orderly shutdown or reboot before\\n *\\t\\t\\ttriggering it. Negative value disables the forced\\n *\\t\\t\\tshutdown or reboot.\\n * @shutdown:\\t\\tIf true, indicates that a shutdown will happen\\n *\\t\\t\\tafter the critical tempeature is reached.\\n *\\t\\t\\tIf false, indicates that a reboot will happen\\n *\\t\\t\\tafter the critical tempeature is reached.\\n *\\n * Initiate an emergency system shutdown or reboot in order to protect\\n * hardware from further damage. Usage examples include a thermal protection.\\n * NOTE: The request is ignored if protection shutdown or reboot is already\\n * pending even if the previous request has given a large timeout for forced\\n * shutdown/reboot.\\n */\\nvoid __hw_protection_shutdown(const char *reason, int ms_until_forced, bool shutdown)\\n{\\n\\tstatic atomic_t allow_proceed = ATOMIC_INIT(1);\\n\\n\\tpr_emerg(\"HARDWARE PROTECTION shutdown (%s)\\\\n\", reason);\\n\\n\\t/* Shutdown should be initiated only once. */\\n\\tif (!atomic_dec_and_test(&allow_proceed))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Queue a backup emergency shutdown in the event of\\n\\t * orderly_poweroff failure\\n\\t */\\n\\thw_failure_emergency_poweroff(ms_until_forced);\\n\\tif (shutdown)\\n\\t\\torderly_poweroff(true);\\n\\telse\\n\\t\\torderly_reboot();\\n}\\nEXPORT_SYMBOL_GPL(__hw_protection_shutdown);\\n\\nstatic int __init reboot_setup(char *str)\\n{\\n\\tfor (;;) {\\n\\t\\tenum reboot_mode *mode;\\n\\n\\t\\t/*\\n\\t\\t * Having anything passed on the command line via\\n\\t\\t * reboot= will cause us to disable DMI checking\\n\\t\\t * below.\\n\\t\\t */\\n\\t\\treboot_default = 0;\\n\\n\\t\\tif (!strncmp(str, \"panic_\", 6)) {\\n\\t\\t\\tmode = &panic_reboot_mode;\\n\\t\\t\\tstr += 6;\\n\\t\\t} else {\\n\\t\\t\\tmode = &reboot_mode;\\n\\t\\t}\\n\\n\\t\\tswitch (*str) {\\n\\t\\tcase \\'w\\':\\n\\t\\t\\t*mode = REBOOT_WARM;\\n\\t\\t\\tbreak;\\n\\n\\t\\tcase \\'c\\':\\n\\t\\t\\t*mode = REBOOT_COLD;\\n\\t\\t\\tbreak;\\n\\n\\t\\tcase \\'h\\':\\n\\t\\t\\t*mode = REBOOT_HARD;\\n\\t\\t\\tbreak;\\n\\n\\t\\tcase \\'s\\':\\n\\t\\t\\t/*\\n\\t\\t\\t * reboot_cpu is s[mp]#### with #### being the processor\\n\\t\\t\\t * to be used for rebooting. Skip \\'s\\' or \\'smp\\' prefix.\\n\\t\\t\\t */\\n\\t\\t\\tstr += str[1] == \\'m\\' && str[2] == \\'p\\' ? 3 : 1;\\n\\n\\t\\t\\tif (isdigit(str[0])) {\\n\\t\\t\\t\\tint cpu = simple_strtoul(str, NULL, 0);\\n\\n\\t\\t\\t\\tif (cpu >= num_possible_cpus()) {\\n\\t\\t\\t\\t\\tpr_err(\"Ignoring the CPU number in reboot= option. \"\\n\\t\\t\\t\\t\\t\"CPU %d exceeds possible cpu number %d\\\\n\",\\n\\t\\t\\t\\t\\tcpu, num_possible_cpus());\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\treboot_cpu = cpu;\\n\\t\\t\\t} else\\n\\t\\t\\t\\t*mode = REBOOT_SOFT;\\n\\t\\t\\tbreak;\\n\\n\\t\\tcase \\'g\\':\\n\\t\\t\\t*mode = REBOOT_GPIO;\\n\\t\\t\\tbreak;\\n\\n\\t\\tcase \\'b\\':\\n\\t\\tcase \\'a\\':\\n\\t\\tcase \\'k\\':\\n\\t\\tcase \\'t\\':\\n\\t\\tcase \\'e\\':\\n\\t\\tcase \\'p\\':\\n\\t\\t\\treboot_type = *str;\\n\\t\\t\\tbreak;\\n\\n\\t\\tcase \\'f\\':\\n\\t\\t\\treboot_force = 1;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tstr = strchr(str, \\',\\');\\n\\t\\tif (str)\\n\\t\\t\\tstr++;\\n\\t\\telse\\n\\t\\t\\tbreak;\\n\\t}\\n\\treturn 1;\\n}\\n__setup(\"reboot=\", reboot_setup);\\n\\n#ifdef CONFIG_SYSFS\\n\\n#define REBOOT_COLD_STR\\t\\t\"cold\"\\n#define REBOOT_WARM_STR\\t\\t\"warm\"\\n#define REBOOT_HARD_STR\\t\\t\"hard\"\\n#define REBOOT_SOFT_STR\\t\\t\"soft\"\\n#define REBOOT_GPIO_STR\\t\\t\"gpio\"\\n#define REBOOT_UNDEFINED_STR\\t\"undefined\"\\n\\n#define BOOT_TRIPLE_STR\\t\\t\"triple\"\\n#define BOOT_KBD_STR\\t\\t\"kbd\"\\n#define BOOT_BIOS_STR\\t\\t\"bios\"\\n#define BOOT_ACPI_STR\\t\\t\"acpi\"\\n#define BOOT_EFI_STR\\t\\t\"efi\"\\n#define BOOT_PCI_STR\\t\\t\"pci\"\\n\\nstatic ssize_t mode_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)\\n{\\n\\tconst char *val;\\n\\n\\tswitch (reboot_mode) {\\n\\tcase REBOOT_COLD:\\n\\t\\tval = REBOOT_COLD_STR;\\n\\t\\tbreak;\\n\\tcase REBOOT_WARM:\\n\\t\\tval = REBOOT_WARM_STR;\\n\\t\\tbreak;\\n\\tcase REBOOT_HARD:\\n\\t\\tval = REBOOT_HARD_STR;\\n\\t\\tbreak;\\n\\tcase REBOOT_SOFT:\\n\\t\\tval = REBOOT_SOFT_STR;\\n\\t\\tbreak;\\n\\tcase REBOOT_GPIO:\\n\\t\\tval = REBOOT_GPIO_STR;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tval = REBOOT_UNDEFINED_STR;\\n\\t}\\n\\n\\treturn sysfs_emit(buf, \"%s\\\\n\", val);\\n}\\nstatic ssize_t mode_store(struct kobject *kobj, struct kobj_attribute *attr,\\n\\t\\t\\t  const char *buf, size_t count)\\n{\\n\\tif (!capable(CAP_SYS_BOOT))\\n\\t\\treturn -EPERM;\\n\\n\\tif (!strncmp(buf, REBOOT_COLD_STR, strlen(REBOOT_COLD_STR)))\\n\\t\\treboot_mode = REBOOT_COLD;\\n\\telse if (!strncmp(buf, REBOOT_WARM_STR, strlen(REBOOT_WARM_STR)))\\n\\t\\treboot_mode = REBOOT_WARM;\\n\\telse if (!strncmp(buf, REBOOT_HARD_STR, strlen(REBOOT_HARD_STR)))\\n\\t\\treboot_mode = REBOOT_HARD;\\n\\telse if (!strncmp(buf, REBOOT_SOFT_STR, strlen(REBOOT_SOFT_STR)))\\n\\t\\treboot_mode = REBOOT_SOFT;\\n\\telse if (!strncmp(buf, REBOOT_GPIO_STR, strlen(REBOOT_GPIO_STR)))\\n\\t\\treboot_mode = REBOOT_GPIO;\\n\\telse\\n\\t\\treturn -EINVAL;\\n\\n\\treboot_default = 0;\\n\\n\\treturn count;\\n}\\nstatic struct kobj_attribute reboot_mode_attr = __ATTR_RW(mode);\\n\\n#ifdef CONFIG_X86\\nstatic ssize_t force_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", reboot_force);\\n}\\nstatic ssize_t force_store(struct kobject *kobj, struct kobj_attribute *attr,\\n\\t\\t\\t  const char *buf, size_t count)\\n{\\n\\tbool res;\\n\\n\\tif (!capable(CAP_SYS_BOOT))\\n\\t\\treturn -EPERM;\\n\\n\\tif (kstrtobool(buf, &res))\\n\\t\\treturn -EINVAL;\\n\\n\\treboot_default = 0;\\n\\treboot_force = res;\\n\\n\\treturn count;\\n}\\nstatic struct kobj_attribute reboot_force_attr = __ATTR_RW(force);\\n\\nstatic ssize_t type_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)\\n{\\n\\tconst char *val;\\n\\n\\tswitch (reboot_type) {\\n\\tcase BOOT_TRIPLE:\\n\\t\\tval = BOOT_TRIPLE_STR;\\n\\t\\tbreak;\\n\\tcase BOOT_KBD:\\n\\t\\tval = BOOT_KBD_STR;\\n\\t\\tbreak;\\n\\tcase BOOT_BIOS:\\n\\t\\tval = BOOT_BIOS_STR;\\n\\t\\tbreak;\\n\\tcase BOOT_ACPI:\\n\\t\\tval = BOOT_ACPI_STR;\\n\\t\\tbreak;\\n\\tcase BOOT_EFI:\\n\\t\\tval = BOOT_EFI_STR;\\n\\t\\tbreak;\\n\\tcase BOOT_CF9_FORCE:\\n\\t\\tval = BOOT_PCI_STR;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tval = REBOOT_UNDEFINED_STR;\\n\\t}\\n\\n\\treturn sysfs_emit(buf, \"%s\\\\n\", val);\\n}\\nstatic ssize_t type_store(struct kobject *kobj, struct kobj_attribute *attr,\\n\\t\\t\\t  const char *buf, size_t count)\\n{\\n\\tif (!capable(CAP_SYS_BOOT))\\n\\t\\treturn -EPERM;\\n\\n\\tif (!strncmp(buf, BOOT_TRIPLE_STR, strlen(BOOT_TRIPLE_STR)))\\n\\t\\treboot_type = BOOT_TRIPLE;\\n\\telse if (!strncmp(buf, BOOT_KBD_STR, strlen(BOOT_KBD_STR)))\\n\\t\\treboot_type = BOOT_KBD;\\n\\telse if (!strncmp(buf, BOOT_BIOS_STR, strlen(BOOT_BIOS_STR)))\\n\\t\\treboot_type = BOOT_BIOS;\\n\\telse if (!strncmp(buf, BOOT_ACPI_STR, strlen(BOOT_ACPI_STR)))\\n\\t\\treboot_type = BOOT_ACPI;\\n\\telse if (!strncmp(buf, BOOT_EFI_STR, strlen(BOOT_EFI_STR)))\\n\\t\\treboot_type = BOOT_EFI;\\n\\telse if (!strncmp(buf, BOOT_PCI_STR, strlen(BOOT_PCI_STR)))\\n\\t\\treboot_type = BOOT_CF9_FORCE;\\n\\telse\\n\\t\\treturn -EINVAL;\\n\\n\\treboot_default = 0;\\n\\n\\treturn count;\\n}\\nstatic struct kobj_attribute reboot_type_attr = __ATTR_RW(type);\\n#endif\\n\\n#ifdef CONFIG_SMP\\nstatic ssize_t cpu_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)\\n{\\n\\treturn sysfs_emit(buf, \"%d\\\\n\", reboot_cpu);\\n}\\nstatic ssize_t cpu_store(struct kobject *kobj, struct kobj_attribute *attr,\\n\\t\\t\\t  const char *buf, size_t count)\\n{\\n\\tunsigned int cpunum;\\n\\tint rc;\\n\\n\\tif (!capable(CAP_SYS_BOOT))\\n\\t\\treturn -EPERM;\\n\\n\\trc = kstrtouint(buf, 0, &cpunum);\\n\\n\\tif (rc)\\n\\t\\treturn rc;\\n\\n\\tif (cpunum >= num_possible_cpus())\\n\\t\\treturn -ERANGE;\\n\\n\\treboot_default = 0;\\n\\treboot_cpu = cpunum;\\n\\n\\treturn count;\\n}\\nstatic struct kobj_attribute reboot_cpu_attr = __ATTR_RW(cpu);\\n#endif\\n\\nstatic struct attribute *reboot_attrs[] = {\\n\\t&reboot_mode_attr.attr,\\n#ifdef CONFIG_X86\\n\\t&reboot_force_attr.attr,\\n\\t&reboot_type_attr.attr,\\n#endif\\n#ifdef CONFIG_SMP\\n\\t&reboot_cpu_attr.attr,\\n#endif\\n\\tNULL,\\n};\\n\\n#ifdef CONFIG_SYSCTL\\nstatic struct ctl_table kern_reboot_table[] = {\\n\\t{\\n\\t\\t.procname       = \"poweroff_cmd\",\\n\\t\\t.data           = &poweroff_cmd,\\n\\t\\t.maxlen         = POWEROFF_CMD_PATH_LEN,\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = proc_dostring,\\n\\t},\\n\\t{\\n\\t\\t.procname       = \"ctrl-alt-del\",\\n\\t\\t.data           = &C_A_D,\\n\\t\\t.maxlen         = sizeof(int),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = proc_dointvec,\\n\\t},\\n};\\n\\nstatic void __init kernel_reboot_sysctls_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", kern_reboot_table);\\n}\\n#else\\n#define kernel_reboot_sysctls_init() do { } while (0)\\n#endif /* CONFIG_SYSCTL */\\n\\nstatic const struct attribute_group reboot_attr_group = {\\n\\t.attrs = reboot_attrs,\\n};\\n\\nstatic int __init reboot_ksysfs_init(void)\\n{\\n\\tstruct kobject *reboot_kobj;\\n\\tint ret;\\n\\n\\treboot_kobj = kobject_create_and_add(\"reboot\", kernel_kobj);\\n\\tif (!reboot_kobj)\\n\\t\\treturn -ENOMEM;\\n\\n\\tret = sysfs_create_group(reboot_kobj, &reboot_attr_group);\\n\\tif (ret) {\\n\\t\\tkobject_put(reboot_kobj);\\n\\t\\treturn ret;\\n\\t}\\n\\n\\tkernel_reboot_sysctls_init();\\n\\n\\treturn 0;\\n}\\nlate_initcall(reboot_ksysfs_init);\\n\\n#endif\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n\\n#include <linux/stat.h>\\n#include <linux/sysctl.h>\\n#include <linux/slab.h>\\n#include <linux/cred.h>\\n#include <linux/hash.h>\\n#include <linux/kmemleak.h>\\n#include <linux/user_namespace.h>\\n\\nstruct ucounts init_ucounts = {\\n\\t.ns    = &init_user_ns,\\n\\t.uid   = GLOBAL_ROOT_UID,\\n\\t.count = ATOMIC_INIT(1),\\n};\\n\\n#define UCOUNTS_HASHTABLE_BITS 10\\nstatic struct hlist_head ucounts_hashtable[(1 << UCOUNTS_HASHTABLE_BITS)];\\nstatic DEFINE_SPINLOCK(ucounts_lock);\\n\\n#define ucounts_hashfn(ns, uid)\\t\\t\\t\\t\\t\\t\\\\\\n\\thash_long((unsigned long)__kuid_val(uid) + (unsigned long)(ns), \\\\\\n\\t\\t  UCOUNTS_HASHTABLE_BITS)\\n#define ucounts_hashentry(ns, uid)\\t\\\\\\n\\t(ucounts_hashtable + ucounts_hashfn(ns, uid))\\n\\n\\n#ifdef CONFIG_SYSCTL\\nstatic struct ctl_table_set *\\nset_lookup(struct ctl_table_root *root)\\n{\\n\\treturn &current_user_ns()->set;\\n}\\n\\nstatic int set_is_seen(struct ctl_table_set *set)\\n{\\n\\treturn &current_user_ns()->set == set;\\n}\\n\\nstatic int set_permissions(struct ctl_table_header *head,\\n\\t\\t\\t   const struct ctl_table *table)\\n{\\n\\tstruct user_namespace *user_ns =\\n\\t\\tcontainer_of(head->set, struct user_namespace, set);\\n\\tint mode;\\n\\n\\t/* Allow users with CAP_SYS_RESOURCE unrestrained access */\\n\\tif (ns_capable(user_ns, CAP_SYS_RESOURCE))\\n\\t\\tmode = (table->mode & S_IRWXU) >> 6;\\n\\telse\\n\\t/* Allow all others at most read-only access */\\n\\t\\tmode = table->mode & S_IROTH;\\n\\treturn (mode << 6) | (mode << 3) | mode;\\n}\\n\\nstatic struct ctl_table_root set_root = {\\n\\t.lookup = set_lookup,\\n\\t.permissions = set_permissions,\\n};\\n\\nstatic long ue_zero = 0;\\nstatic long ue_int_max = INT_MAX;\\n\\n#define UCOUNT_ENTRY(name)\\t\\t\\t\\t\\t\\\\\\n\\t{\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\t\\t.procname\\t= name,\\t\\t\\t\\t\\\\\\n\\t\\t.maxlen\\t\\t= sizeof(long),\\t\\t\\t\\\\\\n\\t\\t.mode\\t\\t= 0644,\\t\\t\\t\\t\\\\\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\t\\\\\\n\\t\\t.extra1\\t\\t= &ue_zero,\\t\\t\\t\\\\\\n\\t\\t.extra2\\t\\t= &ue_int_max,\\t\\t\\t\\\\\\n\\t}\\nstatic const struct ctl_table user_table[] = {\\n\\tUCOUNT_ENTRY(\"max_user_namespaces\"),\\n\\tUCOUNT_ENTRY(\"max_pid_namespaces\"),\\n\\tUCOUNT_ENTRY(\"max_uts_namespaces\"),\\n\\tUCOUNT_ENTRY(\"max_ipc_namespaces\"),\\n\\tUCOUNT_ENTRY(\"max_net_namespaces\"),\\n\\tUCOUNT_ENTRY(\"max_mnt_namespaces\"),\\n\\tUCOUNT_ENTRY(\"max_cgroup_namespaces\"),\\n\\tUCOUNT_ENTRY(\"max_time_namespaces\"),\\n#ifdef CONFIG_INOTIFY_USER\\n\\tUCOUNT_ENTRY(\"max_inotify_instances\"),\\n\\tUCOUNT_ENTRY(\"max_inotify_watches\"),\\n#endif\\n#ifdef CONFIG_FANOTIFY\\n\\tUCOUNT_ENTRY(\"max_fanotify_groups\"),\\n\\tUCOUNT_ENTRY(\"max_fanotify_marks\"),\\n#endif\\n};\\n#endif /* CONFIG_SYSCTL */\\n\\nbool setup_userns_sysctls(struct user_namespace *ns)\\n{\\n#ifdef CONFIG_SYSCTL\\n\\tstruct ctl_table *tbl;\\n\\n\\tBUILD_BUG_ON(ARRAY_SIZE(user_table) != UCOUNT_COUNTS);\\n\\tsetup_sysctl_set(&ns->set, &set_root, set_is_seen);\\n\\ttbl = kmemdup(user_table, sizeof(user_table), GFP_KERNEL);\\n\\tif (tbl) {\\n\\t\\tint i;\\n\\t\\tfor (i = 0; i < UCOUNT_COUNTS; i++) {\\n\\t\\t\\ttbl[i].data = &ns->ucount_max[i];\\n\\t\\t}\\n\\t\\tns->sysctls = __register_sysctl_table(&ns->set, \"user\", tbl,\\n\\t\\t\\t\\t\\t\\t      ARRAY_SIZE(user_table));\\n\\t}\\n\\tif (!ns->sysctls) {\\n\\t\\tkfree(tbl);\\n\\t\\tretire_sysctl_set(&ns->set);\\n\\t\\treturn false;\\n\\t}\\n#endif\\n\\treturn true;\\n}\\n\\nvoid retire_userns_sysctls(struct user_namespace *ns)\\n{\\n#ifdef CONFIG_SYSCTL\\n\\tconst struct ctl_table *tbl;\\n\\n\\ttbl = ns->sysctls->ctl_table_arg;\\n\\tunregister_sysctl_table(ns->sysctls);\\n\\tretire_sysctl_set(&ns->set);\\n\\tkfree(tbl);\\n#endif\\n}\\n\\nstatic struct ucounts *find_ucounts(struct user_namespace *ns, kuid_t uid, struct hlist_head *hashent)\\n{\\n\\tstruct ucounts *ucounts;\\n\\n\\thlist_for_each_entry(ucounts, hashent, node) {\\n\\t\\tif (uid_eq(ucounts->uid, uid) && (ucounts->ns == ns))\\n\\t\\t\\treturn ucounts;\\n\\t}\\n\\treturn NULL;\\n}\\n\\nstatic void hlist_add_ucounts(struct ucounts *ucounts)\\n{\\n\\tstruct hlist_head *hashent = ucounts_hashentry(ucounts->ns, ucounts->uid);\\n\\tspin_lock_irq(&ucounts_lock);\\n\\thlist_add_head(&ucounts->node, hashent);\\n\\tspin_unlock_irq(&ucounts_lock);\\n}\\n\\nstatic inline bool get_ucounts_or_wrap(struct ucounts *ucounts)\\n{\\n\\t/* Returns true on a successful get, false if the count wraps. */\\n\\treturn !atomic_add_negative(1, &ucounts->count);\\n}\\n\\nstruct ucounts *get_ucounts(struct ucounts *ucounts)\\n{\\n\\tif (!get_ucounts_or_wrap(ucounts)) {\\n\\t\\tput_ucounts(ucounts);\\n\\t\\tucounts = NULL;\\n\\t}\\n\\treturn ucounts;\\n}\\n\\nstruct ucounts *alloc_ucounts(struct user_namespace *ns, kuid_t uid)\\n{\\n\\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\\n\\tstruct ucounts *ucounts, *new;\\n\\tbool wrapped;\\n\\n\\tspin_lock_irq(&ucounts_lock);\\n\\tucounts = find_ucounts(ns, uid, hashent);\\n\\tif (!ucounts) {\\n\\t\\tspin_unlock_irq(&ucounts_lock);\\n\\n\\t\\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\\n\\t\\tif (!new)\\n\\t\\t\\treturn NULL;\\n\\n\\t\\tnew->ns = ns;\\n\\t\\tnew->uid = uid;\\n\\t\\tatomic_set(&new->count, 1);\\n\\n\\t\\tspin_lock_irq(&ucounts_lock);\\n\\t\\tucounts = find_ucounts(ns, uid, hashent);\\n\\t\\tif (ucounts) {\\n\\t\\t\\tkfree(new);\\n\\t\\t} else {\\n\\t\\t\\thlist_add_head(&new->node, hashent);\\n\\t\\t\\tget_user_ns(new->ns);\\n\\t\\t\\tspin_unlock_irq(&ucounts_lock);\\n\\t\\t\\treturn new;\\n\\t\\t}\\n\\t}\\n\\twrapped = !get_ucounts_or_wrap(ucounts);\\n\\tspin_unlock_irq(&ucounts_lock);\\n\\tif (wrapped) {\\n\\t\\tput_ucounts(ucounts);\\n\\t\\treturn NULL;\\n\\t}\\n\\treturn ucounts;\\n}\\n\\nvoid put_ucounts(struct ucounts *ucounts)\\n{\\n\\tunsigned long flags;\\n\\n\\tif (atomic_dec_and_lock_irqsave(&ucounts->count, &ucounts_lock, flags)) {\\n\\t\\thlist_del_init(&ucounts->node);\\n\\t\\tspin_unlock_irqrestore(&ucounts_lock, flags);\\n\\t\\tput_user_ns(ucounts->ns);\\n\\t\\tkfree(ucounts);\\n\\t}\\n}\\n\\nstatic inline bool atomic_long_inc_below(atomic_long_t *v, int u)\\n{\\n\\tlong c, old;\\n\\tc = atomic_long_read(v);\\n\\tfor (;;) {\\n\\t\\tif (unlikely(c >= u))\\n\\t\\t\\treturn false;\\n\\t\\told = atomic_long_cmpxchg(v, c, c+1);\\n\\t\\tif (likely(old == c))\\n\\t\\t\\treturn true;\\n\\t\\tc = old;\\n\\t}\\n}\\n\\nstruct ucounts *inc_ucount(struct user_namespace *ns, kuid_t uid,\\n\\t\\t\\t   enum ucount_type type)\\n{\\n\\tstruct ucounts *ucounts, *iter, *bad;\\n\\tstruct user_namespace *tns;\\n\\tucounts = alloc_ucounts(ns, uid);\\n\\tfor (iter = ucounts; iter; iter = tns->ucounts) {\\n\\t\\tlong max;\\n\\t\\ttns = iter->ns;\\n\\t\\tmax = READ_ONCE(tns->ucount_max[type]);\\n\\t\\tif (!atomic_long_inc_below(&iter->ucount[type], max))\\n\\t\\t\\tgoto fail;\\n\\t}\\n\\treturn ucounts;\\nfail:\\n\\tbad = iter;\\n\\tfor (iter = ucounts; iter != bad; iter = iter->ns->ucounts)\\n\\t\\tatomic_long_dec(&iter->ucount[type]);\\n\\n\\tput_ucounts(ucounts);\\n\\treturn NULL;\\n}\\n\\nvoid dec_ucount(struct ucounts *ucounts, enum ucount_type type)\\n{\\n\\tstruct ucounts *iter;\\n\\tfor (iter = ucounts; iter; iter = iter->ns->ucounts) {\\n\\t\\tlong dec = atomic_long_dec_if_positive(&iter->ucount[type]);\\n\\t\\tWARN_ON_ONCE(dec < 0);\\n\\t}\\n\\tput_ucounts(ucounts);\\n}\\n\\nlong inc_rlimit_ucounts(struct ucounts *ucounts, enum rlimit_type type, long v)\\n{\\n\\tstruct ucounts *iter;\\n\\tlong max = LONG_MAX;\\n\\tlong ret = 0;\\n\\n\\tfor (iter = ucounts; iter; iter = iter->ns->ucounts) {\\n\\t\\tlong new = atomic_long_add_return(v, &iter->rlimit[type]);\\n\\t\\tif (new < 0 || new > max)\\n\\t\\t\\tret = LONG_MAX;\\n\\t\\telse if (iter == ucounts)\\n\\t\\t\\tret = new;\\n\\t\\tmax = get_userns_rlimit_max(iter->ns, type);\\n\\t}\\n\\treturn ret;\\n}\\n\\nbool dec_rlimit_ucounts(struct ucounts *ucounts, enum rlimit_type type, long v)\\n{\\n\\tstruct ucounts *iter;\\n\\tlong new = -1; /* Silence compiler warning */\\n\\tfor (iter = ucounts; iter; iter = iter->ns->ucounts) {\\n\\t\\tlong dec = atomic_long_sub_return(v, &iter->rlimit[type]);\\n\\t\\tWARN_ON_ONCE(dec < 0);\\n\\t\\tif (iter == ucounts)\\n\\t\\t\\tnew = dec;\\n\\t}\\n\\treturn (new == 0);\\n}\\n\\nstatic void do_dec_rlimit_put_ucounts(struct ucounts *ucounts,\\n\\t\\t\\t\\tstruct ucounts *last, enum rlimit_type type)\\n{\\n\\tstruct ucounts *iter, *next;\\n\\tfor (iter = ucounts; iter != last; iter = next) {\\n\\t\\tlong dec = atomic_long_sub_return(1, &iter->rlimit[type]);\\n\\t\\tWARN_ON_ONCE(dec < 0);\\n\\t\\tnext = iter->ns->ucounts;\\n\\t\\tif (dec == 0)\\n\\t\\t\\tput_ucounts(iter);\\n\\t}\\n}\\n\\nvoid dec_rlimit_put_ucounts(struct ucounts *ucounts, enum rlimit_type type)\\n{\\n\\tdo_dec_rlimit_put_ucounts(ucounts, NULL, type);\\n}\\n\\nlong inc_rlimit_get_ucounts(struct ucounts *ucounts, enum rlimit_type type,\\n\\t\\t\\t    bool override_rlimit)\\n{\\n\\t/* Caller must hold a reference to ucounts */\\n\\tstruct ucounts *iter;\\n\\tlong max = LONG_MAX;\\n\\tlong dec, ret = 0;\\n\\n\\tfor (iter = ucounts; iter; iter = iter->ns->ucounts) {\\n\\t\\tlong new = atomic_long_add_return(1, &iter->rlimit[type]);\\n\\t\\tif (new < 0 || new > max)\\n\\t\\t\\tgoto dec_unwind;\\n\\t\\tif (iter == ucounts)\\n\\t\\t\\tret = new;\\n\\t\\tif (!override_rlimit)\\n\\t\\t\\tmax = get_userns_rlimit_max(iter->ns, type);\\n\\t\\t/*\\n\\t\\t * Grab an extra ucount reference for the caller when\\n\\t\\t * the rlimit count was previously 0.\\n\\t\\t */\\n\\t\\tif (new != 1)\\n\\t\\t\\tcontinue;\\n\\t\\tif (!get_ucounts(iter))\\n\\t\\t\\tgoto dec_unwind;\\n\\t}\\n\\treturn ret;\\ndec_unwind:\\n\\tdec = atomic_long_sub_return(1, &iter->rlimit[type]);\\n\\tWARN_ON_ONCE(dec < 0);\\n\\tdo_dec_rlimit_put_ucounts(ucounts, iter, type);\\n\\treturn 0;\\n}\\n\\nbool is_rlimit_overlimit(struct ucounts *ucounts, enum rlimit_type type, unsigned long rlimit)\\n{\\n\\tstruct ucounts *iter;\\n\\tlong max = rlimit;\\n\\tif (rlimit > LONG_MAX)\\n\\t\\tmax = LONG_MAX;\\n\\tfor (iter = ucounts; iter; iter = iter->ns->ucounts) {\\n\\t\\tlong val = get_rlimit_value(iter, type);\\n\\t\\tif (val < 0 || val > max)\\n\\t\\t\\treturn true;\\n\\t\\tmax = get_userns_rlimit_max(iter->ns, type);\\n\\t}\\n\\treturn false;\\n}\\n\\nstatic __init int user_namespace_sysctl_init(void)\\n{\\n#ifdef CONFIG_SYSCTL\\n\\tstatic struct ctl_table_header *user_header;\\n\\tstatic struct ctl_table empty[1];\\n\\t/*\\n\\t * It is necessary to register the user directory in the\\n\\t * default set so that registrations in the child sets work\\n\\t * properly.\\n\\t */\\n\\tuser_header = register_sysctl_sz(\"user\", empty, 0);\\n\\tkmemleak_ignore(user_header);\\n\\tBUG_ON(!user_header);\\n\\tBUG_ON(!setup_userns_sysctls(&init_user_ns));\\n#endif\\n\\thlist_add_ucounts(&init_ucounts);\\n\\tinc_rlimit_ucounts(&init_ucounts, UCOUNT_RLIMIT_NPROC, 1);\\n\\treturn 0;\\n}\\nsubsys_initcall(user_namespace_sysctl_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Generic pidhash and scalable, time-bounded PID allocator\\n *\\n * (C) 2002-2003 Nadia Yvette Chambers, IBM\\n * (C) 2004 Nadia Yvette Chambers, Oracle\\n * (C) 2002-2004 Ingo Molnar, Red Hat\\n *\\n * pid-structures are backing objects for tasks sharing a given ID to chain\\n * against. There is very little to them aside from hashing them and\\n * parking tasks using given ID\\'s on a list.\\n *\\n * The hash is always changed with the tasklist_lock write-acquired,\\n * and the hash is only accessed with the tasklist_lock at least\\n * read-acquired, so there\\'s no additional SMP locking needed here.\\n *\\n * We have a list of bitmap pages, which bitmaps represent the PID space.\\n * Allocating and freeing PIDs is completely lockless. The worst-case\\n * allocation scenario when all but one out of 1 million PIDs possible are\\n * allocated already: the scanning of 32 list entries and at most PAGE_SIZE\\n * bytes. The typical fastpath is a single successful setbit. Freeing is O(1).\\n *\\n * Pid namespaces:\\n *    (C) 2007 Pavel Emelyanov <xemul@openvz.org>, OpenVZ, SWsoft Inc.\\n *    (C) 2007 Sukadev Bhattiprolu <sukadev@us.ibm.com>, IBM\\n *     Many thanks to Oleg Nesterov for comments and help\\n *\\n */\\n\\n#include <linux/mm.h>\\n#include <linux/export.h>\\n#include <linux/slab.h>\\n#include <linux/init.h>\\n#include <linux/rculist.h>\\n#include <linux/memblock.h>\\n#include <linux/pid_namespace.h>\\n#include <linux/init_task.h>\\n#include <linux/syscalls.h>\\n#include <linux/proc_ns.h>\\n#include <linux/refcount.h>\\n#include <linux/anon_inodes.h>\\n#include <linux/sched/signal.h>\\n#include <linux/sched/task.h>\\n#include <linux/idr.h>\\n#include <linux/pidfs.h>\\n#include <net/sock.h>\\n#include <uapi/linux/pidfd.h>\\n\\nstruct pid init_struct_pid = {\\n\\t.count\\t\\t= REFCOUNT_INIT(1),\\n\\t.tasks\\t\\t= {\\n\\t\\t{ .first = NULL },\\n\\t\\t{ .first = NULL },\\n\\t\\t{ .first = NULL },\\n\\t},\\n\\t.level\\t\\t= 0,\\n\\t.numbers\\t= { {\\n\\t\\t.nr\\t\\t= 0,\\n\\t\\t.ns\\t\\t= &init_pid_ns,\\n\\t}, }\\n};\\n\\nint pid_max = PID_MAX_DEFAULT;\\n\\nint pid_max_min = RESERVED_PIDS + 1;\\nint pid_max_max = PID_MAX_LIMIT;\\n/*\\n * Pseudo filesystems start inode numbering after one. We use Reserved\\n * PIDs as a natural offset.\\n */\\nstatic u64 pidfs_ino = RESERVED_PIDS;\\n\\n/*\\n * PID-map pages start out as NULL, they get allocated upon\\n * first use and are never deallocated. This way a low pid_max\\n * value does not cause lots of bitmaps to be allocated, but\\n * the scheme scales to up to 4 million PIDs, runtime.\\n */\\nstruct pid_namespace init_pid_ns = {\\n\\t.ns.count = REFCOUNT_INIT(2),\\n\\t.idr = IDR_INIT(init_pid_ns.idr),\\n\\t.pid_allocated = PIDNS_ADDING,\\n\\t.level = 0,\\n\\t.child_reaper = &init_task,\\n\\t.user_ns = &init_user_ns,\\n\\t.ns.inum = PROC_PID_INIT_INO,\\n#ifdef CONFIG_PID_NS\\n\\t.ns.ops = &pidns_operations,\\n#endif\\n#if defined(CONFIG_SYSCTL) && defined(CONFIG_MEMFD_CREATE)\\n\\t.memfd_noexec_scope = MEMFD_NOEXEC_SCOPE_EXEC,\\n#endif\\n};\\nEXPORT_SYMBOL_GPL(init_pid_ns);\\n\\n/*\\n * Note: disable interrupts while the pidmap_lock is held as an\\n * interrupt might come in and do read_lock(&tasklist_lock).\\n *\\n * If we don\\'t disable interrupts there is a nasty deadlock between\\n * detach_pid()->free_pid() and another cpu that does\\n * spin_lock(&pidmap_lock) followed by an interrupt routine that does\\n * read_lock(&tasklist_lock);\\n *\\n * After we clean up the tasklist_lock and know there are no\\n * irq handlers that take it we can leave the interrupts enabled.\\n * For now it is easier to be safe than to prove it can\\'t happen.\\n */\\n\\nstatic  __cacheline_aligned_in_smp DEFINE_SPINLOCK(pidmap_lock);\\n\\nvoid put_pid(struct pid *pid)\\n{\\n\\tstruct pid_namespace *ns;\\n\\n\\tif (!pid)\\n\\t\\treturn;\\n\\n\\tns = pid->numbers[pid->level].ns;\\n\\tif (refcount_dec_and_test(&pid->count)) {\\n\\t\\tkmem_cache_free(ns->pid_cachep, pid);\\n\\t\\tput_pid_ns(ns);\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(put_pid);\\n\\nstatic void delayed_put_pid(struct rcu_head *rhp)\\n{\\n\\tstruct pid *pid = container_of(rhp, struct pid, rcu);\\n\\tput_pid(pid);\\n}\\n\\nvoid free_pid(struct pid *pid)\\n{\\n\\t/* We can be called with write_lock_irq(&tasklist_lock) held */\\n\\tint i;\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&pidmap_lock, flags);\\n\\tfor (i = 0; i <= pid->level; i++) {\\n\\t\\tstruct upid *upid = pid->numbers + i;\\n\\t\\tstruct pid_namespace *ns = upid->ns;\\n\\t\\tswitch (--ns->pid_allocated) {\\n\\t\\tcase 2:\\n\\t\\tcase 1:\\n\\t\\t\\t/* When all that is left in the pid namespace\\n\\t\\t\\t * is the reaper wake up the reaper.  The reaper\\n\\t\\t\\t * may be sleeping in zap_pid_ns_processes().\\n\\t\\t\\t */\\n\\t\\t\\twake_up_process(ns->child_reaper);\\n\\t\\t\\tbreak;\\n\\t\\tcase PIDNS_ADDING:\\n\\t\\t\\t/* Handle a fork failure of the first process */\\n\\t\\t\\tWARN_ON(ns->child_reaper);\\n\\t\\t\\tns->pid_allocated = 0;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tidr_remove(&ns->idr, upid->nr);\\n\\t}\\n\\tspin_unlock_irqrestore(&pidmap_lock, flags);\\n\\n\\tcall_rcu(&pid->rcu, delayed_put_pid);\\n}\\n\\nstruct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid,\\n\\t\\t      size_t set_tid_size)\\n{\\n\\tstruct pid *pid;\\n\\tenum pid_type type;\\n\\tint i, nr;\\n\\tstruct pid_namespace *tmp;\\n\\tstruct upid *upid;\\n\\tint retval = -ENOMEM;\\n\\n\\t/*\\n\\t * set_tid_size contains the size of the set_tid array. Starting at\\n\\t * the most nested currently active PID namespace it tells alloc_pid()\\n\\t * which PID to set for a process in that most nested PID namespace\\n\\t * up to set_tid_size PID namespaces. It does not have to set the PID\\n\\t * for a process in all nested PID namespaces but set_tid_size must\\n\\t * never be greater than the current ns->level + 1.\\n\\t */\\n\\tif (set_tid_size > ns->level + 1)\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\tpid = kmem_cache_alloc(ns->pid_cachep, GFP_KERNEL);\\n\\tif (!pid)\\n\\t\\treturn ERR_PTR(retval);\\n\\n\\ttmp = ns;\\n\\tpid->level = ns->level;\\n\\n\\tfor (i = ns->level; i >= 0; i--) {\\n\\t\\tint tid = 0;\\n\\n\\t\\tif (set_tid_size) {\\n\\t\\t\\ttid = set_tid[ns->level - i];\\n\\n\\t\\t\\tretval = -EINVAL;\\n\\t\\t\\tif (tid < 1 || tid >= pid_max)\\n\\t\\t\\t\\tgoto out_free;\\n\\t\\t\\t/*\\n\\t\\t\\t * Also fail if a PID != 1 is requested and\\n\\t\\t\\t * no PID 1 exists.\\n\\t\\t\\t */\\n\\t\\t\\tif (tid != 1 && !tmp->child_reaper)\\n\\t\\t\\t\\tgoto out_free;\\n\\t\\t\\tretval = -EPERM;\\n\\t\\t\\tif (!checkpoint_restore_ns_capable(tmp->user_ns))\\n\\t\\t\\t\\tgoto out_free;\\n\\t\\t\\tset_tid_size--;\\n\\t\\t}\\n\\n\\t\\tidr_preload(GFP_KERNEL);\\n\\t\\tspin_lock_irq(&pidmap_lock);\\n\\n\\t\\tif (tid) {\\n\\t\\t\\tnr = idr_alloc(&tmp->idr, NULL, tid,\\n\\t\\t\\t\\t       tid + 1, GFP_ATOMIC);\\n\\t\\t\\t/*\\n\\t\\t\\t * If ENOSPC is returned it means that the PID is\\n\\t\\t\\t * alreay in use. Return EEXIST in that case.\\n\\t\\t\\t */\\n\\t\\t\\tif (nr == -ENOSPC)\\n\\t\\t\\t\\tnr = -EEXIST;\\n\\t\\t} else {\\n\\t\\t\\tint pid_min = 1;\\n\\t\\t\\t/*\\n\\t\\t\\t * init really needs pid 1, but after reaching the\\n\\t\\t\\t * maximum wrap back to RESERVED_PIDS\\n\\t\\t\\t */\\n\\t\\t\\tif (idr_get_cursor(&tmp->idr) > RESERVED_PIDS)\\n\\t\\t\\t\\tpid_min = RESERVED_PIDS;\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * Store a null pointer so find_pid_ns does not find\\n\\t\\t\\t * a partially initialized PID (see below).\\n\\t\\t\\t */\\n\\t\\t\\tnr = idr_alloc_cyclic(&tmp->idr, NULL, pid_min,\\n\\t\\t\\t\\t\\t      pid_max, GFP_ATOMIC);\\n\\t\\t}\\n\\t\\tspin_unlock_irq(&pidmap_lock);\\n\\t\\tidr_preload_end();\\n\\n\\t\\tif (nr < 0) {\\n\\t\\t\\tretval = (nr == -ENOSPC) ? -EAGAIN : nr;\\n\\t\\t\\tgoto out_free;\\n\\t\\t}\\n\\n\\t\\tpid->numbers[i].nr = nr;\\n\\t\\tpid->numbers[i].ns = tmp;\\n\\t\\ttmp = tmp->parent;\\n\\t}\\n\\n\\t/*\\n\\t * ENOMEM is not the most obvious choice especially for the case\\n\\t * where the child subreaper has already exited and the pid\\n\\t * namespace denies the creation of any new processes. But ENOMEM\\n\\t * is what we have exposed to userspace for a long time and it is\\n\\t * documented behavior for pid namespaces. So we can\\'t easily\\n\\t * change it even if there were an error code better suited.\\n\\t */\\n\\tretval = -ENOMEM;\\n\\n\\tget_pid_ns(ns);\\n\\trefcount_set(&pid->count, 1);\\n\\tspin_lock_init(&pid->lock);\\n\\tfor (type = 0; type < PIDTYPE_MAX; ++type)\\n\\t\\tINIT_HLIST_HEAD(&pid->tasks[type]);\\n\\n\\tinit_waitqueue_head(&pid->wait_pidfd);\\n\\tINIT_HLIST_HEAD(&pid->inodes);\\n\\n\\tupid = pid->numbers + ns->level;\\n\\tspin_lock_irq(&pidmap_lock);\\n\\tif (!(ns->pid_allocated & PIDNS_ADDING))\\n\\t\\tgoto out_unlock;\\n\\tpid->stashed = NULL;\\n\\tpid->ino = ++pidfs_ino;\\n\\tfor ( ; upid >= pid->numbers; --upid) {\\n\\t\\t/* Make the PID visible to find_pid_ns. */\\n\\t\\tidr_replace(&upid->ns->idr, pid, upid->nr);\\n\\t\\tupid->ns->pid_allocated++;\\n\\t}\\n\\tspin_unlock_irq(&pidmap_lock);\\n\\n\\treturn pid;\\n\\nout_unlock:\\n\\tspin_unlock_irq(&pidmap_lock);\\n\\tput_pid_ns(ns);\\n\\nout_free:\\n\\tspin_lock_irq(&pidmap_lock);\\n\\twhile (++i <= ns->level) {\\n\\t\\tupid = pid->numbers + i;\\n\\t\\tidr_remove(&upid->ns->idr, upid->nr);\\n\\t}\\n\\n\\t/* On failure to allocate the first pid, reset the state */\\n\\tif (ns->pid_allocated == PIDNS_ADDING)\\n\\t\\tidr_set_cursor(&ns->idr, 0);\\n\\n\\tspin_unlock_irq(&pidmap_lock);\\n\\n\\tkmem_cache_free(ns->pid_cachep, pid);\\n\\treturn ERR_PTR(retval);\\n}\\n\\nvoid disable_pid_allocation(struct pid_namespace *ns)\\n{\\n\\tspin_lock_irq(&pidmap_lock);\\n\\tns->pid_allocated &= ~PIDNS_ADDING;\\n\\tspin_unlock_irq(&pidmap_lock);\\n}\\n\\nstruct pid *find_pid_ns(int nr, struct pid_namespace *ns)\\n{\\n\\treturn idr_find(&ns->idr, nr);\\n}\\nEXPORT_SYMBOL_GPL(find_pid_ns);\\n\\nstruct pid *find_vpid(int nr)\\n{\\n\\treturn find_pid_ns(nr, task_active_pid_ns(current));\\n}\\nEXPORT_SYMBOL_GPL(find_vpid);\\n\\nstatic struct pid **task_pid_ptr(struct task_struct *task, enum pid_type type)\\n{\\n\\treturn (type == PIDTYPE_PID) ?\\n\\t\\t&task->thread_pid :\\n\\t\\t&task->signal->pids[type];\\n}\\n\\n/*\\n * attach_pid() must be called with the tasklist_lock write-held.\\n */\\nvoid attach_pid(struct task_struct *task, enum pid_type type)\\n{\\n\\tstruct pid *pid = *task_pid_ptr(task, type);\\n\\thlist_add_head_rcu(&task->pid_links[type], &pid->tasks[type]);\\n}\\n\\nstatic void __change_pid(struct task_struct *task, enum pid_type type,\\n\\t\\t\\tstruct pid *new)\\n{\\n\\tstruct pid **pid_ptr = task_pid_ptr(task, type);\\n\\tstruct pid *pid;\\n\\tint tmp;\\n\\n\\tpid = *pid_ptr;\\n\\n\\thlist_del_rcu(&task->pid_links[type]);\\n\\t*pid_ptr = new;\\n\\n\\tif (type == PIDTYPE_PID) {\\n\\t\\tWARN_ON_ONCE(pid_has_task(pid, PIDTYPE_PID));\\n\\t\\twake_up_all(&pid->wait_pidfd);\\n\\t}\\n\\n\\tfor (tmp = PIDTYPE_MAX; --tmp >= 0; )\\n\\t\\tif (pid_has_task(pid, tmp))\\n\\t\\t\\treturn;\\n\\n\\tfree_pid(pid);\\n}\\n\\nvoid detach_pid(struct task_struct *task, enum pid_type type)\\n{\\n\\t__change_pid(task, type, NULL);\\n}\\n\\nvoid change_pid(struct task_struct *task, enum pid_type type,\\n\\t\\tstruct pid *pid)\\n{\\n\\t__change_pid(task, type, pid);\\n\\tattach_pid(task, type);\\n}\\n\\nvoid exchange_tids(struct task_struct *left, struct task_struct *right)\\n{\\n\\tstruct pid *pid1 = left->thread_pid;\\n\\tstruct pid *pid2 = right->thread_pid;\\n\\tstruct hlist_head *head1 = &pid1->tasks[PIDTYPE_PID];\\n\\tstruct hlist_head *head2 = &pid2->tasks[PIDTYPE_PID];\\n\\n\\t/* Swap the single entry tid lists */\\n\\thlists_swap_heads_rcu(head1, head2);\\n\\n\\t/* Swap the per task_struct pid */\\n\\trcu_assign_pointer(left->thread_pid, pid2);\\n\\trcu_assign_pointer(right->thread_pid, pid1);\\n\\n\\t/* Swap the cached value */\\n\\tWRITE_ONCE(left->pid, pid_nr(pid2));\\n\\tWRITE_ONCE(right->pid, pid_nr(pid1));\\n}\\n\\n/* transfer_pid is an optimization of attach_pid(new), detach_pid(old) */\\nvoid transfer_pid(struct task_struct *old, struct task_struct *new,\\n\\t\\t\\t   enum pid_type type)\\n{\\n\\tWARN_ON_ONCE(type == PIDTYPE_PID);\\n\\thlist_replace_rcu(&old->pid_links[type], &new->pid_links[type]);\\n}\\n\\nstruct task_struct *pid_task(struct pid *pid, enum pid_type type)\\n{\\n\\tstruct task_struct *result = NULL;\\n\\tif (pid) {\\n\\t\\tstruct hlist_node *first;\\n\\t\\tfirst = rcu_dereference_check(hlist_first_rcu(&pid->tasks[type]),\\n\\t\\t\\t\\t\\t      lockdep_tasklist_lock_is_held());\\n\\t\\tif (first)\\n\\t\\t\\tresult = hlist_entry(first, struct task_struct, pid_links[(type)]);\\n\\t}\\n\\treturn result;\\n}\\nEXPORT_SYMBOL(pid_task);\\n\\n/*\\n * Must be called under rcu_read_lock().\\n */\\nstruct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns)\\n{\\n\\tRCU_LOCKDEP_WARN(!rcu_read_lock_held(),\\n\\t\\t\\t \"find_task_by_pid_ns() needs rcu_read_lock() protection\");\\n\\treturn pid_task(find_pid_ns(nr, ns), PIDTYPE_PID);\\n}\\n\\nstruct task_struct *find_task_by_vpid(pid_t vnr)\\n{\\n\\treturn find_task_by_pid_ns(vnr, task_active_pid_ns(current));\\n}\\n\\nstruct task_struct *find_get_task_by_vpid(pid_t nr)\\n{\\n\\tstruct task_struct *task;\\n\\n\\trcu_read_lock();\\n\\ttask = find_task_by_vpid(nr);\\n\\tif (task)\\n\\t\\tget_task_struct(task);\\n\\trcu_read_unlock();\\n\\n\\treturn task;\\n}\\n\\nstruct pid *get_task_pid(struct task_struct *task, enum pid_type type)\\n{\\n\\tstruct pid *pid;\\n\\trcu_read_lock();\\n\\tpid = get_pid(rcu_dereference(*task_pid_ptr(task, type)));\\n\\trcu_read_unlock();\\n\\treturn pid;\\n}\\nEXPORT_SYMBOL_GPL(get_task_pid);\\n\\nstruct task_struct *get_pid_task(struct pid *pid, enum pid_type type)\\n{\\n\\tstruct task_struct *result;\\n\\trcu_read_lock();\\n\\tresult = pid_task(pid, type);\\n\\tif (result)\\n\\t\\tget_task_struct(result);\\n\\trcu_read_unlock();\\n\\treturn result;\\n}\\nEXPORT_SYMBOL_GPL(get_pid_task);\\n\\nstruct pid *find_get_pid(pid_t nr)\\n{\\n\\tstruct pid *pid;\\n\\n\\trcu_read_lock();\\n\\tpid = get_pid(find_vpid(nr));\\n\\trcu_read_unlock();\\n\\n\\treturn pid;\\n}\\nEXPORT_SYMBOL_GPL(find_get_pid);\\n\\npid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns)\\n{\\n\\tstruct upid *upid;\\n\\tpid_t nr = 0;\\n\\n\\tif (pid && ns->level <= pid->level) {\\n\\t\\tupid = &pid->numbers[ns->level];\\n\\t\\tif (upid->ns == ns)\\n\\t\\t\\tnr = upid->nr;\\n\\t}\\n\\treturn nr;\\n}\\nEXPORT_SYMBOL_GPL(pid_nr_ns);\\n\\npid_t pid_vnr(struct pid *pid)\\n{\\n\\treturn pid_nr_ns(pid, task_active_pid_ns(current));\\n}\\nEXPORT_SYMBOL_GPL(pid_vnr);\\n\\npid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,\\n\\t\\t\\tstruct pid_namespace *ns)\\n{\\n\\tpid_t nr = 0;\\n\\n\\trcu_read_lock();\\n\\tif (!ns)\\n\\t\\tns = task_active_pid_ns(current);\\n\\tnr = pid_nr_ns(rcu_dereference(*task_pid_ptr(task, type)), ns);\\n\\trcu_read_unlock();\\n\\n\\treturn nr;\\n}\\nEXPORT_SYMBOL(__task_pid_nr_ns);\\n\\nstruct pid_namespace *task_active_pid_ns(struct task_struct *tsk)\\n{\\n\\treturn ns_of_pid(task_pid(tsk));\\n}\\nEXPORT_SYMBOL_GPL(task_active_pid_ns);\\n\\n/*\\n * Used by proc to find the first pid that is greater than or equal to nr.\\n *\\n * If there is a pid at nr this function is exactly the same as find_pid_ns.\\n */\\nstruct pid *find_ge_pid(int nr, struct pid_namespace *ns)\\n{\\n\\treturn idr_get_next(&ns->idr, &nr);\\n}\\nEXPORT_SYMBOL_GPL(find_ge_pid);\\n\\nstruct pid *pidfd_get_pid(unsigned int fd, unsigned int *flags)\\n{\\n\\tCLASS(fd, f)(fd);\\n\\tstruct pid *pid;\\n\\n\\tif (fd_empty(f))\\n\\t\\treturn ERR_PTR(-EBADF);\\n\\n\\tpid = pidfd_pid(fd_file(f));\\n\\tif (!IS_ERR(pid)) {\\n\\t\\tget_pid(pid);\\n\\t\\t*flags = fd_file(f)->f_flags;\\n\\t}\\n\\treturn pid;\\n}\\n\\n/**\\n * pidfd_get_task() - Get the task associated with a pidfd\\n *\\n * @pidfd: pidfd for which to get the task\\n * @flags: flags associated with this pidfd\\n *\\n * Return the task associated with @pidfd. The function takes a reference on\\n * the returned task. The caller is responsible for releasing that reference.\\n *\\n * Return: On success, the task_struct associated with the pidfd.\\n *\\t   On error, a negative errno number will be returned.\\n */\\nstruct task_struct *pidfd_get_task(int pidfd, unsigned int *flags)\\n{\\n\\tunsigned int f_flags;\\n\\tstruct pid *pid;\\n\\tstruct task_struct *task;\\n\\n\\tpid = pidfd_get_pid(pidfd, &f_flags);\\n\\tif (IS_ERR(pid))\\n\\t\\treturn ERR_CAST(pid);\\n\\n\\ttask = get_pid_task(pid, PIDTYPE_TGID);\\n\\tput_pid(pid);\\n\\tif (!task)\\n\\t\\treturn ERR_PTR(-ESRCH);\\n\\n\\t*flags = f_flags;\\n\\treturn task;\\n}\\n\\n/**\\n * pidfd_create() - Create a new pid file descriptor.\\n *\\n * @pid:   struct pid that the pidfd will reference\\n * @flags: flags to pass\\n *\\n * This creates a new pid file descriptor with the O_CLOEXEC flag set.\\n *\\n * Note, that this function can only be called after the fd table has\\n * been unshared to avoid leaking the pidfd to the new process.\\n *\\n * This symbol should not be explicitly exported to loadable modules.\\n *\\n * Return: On success, a cloexec pidfd is returned.\\n *         On error, a negative errno number will be returned.\\n */\\nstatic int pidfd_create(struct pid *pid, unsigned int flags)\\n{\\n\\tint pidfd;\\n\\tstruct file *pidfd_file;\\n\\n\\tpidfd = pidfd_prepare(pid, flags, &pidfd_file);\\n\\tif (pidfd < 0)\\n\\t\\treturn pidfd;\\n\\n\\tfd_install(pidfd, pidfd_file);\\n\\treturn pidfd;\\n}\\n\\n/**\\n * sys_pidfd_open() - Open new pid file descriptor.\\n *\\n * @pid:   pid for which to retrieve a pidfd\\n * @flags: flags to pass\\n *\\n * This creates a new pid file descriptor with the O_CLOEXEC flag set for\\n * the task identified by @pid. Without PIDFD_THREAD flag the target task\\n * must be a thread-group leader.\\n *\\n * Return: On success, a cloexec pidfd is returned.\\n *         On error, a negative errno number will be returned.\\n */\\nSYSCALL_DEFINE2(pidfd_open, pid_t, pid, unsigned int, flags)\\n{\\n\\tint fd;\\n\\tstruct pid *p;\\n\\n\\tif (flags & ~(PIDFD_NONBLOCK | PIDFD_THREAD))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (pid <= 0)\\n\\t\\treturn -EINVAL;\\n\\n\\tp = find_get_pid(pid);\\n\\tif (!p)\\n\\t\\treturn -ESRCH;\\n\\n\\tfd = pidfd_create(p, flags);\\n\\n\\tput_pid(p);\\n\\treturn fd;\\n}\\n\\nvoid __init pid_idr_init(void)\\n{\\n\\t/* Verify no one has done anything silly: */\\n\\tBUILD_BUG_ON(PID_MAX_LIMIT >= PIDNS_ADDING);\\n\\n\\t/* bump default and minimum pid_max based on number of cpus */\\n\\tpid_max = min(pid_max_max, max_t(int, pid_max,\\n\\t\\t\\t\\tPIDS_PER_CPU_DEFAULT * num_possible_cpus()));\\n\\tpid_max_min = max_t(int, pid_max_min,\\n\\t\\t\\t\\tPIDS_PER_CPU_MIN * num_possible_cpus());\\n\\tpr_info(\"pid_max: default: %u minimum: %u\\\\n\", pid_max, pid_max_min);\\n\\n\\tidr_init(&init_pid_ns.idr);\\n\\n\\tinit_pid_ns.pid_cachep = kmem_cache_create(\"pid\",\\n\\t\\t\\tstruct_size_t(struct pid, numbers, 1),\\n\\t\\t\\t__alignof__(struct pid),\\n\\t\\t\\tSLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT,\\n\\t\\t\\tNULL);\\n}\\n\\nstatic struct file *__pidfd_fget(struct task_struct *task, int fd)\\n{\\n\\tstruct file *file;\\n\\tint ret;\\n\\n\\tret = down_read_killable(&task->signal->exec_update_lock);\\n\\tif (ret)\\n\\t\\treturn ERR_PTR(ret);\\n\\n\\tif (ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS))\\n\\t\\tfile = fget_task(task, fd);\\n\\telse\\n\\t\\tfile = ERR_PTR(-EPERM);\\n\\n\\tup_read(&task->signal->exec_update_lock);\\n\\n\\tif (!file) {\\n\\t\\t/*\\n\\t\\t * It is possible that the target thread is exiting; it can be\\n\\t\\t * either:\\n\\t\\t * 1. before exit_signals(), which gives a real fd\\n\\t\\t * 2. before exit_files() takes the task_lock() gives a real fd\\n\\t\\t * 3. after exit_files() releases task_lock(), ->files is NULL;\\n\\t\\t *    this has PF_EXITING, since it was set in exit_signals(),\\n\\t\\t *    __pidfd_fget() returns EBADF.\\n\\t\\t * In case 3 we get EBADF, but that really means ESRCH, since\\n\\t\\t * the task is currently exiting and has freed its files\\n\\t\\t * struct, so we fix it up.\\n\\t\\t */\\n\\t\\tif (task->flags & PF_EXITING)\\n\\t\\t\\tfile = ERR_PTR(-ESRCH);\\n\\t\\telse\\n\\t\\t\\tfile = ERR_PTR(-EBADF);\\n\\t}\\n\\n\\treturn file;\\n}\\n\\nstatic int pidfd_getfd(struct pid *pid, int fd)\\n{\\n\\tstruct task_struct *task;\\n\\tstruct file *file;\\n\\tint ret;\\n\\n\\ttask = get_pid_task(pid, PIDTYPE_PID);\\n\\tif (!task)\\n\\t\\treturn -ESRCH;\\n\\n\\tfile = __pidfd_fget(task, fd);\\n\\tput_task_struct(task);\\n\\tif (IS_ERR(file))\\n\\t\\treturn PTR_ERR(file);\\n\\n\\tret = receive_fd(file, NULL, O_CLOEXEC);\\n\\tfput(file);\\n\\n\\treturn ret;\\n}\\n\\n/**\\n * sys_pidfd_getfd() - Get a file descriptor from another process\\n *\\n * @pidfd:\\tthe pidfd file descriptor of the process\\n * @fd:\\t\\tthe file descriptor number to get\\n * @flags:\\tflags on how to get the fd (reserved)\\n *\\n * This syscall gets a copy of a file descriptor from another process\\n * based on the pidfd, and file descriptor number. It requires that\\n * the calling process has the ability to ptrace the process represented\\n * by the pidfd. The process which is having its file descriptor copied\\n * is otherwise unaffected.\\n *\\n * Return: On success, a cloexec file descriptor is returned.\\n *         On error, a negative errno number will be returned.\\n */\\nSYSCALL_DEFINE3(pidfd_getfd, int, pidfd, int, fd,\\n\\t\\tunsigned int, flags)\\n{\\n\\tstruct pid *pid;\\n\\n\\t/* flags is currently unused - make sure it\\'s unset */\\n\\tif (flags)\\n\\t\\treturn -EINVAL;\\n\\n\\tCLASS(fd, f)(pidfd);\\n\\tif (fd_empty(f))\\n\\t\\treturn -EBADF;\\n\\n\\tpid = pidfd_pid(fd_file(f));\\n\\tif (IS_ERR(pid))\\n\\t\\treturn PTR_ERR(pid);\\n\\n\\treturn pidfd_getfd(pid, fd);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Common SMP CPU bringup/teardown functions\\n */\\n#include <linux/cpu.h>\\n#include <linux/err.h>\\n#include <linux/smp.h>\\n#include <linux/delay.h>\\n#include <linux/init.h>\\n#include <linux/list.h>\\n#include <linux/slab.h>\\n#include <linux/sched.h>\\n#include <linux/sched/task.h>\\n#include <linux/export.h>\\n#include <linux/percpu.h>\\n#include <linux/kthread.h>\\n#include <linux/smpboot.h>\\n\\n#include \"smpboot.h\"\\n\\n#ifdef CONFIG_SMP\\n\\n#ifdef CONFIG_GENERIC_SMP_IDLE_THREAD\\n/*\\n * For the hotplug case we keep the task structs around and reuse\\n * them.\\n */\\nstatic DEFINE_PER_CPU(struct task_struct *, idle_threads);\\n\\nstruct task_struct *idle_thread_get(unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = per_cpu(idle_threads, cpu);\\n\\n\\tif (!tsk)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\treturn tsk;\\n}\\n\\nvoid __init idle_thread_set_boot_cpu(void)\\n{\\n\\tper_cpu(idle_threads, smp_processor_id()) = current;\\n}\\n\\n/**\\n * idle_init - Initialize the idle thread for a cpu\\n * @cpu:\\tThe cpu for which the idle thread should be initialized\\n *\\n * Creates the thread if it does not exist.\\n */\\nstatic __always_inline void idle_init(unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = per_cpu(idle_threads, cpu);\\n\\n\\tif (!tsk) {\\n\\t\\ttsk = fork_idle(cpu);\\n\\t\\tif (IS_ERR(tsk))\\n\\t\\t\\tpr_err(\"SMP: fork_idle() failed for CPU %u\\\\n\", cpu);\\n\\t\\telse\\n\\t\\t\\tper_cpu(idle_threads, cpu) = tsk;\\n\\t}\\n}\\n\\n/**\\n * idle_threads_init - Initialize idle threads for all cpus\\n */\\nvoid __init idle_threads_init(void)\\n{\\n\\tunsigned int cpu, boot_cpu;\\n\\n\\tboot_cpu = smp_processor_id();\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tif (cpu != boot_cpu)\\n\\t\\t\\tidle_init(cpu);\\n\\t}\\n}\\n#endif\\n\\n#endif /* #ifdef CONFIG_SMP */\\n\\nstatic LIST_HEAD(hotplug_threads);\\nstatic DEFINE_MUTEX(smpboot_threads_lock);\\n\\nstruct smpboot_thread_data {\\n\\tunsigned int\\t\\t\\tcpu;\\n\\tunsigned int\\t\\t\\tstatus;\\n\\tstruct smp_hotplug_thread\\t*ht;\\n};\\n\\nenum {\\n\\tHP_THREAD_NONE = 0,\\n\\tHP_THREAD_ACTIVE,\\n\\tHP_THREAD_PARKED,\\n};\\n\\n/**\\n * smpboot_thread_fn - percpu hotplug thread loop function\\n * @data:\\tthread data pointer\\n *\\n * Checks for thread stop and park conditions. Calls the necessary\\n * setup, cleanup, park and unpark functions for the registered\\n * thread.\\n *\\n * Returns 1 when the thread should exit, 0 otherwise.\\n */\\nstatic int smpboot_thread_fn(void *data)\\n{\\n\\tstruct smpboot_thread_data *td = data;\\n\\tstruct smp_hotplug_thread *ht = td->ht;\\n\\n\\twhile (1) {\\n\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\t\\tpreempt_disable();\\n\\t\\tif (kthread_should_stop()) {\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\t/* cleanup must mirror setup */\\n\\t\\t\\tif (ht->cleanup && td->status != HP_THREAD_NONE)\\n\\t\\t\\t\\tht->cleanup(td->cpu, cpu_online(td->cpu));\\n\\t\\t\\tkfree(td);\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\n\\t\\tif (kthread_should_park()) {\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tif (ht->park && td->status == HP_THREAD_ACTIVE) {\\n\\t\\t\\t\\tBUG_ON(td->cpu != smp_processor_id());\\n\\t\\t\\t\\tht->park(td->cpu);\\n\\t\\t\\t\\ttd->status = HP_THREAD_PARKED;\\n\\t\\t\\t}\\n\\t\\t\\tkthread_parkme();\\n\\t\\t\\t/* We might have been woken for stop */\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tBUG_ON(td->cpu != smp_processor_id());\\n\\n\\t\\t/* Check for state change setup */\\n\\t\\tswitch (td->status) {\\n\\t\\tcase HP_THREAD_NONE:\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tif (ht->setup)\\n\\t\\t\\t\\tht->setup(td->cpu);\\n\\t\\t\\ttd->status = HP_THREAD_ACTIVE;\\n\\t\\t\\tcontinue;\\n\\n\\t\\tcase HP_THREAD_PARKED:\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tif (ht->unpark)\\n\\t\\t\\t\\tht->unpark(td->cpu);\\n\\t\\t\\ttd->status = HP_THREAD_ACTIVE;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tif (!ht->thread_should_run(td->cpu)) {\\n\\t\\t\\tpreempt_enable_no_resched();\\n\\t\\t\\tschedule();\\n\\t\\t} else {\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tht->thread_fn(td->cpu);\\n\\t\\t}\\n\\t}\\n}\\n\\nstatic int\\n__smpboot_create_thread(struct smp_hotplug_thread *ht, unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);\\n\\tstruct smpboot_thread_data *td;\\n\\n\\tif (tsk)\\n\\t\\treturn 0;\\n\\n\\ttd = kzalloc_node(sizeof(*td), GFP_KERNEL, cpu_to_node(cpu));\\n\\tif (!td)\\n\\t\\treturn -ENOMEM;\\n\\ttd->cpu = cpu;\\n\\ttd->ht = ht;\\n\\n\\ttsk = kthread_create_on_cpu(smpboot_thread_fn, td, cpu,\\n\\t\\t\\t\\t    ht->thread_comm);\\n\\tif (IS_ERR(tsk)) {\\n\\t\\tkfree(td);\\n\\t\\treturn PTR_ERR(tsk);\\n\\t}\\n\\tkthread_set_per_cpu(tsk, cpu);\\n\\t/*\\n\\t * Park the thread so that it could start right on the CPU\\n\\t * when it is available.\\n\\t */\\n\\tkthread_park(tsk);\\n\\tget_task_struct(tsk);\\n\\t*per_cpu_ptr(ht->store, cpu) = tsk;\\n\\tif (ht->create) {\\n\\t\\t/*\\n\\t\\t * Make sure that the task has actually scheduled out\\n\\t\\t * into park position, before calling the create\\n\\t\\t * callback. At least the migration thread callback\\n\\t\\t * requires that the task is off the runqueue.\\n\\t\\t */\\n\\t\\tif (!wait_task_inactive(tsk, TASK_PARKED))\\n\\t\\t\\tWARN_ON(1);\\n\\t\\telse\\n\\t\\t\\tht->create(cpu);\\n\\t}\\n\\treturn 0;\\n}\\n\\nint smpboot_create_threads(unsigned int cpu)\\n{\\n\\tstruct smp_hotplug_thread *cur;\\n\\tint ret = 0;\\n\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tlist_for_each_entry(cur, &hotplug_threads, list) {\\n\\t\\tret = __smpboot_create_thread(cur, cpu);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t}\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\treturn ret;\\n}\\n\\nstatic void smpboot_unpark_thread(struct smp_hotplug_thread *ht, unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);\\n\\n\\tif (!ht->selfparking)\\n\\t\\tkthread_unpark(tsk);\\n}\\n\\nint smpboot_unpark_threads(unsigned int cpu)\\n{\\n\\tstruct smp_hotplug_thread *cur;\\n\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tlist_for_each_entry(cur, &hotplug_threads, list)\\n\\t\\tsmpboot_unpark_thread(cur, cpu);\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\treturn 0;\\n}\\n\\nstatic void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);\\n\\n\\tif (tsk && !ht->selfparking)\\n\\t\\tkthread_park(tsk);\\n}\\n\\nint smpboot_park_threads(unsigned int cpu)\\n{\\n\\tstruct smp_hotplug_thread *cur;\\n\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tlist_for_each_entry_reverse(cur, &hotplug_threads, list)\\n\\t\\tsmpboot_park_thread(cur, cpu);\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\treturn 0;\\n}\\n\\nstatic void smpboot_destroy_threads(struct smp_hotplug_thread *ht)\\n{\\n\\tunsigned int cpu;\\n\\n\\t/* We need to destroy also the parked threads of offline cpus */\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tstruct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);\\n\\n\\t\\tif (tsk) {\\n\\t\\t\\tkthread_stop_put(tsk);\\n\\t\\t\\t*per_cpu_ptr(ht->store, cpu) = NULL;\\n\\t\\t}\\n\\t}\\n}\\n\\n/**\\n * smpboot_register_percpu_thread - Register a per_cpu thread related\\n * \\t\\t\\t\\t\\t    to hotplug\\n * @plug_thread:\\tHotplug thread descriptor\\n *\\n * Creates and starts the threads on all online cpus.\\n */\\nint smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)\\n{\\n\\tunsigned int cpu;\\n\\tint ret = 0;\\n\\n\\tcpus_read_lock();\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tret = __smpboot_create_thread(plug_thread, cpu);\\n\\t\\tif (ret) {\\n\\t\\t\\tsmpboot_destroy_threads(plug_thread);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tsmpboot_unpark_thread(plug_thread, cpu);\\n\\t}\\n\\tlist_add(&plug_thread->list, &hotplug_threads);\\nout:\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(smpboot_register_percpu_thread);\\n\\n/**\\n * smpboot_unregister_percpu_thread - Unregister a per_cpu thread related to hotplug\\n * @plug_thread:\\tHotplug thread descriptor\\n *\\n * Stops all threads on all possible cpus.\\n */\\nvoid smpboot_unregister_percpu_thread(struct smp_hotplug_thread *plug_thread)\\n{\\n\\tcpus_read_lock();\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tlist_del(&plug_thread->list);\\n\\tsmpboot_destroy_threads(plug_thread);\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\tcpus_read_unlock();\\n}\\nEXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);\\n\\n// SPDX-License-Identifier: GPL-2.0+\\n/*\\n * Test cases for API provided by resource.c and ioport.h\\n */\\n\\n#include <kunit/test.h>\\n#include <linux/ioport.h>\\n#include <linux/kernel.h>\\n#include <linux/string.h>\\n#include <linux/sizes.h>\\n#include <linux/mm.h>\\n\\n#define R0_START\\t0x0000\\n#define R0_END\\t\\t0xffff\\n#define R1_START\\t0x1234\\n#define R1_END\\t\\t0x2345\\n#define R2_START\\t0x4567\\n#define R2_END\\t\\t0x5678\\n#define R3_START\\t0x6789\\n#define R3_END\\t\\t0x789a\\n#define R4_START\\t0x2000\\n#define R4_END\\t\\t0x7000\\n\\nstatic struct resource r0 = { .start = R0_START, .end = R0_END };\\nstatic struct resource r1 = { .start = R1_START, .end = R1_END };\\nstatic struct resource r2 = { .start = R2_START, .end = R2_END };\\nstatic struct resource r3 = { .start = R3_START, .end = R3_END };\\nstatic struct resource r4 = { .start = R4_START, .end = R4_END };\\n\\nstruct result {\\n\\tstruct resource *r1;\\n\\tstruct resource *r2;\\n\\tstruct resource r;\\n\\tbool ret;\\n};\\n\\nstatic struct result results_for_union[] = {\\n\\t{\\n\\t\\t.r1 = &r1, .r2 = &r0, .r.start = R0_START, .r.end = R0_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r2, .r2 = &r0, .r.start = R0_START, .r.end = R0_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r3, .r2 = &r0, .r.start = R0_START, .r.end = R0_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r4, .r2 = &r0, .r.start = R0_START, .r.end = R0_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r2, .r2 = &r1, .ret = false,\\n\\t}, {\\n\\t\\t.r1 = &r3, .r2 = &r1, .ret = false,\\n\\t}, {\\n\\t\\t.r1 = &r4, .r2 = &r1, .r.start = R1_START, .r.end = R4_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r2, .r2 = &r3, .ret = false,\\n\\t}, {\\n\\t\\t.r1 = &r2, .r2 = &r4, .r.start = R4_START, .r.end = R4_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r3, .r2 = &r4, .r.start = R4_START, .r.end = R3_END, .ret = true,\\n\\t},\\n};\\n\\nstatic struct result results_for_intersection[] = {\\n\\t{\\n\\t\\t.r1 = &r1, .r2 = &r0, .r.start = R1_START, .r.end = R1_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r2, .r2 = &r0, .r.start = R2_START, .r.end = R2_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r3, .r2 = &r0, .r.start = R3_START, .r.end = R3_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r4, .r2 = &r0, .r.start = R4_START, .r.end = R4_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r2, .r2 = &r1, .ret = false,\\n\\t}, {\\n\\t\\t.r1 = &r3, .r2 = &r1, .ret = false,\\n\\t}, {\\n\\t\\t.r1 = &r4, .r2 = &r1, .r.start = R4_START, .r.end = R1_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r2, .r2 = &r3, .ret = false,\\n\\t}, {\\n\\t\\t.r1 = &r2, .r2 = &r4, .r.start = R2_START, .r.end = R2_END, .ret = true,\\n\\t}, {\\n\\t\\t.r1 = &r3, .r2 = &r4, .r.start = R3_START, .r.end = R4_END, .ret = true,\\n\\t},\\n};\\n\\nstatic void resource_do_test(struct kunit *test, bool ret, struct resource *r,\\n\\t\\t\\t     bool exp_ret, struct resource *exp_r,\\n\\t\\t\\t     struct resource *r1, struct resource *r2)\\n{\\n\\tKUNIT_EXPECT_EQ_MSG(test, ret, exp_ret, \"Resources %pR %pR\", r1, r2);\\n\\tKUNIT_EXPECT_EQ_MSG(test, r->start, exp_r->start, \"Start elements are not equal\");\\n\\tKUNIT_EXPECT_EQ_MSG(test, r->end, exp_r->end, \"End elements are not equal\");\\n}\\n\\nstatic void resource_do_union_test(struct kunit *test, struct result *r)\\n{\\n\\tstruct resource result;\\n\\tbool ret;\\n\\n\\tmemset(&result, 0, sizeof(result));\\n\\tret = resource_union(r->r1, r->r2, &result);\\n\\tresource_do_test(test, ret, &result, r->ret, &r->r, r->r1, r->r2);\\n\\n\\tmemset(&result, 0, sizeof(result));\\n\\tret = resource_union(r->r2, r->r1, &result);\\n\\tresource_do_test(test, ret, &result, r->ret, &r->r, r->r2, r->r1);\\n}\\n\\nstatic void resource_test_union(struct kunit *test)\\n{\\n\\tstruct result *r = results_for_union;\\n\\tunsigned int i = 0;\\n\\n\\tdo {\\n\\t\\tresource_do_union_test(test, &r[i]);\\n\\t} while (++i < ARRAY_SIZE(results_for_union));\\n}\\n\\nstatic void resource_do_intersection_test(struct kunit *test, struct result *r)\\n{\\n\\tstruct resource result;\\n\\tbool ret;\\n\\n\\tmemset(&result, 0, sizeof(result));\\n\\tret = resource_intersection(r->r1, r->r2, &result);\\n\\tresource_do_test(test, ret, &result, r->ret, &r->r, r->r1, r->r2);\\n\\n\\tmemset(&result, 0, sizeof(result));\\n\\tret = resource_intersection(r->r2, r->r1, &result);\\n\\tresource_do_test(test, ret, &result, r->ret, &r->r, r->r2, r->r1);\\n}\\n\\nstatic void resource_test_intersection(struct kunit *test)\\n{\\n\\tstruct result *r = results_for_intersection;\\n\\tunsigned int i = 0;\\n\\n\\tdo {\\n\\t\\tresource_do_intersection_test(test, &r[i]);\\n\\t} while (++i < ARRAY_SIZE(results_for_intersection));\\n}\\n\\n/*\\n * The test resource tree for region_intersects() test:\\n *\\n * BASE-BASE+1M-1 : Test System RAM 0\\n *\\t\\t  # hole 0 (BASE+1M-BASE+2M)\\n * BASE+2M-BASE+3M-1 : Test CXL Window 0\\n * BASE+3M-BASE+4M-1 : Test System RAM 1\\n * BASE+4M-BASE+7M-1 : Test CXL Window 1\\n *   BASE+4M-BASE+5M-1 : Test System RAM 2\\n *     BASE+4M+128K-BASE+4M+256K-1: Test Code\\n *   BASE+5M-BASE+6M-1 : Test System RAM 3\\n */\\n#define RES_TEST_RAM0_OFFSET\\t0\\n#define RES_TEST_RAM0_SIZE\\tSZ_1M\\n#define RES_TEST_HOLE0_OFFSET\\t(RES_TEST_RAM0_OFFSET + RES_TEST_RAM0_SIZE)\\n#define RES_TEST_HOLE0_SIZE\\tSZ_1M\\n#define RES_TEST_WIN0_OFFSET\\t(RES_TEST_HOLE0_OFFSET + RES_TEST_HOLE0_SIZE)\\n#define RES_TEST_WIN0_SIZE\\tSZ_1M\\n#define RES_TEST_RAM1_OFFSET\\t(RES_TEST_WIN0_OFFSET + RES_TEST_WIN0_SIZE)\\n#define RES_TEST_RAM1_SIZE\\tSZ_1M\\n#define RES_TEST_WIN1_OFFSET\\t(RES_TEST_RAM1_OFFSET + RES_TEST_RAM1_SIZE)\\n#define RES_TEST_WIN1_SIZE\\t(SZ_1M * 3)\\n#define RES_TEST_RAM2_OFFSET\\tRES_TEST_WIN1_OFFSET\\n#define RES_TEST_RAM2_SIZE\\tSZ_1M\\n#define RES_TEST_CODE_OFFSET\\t(RES_TEST_RAM2_OFFSET + SZ_128K)\\n#define RES_TEST_CODE_SIZE\\tSZ_128K\\n#define RES_TEST_RAM3_OFFSET\\t(RES_TEST_RAM2_OFFSET + RES_TEST_RAM2_SIZE)\\n#define RES_TEST_RAM3_SIZE\\tSZ_1M\\n#define RES_TEST_TOTAL_SIZE\\t((RES_TEST_WIN1_OFFSET + RES_TEST_WIN1_SIZE))\\n\\nKUNIT_DEFINE_ACTION_WRAPPER(kfree_wrapper, kfree, const void *);\\n\\nstatic void remove_free_resource(void *ctx)\\n{\\n\\tstruct resource *res = (struct resource *)ctx;\\n\\n\\tremove_resource(res);\\n\\tkfree(res);\\n}\\n\\nstatic void resource_test_add_action_or_abort(\\n\\tstruct kunit *test, void (*action)(void *), void *ctx)\\n{\\n\\tKUNIT_ASSERT_EQ_MSG(test, 0,\\n\\t\\t\\t    kunit_add_action_or_reset(test, action, ctx),\\n\\t\\t\\t    \"Fail to add action\");\\n}\\n\\nstatic void resource_test_request_region(struct kunit *test, struct resource *parent,\\n\\t\\t\\t\\t\\t resource_size_t start, resource_size_t size,\\n\\t\\t\\t\\t\\t const char *name, unsigned long flags)\\n{\\n\\tstruct resource *res;\\n\\n\\tres = __request_region(parent, start, size, name, flags);\\n\\tKUNIT_ASSERT_NOT_NULL(test, res);\\n\\tresource_test_add_action_or_abort(test, remove_free_resource, res);\\n}\\n\\nstatic void resource_test_insert_resource(struct kunit *test, struct resource *parent,\\n\\t\\t\\t\\t\\t  resource_size_t start, resource_size_t size,\\n\\t\\t\\t\\t\\t  const char *name, unsigned long flags)\\n{\\n\\tstruct resource *res;\\n\\n\\tres = kzalloc(sizeof(*res), GFP_KERNEL);\\n\\tKUNIT_ASSERT_NOT_NULL(test, res);\\n\\n\\tres->name = name;\\n\\tres->start = start;\\n\\tres->end = start + size - 1;\\n\\tres->flags = flags;\\n\\tif (insert_resource(parent, res)) {\\n\\t\\tresource_test_add_action_or_abort(test, kfree_wrapper, res);\\n\\t\\tKUNIT_FAIL_AND_ABORT(test, \"Fail to insert resource %pR\\\\n\", res);\\n\\t}\\n\\n\\tresource_test_add_action_or_abort(test, remove_free_resource, res);\\n}\\n\\nstatic void resource_test_region_intersects(struct kunit *test)\\n{\\n\\tunsigned long flags =  IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\\n\\tstruct resource *parent;\\n\\tresource_size_t start;\\n\\n\\t/* Find an iomem_resource hole to hold test resources */\\n\\tparent = alloc_free_mem_region(&iomem_resource, RES_TEST_TOTAL_SIZE, SZ_1M,\\n\\t\\t\\t\\t       \"test resources\");\\n\\tKUNIT_ASSERT_NOT_ERR_OR_NULL(test, parent);\\n\\tstart = parent->start;\\n\\tresource_test_add_action_or_abort(test, remove_free_resource, parent);\\n\\n\\tresource_test_request_region(test, parent, start + RES_TEST_RAM0_OFFSET,\\n\\t\\t\\t\\t     RES_TEST_RAM0_SIZE, \"Test System RAM 0\", flags);\\n\\tresource_test_insert_resource(test, parent, start + RES_TEST_WIN0_OFFSET,\\n\\t\\t\\t\\t      RES_TEST_WIN0_SIZE, \"Test CXL Window 0\",\\n\\t\\t\\t\\t      IORESOURCE_MEM);\\n\\tresource_test_request_region(test, parent, start + RES_TEST_RAM1_OFFSET,\\n\\t\\t\\t\\t     RES_TEST_RAM1_SIZE, \"Test System RAM 1\", flags);\\n\\tresource_test_insert_resource(test, parent, start + RES_TEST_WIN1_OFFSET,\\n\\t\\t\\t\\t      RES_TEST_WIN1_SIZE, \"Test CXL Window 1\",\\n\\t\\t\\t\\t      IORESOURCE_MEM);\\n\\tresource_test_request_region(test, parent, start + RES_TEST_RAM2_OFFSET,\\n\\t\\t\\t\\t     RES_TEST_RAM2_SIZE, \"Test System RAM 2\", flags);\\n\\tresource_test_insert_resource(test, parent, start + RES_TEST_CODE_OFFSET,\\n\\t\\t\\t\\t      RES_TEST_CODE_SIZE, \"Test Code\", flags);\\n\\tresource_test_request_region(test, parent, start + RES_TEST_RAM3_OFFSET,\\n\\t\\t\\t\\t     RES_TEST_RAM3_SIZE, \"Test System RAM 3\", flags);\\n\\tkunit_release_action(test, remove_free_resource, parent);\\n\\n\\tKUNIT_EXPECT_EQ(test, REGION_INTERSECTS,\\n\\t\\t\\tregion_intersects(start + RES_TEST_RAM0_OFFSET, PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_INTERSECTS,\\n\\t\\t\\tregion_intersects(start + RES_TEST_RAM0_OFFSET +\\n\\t\\t\\t\\t\\t  RES_TEST_RAM0_SIZE - PAGE_SIZE, 2 * PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_DISJOINT,\\n\\t\\t\\tregion_intersects(start + RES_TEST_HOLE0_OFFSET, PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_DISJOINT,\\n\\t\\t\\tregion_intersects(start + RES_TEST_HOLE0_OFFSET +\\n\\t\\t\\t\\t\\t  RES_TEST_HOLE0_SIZE - PAGE_SIZE, 2 * PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_MIXED,\\n\\t\\t\\tregion_intersects(start + RES_TEST_WIN0_OFFSET +\\n\\t\\t\\t\\t\\t  RES_TEST_WIN0_SIZE - PAGE_SIZE, 2 * PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_INTERSECTS,\\n\\t\\t\\tregion_intersects(start + RES_TEST_RAM1_OFFSET +\\n\\t\\t\\t\\t\\t  RES_TEST_RAM1_SIZE - PAGE_SIZE, 2 * PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_INTERSECTS,\\n\\t\\t\\tregion_intersects(start + RES_TEST_RAM2_OFFSET +\\n\\t\\t\\t\\t\\t  RES_TEST_RAM2_SIZE - PAGE_SIZE, 2 * PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_INTERSECTS,\\n\\t\\t\\tregion_intersects(start + RES_TEST_CODE_OFFSET, PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_INTERSECTS,\\n\\t\\t\\tregion_intersects(start + RES_TEST_RAM2_OFFSET,\\n\\t\\t\\t\\t\\t  RES_TEST_RAM2_SIZE + PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n\\tKUNIT_EXPECT_EQ(test, REGION_MIXED,\\n\\t\\t\\tregion_intersects(start + RES_TEST_RAM3_OFFSET,\\n\\t\\t\\t\\t\\t  RES_TEST_RAM3_SIZE + PAGE_SIZE,\\n\\t\\t\\t\\t\\t  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE));\\n}\\n\\nstatic struct kunit_case resource_test_cases[] = {\\n\\tKUNIT_CASE(resource_test_union),\\n\\tKUNIT_CASE(resource_test_intersection),\\n\\tKUNIT_CASE(resource_test_region_intersects),\\n\\t{}\\n};\\n\\nstatic struct kunit_suite resource_test_suite = {\\n\\t.name = \"resource\",\\n\\t.test_cases = resource_test_cases,\\n};\\nkunit_test_suite(resource_test_suite);\\n\\nMODULE_DESCRIPTION(\"I/O Port & Memory Resource manager unit tests\");\\nMODULE_LICENSE(\"GPL\");\\n\\n/*\\n * Public API and common code for kernel->userspace relay file support.\\n *\\n * See Documentation/filesystems/relay.rst for an overview.\\n *\\n * Copyright (C) 2002-2005 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp\\n * Copyright (C) 1999-2005 - Karim Yaghmour (karim@opersys.com)\\n *\\n * Moved to kernel/relay.c by Paul Mundt, 2006.\\n * November 2006 - CPU hotplug support by Mathieu Desnoyers\\n * \\t(mathieu.desnoyers@polymtl.ca)\\n *\\n * This file is released under the GPL.\\n */\\n#include <linux/errno.h>\\n#include <linux/stddef.h>\\n#include <linux/slab.h>\\n#include <linux/export.h>\\n#include <linux/string.h>\\n#include <linux/relay.h>\\n#include <linux/vmalloc.h>\\n#include <linux/mm.h>\\n#include <linux/cpu.h>\\n#include <linux/splice.h>\\n\\n/* list of open channels, for cpu hotplug */\\nstatic DEFINE_MUTEX(relay_channels_mutex);\\nstatic LIST_HEAD(relay_channels);\\n\\n/*\\n * fault() vm_op implementation for relay file mapping.\\n */\\nstatic vm_fault_t relay_buf_fault(struct vm_fault *vmf)\\n{\\n\\tstruct page *page;\\n\\tstruct rchan_buf *buf = vmf->vma->vm_private_data;\\n\\tpgoff_t pgoff = vmf->pgoff;\\n\\n\\tif (!buf)\\n\\t\\treturn VM_FAULT_OOM;\\n\\n\\tpage = vmalloc_to_page(buf->start + (pgoff << PAGE_SHIFT));\\n\\tif (!page)\\n\\t\\treturn VM_FAULT_SIGBUS;\\n\\tget_page(page);\\n\\tvmf->page = page;\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * vm_ops for relay file mappings.\\n */\\nstatic const struct vm_operations_struct relay_file_mmap_ops = {\\n\\t.fault = relay_buf_fault,\\n};\\n\\n/*\\n * allocate an array of pointers of struct page\\n */\\nstatic struct page **relay_alloc_page_array(unsigned int n_pages)\\n{\\n\\treturn kvcalloc(n_pages, sizeof(struct page *), GFP_KERNEL);\\n}\\n\\n/*\\n * free an array of pointers of struct page\\n */\\nstatic void relay_free_page_array(struct page **array)\\n{\\n\\tkvfree(array);\\n}\\n\\n/**\\n *\\trelay_mmap_buf: - mmap channel buffer to process address space\\n *\\t@buf: relay channel buffer\\n *\\t@vma: vm_area_struct describing memory to be mapped\\n *\\n *\\tReturns 0 if ok, negative on error\\n *\\n *\\tCaller should already have grabbed mmap_lock.\\n */\\nstatic int relay_mmap_buf(struct rchan_buf *buf, struct vm_area_struct *vma)\\n{\\n\\tunsigned long length = vma->vm_end - vma->vm_start;\\n\\n\\tif (!buf)\\n\\t\\treturn -EBADF;\\n\\n\\tif (length != (unsigned long)buf->chan->alloc_size)\\n\\t\\treturn -EINVAL;\\n\\n\\tvma->vm_ops = &relay_file_mmap_ops;\\n\\tvm_flags_set(vma, VM_DONTEXPAND);\\n\\tvma->vm_private_data = buf;\\n\\n\\treturn 0;\\n}\\n\\n/**\\n *\\trelay_alloc_buf - allocate a channel buffer\\n *\\t@buf: the buffer struct\\n *\\t@size: total size of the buffer\\n *\\n *\\tReturns a pointer to the resulting buffer, %NULL if unsuccessful. The\\n *\\tpassed in size will get page aligned, if it isn\\'t already.\\n */\\nstatic void *relay_alloc_buf(struct rchan_buf *buf, size_t *size)\\n{\\n\\tvoid *mem;\\n\\tunsigned int i, j, n_pages;\\n\\n\\t*size = PAGE_ALIGN(*size);\\n\\tn_pages = *size >> PAGE_SHIFT;\\n\\n\\tbuf->page_array = relay_alloc_page_array(n_pages);\\n\\tif (!buf->page_array)\\n\\t\\treturn NULL;\\n\\n\\tfor (i = 0; i < n_pages; i++) {\\n\\t\\tbuf->page_array[i] = alloc_page(GFP_KERNEL);\\n\\t\\tif (unlikely(!buf->page_array[i]))\\n\\t\\t\\tgoto depopulate;\\n\\t\\tset_page_private(buf->page_array[i], (unsigned long)buf);\\n\\t}\\n\\tmem = vmap(buf->page_array, n_pages, VM_MAP, PAGE_KERNEL);\\n\\tif (!mem)\\n\\t\\tgoto depopulate;\\n\\n\\tmemset(mem, 0, *size);\\n\\tbuf->page_count = n_pages;\\n\\treturn mem;\\n\\ndepopulate:\\n\\tfor (j = 0; j < i; j++)\\n\\t\\t__free_page(buf->page_array[j]);\\n\\trelay_free_page_array(buf->page_array);\\n\\treturn NULL;\\n}\\n\\n/**\\n *\\trelay_create_buf - allocate and initialize a channel buffer\\n *\\t@chan: the relay channel\\n *\\n *\\tReturns channel buffer if successful, %NULL otherwise.\\n */\\nstatic struct rchan_buf *relay_create_buf(struct rchan *chan)\\n{\\n\\tstruct rchan_buf *buf;\\n\\n\\tif (chan->n_subbufs > KMALLOC_MAX_SIZE / sizeof(size_t))\\n\\t\\treturn NULL;\\n\\n\\tbuf = kzalloc(sizeof(struct rchan_buf), GFP_KERNEL);\\n\\tif (!buf)\\n\\t\\treturn NULL;\\n\\tbuf->padding = kmalloc_array(chan->n_subbufs, sizeof(size_t),\\n\\t\\t\\t\\t     GFP_KERNEL);\\n\\tif (!buf->padding)\\n\\t\\tgoto free_buf;\\n\\n\\tbuf->start = relay_alloc_buf(buf, &chan->alloc_size);\\n\\tif (!buf->start)\\n\\t\\tgoto free_buf;\\n\\n\\tbuf->chan = chan;\\n\\tkref_get(&buf->chan->kref);\\n\\treturn buf;\\n\\nfree_buf:\\n\\tkfree(buf->padding);\\n\\tkfree(buf);\\n\\treturn NULL;\\n}\\n\\n/**\\n *\\trelay_destroy_channel - free the channel struct\\n *\\t@kref: target kernel reference that contains the relay channel\\n *\\n *\\tShould only be called from kref_put().\\n */\\nstatic void relay_destroy_channel(struct kref *kref)\\n{\\n\\tstruct rchan *chan = container_of(kref, struct rchan, kref);\\n\\tfree_percpu(chan->buf);\\n\\tkfree(chan);\\n}\\n\\n/**\\n *\\trelay_destroy_buf - destroy an rchan_buf struct and associated buffer\\n *\\t@buf: the buffer struct\\n */\\nstatic void relay_destroy_buf(struct rchan_buf *buf)\\n{\\n\\tstruct rchan *chan = buf->chan;\\n\\tunsigned int i;\\n\\n\\tif (likely(buf->start)) {\\n\\t\\tvunmap(buf->start);\\n\\t\\tfor (i = 0; i < buf->page_count; i++)\\n\\t\\t\\t__free_page(buf->page_array[i]);\\n\\t\\trelay_free_page_array(buf->page_array);\\n\\t}\\n\\t*per_cpu_ptr(chan->buf, buf->cpu) = NULL;\\n\\tkfree(buf->padding);\\n\\tkfree(buf);\\n\\tkref_put(&chan->kref, relay_destroy_channel);\\n}\\n\\n/**\\n *\\trelay_remove_buf - remove a channel buffer\\n *\\t@kref: target kernel reference that contains the relay buffer\\n *\\n *\\tRemoves the file from the filesystem, which also frees the\\n *\\trchan_buf_struct and the channel buffer.  Should only be called from\\n *\\tkref_put().\\n */\\nstatic void relay_remove_buf(struct kref *kref)\\n{\\n\\tstruct rchan_buf *buf = container_of(kref, struct rchan_buf, kref);\\n\\trelay_destroy_buf(buf);\\n}\\n\\n/**\\n *\\trelay_buf_empty - boolean, is the channel buffer empty?\\n *\\t@buf: channel buffer\\n *\\n *\\tReturns 1 if the buffer is empty, 0 otherwise.\\n */\\nstatic int relay_buf_empty(struct rchan_buf *buf)\\n{\\n\\treturn (buf->subbufs_produced - buf->subbufs_consumed) ? 0 : 1;\\n}\\n\\n/**\\n *\\trelay_buf_full - boolean, is the channel buffer full?\\n *\\t@buf: channel buffer\\n *\\n *\\tReturns 1 if the buffer is full, 0 otherwise.\\n */\\nint relay_buf_full(struct rchan_buf *buf)\\n{\\n\\tsize_t ready = buf->subbufs_produced - buf->subbufs_consumed;\\n\\treturn (ready >= buf->chan->n_subbufs) ? 1 : 0;\\n}\\nEXPORT_SYMBOL_GPL(relay_buf_full);\\n\\n/*\\n * High-level relay kernel API and associated functions.\\n */\\n\\nstatic int relay_subbuf_start(struct rchan_buf *buf, void *subbuf,\\n\\t\\t\\t      void *prev_subbuf, size_t prev_padding)\\n{\\n\\tif (!buf->chan->cb->subbuf_start)\\n\\t\\treturn !relay_buf_full(buf);\\n\\n\\treturn buf->chan->cb->subbuf_start(buf, subbuf,\\n\\t\\t\\t\\t\\t   prev_subbuf, prev_padding);\\n}\\n\\n/**\\n *\\twakeup_readers - wake up readers waiting on a channel\\n *\\t@work: contains the channel buffer\\n *\\n *\\tThis is the function used to defer reader waking\\n */\\nstatic void wakeup_readers(struct irq_work *work)\\n{\\n\\tstruct rchan_buf *buf;\\n\\n\\tbuf = container_of(work, struct rchan_buf, wakeup_work);\\n\\twake_up_interruptible(&buf->read_wait);\\n}\\n\\n/**\\n *\\t__relay_reset - reset a channel buffer\\n *\\t@buf: the channel buffer\\n *\\t@init: 1 if this is a first-time initialization\\n *\\n *\\tSee relay_reset() for description of effect.\\n */\\nstatic void __relay_reset(struct rchan_buf *buf, unsigned int init)\\n{\\n\\tsize_t i;\\n\\n\\tif (init) {\\n\\t\\tinit_waitqueue_head(&buf->read_wait);\\n\\t\\tkref_init(&buf->kref);\\n\\t\\tinit_irq_work(&buf->wakeup_work, wakeup_readers);\\n\\t} else {\\n\\t\\tirq_work_sync(&buf->wakeup_work);\\n\\t}\\n\\n\\tbuf->subbufs_produced = 0;\\n\\tbuf->subbufs_consumed = 0;\\n\\tbuf->bytes_consumed = 0;\\n\\tbuf->finalized = 0;\\n\\tbuf->data = buf->start;\\n\\tbuf->offset = 0;\\n\\n\\tfor (i = 0; i < buf->chan->n_subbufs; i++)\\n\\t\\tbuf->padding[i] = 0;\\n\\n\\trelay_subbuf_start(buf, buf->data, NULL, 0);\\n}\\n\\n/**\\n *\\trelay_reset - reset the channel\\n *\\t@chan: the channel\\n *\\n *\\tThis has the effect of erasing all data from all channel buffers\\n *\\tand restarting the channel in its initial state.  The buffers\\n *\\tare not freed, so any mappings are still in effect.\\n *\\n *\\tNOTE. Care should be taken that the channel isn\\'t actually\\n *\\tbeing used by anything when this call is made.\\n */\\nvoid relay_reset(struct rchan *chan)\\n{\\n\\tstruct rchan_buf *buf;\\n\\tunsigned int i;\\n\\n\\tif (!chan)\\n\\t\\treturn;\\n\\n\\tif (chan->is_global && (buf = *per_cpu_ptr(chan->buf, 0))) {\\n\\t\\t__relay_reset(buf, 0);\\n\\t\\treturn;\\n\\t}\\n\\n\\tmutex_lock(&relay_channels_mutex);\\n\\tfor_each_possible_cpu(i)\\n\\t\\tif ((buf = *per_cpu_ptr(chan->buf, i)))\\n\\t\\t\\t__relay_reset(buf, 0);\\n\\tmutex_unlock(&relay_channels_mutex);\\n}\\nEXPORT_SYMBOL_GPL(relay_reset);\\n\\nstatic inline void relay_set_buf_dentry(struct rchan_buf *buf,\\n\\t\\t\\t\\t\\tstruct dentry *dentry)\\n{\\n\\tbuf->dentry = dentry;\\n\\td_inode(buf->dentry)->i_size = buf->early_bytes;\\n}\\n\\nstatic struct dentry *relay_create_buf_file(struct rchan *chan,\\n\\t\\t\\t\\t\\t    struct rchan_buf *buf,\\n\\t\\t\\t\\t\\t    unsigned int cpu)\\n{\\n\\tstruct dentry *dentry;\\n\\tchar *tmpname;\\n\\n\\ttmpname = kzalloc(NAME_MAX + 1, GFP_KERNEL);\\n\\tif (!tmpname)\\n\\t\\treturn NULL;\\n\\tsnprintf(tmpname, NAME_MAX, \"%s%d\", chan->base_filename, cpu);\\n\\n\\t/* Create file in fs */\\n\\tdentry = chan->cb->create_buf_file(tmpname, chan->parent,\\n\\t\\t\\t\\t\\t   S_IRUSR, buf,\\n\\t\\t\\t\\t\\t   &chan->is_global);\\n\\tif (IS_ERR(dentry))\\n\\t\\tdentry = NULL;\\n\\n\\tkfree(tmpname);\\n\\n\\treturn dentry;\\n}\\n\\n/*\\n *\\trelay_open_buf - create a new relay channel buffer\\n *\\n *\\tused by relay_open() and CPU hotplug.\\n */\\nstatic struct rchan_buf *relay_open_buf(struct rchan *chan, unsigned int cpu)\\n{\\n\\tstruct rchan_buf *buf;\\n\\tstruct dentry *dentry;\\n\\n \\tif (chan->is_global)\\n\\t\\treturn *per_cpu_ptr(chan->buf, 0);\\n\\n\\tbuf = relay_create_buf(chan);\\n\\tif (!buf)\\n\\t\\treturn NULL;\\n\\n\\tif (chan->has_base_filename) {\\n\\t\\tdentry = relay_create_buf_file(chan, buf, cpu);\\n\\t\\tif (!dentry)\\n\\t\\t\\tgoto free_buf;\\n\\t\\trelay_set_buf_dentry(buf, dentry);\\n\\t} else {\\n\\t\\t/* Only retrieve global info, nothing more, nothing less */\\n\\t\\tdentry = chan->cb->create_buf_file(NULL, NULL,\\n\\t\\t\\t\\t\\t\\t   S_IRUSR, buf,\\n\\t\\t\\t\\t\\t\\t   &chan->is_global);\\n\\t\\tif (IS_ERR_OR_NULL(dentry))\\n\\t\\t\\tgoto free_buf;\\n\\t}\\n\\n \\tbuf->cpu = cpu;\\n \\t__relay_reset(buf, 1);\\n\\n \\tif(chan->is_global) {\\n\\t\\t*per_cpu_ptr(chan->buf, 0) = buf;\\n \\t\\tbuf->cpu = 0;\\n  \\t}\\n\\n\\treturn buf;\\n\\nfree_buf:\\n \\trelay_destroy_buf(buf);\\n\\treturn NULL;\\n}\\n\\n/**\\n *\\trelay_close_buf - close a channel buffer\\n *\\t@buf: channel buffer\\n *\\n *\\tMarks the buffer finalized and restores the default callbacks.\\n *\\tThe channel buffer and channel buffer data structure are then freed\\n *\\tautomatically when the last reference is given up.\\n */\\nstatic void relay_close_buf(struct rchan_buf *buf)\\n{\\n\\tbuf->finalized = 1;\\n\\tirq_work_sync(&buf->wakeup_work);\\n\\tbuf->chan->cb->remove_buf_file(buf->dentry);\\n\\tkref_put(&buf->kref, relay_remove_buf);\\n}\\n\\nint relay_prepare_cpu(unsigned int cpu)\\n{\\n\\tstruct rchan *chan;\\n\\tstruct rchan_buf *buf;\\n\\n\\tmutex_lock(&relay_channels_mutex);\\n\\tlist_for_each_entry(chan, &relay_channels, list) {\\n\\t\\tif (*per_cpu_ptr(chan->buf, cpu))\\n\\t\\t\\tcontinue;\\n\\t\\tbuf = relay_open_buf(chan, cpu);\\n\\t\\tif (!buf) {\\n\\t\\t\\tpr_err(\"relay: cpu %d buffer creation failed\\\\n\", cpu);\\n\\t\\t\\tmutex_unlock(&relay_channels_mutex);\\n\\t\\t\\treturn -ENOMEM;\\n\\t\\t}\\n\\t\\t*per_cpu_ptr(chan->buf, cpu) = buf;\\n\\t}\\n\\tmutex_unlock(&relay_channels_mutex);\\n\\treturn 0;\\n}\\n\\n/**\\n *\\trelay_open - create a new relay channel\\n *\\t@base_filename: base name of files to create, %NULL for buffering only\\n *\\t@parent: dentry of parent directory, %NULL for root directory or buffer\\n *\\t@subbuf_size: size of sub-buffers\\n *\\t@n_subbufs: number of sub-buffers\\n *\\t@cb: client callback functions\\n *\\t@private_data: user-defined data\\n *\\n *\\tReturns channel pointer if successful, %NULL otherwise.\\n *\\n *\\tCreates a channel buffer for each cpu using the sizes and\\n *\\tattributes specified.  The created channel buffer files\\n *\\twill be named base_filename0...base_filenameN-1.  File\\n *\\tpermissions will be %S_IRUSR.\\n *\\n *\\tIf opening a buffer (@parent = NULL) that you later wish to register\\n *\\tin a filesystem, call relay_late_setup_files() once the @parent dentry\\n *\\tis available.\\n */\\nstruct rchan *relay_open(const char *base_filename,\\n\\t\\t\\t struct dentry *parent,\\n\\t\\t\\t size_t subbuf_size,\\n\\t\\t\\t size_t n_subbufs,\\n\\t\\t\\t const struct rchan_callbacks *cb,\\n\\t\\t\\t void *private_data)\\n{\\n\\tunsigned int i;\\n\\tstruct rchan *chan;\\n\\tstruct rchan_buf *buf;\\n\\n\\tif (!(subbuf_size && n_subbufs))\\n\\t\\treturn NULL;\\n\\tif (subbuf_size > UINT_MAX / n_subbufs)\\n\\t\\treturn NULL;\\n\\tif (!cb || !cb->create_buf_file || !cb->remove_buf_file)\\n\\t\\treturn NULL;\\n\\n\\tchan = kzalloc(sizeof(struct rchan), GFP_KERNEL);\\n\\tif (!chan)\\n\\t\\treturn NULL;\\n\\n\\tchan->buf = alloc_percpu(struct rchan_buf *);\\n\\tif (!chan->buf) {\\n\\t\\tkfree(chan);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tchan->version = RELAYFS_CHANNEL_VERSION;\\n\\tchan->n_subbufs = n_subbufs;\\n\\tchan->subbuf_size = subbuf_size;\\n\\tchan->alloc_size = PAGE_ALIGN(subbuf_size * n_subbufs);\\n\\tchan->parent = parent;\\n\\tchan->private_data = private_data;\\n\\tif (base_filename) {\\n\\t\\tchan->has_base_filename = 1;\\n\\t\\tstrscpy(chan->base_filename, base_filename, NAME_MAX);\\n\\t}\\n\\tchan->cb = cb;\\n\\tkref_init(&chan->kref);\\n\\n\\tmutex_lock(&relay_channels_mutex);\\n\\tfor_each_online_cpu(i) {\\n\\t\\tbuf = relay_open_buf(chan, i);\\n\\t\\tif (!buf)\\n\\t\\t\\tgoto free_bufs;\\n\\t\\t*per_cpu_ptr(chan->buf, i) = buf;\\n\\t}\\n\\tlist_add(&chan->list, &relay_channels);\\n\\tmutex_unlock(&relay_channels_mutex);\\n\\n\\treturn chan;\\n\\nfree_bufs:\\n\\tfor_each_possible_cpu(i) {\\n\\t\\tif ((buf = *per_cpu_ptr(chan->buf, i)))\\n\\t\\t\\trelay_close_buf(buf);\\n\\t}\\n\\n\\tkref_put(&chan->kref, relay_destroy_channel);\\n\\tmutex_unlock(&relay_channels_mutex);\\n\\treturn NULL;\\n}\\nEXPORT_SYMBOL_GPL(relay_open);\\n\\nstruct rchan_percpu_buf_dispatcher {\\n\\tstruct rchan_buf *buf;\\n\\tstruct dentry *dentry;\\n};\\n\\n/* Called in atomic context. */\\nstatic void __relay_set_buf_dentry(void *info)\\n{\\n\\tstruct rchan_percpu_buf_dispatcher *p = info;\\n\\n\\trelay_set_buf_dentry(p->buf, p->dentry);\\n}\\n\\n/**\\n *\\trelay_late_setup_files - triggers file creation\\n *\\t@chan: channel to operate on\\n *\\t@base_filename: base name of files to create\\n *\\t@parent: dentry of parent directory, %NULL for root directory\\n *\\n *\\tReturns 0 if successful, non-zero otherwise.\\n *\\n *\\tUse to setup files for a previously buffer-only channel created\\n *\\tby relay_open() with a NULL parent dentry.\\n *\\n *\\tFor example, this is useful for perfomring early tracing in kernel,\\n *\\tbefore VFS is up and then exposing the early results once the dentry\\n *\\tis available.\\n */\\nint relay_late_setup_files(struct rchan *chan,\\n\\t\\t\\t   const char *base_filename,\\n\\t\\t\\t   struct dentry *parent)\\n{\\n\\tint err = 0;\\n\\tunsigned int i, curr_cpu;\\n\\tunsigned long flags;\\n\\tstruct dentry *dentry;\\n\\tstruct rchan_buf *buf;\\n\\tstruct rchan_percpu_buf_dispatcher disp;\\n\\n\\tif (!chan || !base_filename)\\n\\t\\treturn -EINVAL;\\n\\n\\tstrscpy(chan->base_filename, base_filename, NAME_MAX);\\n\\n\\tmutex_lock(&relay_channels_mutex);\\n\\t/* Is chan already set up? */\\n\\tif (unlikely(chan->has_base_filename)) {\\n\\t\\tmutex_unlock(&relay_channels_mutex);\\n\\t\\treturn -EEXIST;\\n\\t}\\n\\tchan->has_base_filename = 1;\\n\\tchan->parent = parent;\\n\\n\\tif (chan->is_global) {\\n\\t\\terr = -EINVAL;\\n\\t\\tbuf = *per_cpu_ptr(chan->buf, 0);\\n\\t\\tif (!WARN_ON_ONCE(!buf)) {\\n\\t\\t\\tdentry = relay_create_buf_file(chan, buf, 0);\\n\\t\\t\\tif (dentry && !WARN_ON_ONCE(!chan->is_global)) {\\n\\t\\t\\t\\trelay_set_buf_dentry(buf, dentry);\\n\\t\\t\\t\\terr = 0;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tmutex_unlock(&relay_channels_mutex);\\n\\t\\treturn err;\\n\\t}\\n\\n\\tcurr_cpu = get_cpu();\\n\\t/*\\n\\t * The CPU hotplug notifier ran before us and created buffers with\\n\\t * no files associated. So it\\'s safe to call relay_setup_buf_file()\\n\\t * on all currently online CPUs.\\n\\t */\\n\\tfor_each_online_cpu(i) {\\n\\t\\tbuf = *per_cpu_ptr(chan->buf, i);\\n\\t\\tif (unlikely(!buf)) {\\n\\t\\t\\tWARN_ONCE(1, KERN_ERR \"CPU has no buffer!\\\\n\");\\n\\t\\t\\terr = -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tdentry = relay_create_buf_file(chan, buf, i);\\n\\t\\tif (unlikely(!dentry)) {\\n\\t\\t\\terr = -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tif (curr_cpu == i) {\\n\\t\\t\\tlocal_irq_save(flags);\\n\\t\\t\\trelay_set_buf_dentry(buf, dentry);\\n\\t\\t\\tlocal_irq_restore(flags);\\n\\t\\t} else {\\n\\t\\t\\tdisp.buf = buf;\\n\\t\\t\\tdisp.dentry = dentry;\\n\\t\\t\\tsmp_mb();\\n\\t\\t\\t/* relay_channels_mutex must be held, so wait. */\\n\\t\\t\\terr = smp_call_function_single(i,\\n\\t\\t\\t\\t\\t\\t       __relay_set_buf_dentry,\\n\\t\\t\\t\\t\\t\\t       &disp, 1);\\n\\t\\t}\\n\\t\\tif (unlikely(err))\\n\\t\\t\\tbreak;\\n\\t}\\n\\tput_cpu();\\n\\tmutex_unlock(&relay_channels_mutex);\\n\\n\\treturn err;\\n}\\nEXPORT_SYMBOL_GPL(relay_late_setup_files);\\n\\n/**\\n *\\trelay_switch_subbuf - switch to a new sub-buffer\\n *\\t@buf: channel buffer\\n *\\t@length: size of current event\\n *\\n *\\tReturns either the length passed in or 0 if full.\\n *\\n *\\tPerforms sub-buffer-switch tasks such as invoking callbacks,\\n *\\tupdating padding counts, waking up readers, etc.\\n */\\nsize_t relay_switch_subbuf(struct rchan_buf *buf, size_t length)\\n{\\n\\tvoid *old, *new;\\n\\tsize_t old_subbuf, new_subbuf;\\n\\n\\tif (unlikely(length > buf->chan->subbuf_size))\\n\\t\\tgoto toobig;\\n\\n\\tif (buf->offset != buf->chan->subbuf_size + 1) {\\n\\t\\tbuf->prev_padding = buf->chan->subbuf_size - buf->offset;\\n\\t\\told_subbuf = buf->subbufs_produced % buf->chan->n_subbufs;\\n\\t\\tbuf->padding[old_subbuf] = buf->prev_padding;\\n\\t\\tbuf->subbufs_produced++;\\n\\t\\tif (buf->dentry)\\n\\t\\t\\td_inode(buf->dentry)->i_size +=\\n\\t\\t\\t\\tbuf->chan->subbuf_size -\\n\\t\\t\\t\\tbuf->padding[old_subbuf];\\n\\t\\telse\\n\\t\\t\\tbuf->early_bytes += buf->chan->subbuf_size -\\n\\t\\t\\t\\t\\t    buf->padding[old_subbuf];\\n\\t\\tsmp_mb();\\n\\t\\tif (waitqueue_active(&buf->read_wait)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Calling wake_up_interruptible() from here\\n\\t\\t\\t * will deadlock if we happen to be logging\\n\\t\\t\\t * from the scheduler (trying to re-grab\\n\\t\\t\\t * rq->lock), so defer it.\\n\\t\\t\\t */\\n\\t\\t\\tirq_work_queue(&buf->wakeup_work);\\n\\t\\t}\\n\\t}\\n\\n\\told = buf->data;\\n\\tnew_subbuf = buf->subbufs_produced % buf->chan->n_subbufs;\\n\\tnew = buf->start + new_subbuf * buf->chan->subbuf_size;\\n\\tbuf->offset = 0;\\n\\tif (!relay_subbuf_start(buf, new, old, buf->prev_padding)) {\\n\\t\\tbuf->offset = buf->chan->subbuf_size + 1;\\n\\t\\treturn 0;\\n\\t}\\n\\tbuf->data = new;\\n\\tbuf->padding[new_subbuf] = 0;\\n\\n\\tif (unlikely(length + buf->offset > buf->chan->subbuf_size))\\n\\t\\tgoto toobig;\\n\\n\\treturn length;\\n\\ntoobig:\\n\\tbuf->chan->last_toobig = length;\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(relay_switch_subbuf);\\n\\n/**\\n *\\trelay_subbufs_consumed - update the buffer\\'s sub-buffers-consumed count\\n *\\t@chan: the channel\\n *\\t@cpu: the cpu associated with the channel buffer to update\\n *\\t@subbufs_consumed: number of sub-buffers to add to current buf\\'s count\\n *\\n *\\tAdds to the channel buffer\\'s consumed sub-buffer count.\\n *\\tsubbufs_consumed should be the number of sub-buffers newly consumed,\\n *\\tnot the total consumed.\\n *\\n *\\tNOTE. Kernel clients don\\'t need to call this function if the channel\\n *\\tmode is \\'overwrite\\'.\\n */\\nvoid relay_subbufs_consumed(struct rchan *chan,\\n\\t\\t\\t    unsigned int cpu,\\n\\t\\t\\t    size_t subbufs_consumed)\\n{\\n\\tstruct rchan_buf *buf;\\n\\n\\tif (!chan || cpu >= NR_CPUS)\\n\\t\\treturn;\\n\\n\\tbuf = *per_cpu_ptr(chan->buf, cpu);\\n\\tif (!buf || subbufs_consumed > chan->n_subbufs)\\n\\t\\treturn;\\n\\n\\tif (subbufs_consumed > buf->subbufs_produced - buf->subbufs_consumed)\\n\\t\\tbuf->subbufs_consumed = buf->subbufs_produced;\\n\\telse\\n\\t\\tbuf->subbufs_consumed += subbufs_consumed;\\n}\\nEXPORT_SYMBOL_GPL(relay_subbufs_consumed);\\n\\n/**\\n *\\trelay_close - close the channel\\n *\\t@chan: the channel\\n *\\n *\\tCloses all channel buffers and frees the channel.\\n */\\nvoid relay_close(struct rchan *chan)\\n{\\n\\tstruct rchan_buf *buf;\\n\\tunsigned int i;\\n\\n\\tif (!chan)\\n\\t\\treturn;\\n\\n\\tmutex_lock(&relay_channels_mutex);\\n\\tif (chan->is_global && (buf = *per_cpu_ptr(chan->buf, 0)))\\n\\t\\trelay_close_buf(buf);\\n\\telse\\n\\t\\tfor_each_possible_cpu(i)\\n\\t\\t\\tif ((buf = *per_cpu_ptr(chan->buf, i)))\\n\\t\\t\\t\\trelay_close_buf(buf);\\n\\n\\tif (chan->last_toobig)\\n\\t\\tprintk(KERN_WARNING \"relay: one or more items not logged \"\\n\\t\\t       \"[item size (%zd) > sub-buffer size (%zd)]\\\\n\",\\n\\t\\t       chan->last_toobig, chan->subbuf_size);\\n\\n\\tlist_del(&chan->list);\\n\\tkref_put(&chan->kref, relay_destroy_channel);\\n\\tmutex_unlock(&relay_channels_mutex);\\n}\\nEXPORT_SYMBOL_GPL(relay_close);\\n\\n/**\\n *\\trelay_flush - close the channel\\n *\\t@chan: the channel\\n *\\n *\\tFlushes all channel buffers, i.e. forces buffer switch.\\n */\\nvoid relay_flush(struct rchan *chan)\\n{\\n\\tstruct rchan_buf *buf;\\n\\tunsigned int i;\\n\\n\\tif (!chan)\\n\\t\\treturn;\\n\\n\\tif (chan->is_global && (buf = *per_cpu_ptr(chan->buf, 0))) {\\n\\t\\trelay_switch_subbuf(buf, 0);\\n\\t\\treturn;\\n\\t}\\n\\n\\tmutex_lock(&relay_channels_mutex);\\n\\tfor_each_possible_cpu(i)\\n\\t\\tif ((buf = *per_cpu_ptr(chan->buf, i)))\\n\\t\\t\\trelay_switch_subbuf(buf, 0);\\n\\tmutex_unlock(&relay_channels_mutex);\\n}\\nEXPORT_SYMBOL_GPL(relay_flush);\\n\\n/**\\n *\\trelay_file_open - open file op for relay files\\n *\\t@inode: the inode\\n *\\t@filp: the file\\n *\\n *\\tIncrements the channel buffer refcount.\\n */\\nstatic int relay_file_open(struct inode *inode, struct file *filp)\\n{\\n\\tstruct rchan_buf *buf = inode->i_private;\\n\\tkref_get(&buf->kref);\\n\\tfilp->private_data = buf;\\n\\n\\treturn nonseekable_open(inode, filp);\\n}\\n\\n/**\\n *\\trelay_file_mmap - mmap file op for relay files\\n *\\t@filp: the file\\n *\\t@vma: the vma describing what to map\\n *\\n *\\tCalls upon relay_mmap_buf() to map the file into user space.\\n */\\nstatic int relay_file_mmap(struct file *filp, struct vm_area_struct *vma)\\n{\\n\\tstruct rchan_buf *buf = filp->private_data;\\n\\treturn relay_mmap_buf(buf, vma);\\n}\\n\\n/**\\n *\\trelay_file_poll - poll file op for relay files\\n *\\t@filp: the file\\n *\\t@wait: poll table\\n *\\n *\\tPoll implemention.\\n */\\nstatic __poll_t relay_file_poll(struct file *filp, poll_table *wait)\\n{\\n\\t__poll_t mask = 0;\\n\\tstruct rchan_buf *buf = filp->private_data;\\n\\n\\tif (buf->finalized)\\n\\t\\treturn EPOLLERR;\\n\\n\\tif (filp->f_mode & FMODE_READ) {\\n\\t\\tpoll_wait(filp, &buf->read_wait, wait);\\n\\t\\tif (!relay_buf_empty(buf))\\n\\t\\t\\tmask |= EPOLLIN | EPOLLRDNORM;\\n\\t}\\n\\n\\treturn mask;\\n}\\n\\n/**\\n *\\trelay_file_release - release file op for relay files\\n *\\t@inode: the inode\\n *\\t@filp: the file\\n *\\n *\\tDecrements the channel refcount, as the filesystem is\\n *\\tno longer using it.\\n */\\nstatic int relay_file_release(struct inode *inode, struct file *filp)\\n{\\n\\tstruct rchan_buf *buf = filp->private_data;\\n\\tkref_put(&buf->kref, relay_remove_buf);\\n\\n\\treturn 0;\\n}\\n\\n/*\\n *\\trelay_file_read_consume - update the consumed count for the buffer\\n */\\nstatic void relay_file_read_consume(struct rchan_buf *buf,\\n\\t\\t\\t\\t    size_t read_pos,\\n\\t\\t\\t\\t    size_t bytes_consumed)\\n{\\n\\tsize_t subbuf_size = buf->chan->subbuf_size;\\n\\tsize_t n_subbufs = buf->chan->n_subbufs;\\n\\tsize_t read_subbuf;\\n\\n\\tif (buf->subbufs_produced == buf->subbufs_consumed &&\\n\\t    buf->offset == buf->bytes_consumed)\\n\\t\\treturn;\\n\\n\\tif (buf->bytes_consumed + bytes_consumed > subbuf_size) {\\n\\t\\trelay_subbufs_consumed(buf->chan, buf->cpu, 1);\\n\\t\\tbuf->bytes_consumed = 0;\\n\\t}\\n\\n\\tbuf->bytes_consumed += bytes_consumed;\\n\\tif (!read_pos)\\n\\t\\tread_subbuf = buf->subbufs_consumed % n_subbufs;\\n\\telse\\n\\t\\tread_subbuf = read_pos / buf->chan->subbuf_size;\\n\\tif (buf->bytes_consumed + buf->padding[read_subbuf] == subbuf_size) {\\n\\t\\tif ((read_subbuf == buf->subbufs_produced % n_subbufs) &&\\n\\t\\t    (buf->offset == subbuf_size))\\n\\t\\t\\treturn;\\n\\t\\trelay_subbufs_consumed(buf->chan, buf->cpu, 1);\\n\\t\\tbuf->bytes_consumed = 0;\\n\\t}\\n}\\n\\n/*\\n *\\trelay_file_read_avail - boolean, are there unconsumed bytes available?\\n */\\nstatic int relay_file_read_avail(struct rchan_buf *buf)\\n{\\n\\tsize_t subbuf_size = buf->chan->subbuf_size;\\n\\tsize_t n_subbufs = buf->chan->n_subbufs;\\n\\tsize_t produced = buf->subbufs_produced;\\n\\tsize_t consumed;\\n\\n\\trelay_file_read_consume(buf, 0, 0);\\n\\n\\tconsumed = buf->subbufs_consumed;\\n\\n\\tif (unlikely(buf->offset > subbuf_size)) {\\n\\t\\tif (produced == consumed)\\n\\t\\t\\treturn 0;\\n\\t\\treturn 1;\\n\\t}\\n\\n\\tif (unlikely(produced - consumed >= n_subbufs)) {\\n\\t\\tconsumed = produced - n_subbufs + 1;\\n\\t\\tbuf->subbufs_consumed = consumed;\\n\\t\\tbuf->bytes_consumed = 0;\\n\\t}\\n\\n\\tproduced = (produced % n_subbufs) * subbuf_size + buf->offset;\\n\\tconsumed = (consumed % n_subbufs) * subbuf_size + buf->bytes_consumed;\\n\\n\\tif (consumed > produced)\\n\\t\\tproduced += n_subbufs * subbuf_size;\\n\\n\\tif (consumed == produced) {\\n\\t\\tif (buf->offset == subbuf_size &&\\n\\t\\t    buf->subbufs_produced > buf->subbufs_consumed)\\n\\t\\t\\treturn 1;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\treturn 1;\\n}\\n\\n/**\\n *\\trelay_file_read_subbuf_avail - return bytes available in sub-buffer\\n *\\t@read_pos: file read position\\n *\\t@buf: relay channel buffer\\n */\\nstatic size_t relay_file_read_subbuf_avail(size_t read_pos,\\n\\t\\t\\t\\t\\t   struct rchan_buf *buf)\\n{\\n\\tsize_t padding, avail = 0;\\n\\tsize_t read_subbuf, read_offset, write_subbuf, write_offset;\\n\\tsize_t subbuf_size = buf->chan->subbuf_size;\\n\\n\\twrite_subbuf = (buf->data - buf->start) / subbuf_size;\\n\\twrite_offset = buf->offset > subbuf_size ? subbuf_size : buf->offset;\\n\\tread_subbuf = read_pos / subbuf_size;\\n\\tread_offset = read_pos % subbuf_size;\\n\\tpadding = buf->padding[read_subbuf];\\n\\n\\tif (read_subbuf == write_subbuf) {\\n\\t\\tif (read_offset + padding < write_offset)\\n\\t\\t\\tavail = write_offset - (read_offset + padding);\\n\\t} else\\n\\t\\tavail = (subbuf_size - padding) - read_offset;\\n\\n\\treturn avail;\\n}\\n\\n/**\\n *\\trelay_file_read_start_pos - find the first available byte to read\\n *\\t@buf: relay channel buffer\\n *\\n *\\tIf the read_pos is in the middle of padding, return the\\n *\\tposition of the first actually available byte, otherwise\\n *\\treturn the original value.\\n */\\nstatic size_t relay_file_read_start_pos(struct rchan_buf *buf)\\n{\\n\\tsize_t read_subbuf, padding, padding_start, padding_end;\\n\\tsize_t subbuf_size = buf->chan->subbuf_size;\\n\\tsize_t n_subbufs = buf->chan->n_subbufs;\\n\\tsize_t consumed = buf->subbufs_consumed % n_subbufs;\\n\\tsize_t read_pos = (consumed * subbuf_size + buf->bytes_consumed)\\n\\t\\t\\t% (n_subbufs * subbuf_size);\\n\\n\\tread_subbuf = read_pos / subbuf_size;\\n\\tpadding = buf->padding[read_subbuf];\\n\\tpadding_start = (read_subbuf + 1) * subbuf_size - padding;\\n\\tpadding_end = (read_subbuf + 1) * subbuf_size;\\n\\tif (read_pos >= padding_start && read_pos < padding_end) {\\n\\t\\tread_subbuf = (read_subbuf + 1) % n_subbufs;\\n\\t\\tread_pos = read_subbuf * subbuf_size;\\n\\t}\\n\\n\\treturn read_pos;\\n}\\n\\n/**\\n *\\trelay_file_read_end_pos - return the new read position\\n *\\t@read_pos: file read position\\n *\\t@buf: relay channel buffer\\n *\\t@count: number of bytes to be read\\n */\\nstatic size_t relay_file_read_end_pos(struct rchan_buf *buf,\\n\\t\\t\\t\\t      size_t read_pos,\\n\\t\\t\\t\\t      size_t count)\\n{\\n\\tsize_t read_subbuf, padding, end_pos;\\n\\tsize_t subbuf_size = buf->chan->subbuf_size;\\n\\tsize_t n_subbufs = buf->chan->n_subbufs;\\n\\n\\tread_subbuf = read_pos / subbuf_size;\\n\\tpadding = buf->padding[read_subbuf];\\n\\tif (read_pos % subbuf_size + count + padding == subbuf_size)\\n\\t\\tend_pos = (read_subbuf + 1) * subbuf_size;\\n\\telse\\n\\t\\tend_pos = read_pos + count;\\n\\tif (end_pos >= subbuf_size * n_subbufs)\\n\\t\\tend_pos = 0;\\n\\n\\treturn end_pos;\\n}\\n\\nstatic ssize_t relay_file_read(struct file *filp,\\n\\t\\t\\t       char __user *buffer,\\n\\t\\t\\t       size_t count,\\n\\t\\t\\t       loff_t *ppos)\\n{\\n\\tstruct rchan_buf *buf = filp->private_data;\\n\\tsize_t read_start, avail;\\n\\tsize_t written = 0;\\n\\tint ret;\\n\\n\\tif (!count)\\n\\t\\treturn 0;\\n\\n\\tinode_lock(file_inode(filp));\\n\\tdo {\\n\\t\\tvoid *from;\\n\\n\\t\\tif (!relay_file_read_avail(buf))\\n\\t\\t\\tbreak;\\n\\n\\t\\tread_start = relay_file_read_start_pos(buf);\\n\\t\\tavail = relay_file_read_subbuf_avail(read_start, buf);\\n\\t\\tif (!avail)\\n\\t\\t\\tbreak;\\n\\n\\t\\tavail = min(count, avail);\\n\\t\\tfrom = buf->start + read_start;\\n\\t\\tret = avail;\\n\\t\\tif (copy_to_user(buffer, from, avail))\\n\\t\\t\\tbreak;\\n\\n\\t\\tbuffer += ret;\\n\\t\\twritten += ret;\\n\\t\\tcount -= ret;\\n\\n\\t\\trelay_file_read_consume(buf, read_start, ret);\\n\\t\\t*ppos = relay_file_read_end_pos(buf, read_start, ret);\\n\\t} while (count);\\n\\tinode_unlock(file_inode(filp));\\n\\n\\treturn written;\\n}\\n\\n\\nconst struct file_operations relay_file_operations = {\\n\\t.open\\t\\t= relay_file_open,\\n\\t.poll\\t\\t= relay_file_poll,\\n\\t.mmap\\t\\t= relay_file_mmap,\\n\\t.read\\t\\t= relay_file_read,\\n\\t.release\\t= relay_file_release,\\n};\\nEXPORT_SYMBOL_GPL(relay_file_operations);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Range add and subtract\\n */\\n#include <linux/init.h>\\n#include <linux/minmax.h>\\n#include <linux/printk.h>\\n#include <linux/sort.h>\\n#include <linux/string.h>\\n#include <linux/range.h>\\n\\nint add_range(struct range *range, int az, int nr_range, u64 start, u64 end)\\n{\\n\\tif (start >= end)\\n\\t\\treturn nr_range;\\n\\n\\t/* Out of slots: */\\n\\tif (nr_range >= az)\\n\\t\\treturn nr_range;\\n\\n\\trange[nr_range].start = start;\\n\\trange[nr_range].end = end;\\n\\n\\tnr_range++;\\n\\n\\treturn nr_range;\\n}\\n\\nint add_range_with_merge(struct range *range, int az, int nr_range,\\n\\t\\t     u64 start, u64 end)\\n{\\n\\tint i;\\n\\n\\tif (start >= end)\\n\\t\\treturn nr_range;\\n\\n\\t/* get new start/end: */\\n\\tfor (i = 0; i < nr_range; i++) {\\n\\t\\tu64 common_start, common_end;\\n\\n\\t\\tif (!range[i].end)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tcommon_start = max(range[i].start, start);\\n\\t\\tcommon_end = min(range[i].end, end);\\n\\t\\tif (common_start > common_end)\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* new start/end, will add it back at last */\\n\\t\\tstart = min(range[i].start, start);\\n\\t\\tend = max(range[i].end, end);\\n\\n\\t\\tmemmove(&range[i], &range[i + 1],\\n\\t\\t\\t(nr_range - (i + 1)) * sizeof(range[i]));\\n\\t\\trange[nr_range - 1].start = 0;\\n\\t\\trange[nr_range - 1].end   = 0;\\n\\t\\tnr_range--;\\n\\t\\ti--;\\n\\t}\\n\\n\\t/* Need to add it: */\\n\\treturn add_range(range, az, nr_range, start, end);\\n}\\n\\nvoid subtract_range(struct range *range, int az, u64 start, u64 end)\\n{\\n\\tint i, j;\\n\\n\\tif (start >= end)\\n\\t\\treturn;\\n\\n\\tfor (j = 0; j < az; j++) {\\n\\t\\tif (!range[j].end)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (start <= range[j].start && end >= range[j].end) {\\n\\t\\t\\trange[j].start = 0;\\n\\t\\t\\trange[j].end = 0;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tif (start <= range[j].start && end < range[j].end &&\\n\\t\\t    range[j].start < end) {\\n\\t\\t\\trange[j].start = end;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\n\\t\\tif (start > range[j].start && end >= range[j].end &&\\n\\t\\t    range[j].end > start) {\\n\\t\\t\\trange[j].end = start;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tif (start > range[j].start && end < range[j].end) {\\n\\t\\t\\t/* Find the new spare: */\\n\\t\\t\\tfor (i = 0; i < az; i++) {\\n\\t\\t\\t\\tif (range[i].end == 0)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tif (i < az) {\\n\\t\\t\\t\\trange[i].end = range[j].end;\\n\\t\\t\\t\\trange[i].start = end;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tpr_err(\"%s: run out of slot in ranges\\\\n\",\\n\\t\\t\\t\\t\\t__func__);\\n\\t\\t\\t}\\n\\t\\t\\trange[j].end = start;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t}\\n}\\n\\nstatic int cmp_range(const void *x1, const void *x2)\\n{\\n\\tconst struct range *r1 = x1;\\n\\tconst struct range *r2 = x2;\\n\\n\\tif (r1->start < r2->start)\\n\\t\\treturn -1;\\n\\tif (r1->start > r2->start)\\n\\t\\treturn 1;\\n\\treturn 0;\\n}\\n\\nint clean_sort_range(struct range *range, int az)\\n{\\n\\tint i, j, k = az - 1, nr_range = az;\\n\\n\\tfor (i = 0; i < k; i++) {\\n\\t\\tif (range[i].end)\\n\\t\\t\\tcontinue;\\n\\t\\tfor (j = k; j > i; j--) {\\n\\t\\t\\tif (range[j].end) {\\n\\t\\t\\t\\tk = j;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif (j == i)\\n\\t\\t\\tbreak;\\n\\t\\trange[i].start = range[k].start;\\n\\t\\trange[i].end   = range[k].end;\\n\\t\\trange[k].start = 0;\\n\\t\\trange[k].end   = 0;\\n\\t\\tk--;\\n\\t}\\n\\t/* count it */\\n\\tfor (i = 0; i < az; i++) {\\n\\t\\tif (!range[i].end) {\\n\\t\\t\\tnr_range = i;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\t/* sort them */\\n\\tsort(range, nr_range, sizeof(struct range), cmp_range, NULL);\\n\\n\\treturn nr_range;\\n}\\n\\nvoid sort_range(struct range *range, int nr_range)\\n{\\n\\t/* sort them */\\n\\tsort(range, nr_range, sizeof(struct range), cmp_range, NULL);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Generic helpers for smp ipi calls\\n *\\n * (C) Jens Axboe <jens.axboe@oracle.com> 2008\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/irq_work.h>\\n#include <linux/rcupdate.h>\\n#include <linux/rculist.h>\\n#include <linux/kernel.h>\\n#include <linux/export.h>\\n#include <linux/percpu.h>\\n#include <linux/init.h>\\n#include <linux/interrupt.h>\\n#include <linux/gfp.h>\\n#include <linux/smp.h>\\n#include <linux/cpu.h>\\n#include <linux/sched.h>\\n#include <linux/sched/idle.h>\\n#include <linux/hypervisor.h>\\n#include <linux/sched/clock.h>\\n#include <linux/nmi.h>\\n#include <linux/sched/debug.h>\\n#include <linux/jump_label.h>\\n#include <linux/string_choices.h>\\n\\n#include <trace/events/ipi.h>\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/csd.h>\\n#undef CREATE_TRACE_POINTS\\n\\n#include \"smpboot.h\"\\n#include \"sched/smp.h\"\\n\\n#define CSD_TYPE(_csd)\\t((_csd)->node.u_flags & CSD_FLAG_TYPE_MASK)\\n\\nstruct call_function_data {\\n\\tcall_single_data_t\\t__percpu *csd;\\n\\tcpumask_var_t\\t\\tcpumask;\\n\\tcpumask_var_t\\t\\tcpumask_ipi;\\n};\\n\\nstatic DEFINE_PER_CPU_ALIGNED(struct call_function_data, cfd_data);\\n\\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);\\n\\nstatic DEFINE_PER_CPU(atomic_t, trigger_backtrace) = ATOMIC_INIT(1);\\n\\nstatic void __flush_smp_call_function_queue(bool warn_cpu_offline);\\n\\nint smpcfd_prepare_cpu(unsigned int cpu)\\n{\\n\\tstruct call_function_data *cfd = &per_cpu(cfd_data, cpu);\\n\\n\\tif (!zalloc_cpumask_var_node(&cfd->cpumask, GFP_KERNEL,\\n\\t\\t\\t\\t     cpu_to_node(cpu)))\\n\\t\\treturn -ENOMEM;\\n\\tif (!zalloc_cpumask_var_node(&cfd->cpumask_ipi, GFP_KERNEL,\\n\\t\\t\\t\\t     cpu_to_node(cpu))) {\\n\\t\\tfree_cpumask_var(cfd->cpumask);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\tcfd->csd = alloc_percpu(call_single_data_t);\\n\\tif (!cfd->csd) {\\n\\t\\tfree_cpumask_var(cfd->cpumask);\\n\\t\\tfree_cpumask_var(cfd->cpumask_ipi);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nint smpcfd_dead_cpu(unsigned int cpu)\\n{\\n\\tstruct call_function_data *cfd = &per_cpu(cfd_data, cpu);\\n\\n\\tfree_cpumask_var(cfd->cpumask);\\n\\tfree_cpumask_var(cfd->cpumask_ipi);\\n\\tfree_percpu(cfd->csd);\\n\\treturn 0;\\n}\\n\\nint smpcfd_dying_cpu(unsigned int cpu)\\n{\\n\\t/*\\n\\t * The IPIs for the smp-call-function callbacks queued by other\\n\\t * CPUs might arrive late, either due to hardware latencies or\\n\\t * because this CPU disabled interrupts (inside stop-machine)\\n\\t * before the IPIs were sent. So flush out any pending callbacks\\n\\t * explicitly (without waiting for the IPIs to arrive), to\\n\\t * ensure that the outgoing CPU doesn\\'t go offline with work\\n\\t * still pending.\\n\\t */\\n\\t__flush_smp_call_function_queue(false);\\n\\tirq_work_run();\\n\\treturn 0;\\n}\\n\\nvoid __init call_function_init(void)\\n{\\n\\tint i;\\n\\n\\tfor_each_possible_cpu(i)\\n\\t\\tinit_llist_head(&per_cpu(call_single_queue, i));\\n\\n\\tsmpcfd_prepare_cpu(smp_processor_id());\\n}\\n\\nstatic __always_inline void\\nsend_call_function_single_ipi(int cpu)\\n{\\n\\tif (call_function_single_prep_ipi(cpu)) {\\n\\t\\ttrace_ipi_send_cpu(cpu, _RET_IP_,\\n\\t\\t\\t\\t   generic_smp_call_function_single_interrupt);\\n\\t\\tarch_send_call_function_single_ipi(cpu);\\n\\t}\\n}\\n\\nstatic __always_inline void\\nsend_call_function_ipi_mask(struct cpumask *mask)\\n{\\n\\ttrace_ipi_send_cpumask(mask, _RET_IP_,\\n\\t\\t\\t       generic_smp_call_function_single_interrupt);\\n\\tarch_send_call_function_ipi_mask(mask);\\n}\\n\\nstatic __always_inline void\\ncsd_do_func(smp_call_func_t func, void *info, call_single_data_t *csd)\\n{\\n\\ttrace_csd_function_entry(func, csd);\\n\\tfunc(info);\\n\\ttrace_csd_function_exit(func, csd);\\n}\\n\\n#ifdef CONFIG_CSD_LOCK_WAIT_DEBUG\\n\\nstatic DEFINE_STATIC_KEY_MAYBE(CONFIG_CSD_LOCK_WAIT_DEBUG_DEFAULT, csdlock_debug_enabled);\\n\\n/*\\n * Parse the csdlock_debug= kernel boot parameter.\\n *\\n * If you need to restore the old \"ext\" value that once provided\\n * additional debugging information, reapply the following commits:\\n *\\n * de7b09ef658d (\"locking/csd_lock: Prepare more CSD lock debugging\")\\n * a5aabace5fb8 (\"locking/csd_lock: Add more data to CSD lock debugging\")\\n */\\nstatic int __init csdlock_debug(char *str)\\n{\\n\\tint ret;\\n\\tunsigned int val = 0;\\n\\n\\tret = get_option(&str, &val);\\n\\tif (ret) {\\n\\t\\tif (val)\\n\\t\\t\\tstatic_branch_enable(&csdlock_debug_enabled);\\n\\t\\telse\\n\\t\\t\\tstatic_branch_disable(&csdlock_debug_enabled);\\n\\t}\\n\\n\\treturn 1;\\n}\\n__setup(\"csdlock_debug=\", csdlock_debug);\\n\\nstatic DEFINE_PER_CPU(call_single_data_t *, cur_csd);\\nstatic DEFINE_PER_CPU(smp_call_func_t, cur_csd_func);\\nstatic DEFINE_PER_CPU(void *, cur_csd_info);\\n\\nstatic ulong csd_lock_timeout = 5000;  /* CSD lock timeout in milliseconds. */\\nmodule_param(csd_lock_timeout, ulong, 0444);\\nstatic int panic_on_ipistall;  /* CSD panic timeout in milliseconds, 300000 for five minutes. */\\nmodule_param(panic_on_ipistall, int, 0444);\\n\\nstatic atomic_t csd_bug_count = ATOMIC_INIT(0);\\n\\n/* Record current CSD work for current CPU, NULL to erase. */\\nstatic void __csd_lock_record(call_single_data_t *csd)\\n{\\n\\tif (!csd) {\\n\\t\\tsmp_mb(); /* NULL cur_csd after unlock. */\\n\\t\\t__this_cpu_write(cur_csd, NULL);\\n\\t\\treturn;\\n\\t}\\n\\t__this_cpu_write(cur_csd_func, csd->func);\\n\\t__this_cpu_write(cur_csd_info, csd->info);\\n\\tsmp_wmb(); /* func and info before csd. */\\n\\t__this_cpu_write(cur_csd, csd);\\n\\tsmp_mb(); /* Update cur_csd before function call. */\\n\\t\\t  /* Or before unlock, as the case may be. */\\n}\\n\\nstatic __always_inline void csd_lock_record(call_single_data_t *csd)\\n{\\n\\tif (static_branch_unlikely(&csdlock_debug_enabled))\\n\\t\\t__csd_lock_record(csd);\\n}\\n\\nstatic int csd_lock_wait_getcpu(call_single_data_t *csd)\\n{\\n\\tunsigned int csd_type;\\n\\n\\tcsd_type = CSD_TYPE(csd);\\n\\tif (csd_type == CSD_TYPE_ASYNC || csd_type == CSD_TYPE_SYNC)\\n\\t\\treturn csd->node.dst; /* Other CSD_TYPE_ values might not have ->dst. */\\n\\treturn -1;\\n}\\n\\nstatic atomic_t n_csd_lock_stuck;\\n\\n/**\\n * csd_lock_is_stuck - Has a CSD-lock acquisition been stuck too long?\\n *\\n * Returns @true if a CSD-lock acquisition is stuck and has been stuck\\n * long enough for a \"non-responsive CSD lock\" message to be printed.\\n */\\nbool csd_lock_is_stuck(void)\\n{\\n\\treturn !!atomic_read(&n_csd_lock_stuck);\\n}\\n\\n/*\\n * Complain if too much time spent waiting.  Note that only\\n * the CSD_TYPE_SYNC/ASYNC types provide the destination CPU,\\n * so waiting on other types gets much less information.\\n */\\nstatic bool csd_lock_wait_toolong(call_single_data_t *csd, u64 ts0, u64 *ts1, int *bug_id, unsigned long *nmessages)\\n{\\n\\tint cpu = -1;\\n\\tint cpux;\\n\\tbool firsttime;\\n\\tu64 ts2, ts_delta;\\n\\tcall_single_data_t *cpu_cur_csd;\\n\\tunsigned int flags = READ_ONCE(csd->node.u_flags);\\n\\tunsigned long long csd_lock_timeout_ns = csd_lock_timeout * NSEC_PER_MSEC;\\n\\n\\tif (!(flags & CSD_FLAG_LOCK)) {\\n\\t\\tif (!unlikely(*bug_id))\\n\\t\\t\\treturn true;\\n\\t\\tcpu = csd_lock_wait_getcpu(csd);\\n\\t\\tpr_alert(\"csd: CSD lock (#%d) got unstuck on CPU#%02d, CPU#%02d released the lock.\\\\n\",\\n\\t\\t\\t *bug_id, raw_smp_processor_id(), cpu);\\n\\t\\tatomic_dec(&n_csd_lock_stuck);\\n\\t\\treturn true;\\n\\t}\\n\\n\\tts2 = ktime_get_mono_fast_ns();\\n\\t/* How long since we last checked for a stuck CSD lock.*/\\n\\tts_delta = ts2 - *ts1;\\n\\tif (likely(ts_delta <= csd_lock_timeout_ns * (*nmessages + 1) *\\n\\t\\t\\t       (!*nmessages ? 1 : (ilog2(num_online_cpus()) / 2 + 1)) ||\\n\\t\\t   csd_lock_timeout_ns == 0))\\n\\t\\treturn false;\\n\\n\\tif (ts0 > ts2) {\\n\\t\\t/* Our own sched_clock went backward; don\\'t blame another CPU. */\\n\\t\\tts_delta = ts0 - ts2;\\n\\t\\tpr_alert(\"sched_clock on CPU %d went backward by %llu ns\\\\n\", raw_smp_processor_id(), ts_delta);\\n\\t\\t*ts1 = ts2;\\n\\t\\treturn false;\\n\\t}\\n\\n\\tfirsttime = !*bug_id;\\n\\tif (firsttime)\\n\\t\\t*bug_id = atomic_inc_return(&csd_bug_count);\\n\\tcpu = csd_lock_wait_getcpu(csd);\\n\\tif (WARN_ONCE(cpu < 0 || cpu >= nr_cpu_ids, \"%s: cpu = %d\\\\n\", __func__, cpu))\\n\\t\\tcpux = 0;\\n\\telse\\n\\t\\tcpux = cpu;\\n\\tcpu_cur_csd = smp_load_acquire(&per_cpu(cur_csd, cpux)); /* Before func and info. */\\n\\t/* How long since this CSD lock was stuck. */\\n\\tts_delta = ts2 - ts0;\\n\\tpr_alert(\"csd: %s non-responsive CSD lock (#%d) on CPU#%d, waiting %lld ns for CPU#%02d %pS(%ps).\\\\n\",\\n\\t\\t firsttime ? \"Detected\" : \"Continued\", *bug_id, raw_smp_processor_id(), (s64)ts_delta,\\n\\t\\t cpu, csd->func, csd->info);\\n\\t(*nmessages)++;\\n\\tif (firsttime)\\n\\t\\tatomic_inc(&n_csd_lock_stuck);\\n\\t/*\\n\\t * If the CSD lock is still stuck after 5 minutes, it is unlikely\\n\\t * to become unstuck. Use a signed comparison to avoid triggering\\n\\t * on underflows when the TSC is out of sync between sockets.\\n\\t */\\n\\tBUG_ON(panic_on_ipistall > 0 && (s64)ts_delta > ((s64)panic_on_ipistall * NSEC_PER_MSEC));\\n\\tif (cpu_cur_csd && csd != cpu_cur_csd) {\\n\\t\\tpr_alert(\"\\\\tcsd: CSD lock (#%d) handling prior %pS(%ps) request.\\\\n\",\\n\\t\\t\\t *bug_id, READ_ONCE(per_cpu(cur_csd_func, cpux)),\\n\\t\\t\\t READ_ONCE(per_cpu(cur_csd_info, cpux)));\\n\\t} else {\\n\\t\\tpr_alert(\"\\\\tcsd: CSD lock (#%d) %s.\\\\n\",\\n\\t\\t\\t *bug_id, !cpu_cur_csd ? \"unresponsive\" : \"handling this request\");\\n\\t}\\n\\tif (cpu >= 0) {\\n\\t\\tif (atomic_cmpxchg_acquire(&per_cpu(trigger_backtrace, cpu), 1, 0))\\n\\t\\t\\tdump_cpu_task(cpu);\\n\\t\\tif (!cpu_cur_csd) {\\n\\t\\t\\tpr_alert(\"csd: Re-sending CSD lock (#%d) IPI from CPU#%02d to CPU#%02d\\\\n\", *bug_id, raw_smp_processor_id(), cpu);\\n\\t\\t\\tarch_send_call_function_single_ipi(cpu);\\n\\t\\t}\\n\\t}\\n\\tif (firsttime)\\n\\t\\tdump_stack();\\n\\t*ts1 = ts2;\\n\\n\\treturn false;\\n}\\n\\n/*\\n * csd_lock/csd_unlock used to serialize access to per-cpu csd resources\\n *\\n * For non-synchronous ipi calls the csd can still be in use by the\\n * previous function call. For multi-cpu calls its even more interesting\\n * as we\\'ll have to ensure no other cpu is observing our csd.\\n */\\nstatic void __csd_lock_wait(call_single_data_t *csd)\\n{\\n\\tunsigned long nmessages = 0;\\n\\tint bug_id = 0;\\n\\tu64 ts0, ts1;\\n\\n\\tts1 = ts0 = ktime_get_mono_fast_ns();\\n\\tfor (;;) {\\n\\t\\tif (csd_lock_wait_toolong(csd, ts0, &ts1, &bug_id, &nmessages))\\n\\t\\t\\tbreak;\\n\\t\\tcpu_relax();\\n\\t}\\n\\tsmp_acquire__after_ctrl_dep();\\n}\\n\\nstatic __always_inline void csd_lock_wait(call_single_data_t *csd)\\n{\\n\\tif (static_branch_unlikely(&csdlock_debug_enabled)) {\\n\\t\\t__csd_lock_wait(csd);\\n\\t\\treturn;\\n\\t}\\n\\n\\tsmp_cond_load_acquire(&csd->node.u_flags, !(VAL & CSD_FLAG_LOCK));\\n}\\n#else\\nstatic void csd_lock_record(call_single_data_t *csd)\\n{\\n}\\n\\nstatic __always_inline void csd_lock_wait(call_single_data_t *csd)\\n{\\n\\tsmp_cond_load_acquire(&csd->node.u_flags, !(VAL & CSD_FLAG_LOCK));\\n}\\n#endif\\n\\nstatic __always_inline void csd_lock(call_single_data_t *csd)\\n{\\n\\tcsd_lock_wait(csd);\\n\\tcsd->node.u_flags |= CSD_FLAG_LOCK;\\n\\n\\t/*\\n\\t * prevent CPU from reordering the above assignment\\n\\t * to ->flags with any subsequent assignments to other\\n\\t * fields of the specified call_single_data_t structure:\\n\\t */\\n\\tsmp_wmb();\\n}\\n\\nstatic __always_inline void csd_unlock(call_single_data_t *csd)\\n{\\n\\tWARN_ON(!(csd->node.u_flags & CSD_FLAG_LOCK));\\n\\n\\t/*\\n\\t * ensure we\\'re all done before releasing data:\\n\\t */\\n\\tsmp_store_release(&csd->node.u_flags, 0);\\n}\\n\\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(call_single_data_t, csd_data);\\n\\nvoid __smp_call_single_queue(int cpu, struct llist_node *node)\\n{\\n\\t/*\\n\\t * We have to check the type of the CSD before queueing it, because\\n\\t * once queued it can have its flags cleared by\\n\\t *   flush_smp_call_function_queue()\\n\\t * even if we haven\\'t sent the smp_call IPI yet (e.g. the stopper\\n\\t * executes migration_cpu_stop() on the remote CPU).\\n\\t */\\n\\tif (trace_csd_queue_cpu_enabled()) {\\n\\t\\tcall_single_data_t *csd;\\n\\t\\tsmp_call_func_t func;\\n\\n\\t\\tcsd = container_of(node, call_single_data_t, node.llist);\\n\\t\\tfunc = CSD_TYPE(csd) == CSD_TYPE_TTWU ?\\n\\t\\t\\tsched_ttwu_pending : csd->func;\\n\\n\\t\\ttrace_csd_queue_cpu(cpu, _RET_IP_, func, csd);\\n\\t}\\n\\n\\t/*\\n\\t * The list addition should be visible to the target CPU when it pops\\n\\t * the head of the list to pull the entry off it in the IPI handler\\n\\t * because of normal cache coherency rules implied by the underlying\\n\\t * llist ops.\\n\\t *\\n\\t * If IPIs can go out of order to the cache coherency protocol\\n\\t * in an architecture, sufficient synchronisation should be added\\n\\t * to arch code to make it appear to obey cache coherency WRT\\n\\t * locking and barrier primitives. Generic code isn\\'t really\\n\\t * equipped to do the right thing...\\n\\t */\\n\\tif (llist_add(node, &per_cpu(call_single_queue, cpu)))\\n\\t\\tsend_call_function_single_ipi(cpu);\\n}\\n\\n/*\\n * Insert a previously allocated call_single_data_t element\\n * for execution on the given CPU. data must already have\\n * ->func, ->info, and ->flags set.\\n */\\nstatic int generic_exec_single(int cpu, call_single_data_t *csd)\\n{\\n\\tif (cpu == smp_processor_id()) {\\n\\t\\tsmp_call_func_t func = csd->func;\\n\\t\\tvoid *info = csd->info;\\n\\t\\tunsigned long flags;\\n\\n\\t\\t/*\\n\\t\\t * We can unlock early even for the synchronous on-stack case,\\n\\t\\t * since we\\'re doing this from the same CPU..\\n\\t\\t */\\n\\t\\tcsd_lock_record(csd);\\n\\t\\tcsd_unlock(csd);\\n\\t\\tlocal_irq_save(flags);\\n\\t\\tcsd_do_func(func, info, NULL);\\n\\t\\tcsd_lock_record(NULL);\\n\\t\\tlocal_irq_restore(flags);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif ((unsigned)cpu >= nr_cpu_ids || !cpu_online(cpu)) {\\n\\t\\tcsd_unlock(csd);\\n\\t\\treturn -ENXIO;\\n\\t}\\n\\n\\t__smp_call_single_queue(cpu, &csd->node.llist);\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * generic_smp_call_function_single_interrupt - Execute SMP IPI callbacks\\n *\\n * Invoked by arch to handle an IPI for call function single.\\n * Must be called with interrupts disabled.\\n */\\nvoid generic_smp_call_function_single_interrupt(void)\\n{\\n\\t__flush_smp_call_function_queue(true);\\n}\\n\\n/**\\n * __flush_smp_call_function_queue - Flush pending smp-call-function callbacks\\n *\\n * @warn_cpu_offline: If set to \\'true\\', warn if callbacks were queued on an\\n *\\t\\t      offline CPU. Skip this check if set to \\'false\\'.\\n *\\n * Flush any pending smp-call-function callbacks queued on this CPU. This is\\n * invoked by the generic IPI handler, as well as by a CPU about to go offline,\\n * to ensure that all pending IPI callbacks are run before it goes completely\\n * offline.\\n *\\n * Loop through the call_single_queue and run all the queued callbacks.\\n * Must be called with interrupts disabled.\\n */\\nstatic void __flush_smp_call_function_queue(bool warn_cpu_offline)\\n{\\n\\tcall_single_data_t *csd, *csd_next;\\n\\tstruct llist_node *entry, *prev;\\n\\tstruct llist_head *head;\\n\\tstatic bool warned;\\n\\tatomic_t *tbt;\\n\\n\\tlockdep_assert_irqs_disabled();\\n\\n\\t/* Allow waiters to send backtrace NMI from here onwards */\\n\\ttbt = this_cpu_ptr(&trigger_backtrace);\\n\\tatomic_set_release(tbt, 1);\\n\\n\\thead = this_cpu_ptr(&call_single_queue);\\n\\tentry = llist_del_all(head);\\n\\tentry = llist_reverse_order(entry);\\n\\n\\t/* There shouldn\\'t be any pending callbacks on an offline CPU. */\\n\\tif (unlikely(warn_cpu_offline && !cpu_online(smp_processor_id()) &&\\n\\t\\t     !warned && entry != NULL)) {\\n\\t\\twarned = true;\\n\\t\\tWARN(1, \"IPI on offline CPU %d\\\\n\", smp_processor_id());\\n\\n\\t\\t/*\\n\\t\\t * We don\\'t have to use the _safe() variant here\\n\\t\\t * because we are not invoking the IPI handlers yet.\\n\\t\\t */\\n\\t\\tllist_for_each_entry(csd, entry, node.llist) {\\n\\t\\t\\tswitch (CSD_TYPE(csd)) {\\n\\t\\t\\tcase CSD_TYPE_ASYNC:\\n\\t\\t\\tcase CSD_TYPE_SYNC:\\n\\t\\t\\tcase CSD_TYPE_IRQ_WORK:\\n\\t\\t\\t\\tpr_warn(\"IPI callback %pS sent to offline CPU\\\\n\",\\n\\t\\t\\t\\t\\tcsd->func);\\n\\t\\t\\t\\tbreak;\\n\\n\\t\\t\\tcase CSD_TYPE_TTWU:\\n\\t\\t\\t\\tpr_warn(\"IPI task-wakeup sent to offline CPU\\\\n\");\\n\\t\\t\\t\\tbreak;\\n\\n\\t\\t\\tdefault:\\n\\t\\t\\t\\tpr_warn(\"IPI callback, unknown type %d, sent to offline CPU\\\\n\",\\n\\t\\t\\t\\t\\tCSD_TYPE(csd));\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * First; run all SYNC callbacks, people are waiting for us.\\n\\t */\\n\\tprev = NULL;\\n\\tllist_for_each_entry_safe(csd, csd_next, entry, node.llist) {\\n\\t\\t/* Do we wait until *after* callback? */\\n\\t\\tif (CSD_TYPE(csd) == CSD_TYPE_SYNC) {\\n\\t\\t\\tsmp_call_func_t func = csd->func;\\n\\t\\t\\tvoid *info = csd->info;\\n\\n\\t\\t\\tif (prev) {\\n\\t\\t\\t\\tprev->next = &csd_next->node.llist;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tentry = &csd_next->node.llist;\\n\\t\\t\\t}\\n\\n\\t\\t\\tcsd_lock_record(csd);\\n\\t\\t\\tcsd_do_func(func, info, csd);\\n\\t\\t\\tcsd_unlock(csd);\\n\\t\\t\\tcsd_lock_record(NULL);\\n\\t\\t} else {\\n\\t\\t\\tprev = &csd->node.llist;\\n\\t\\t}\\n\\t}\\n\\n\\tif (!entry)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Second; run all !SYNC callbacks.\\n\\t */\\n\\tprev = NULL;\\n\\tllist_for_each_entry_safe(csd, csd_next, entry, node.llist) {\\n\\t\\tint type = CSD_TYPE(csd);\\n\\n\\t\\tif (type != CSD_TYPE_TTWU) {\\n\\t\\t\\tif (prev) {\\n\\t\\t\\t\\tprev->next = &csd_next->node.llist;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tentry = &csd_next->node.llist;\\n\\t\\t\\t}\\n\\n\\t\\t\\tif (type == CSD_TYPE_ASYNC) {\\n\\t\\t\\t\\tsmp_call_func_t func = csd->func;\\n\\t\\t\\t\\tvoid *info = csd->info;\\n\\n\\t\\t\\t\\tcsd_lock_record(csd);\\n\\t\\t\\t\\tcsd_unlock(csd);\\n\\t\\t\\t\\tcsd_do_func(func, info, csd);\\n\\t\\t\\t\\tcsd_lock_record(NULL);\\n\\t\\t\\t} else if (type == CSD_TYPE_IRQ_WORK) {\\n\\t\\t\\t\\tirq_work_single(csd);\\n\\t\\t\\t}\\n\\n\\t\\t} else {\\n\\t\\t\\tprev = &csd->node.llist;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * Third; only CSD_TYPE_TTWU is left, issue those.\\n\\t */\\n\\tif (entry) {\\n\\t\\tcsd = llist_entry(entry, typeof(*csd), node.llist);\\n\\t\\tcsd_do_func(sched_ttwu_pending, entry, csd);\\n\\t}\\n}\\n\\n\\n/**\\n * flush_smp_call_function_queue - Flush pending smp-call-function callbacks\\n *\\t\\t\\t\\t   from task context (idle, migration thread)\\n *\\n * When TIF_POLLING_NRFLAG is supported and a CPU is in idle and has it\\n * set, then remote CPUs can avoid sending IPIs and wake the idle CPU by\\n * setting TIF_NEED_RESCHED. The idle task on the woken up CPU has to\\n * handle queued SMP function calls before scheduling.\\n *\\n * The migration thread has to ensure that an eventually pending wakeup has\\n * been handled before it migrates a task.\\n */\\nvoid flush_smp_call_function_queue(void)\\n{\\n\\tunsigned int was_pending;\\n\\tunsigned long flags;\\n\\n\\tif (llist_empty(this_cpu_ptr(&call_single_queue)))\\n\\t\\treturn;\\n\\n\\tlocal_irq_save(flags);\\n\\t/* Get the already pending soft interrupts for RT enabled kernels */\\n\\twas_pending = local_softirq_pending();\\n\\t__flush_smp_call_function_queue(true);\\n\\tif (local_softirq_pending())\\n\\t\\tdo_softirq_post_smp_call_flush(was_pending);\\n\\n\\tlocal_irq_restore(flags);\\n}\\n\\n/*\\n * smp_call_function_single - Run a function on a specific CPU\\n * @func: The function to run. This must be fast and non-blocking.\\n * @info: An arbitrary pointer to pass to the function.\\n * @wait: If true, wait until function has completed on other CPUs.\\n *\\n * Returns 0 on success, else a negative status code.\\n */\\nint smp_call_function_single(int cpu, smp_call_func_t func, void *info,\\n\\t\\t\\t     int wait)\\n{\\n\\tcall_single_data_t *csd;\\n\\tcall_single_data_t csd_stack = {\\n\\t\\t.node = { .u_flags = CSD_FLAG_LOCK | CSD_TYPE_SYNC, },\\n\\t};\\n\\tint this_cpu;\\n\\tint err;\\n\\n\\t/*\\n\\t * prevent preemption and reschedule on another processor,\\n\\t * as well as CPU removal\\n\\t */\\n\\tthis_cpu = get_cpu();\\n\\n\\t/*\\n\\t * Can deadlock when called with interrupts disabled.\\n\\t * We allow cpu\\'s that are not yet online though, as no one else can\\n\\t * send smp call function interrupt to this cpu and as such deadlocks\\n\\t * can\\'t happen.\\n\\t */\\n\\tWARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()\\n\\t\\t     && !oops_in_progress);\\n\\n\\t/*\\n\\t * When @wait we can deadlock when we interrupt between llist_add() and\\n\\t * arch_send_call_function_ipi*(); when !@wait we can deadlock due to\\n\\t * csd_lock() on because the interrupt context uses the same csd\\n\\t * storage.\\n\\t */\\n\\tWARN_ON_ONCE(!in_task());\\n\\n\\tcsd = &csd_stack;\\n\\tif (!wait) {\\n\\t\\tcsd = this_cpu_ptr(&csd_data);\\n\\t\\tcsd_lock(csd);\\n\\t}\\n\\n\\tcsd->func = func;\\n\\tcsd->info = info;\\n#ifdef CONFIG_CSD_LOCK_WAIT_DEBUG\\n\\tcsd->node.src = smp_processor_id();\\n\\tcsd->node.dst = cpu;\\n#endif\\n\\n\\terr = generic_exec_single(cpu, csd);\\n\\n\\tif (wait)\\n\\t\\tcsd_lock_wait(csd);\\n\\n\\tput_cpu();\\n\\n\\treturn err;\\n}\\nEXPORT_SYMBOL(smp_call_function_single);\\n\\n/**\\n * smp_call_function_single_async() - Run an asynchronous function on a\\n * \\t\\t\\t         specific CPU.\\n * @cpu: The CPU to run on.\\n * @csd: Pre-allocated and setup data structure\\n *\\n * Like smp_call_function_single(), but the call is asynchonous and\\n * can thus be done from contexts with disabled interrupts.\\n *\\n * The caller passes his own pre-allocated data structure\\n * (ie: embedded in an object) and is responsible for synchronizing it\\n * such that the IPIs performed on the @csd are strictly serialized.\\n *\\n * If the function is called with one csd which has not yet been\\n * processed by previous call to smp_call_function_single_async(), the\\n * function will return immediately with -EBUSY showing that the csd\\n * object is still in progress.\\n *\\n * NOTE: Be careful, there is unfortunately no current debugging facility to\\n * validate the correctness of this serialization.\\n *\\n * Return: %0 on success or negative errno value on error\\n */\\nint smp_call_function_single_async(int cpu, call_single_data_t *csd)\\n{\\n\\tint err = 0;\\n\\n\\tpreempt_disable();\\n\\n\\tif (csd->node.u_flags & CSD_FLAG_LOCK) {\\n\\t\\terr = -EBUSY;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tcsd->node.u_flags = CSD_FLAG_LOCK;\\n\\tsmp_wmb();\\n\\n\\terr = generic_exec_single(cpu, csd);\\n\\nout:\\n\\tpreempt_enable();\\n\\n\\treturn err;\\n}\\nEXPORT_SYMBOL_GPL(smp_call_function_single_async);\\n\\n/*\\n * smp_call_function_any - Run a function on any of the given cpus\\n * @mask: The mask of cpus it can run on.\\n * @func: The function to run. This must be fast and non-blocking.\\n * @info: An arbitrary pointer to pass to the function.\\n * @wait: If true, wait until function has completed.\\n *\\n * Returns 0 on success, else a negative status code (if no cpus were online).\\n *\\n * Selection preference:\\n *\\t1) current cpu if in @mask\\n *\\t2) any cpu of current node if in @mask\\n *\\t3) any other online cpu in @mask\\n */\\nint smp_call_function_any(const struct cpumask *mask,\\n\\t\\t\\t  smp_call_func_t func, void *info, int wait)\\n{\\n\\tunsigned int cpu;\\n\\tconst struct cpumask *nodemask;\\n\\tint ret;\\n\\n\\t/* Try for same CPU (cheapest) */\\n\\tcpu = get_cpu();\\n\\tif (cpumask_test_cpu(cpu, mask))\\n\\t\\tgoto call;\\n\\n\\t/* Try for same node. */\\n\\tnodemask = cpumask_of_node(cpu_to_node(cpu));\\n\\tfor (cpu = cpumask_first_and(nodemask, mask); cpu < nr_cpu_ids;\\n\\t     cpu = cpumask_next_and(cpu, nodemask, mask)) {\\n\\t\\tif (cpu_online(cpu))\\n\\t\\t\\tgoto call;\\n\\t}\\n\\n\\t/* Any online will do: smp_call_function_single handles nr_cpu_ids. */\\n\\tcpu = cpumask_any_and(mask, cpu_online_mask);\\ncall:\\n\\tret = smp_call_function_single(cpu, func, info, wait);\\n\\tput_cpu();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(smp_call_function_any);\\n\\n/*\\n * Flags to be used as scf_flags argument of smp_call_function_many_cond().\\n *\\n * %SCF_WAIT:\\t\\tWait until function execution is completed\\n * %SCF_RUN_LOCAL:\\tRun also locally if local cpu is set in cpumask\\n */\\n#define SCF_WAIT\\t(1U << 0)\\n#define SCF_RUN_LOCAL\\t(1U << 1)\\n\\nstatic void smp_call_function_many_cond(const struct cpumask *mask,\\n\\t\\t\\t\\t\\tsmp_call_func_t func, void *info,\\n\\t\\t\\t\\t\\tunsigned int scf_flags,\\n\\t\\t\\t\\t\\tsmp_cond_func_t cond_func)\\n{\\n\\tint cpu, last_cpu, this_cpu = smp_processor_id();\\n\\tstruct call_function_data *cfd;\\n\\tbool wait = scf_flags & SCF_WAIT;\\n\\tint nr_cpus = 0;\\n\\tbool run_remote = false;\\n\\tbool run_local = false;\\n\\n\\tlockdep_assert_preemption_disabled();\\n\\n\\t/*\\n\\t * Can deadlock when called with interrupts disabled.\\n\\t * We allow cpu\\'s that are not yet online though, as no one else can\\n\\t * send smp call function interrupt to this cpu and as such deadlocks\\n\\t * can\\'t happen.\\n\\t */\\n\\tif (cpu_online(this_cpu) && !oops_in_progress &&\\n\\t    !early_boot_irqs_disabled)\\n\\t\\tlockdep_assert_irqs_enabled();\\n\\n\\t/*\\n\\t * When @wait we can deadlock when we interrupt between llist_add() and\\n\\t * arch_send_call_function_ipi*(); when !@wait we can deadlock due to\\n\\t * csd_lock() on because the interrupt context uses the same csd\\n\\t * storage.\\n\\t */\\n\\tWARN_ON_ONCE(!in_task());\\n\\n\\t/* Check if we need local execution. */\\n\\tif ((scf_flags & SCF_RUN_LOCAL) && cpumask_test_cpu(this_cpu, mask))\\n\\t\\trun_local = true;\\n\\n\\t/* Check if we need remote execution, i.e., any CPU excluding this one. */\\n\\tcpu = cpumask_first_and(mask, cpu_online_mask);\\n\\tif (cpu == this_cpu)\\n\\t\\tcpu = cpumask_next_and(cpu, mask, cpu_online_mask);\\n\\tif (cpu < nr_cpu_ids)\\n\\t\\trun_remote = true;\\n\\n\\tif (run_remote) {\\n\\t\\tcfd = this_cpu_ptr(&cfd_data);\\n\\t\\tcpumask_and(cfd->cpumask, mask, cpu_online_mask);\\n\\t\\t__cpumask_clear_cpu(this_cpu, cfd->cpumask);\\n\\n\\t\\tcpumask_clear(cfd->cpumask_ipi);\\n\\t\\tfor_each_cpu(cpu, cfd->cpumask) {\\n\\t\\t\\tcall_single_data_t *csd = per_cpu_ptr(cfd->csd, cpu);\\n\\n\\t\\t\\tif (cond_func && !cond_func(cpu, info)) {\\n\\t\\t\\t\\t__cpumask_clear_cpu(cpu, cfd->cpumask);\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\n\\t\\t\\tcsd_lock(csd);\\n\\t\\t\\tif (wait)\\n\\t\\t\\t\\tcsd->node.u_flags |= CSD_TYPE_SYNC;\\n\\t\\t\\tcsd->func = func;\\n\\t\\t\\tcsd->info = info;\\n#ifdef CONFIG_CSD_LOCK_WAIT_DEBUG\\n\\t\\t\\tcsd->node.src = smp_processor_id();\\n\\t\\t\\tcsd->node.dst = cpu;\\n#endif\\n\\t\\t\\ttrace_csd_queue_cpu(cpu, _RET_IP_, func, csd);\\n\\n\\t\\t\\tif (llist_add(&csd->node.llist, &per_cpu(call_single_queue, cpu))) {\\n\\t\\t\\t\\t__cpumask_set_cpu(cpu, cfd->cpumask_ipi);\\n\\t\\t\\t\\tnr_cpus++;\\n\\t\\t\\t\\tlast_cpu = cpu;\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Choose the most efficient way to send an IPI. Note that the\\n\\t\\t * number of CPUs might be zero due to concurrent changes to the\\n\\t\\t * provided mask.\\n\\t\\t */\\n\\t\\tif (nr_cpus == 1)\\n\\t\\t\\tsend_call_function_single_ipi(last_cpu);\\n\\t\\telse if (likely(nr_cpus > 1))\\n\\t\\t\\tsend_call_function_ipi_mask(cfd->cpumask_ipi);\\n\\t}\\n\\n\\tif (run_local && (!cond_func || cond_func(this_cpu, info))) {\\n\\t\\tunsigned long flags;\\n\\n\\t\\tlocal_irq_save(flags);\\n\\t\\tcsd_do_func(func, info, NULL);\\n\\t\\tlocal_irq_restore(flags);\\n\\t}\\n\\n\\tif (run_remote && wait) {\\n\\t\\tfor_each_cpu(cpu, cfd->cpumask) {\\n\\t\\t\\tcall_single_data_t *csd;\\n\\n\\t\\t\\tcsd = per_cpu_ptr(cfd->csd, cpu);\\n\\t\\t\\tcsd_lock_wait(csd);\\n\\t\\t}\\n\\t}\\n}\\n\\n/**\\n * smp_call_function_many(): Run a function on a set of CPUs.\\n * @mask: The set of cpus to run on (only runs on online subset).\\n * @func: The function to run. This must be fast and non-blocking.\\n * @info: An arbitrary pointer to pass to the function.\\n * @wait: Bitmask that controls the operation. If %SCF_WAIT is set, wait\\n *        (atomically) until function has completed on other CPUs. If\\n *        %SCF_RUN_LOCAL is set, the function will also be run locally\\n *        if the local CPU is set in the @cpumask.\\n *\\n * If @wait is true, then returns once @func has returned.\\n *\\n * You must not call this function with disabled interrupts or from a\\n * hardware interrupt handler or from a bottom half handler. Preemption\\n * must be disabled when calling this function.\\n */\\nvoid smp_call_function_many(const struct cpumask *mask,\\n\\t\\t\\t    smp_call_func_t func, void *info, bool wait)\\n{\\n\\tsmp_call_function_many_cond(mask, func, info, wait * SCF_WAIT, NULL);\\n}\\nEXPORT_SYMBOL(smp_call_function_many);\\n\\n/**\\n * smp_call_function(): Run a function on all other CPUs.\\n * @func: The function to run. This must be fast and non-blocking.\\n * @info: An arbitrary pointer to pass to the function.\\n * @wait: If true, wait (atomically) until function has completed\\n *        on other CPUs.\\n *\\n * Returns 0.\\n *\\n * If @wait is true, then returns once @func has returned; otherwise\\n * it returns just before the target cpu calls @func.\\n *\\n * You must not call this function with disabled interrupts or from a\\n * hardware interrupt handler or from a bottom half handler.\\n */\\nvoid smp_call_function(smp_call_func_t func, void *info, int wait)\\n{\\n\\tpreempt_disable();\\n\\tsmp_call_function_many(cpu_online_mask, func, info, wait);\\n\\tpreempt_enable();\\n}\\nEXPORT_SYMBOL(smp_call_function);\\n\\n/* Setup configured maximum number of CPUs to activate */\\nunsigned int setup_max_cpus = NR_CPUS;\\nEXPORT_SYMBOL(setup_max_cpus);\\n\\n\\n/*\\n * Setup routine for controlling SMP activation\\n *\\n * Command-line option of \"nosmp\" or \"maxcpus=0\" will disable SMP\\n * activation entirely (the MPS table probe still happens, though).\\n *\\n * Command-line option of \"maxcpus=<NUM>\", where <NUM> is an integer\\n * greater than 0, limits the maximum number of CPUs activated in\\n * SMP mode to <NUM>.\\n */\\n\\nvoid __weak __init arch_disable_smp_support(void) { }\\n\\nstatic int __init nosmp(char *str)\\n{\\n\\tsetup_max_cpus = 0;\\n\\tarch_disable_smp_support();\\n\\n\\treturn 0;\\n}\\n\\nearly_param(\"nosmp\", nosmp);\\n\\n/* this is hard limit */\\nstatic int __init nrcpus(char *str)\\n{\\n\\tint nr_cpus;\\n\\n\\tif (get_option(&str, &nr_cpus) && nr_cpus > 0 && nr_cpus < nr_cpu_ids)\\n\\t\\tset_nr_cpu_ids(nr_cpus);\\n\\n\\treturn 0;\\n}\\n\\nearly_param(\"nr_cpus\", nrcpus);\\n\\nstatic int __init maxcpus(char *str)\\n{\\n\\tget_option(&str, &setup_max_cpus);\\n\\tif (setup_max_cpus == 0)\\n\\t\\tarch_disable_smp_support();\\n\\n\\treturn 0;\\n}\\n\\nearly_param(\"maxcpus\", maxcpus);\\n\\n#if (NR_CPUS > 1) && !defined(CONFIG_FORCE_NR_CPUS)\\n/* Setup number of possible processor ids */\\nunsigned int nr_cpu_ids __read_mostly = NR_CPUS;\\nEXPORT_SYMBOL(nr_cpu_ids);\\n#endif\\n\\n/* An arch may set nr_cpu_ids earlier if needed, so this would be redundant */\\nvoid __init setup_nr_cpu_ids(void)\\n{\\n\\tset_nr_cpu_ids(find_last_bit(cpumask_bits(cpu_possible_mask), NR_CPUS) + 1);\\n}\\n\\n/* Called by boot processor to activate the rest. */\\nvoid __init smp_init(void)\\n{\\n\\tint num_nodes, num_cpus;\\n\\n\\tidle_threads_init();\\n\\tcpuhp_threads_init();\\n\\n\\tpr_info(\"Bringing up secondary CPUs ...\\\\n\");\\n\\n\\tbringup_nonboot_cpus(setup_max_cpus);\\n\\n\\tnum_nodes = num_online_nodes();\\n\\tnum_cpus  = num_online_cpus();\\n\\tpr_info(\"Brought up %d node%s, %d CPU%s\\\\n\",\\n\\t\\tnum_nodes, str_plural(num_nodes), num_cpus, str_plural(num_cpus));\\n\\n\\t/* Any cleanup work */\\n\\tsmp_cpus_done(setup_max_cpus);\\n}\\n\\n/*\\n * on_each_cpu_cond(): Call a function on each processor for which\\n * the supplied function cond_func returns true, optionally waiting\\n * for all the required CPUs to finish. This may include the local\\n * processor.\\n * @cond_func:\\tA callback function that is passed a cpu id and\\n *\\t\\tthe info parameter. The function is called\\n *\\t\\twith preemption disabled. The function should\\n *\\t\\treturn a blooean value indicating whether to IPI\\n *\\t\\tthe specified CPU.\\n * @func:\\tThe function to run on all applicable CPUs.\\n *\\t\\tThis must be fast and non-blocking.\\n * @info:\\tAn arbitrary pointer to pass to both functions.\\n * @wait:\\tIf true, wait (atomically) until function has\\n *\\t\\tcompleted on other CPUs.\\n *\\n * Preemption is disabled to protect against CPUs going offline but not online.\\n * CPUs going online during the call will not be seen or sent an IPI.\\n *\\n * You must not call this function with disabled interrupts or\\n * from a hardware interrupt handler or from a bottom half handler.\\n */\\nvoid on_each_cpu_cond_mask(smp_cond_func_t cond_func, smp_call_func_t func,\\n\\t\\t\\t   void *info, bool wait, const struct cpumask *mask)\\n{\\n\\tunsigned int scf_flags = SCF_RUN_LOCAL;\\n\\n\\tif (wait)\\n\\t\\tscf_flags |= SCF_WAIT;\\n\\n\\tpreempt_disable();\\n\\tsmp_call_function_many_cond(mask, func, info, scf_flags, cond_func);\\n\\tpreempt_enable();\\n}\\nEXPORT_SYMBOL(on_each_cpu_cond_mask);\\n\\nstatic void do_nothing(void *unused)\\n{\\n}\\n\\n/**\\n * kick_all_cpus_sync - Force all cpus out of idle\\n *\\n * Used to synchronize the update of pm_idle function pointer. It\\'s\\n * called after the pointer is updated and returns after the dummy\\n * callback function has been executed on all cpus. The execution of\\n * the function can only happen on the remote cpus after they have\\n * left the idle function which had been called via pm_idle function\\n * pointer. So it\\'s guaranteed that nothing uses the previous pointer\\n * anymore.\\n */\\nvoid kick_all_cpus_sync(void)\\n{\\n\\t/* Make sure the change is visible before we kick the cpus */\\n\\tsmp_mb();\\n\\tsmp_call_function(do_nothing, NULL, 1);\\n}\\nEXPORT_SYMBOL_GPL(kick_all_cpus_sync);\\n\\n/**\\n * wake_up_all_idle_cpus - break all cpus out of idle\\n * wake_up_all_idle_cpus try to break all cpus which is in idle state even\\n * including idle polling cpus, for non-idle cpus, we will do nothing\\n * for them.\\n */\\nvoid wake_up_all_idle_cpus(void)\\n{\\n\\tint cpu;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tpreempt_disable();\\n\\t\\tif (cpu != smp_processor_id() && cpu_online(cpu))\\n\\t\\t\\twake_up_if_idle(cpu);\\n\\t\\tpreempt_enable();\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(wake_up_all_idle_cpus);\\n\\n/**\\n * struct smp_call_on_cpu_struct - Call a function on a specific CPU\\n * @work: &work_struct\\n * @done: &completion to signal\\n * @func: function to call\\n * @data: function\\'s data argument\\n * @ret: return value from @func\\n * @cpu: target CPU (%-1 for any CPU)\\n *\\n * Used to call a function on a specific cpu and wait for it to return.\\n * Optionally make sure the call is done on a specified physical cpu via vcpu\\n * pinning in order to support virtualized environments.\\n */\\nstruct smp_call_on_cpu_struct {\\n\\tstruct work_struct\\twork;\\n\\tstruct completion\\tdone;\\n\\tint\\t\\t\\t(*func)(void *);\\n\\tvoid\\t\\t\\t*data;\\n\\tint\\t\\t\\tret;\\n\\tint\\t\\t\\tcpu;\\n};\\n\\nstatic void smp_call_on_cpu_callback(struct work_struct *work)\\n{\\n\\tstruct smp_call_on_cpu_struct *sscs;\\n\\n\\tsscs = container_of(work, struct smp_call_on_cpu_struct, work);\\n\\tif (sscs->cpu >= 0)\\n\\t\\thypervisor_pin_vcpu(sscs->cpu);\\n\\tsscs->ret = sscs->func(sscs->data);\\n\\tif (sscs->cpu >= 0)\\n\\t\\thypervisor_pin_vcpu(-1);\\n\\n\\tcomplete(&sscs->done);\\n}\\n\\nint smp_call_on_cpu(unsigned int cpu, int (*func)(void *), void *par, bool phys)\\n{\\n\\tstruct smp_call_on_cpu_struct sscs = {\\n\\t\\t.done = COMPLETION_INITIALIZER_ONSTACK(sscs.done),\\n\\t\\t.func = func,\\n\\t\\t.data = par,\\n\\t\\t.cpu  = phys ? cpu : -1,\\n\\t};\\n\\n\\tINIT_WORK_ONSTACK(&sscs.work, smp_call_on_cpu_callback);\\n\\n\\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\\n\\t\\treturn -ENXIO;\\n\\n\\tqueue_work_on(cpu, system_wq, &sscs.work);\\n\\twait_for_completion(&sscs.done);\\n\\tdestroy_work_on_stack(&sscs.work);\\n\\n\\treturn sscs.ret;\\n}\\nEXPORT_SYMBOL_GPL(smp_call_on_cpu);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * ksyms_common.c: A split of kernel/kallsyms.c\\n * Contains a few generic function definations independent of config KALLSYMS.\\n */\\n#include <linux/kallsyms.h>\\n#include <linux/security.h>\\n\\nstatic inline int kallsyms_for_perf(void)\\n{\\n#ifdef CONFIG_PERF_EVENTS\\n\\textern int sysctl_perf_event_paranoid;\\n\\n\\tif (sysctl_perf_event_paranoid <= 1)\\n\\t\\treturn 1;\\n#endif\\n\\treturn 0;\\n}\\n\\n/*\\n * We show kallsyms information even to normal users if we\\'ve enabled\\n * kernel profiling and are explicitly not paranoid (so kptr_restrict\\n * is clear, and sysctl_perf_event_paranoid isn\\'t set).\\n *\\n * Otherwise, require CAP_SYSLOG (assuming kptr_restrict isn\\'t set to\\n * block even that).\\n */\\nbool kallsyms_show_value(const struct cred *cred)\\n{\\n\\tswitch (kptr_restrict) {\\n\\tcase 0:\\n\\t\\tif (kallsyms_for_perf())\\n\\t\\t\\treturn true;\\n\\t\\tfallthrough;\\n\\tcase 1:\\n\\t\\tif (security_capable(cred, &init_user_ns, CAP_SYSLOG,\\n\\t\\t\\t\\t     CAP_OPT_NOAUDIT) == 0)\\n\\t\\t\\treturn true;\\n\\t\\tfallthrough;\\n\\tdefault:\\n\\t\\treturn false;\\n\\t}\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Pid namespaces\\n *\\n * Authors:\\n *    (C) 2007 Pavel Emelyanov <xemul@openvz.org>, OpenVZ, SWsoft Inc.\\n *    (C) 2007 Sukadev Bhattiprolu <sukadev@us.ibm.com>, IBM\\n *     Many thanks to Oleg Nesterov for comments and help\\n *\\n */\\n\\n#include <linux/pid.h>\\n#include <linux/pid_namespace.h>\\n#include <linux/user_namespace.h>\\n#include <linux/syscalls.h>\\n#include <linux/cred.h>\\n#include <linux/err.h>\\n#include <linux/acct.h>\\n#include <linux/slab.h>\\n#include <linux/proc_ns.h>\\n#include <linux/reboot.h>\\n#include <linux/export.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/signal.h>\\n#include <linux/idr.h>\\n#include <uapi/linux/wait.h>\\n#include \"pid_sysctl.h\"\\n\\nstatic DEFINE_MUTEX(pid_caches_mutex);\\nstatic struct kmem_cache *pid_ns_cachep;\\n/* Write once array, filled from the beginning. */\\nstatic struct kmem_cache *pid_cache[MAX_PID_NS_LEVEL];\\n\\n/*\\n * creates the kmem cache to allocate pids from.\\n * @level: pid namespace level\\n */\\n\\nstatic struct kmem_cache *create_pid_cachep(unsigned int level)\\n{\\n\\t/* Level 0 is init_pid_ns.pid_cachep */\\n\\tstruct kmem_cache **pkc = &pid_cache[level - 1];\\n\\tstruct kmem_cache *kc;\\n\\tchar name[4 + 10 + 1];\\n\\tunsigned int len;\\n\\n\\tkc = READ_ONCE(*pkc);\\n\\tif (kc)\\n\\t\\treturn kc;\\n\\n\\tsnprintf(name, sizeof(name), \"pid_%u\", level + 1);\\n\\tlen = struct_size_t(struct pid, numbers, level + 1);\\n\\tmutex_lock(&pid_caches_mutex);\\n\\t/* Name collision forces to do allocation under mutex. */\\n\\tif (!*pkc)\\n\\t\\t*pkc = kmem_cache_create(name, len, 0,\\n\\t\\t\\t\\t\\t SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT, NULL);\\n\\tmutex_unlock(&pid_caches_mutex);\\n\\t/* current can fail, but someone else can succeed. */\\n\\treturn READ_ONCE(*pkc);\\n}\\n\\nstatic struct ucounts *inc_pid_namespaces(struct user_namespace *ns)\\n{\\n\\treturn inc_ucount(ns, current_euid(), UCOUNT_PID_NAMESPACES);\\n}\\n\\nstatic void dec_pid_namespaces(struct ucounts *ucounts)\\n{\\n\\tdec_ucount(ucounts, UCOUNT_PID_NAMESPACES);\\n}\\n\\nstatic struct pid_namespace *create_pid_namespace(struct user_namespace *user_ns,\\n\\tstruct pid_namespace *parent_pid_ns)\\n{\\n\\tstruct pid_namespace *ns;\\n\\tunsigned int level = parent_pid_ns->level + 1;\\n\\tstruct ucounts *ucounts;\\n\\tint err;\\n\\n\\terr = -EINVAL;\\n\\tif (!in_userns(parent_pid_ns->user_ns, user_ns))\\n\\t\\tgoto out;\\n\\n\\terr = -ENOSPC;\\n\\tif (level > MAX_PID_NS_LEVEL)\\n\\t\\tgoto out;\\n\\tucounts = inc_pid_namespaces(user_ns);\\n\\tif (!ucounts)\\n\\t\\tgoto out;\\n\\n\\terr = -ENOMEM;\\n\\tns = kmem_cache_zalloc(pid_ns_cachep, GFP_KERNEL);\\n\\tif (ns == NULL)\\n\\t\\tgoto out_dec;\\n\\n\\tidr_init(&ns->idr);\\n\\n\\tns->pid_cachep = create_pid_cachep(level);\\n\\tif (ns->pid_cachep == NULL)\\n\\t\\tgoto out_free_idr;\\n\\n\\terr = ns_alloc_inum(&ns->ns);\\n\\tif (err)\\n\\t\\tgoto out_free_idr;\\n\\tns->ns.ops = &pidns_operations;\\n\\n\\trefcount_set(&ns->ns.count, 1);\\n\\tns->level = level;\\n\\tns->parent = get_pid_ns(parent_pid_ns);\\n\\tns->user_ns = get_user_ns(user_ns);\\n\\tns->ucounts = ucounts;\\n\\tns->pid_allocated = PIDNS_ADDING;\\n#if defined(CONFIG_SYSCTL) && defined(CONFIG_MEMFD_CREATE)\\n\\tns->memfd_noexec_scope = pidns_memfd_noexec_scope(parent_pid_ns);\\n#endif\\n\\treturn ns;\\n\\nout_free_idr:\\n\\tidr_destroy(&ns->idr);\\n\\tkmem_cache_free(pid_ns_cachep, ns);\\nout_dec:\\n\\tdec_pid_namespaces(ucounts);\\nout:\\n\\treturn ERR_PTR(err);\\n}\\n\\nstatic void delayed_free_pidns(struct rcu_head *p)\\n{\\n\\tstruct pid_namespace *ns = container_of(p, struct pid_namespace, rcu);\\n\\n\\tdec_pid_namespaces(ns->ucounts);\\n\\tput_user_ns(ns->user_ns);\\n\\n\\tkmem_cache_free(pid_ns_cachep, ns);\\n}\\n\\nstatic void destroy_pid_namespace(struct pid_namespace *ns)\\n{\\n\\tns_free_inum(&ns->ns);\\n\\n\\tidr_destroy(&ns->idr);\\n\\tcall_rcu(&ns->rcu, delayed_free_pidns);\\n}\\n\\nstruct pid_namespace *copy_pid_ns(unsigned long flags,\\n\\tstruct user_namespace *user_ns, struct pid_namespace *old_ns)\\n{\\n\\tif (!(flags & CLONE_NEWPID))\\n\\t\\treturn get_pid_ns(old_ns);\\n\\tif (task_active_pid_ns(current) != old_ns)\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\treturn create_pid_namespace(user_ns, old_ns);\\n}\\n\\nvoid put_pid_ns(struct pid_namespace *ns)\\n{\\n\\tstruct pid_namespace *parent;\\n\\n\\twhile (ns != &init_pid_ns) {\\n\\t\\tparent = ns->parent;\\n\\t\\tif (!refcount_dec_and_test(&ns->ns.count))\\n\\t\\t\\tbreak;\\n\\t\\tdestroy_pid_namespace(ns);\\n\\t\\tns = parent;\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(put_pid_ns);\\n\\nvoid zap_pid_ns_processes(struct pid_namespace *pid_ns)\\n{\\n\\tint nr;\\n\\tint rc;\\n\\tstruct task_struct *task, *me = current;\\n\\tint init_pids = thread_group_leader(me) ? 1 : 2;\\n\\tstruct pid *pid;\\n\\n\\t/* Don\\'t allow any more processes into the pid namespace */\\n\\tdisable_pid_allocation(pid_ns);\\n\\n\\t/*\\n\\t * Ignore SIGCHLD causing any terminated children to autoreap.\\n\\t * This speeds up the namespace shutdown, plus see the comment\\n\\t * below.\\n\\t */\\n\\tspin_lock_irq(&me->sighand->siglock);\\n\\tme->sighand->action[SIGCHLD - 1].sa.sa_handler = SIG_IGN;\\n\\tspin_unlock_irq(&me->sighand->siglock);\\n\\n\\t/*\\n\\t * The last thread in the cgroup-init thread group is terminating.\\n\\t * Find remaining pid_ts in the namespace, signal and wait for them\\n\\t * to exit.\\n\\t *\\n\\t * Note:  This signals each threads in the namespace - even those that\\n\\t * \\t  belong to the same thread group, To avoid this, we would have\\n\\t * \\t  to walk the entire tasklist looking a processes in this\\n\\t * \\t  namespace, but that could be unnecessarily expensive if the\\n\\t * \\t  pid namespace has just a few processes. Or we need to\\n\\t * \\t  maintain a tasklist for each pid namespace.\\n\\t *\\n\\t */\\n\\trcu_read_lock();\\n\\tread_lock(&tasklist_lock);\\n\\tnr = 2;\\n\\tidr_for_each_entry_continue(&pid_ns->idr, pid, nr) {\\n\\t\\ttask = pid_task(pid, PIDTYPE_PID);\\n\\t\\tif (task && !__fatal_signal_pending(task))\\n\\t\\t\\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, task, PIDTYPE_MAX);\\n\\t}\\n\\tread_unlock(&tasklist_lock);\\n\\trcu_read_unlock();\\n\\n\\t/*\\n\\t * Reap the EXIT_ZOMBIE children we had before we ignored SIGCHLD.\\n\\t * kernel_wait4() will also block until our children traced from the\\n\\t * parent namespace are detached and become EXIT_DEAD.\\n\\t */\\n\\tdo {\\n\\t\\tclear_thread_flag(TIF_SIGPENDING);\\n\\t\\tclear_thread_flag(TIF_NOTIFY_SIGNAL);\\n\\t\\trc = kernel_wait4(-1, NULL, __WALL, NULL);\\n\\t} while (rc != -ECHILD);\\n\\n\\t/*\\n\\t * kernel_wait4() misses EXIT_DEAD children, and EXIT_ZOMBIE\\n\\t * process whose parents processes are outside of the pid\\n\\t * namespace.  Such processes are created with setns()+fork().\\n\\t *\\n\\t * If those EXIT_ZOMBIE processes are not reaped by their\\n\\t * parents before their parents exit, they will be reparented\\n\\t * to pid_ns->child_reaper.  Thus pidns->child_reaper needs to\\n\\t * stay valid until they all go away.\\n\\t *\\n\\t * The code relies on the pid_ns->child_reaper ignoring\\n\\t * SIGCHILD to cause those EXIT_ZOMBIE processes to be\\n\\t * autoreaped if reparented.\\n\\t *\\n\\t * Semantically it is also desirable to wait for EXIT_ZOMBIE\\n\\t * processes before allowing the child_reaper to be reaped, as\\n\\t * that gives the invariant that when the init process of a\\n\\t * pid namespace is reaped all of the processes in the pid\\n\\t * namespace are gone.\\n\\t *\\n\\t * Once all of the other tasks are gone from the pid_namespace\\n\\t * free_pid() will awaken this task.\\n\\t */\\n\\tfor (;;) {\\n\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\t\\tif (pid_ns->pid_allocated == init_pids)\\n\\t\\t\\tbreak;\\n\\t\\tschedule();\\n\\t}\\n\\t__set_current_state(TASK_RUNNING);\\n\\n\\tif (pid_ns->reboot)\\n\\t\\tcurrent->signal->group_exit_code = pid_ns->reboot;\\n\\n\\tacct_exit_ns(pid_ns);\\n\\treturn;\\n}\\n\\n#ifdef CONFIG_CHECKPOINT_RESTORE\\nstatic int pid_ns_ctl_handler(const struct ctl_table *table, int write,\\n\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct pid_namespace *pid_ns = task_active_pid_ns(current);\\n\\tstruct ctl_table tmp = *table;\\n\\tint ret, next;\\n\\n\\tif (write && !checkpoint_restore_ns_capable(pid_ns->user_ns))\\n\\t\\treturn -EPERM;\\n\\n\\tnext = idr_get_cursor(&pid_ns->idr) - 1;\\n\\n\\ttmp.data = &next;\\n\\tret = proc_dointvec_minmax(&tmp, write, buffer, lenp, ppos);\\n\\tif (!ret && write)\\n\\t\\tidr_set_cursor(&pid_ns->idr, next + 1);\\n\\n\\treturn ret;\\n}\\n\\nextern int pid_max;\\nstatic struct ctl_table pid_ns_ctl_table[] = {\\n\\t{\\n\\t\\t.procname = \"ns_last_pid\",\\n\\t\\t.maxlen = sizeof(int),\\n\\t\\t.mode = 0666, /* permissions are checked in the handler */\\n\\t\\t.proc_handler = pid_ns_ctl_handler,\\n\\t\\t.extra1 = SYSCTL_ZERO,\\n\\t\\t.extra2 = &pid_max,\\n\\t},\\n};\\n#endif\\t/* CONFIG_CHECKPOINT_RESTORE */\\n\\nint reboot_pid_ns(struct pid_namespace *pid_ns, int cmd)\\n{\\n\\tif (pid_ns == &init_pid_ns)\\n\\t\\treturn 0;\\n\\n\\tswitch (cmd) {\\n\\tcase LINUX_REBOOT_CMD_RESTART2:\\n\\tcase LINUX_REBOOT_CMD_RESTART:\\n\\t\\tpid_ns->reboot = SIGHUP;\\n\\t\\tbreak;\\n\\n\\tcase LINUX_REBOOT_CMD_POWER_OFF:\\n\\tcase LINUX_REBOOT_CMD_HALT:\\n\\t\\tpid_ns->reboot = SIGINT;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tread_lock(&tasklist_lock);\\n\\tsend_sig(SIGKILL, pid_ns->child_reaper, 1);\\n\\tread_unlock(&tasklist_lock);\\n\\n\\tdo_exit(0);\\n\\n\\t/* Not reached */\\n\\treturn 0;\\n}\\n\\nstatic inline struct pid_namespace *to_pid_ns(struct ns_common *ns)\\n{\\n\\treturn container_of(ns, struct pid_namespace, ns);\\n}\\n\\nstatic struct ns_common *pidns_get(struct task_struct *task)\\n{\\n\\tstruct pid_namespace *ns;\\n\\n\\trcu_read_lock();\\n\\tns = task_active_pid_ns(task);\\n\\tif (ns)\\n\\t\\tget_pid_ns(ns);\\n\\trcu_read_unlock();\\n\\n\\treturn ns ? &ns->ns : NULL;\\n}\\n\\nstatic struct ns_common *pidns_for_children_get(struct task_struct *task)\\n{\\n\\tstruct pid_namespace *ns = NULL;\\n\\n\\ttask_lock(task);\\n\\tif (task->nsproxy) {\\n\\t\\tns = task->nsproxy->pid_ns_for_children;\\n\\t\\tget_pid_ns(ns);\\n\\t}\\n\\ttask_unlock(task);\\n\\n\\tif (ns) {\\n\\t\\tread_lock(&tasklist_lock);\\n\\t\\tif (!ns->child_reaper) {\\n\\t\\t\\tput_pid_ns(ns);\\n\\t\\t\\tns = NULL;\\n\\t\\t}\\n\\t\\tread_unlock(&tasklist_lock);\\n\\t}\\n\\n\\treturn ns ? &ns->ns : NULL;\\n}\\n\\nstatic void pidns_put(struct ns_common *ns)\\n{\\n\\tput_pid_ns(to_pid_ns(ns));\\n}\\n\\nstatic int pidns_install(struct nsset *nsset, struct ns_common *ns)\\n{\\n\\tstruct nsproxy *nsproxy = nsset->nsproxy;\\n\\tstruct pid_namespace *active = task_active_pid_ns(current);\\n\\tstruct pid_namespace *ancestor, *new = to_pid_ns(ns);\\n\\n\\tif (!ns_capable(new->user_ns, CAP_SYS_ADMIN) ||\\n\\t    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\t/*\\n\\t * Only allow entering the current active pid namespace\\n\\t * or a child of the current active pid namespace.\\n\\t *\\n\\t * This is required for fork to return a usable pid value and\\n\\t * this maintains the property that processes and their\\n\\t * children can not escape their current pid namespace.\\n\\t */\\n\\tif (new->level < active->level)\\n\\t\\treturn -EINVAL;\\n\\n\\tancestor = new;\\n\\twhile (ancestor->level > active->level)\\n\\t\\tancestor = ancestor->parent;\\n\\tif (ancestor != active)\\n\\t\\treturn -EINVAL;\\n\\n\\tput_pid_ns(nsproxy->pid_ns_for_children);\\n\\tnsproxy->pid_ns_for_children = get_pid_ns(new);\\n\\treturn 0;\\n}\\n\\nstatic struct ns_common *pidns_get_parent(struct ns_common *ns)\\n{\\n\\tstruct pid_namespace *active = task_active_pid_ns(current);\\n\\tstruct pid_namespace *pid_ns, *p;\\n\\n\\t/* See if the parent is in the current namespace */\\n\\tpid_ns = p = to_pid_ns(ns)->parent;\\n\\tfor (;;) {\\n\\t\\tif (!p)\\n\\t\\t\\treturn ERR_PTR(-EPERM);\\n\\t\\tif (p == active)\\n\\t\\t\\tbreak;\\n\\t\\tp = p->parent;\\n\\t}\\n\\n\\treturn &get_pid_ns(pid_ns)->ns;\\n}\\n\\nstatic struct user_namespace *pidns_owner(struct ns_common *ns)\\n{\\n\\treturn to_pid_ns(ns)->user_ns;\\n}\\n\\nconst struct proc_ns_operations pidns_operations = {\\n\\t.name\\t\\t= \"pid\",\\n\\t.type\\t\\t= CLONE_NEWPID,\\n\\t.get\\t\\t= pidns_get,\\n\\t.put\\t\\t= pidns_put,\\n\\t.install\\t= pidns_install,\\n\\t.owner\\t\\t= pidns_owner,\\n\\t.get_parent\\t= pidns_get_parent,\\n};\\n\\nconst struct proc_ns_operations pidns_for_children_operations = {\\n\\t.name\\t\\t= \"pid_for_children\",\\n\\t.real_ns_name\\t= \"pid\",\\n\\t.type\\t\\t= CLONE_NEWPID,\\n\\t.get\\t\\t= pidns_for_children_get,\\n\\t.put\\t\\t= pidns_put,\\n\\t.install\\t= pidns_install,\\n\\t.owner\\t\\t= pidns_owner,\\n\\t.get_parent\\t= pidns_get_parent,\\n};\\n\\nstatic __init int pid_namespaces_init(void)\\n{\\n\\tpid_ns_cachep = KMEM_CACHE(pid_namespace, SLAB_PANIC | SLAB_ACCOUNT);\\n\\n#ifdef CONFIG_CHECKPOINT_RESTORE\\n\\tregister_sysctl_init(\"kernel\", pid_ns_ctl_table);\\n#endif\\n\\n\\tregister_pid_ns_sysctl_table_vm();\\n\\treturn 0;\\n}\\n\\n__initcall(pid_namespaces_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  linux/kernel/profile.c\\n *  Simple profiling. Manages a direct-mapped profile hit count buffer,\\n *  with configurable resolution, support for restricting the cpus on\\n *  which profiling is done, and switching between cpu time and\\n *  schedule() calls via kernel command line parameters passed at boot.\\n *\\n *  Scheduler profiling support, Arjan van de Ven and Ingo Molnar,\\n *\\tRed Hat, July 2004\\n *  Consolidation of architecture support code for profiling,\\n *\\tNadia Yvette Chambers, Oracle, July 2004\\n *  Amortized hit count accounting via per-cpu open-addressed hashtables\\n *\\tto resolve timer interrupt livelocks, Nadia Yvette Chambers,\\n *\\tOracle, 2004\\n */\\n\\n#include <linux/export.h>\\n#include <linux/profile.h>\\n#include <linux/memblock.h>\\n#include <linux/notifier.h>\\n#include <linux/mm.h>\\n#include <linux/cpumask.h>\\n#include <linux/cpu.h>\\n#include <linux/highmem.h>\\n#include <linux/mutex.h>\\n#include <linux/slab.h>\\n#include <linux/vmalloc.h>\\n#include <linux/sched/stat.h>\\n\\n#include <asm/sections.h>\\n#include <asm/irq_regs.h>\\n#include <asm/ptrace.h>\\n\\nstruct profile_hit {\\n\\tu32 pc, hits;\\n};\\n#define PROFILE_GRPSHIFT\\t3\\n#define PROFILE_GRPSZ\\t\\t(1 << PROFILE_GRPSHIFT)\\n#define NR_PROFILE_HIT\\t\\t(PAGE_SIZE/sizeof(struct profile_hit))\\n#define NR_PROFILE_GRP\\t\\t(NR_PROFILE_HIT/PROFILE_GRPSZ)\\n\\nstatic atomic_t *prof_buffer;\\nstatic unsigned long prof_len;\\nstatic unsigned short int prof_shift;\\n\\nint prof_on __read_mostly;\\nEXPORT_SYMBOL_GPL(prof_on);\\n\\nint profile_setup(char *str)\\n{\\n\\tstatic const char schedstr[] = \"schedule\";\\n\\tstatic const char kvmstr[] = \"kvm\";\\n\\tconst char *select = NULL;\\n\\tint par;\\n\\n\\tif (!strncmp(str, schedstr, strlen(schedstr))) {\\n\\t\\tprof_on = SCHED_PROFILING;\\n\\t\\tselect = schedstr;\\n\\t} else if (!strncmp(str, kvmstr, strlen(kvmstr))) {\\n\\t\\tprof_on = KVM_PROFILING;\\n\\t\\tselect = kvmstr;\\n\\t} else if (get_option(&str, &par)) {\\n\\t\\tprof_shift = clamp(par, 0, BITS_PER_LONG - 1);\\n\\t\\tprof_on = CPU_PROFILING;\\n\\t\\tpr_info(\"kernel profiling enabled (shift: %u)\\\\n\",\\n\\t\\t\\tprof_shift);\\n\\t}\\n\\n\\tif (select) {\\n\\t\\tif (str[strlen(select)] == \\',\\')\\n\\t\\t\\tstr += strlen(select) + 1;\\n\\t\\tif (get_option(&str, &par))\\n\\t\\t\\tprof_shift = clamp(par, 0, BITS_PER_LONG - 1);\\n\\t\\tpr_info(\"kernel %s profiling enabled (shift: %u)\\\\n\",\\n\\t\\t\\tselect, prof_shift);\\n\\t}\\n\\n\\treturn 1;\\n}\\n__setup(\"profile=\", profile_setup);\\n\\n\\nint __ref profile_init(void)\\n{\\n\\tint buffer_bytes;\\n\\tif (!prof_on)\\n\\t\\treturn 0;\\n\\n\\t/* only text is profiled */\\n\\tprof_len = (_etext - _stext) >> prof_shift;\\n\\n\\tif (!prof_len) {\\n\\t\\tpr_warn(\"profiling shift: %u too large\\\\n\", prof_shift);\\n\\t\\tprof_on = 0;\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tbuffer_bytes = prof_len*sizeof(atomic_t);\\n\\n\\tprof_buffer = kzalloc(buffer_bytes, GFP_KERNEL|__GFP_NOWARN);\\n\\tif (prof_buffer)\\n\\t\\treturn 0;\\n\\n\\tprof_buffer = alloc_pages_exact(buffer_bytes,\\n\\t\\t\\t\\t\\tGFP_KERNEL|__GFP_ZERO|__GFP_NOWARN);\\n\\tif (prof_buffer)\\n\\t\\treturn 0;\\n\\n\\tprof_buffer = vzalloc(buffer_bytes);\\n\\tif (prof_buffer)\\n\\t\\treturn 0;\\n\\n\\treturn -ENOMEM;\\n}\\n\\nstatic void do_profile_hits(int type, void *__pc, unsigned int nr_hits)\\n{\\n\\tunsigned long pc;\\n\\tpc = ((unsigned long)__pc - (unsigned long)_stext) >> prof_shift;\\n\\tif (pc < prof_len)\\n\\t\\tatomic_add(nr_hits, &prof_buffer[pc]);\\n}\\n\\nvoid profile_hits(int type, void *__pc, unsigned int nr_hits)\\n{\\n\\tif (prof_on != type || !prof_buffer)\\n\\t\\treturn;\\n\\tdo_profile_hits(type, __pc, nr_hits);\\n}\\nEXPORT_SYMBOL_GPL(profile_hits);\\n\\nvoid profile_tick(int type)\\n{\\n\\tstruct pt_regs *regs = get_irq_regs();\\n\\n\\t/* This is the old kernel-only legacy profiling */\\n\\tif (!user_mode(regs))\\n\\t\\tprofile_hit(type, (void *)profile_pc(regs));\\n}\\n\\n#ifdef CONFIG_PROC_FS\\n#include <linux/proc_fs.h>\\n#include <linux/seq_file.h>\\n#include <linux/uaccess.h>\\n\\n/*\\n * This function accesses profiling information. The returned data is\\n * binary: the sampling step and the actual contents of the profile\\n * buffer. Use of the program readprofile is recommended in order to\\n * get meaningful info out of these data.\\n */\\nstatic ssize_t\\nread_profile(struct file *file, char __user *buf, size_t count, loff_t *ppos)\\n{\\n\\tunsigned long p = *ppos;\\n\\tssize_t read;\\n\\tchar *pnt;\\n\\tunsigned long sample_step = 1UL << prof_shift;\\n\\n\\tif (p >= (prof_len+1)*sizeof(unsigned int))\\n\\t\\treturn 0;\\n\\tif (count > (prof_len+1)*sizeof(unsigned int) - p)\\n\\t\\tcount = (prof_len+1)*sizeof(unsigned int) - p;\\n\\tread = 0;\\n\\n\\twhile (p < sizeof(unsigned int) && count > 0) {\\n\\t\\tif (put_user(*((char *)(&sample_step)+p), buf))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tbuf++; p++; count--; read++;\\n\\t}\\n\\tpnt = (char *)prof_buffer + p - sizeof(atomic_t);\\n\\tif (copy_to_user(buf, (void *)pnt, count))\\n\\t\\treturn -EFAULT;\\n\\tread += count;\\n\\t*ppos += read;\\n\\treturn read;\\n}\\n\\n/* default is to not implement this call */\\nint __weak setup_profiling_timer(unsigned mult)\\n{\\n\\treturn -EINVAL;\\n}\\n\\n/*\\n * Writing to /proc/profile resets the counters\\n *\\n * Writing a \\'profiling multiplier\\' value into it also re-sets the profiling\\n * interrupt frequency, on architectures that support this.\\n */\\nstatic ssize_t write_profile(struct file *file, const char __user *buf,\\n\\t\\t\\t     size_t count, loff_t *ppos)\\n{\\n#ifdef CONFIG_SMP\\n\\tif (count == sizeof(int)) {\\n\\t\\tunsigned int multiplier;\\n\\n\\t\\tif (copy_from_user(&multiplier, buf, sizeof(int)))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\tif (setup_profiling_timer(multiplier))\\n\\t\\t\\treturn -EINVAL;\\n\\t}\\n#endif\\n\\tmemset(prof_buffer, 0, prof_len * sizeof(atomic_t));\\n\\treturn count;\\n}\\n\\nstatic const struct proc_ops profile_proc_ops = {\\n\\t.proc_read\\t= read_profile,\\n\\t.proc_write\\t= write_profile,\\n\\t.proc_lseek\\t= default_llseek,\\n};\\n\\nint __ref create_proc_profile(void)\\n{\\n\\tstruct proc_dir_entry *entry;\\n\\tint err = 0;\\n\\n\\tif (!prof_on)\\n\\t\\treturn 0;\\n\\tentry = proc_create(\"profile\", S_IWUSR | S_IRUGO,\\n\\t\\t\\t    NULL, &profile_proc_ops);\\n\\tif (entry)\\n\\t\\tproc_set_size(entry, (1 + prof_len) * sizeof(atomic_t));\\n\\treturn err;\\n}\\nsubsys_initcall(create_proc_profile);\\n#endif /* CONFIG_PROC_FS */\\n\\n// SPDX-License-Identifier: GPL-2.0+\\n/*\\n * Common functions for in-kernel torture tests.\\n *\\n * Copyright (C) IBM Corporation, 2014\\n *\\n * Author: Paul E. McKenney <paulmck@linux.ibm.com>\\n *\\tBased on kernel/rcu/torture.c.\\n */\\n\\n#define pr_fmt(fmt) fmt\\n\\n#include <linux/types.h>\\n#include <linux/kernel.h>\\n#include <linux/init.h>\\n#include <linux/module.h>\\n#include <linux/kthread.h>\\n#include <linux/err.h>\\n#include <linux/spinlock.h>\\n#include <linux/smp.h>\\n#include <linux/interrupt.h>\\n#include <linux/sched.h>\\n#include <linux/sched/clock.h>\\n#include <linux/atomic.h>\\n#include <linux/bitops.h>\\n#include <linux/completion.h>\\n#include <linux/moduleparam.h>\\n#include <linux/percpu.h>\\n#include <linux/notifier.h>\\n#include <linux/reboot.h>\\n#include <linux/freezer.h>\\n#include <linux/cpu.h>\\n#include <linux/delay.h>\\n#include <linux/stat.h>\\n#include <linux/slab.h>\\n#include <linux/trace_clock.h>\\n#include <linux/ktime.h>\\n#include <asm/byteorder.h>\\n#include <linux/torture.h>\\n#include <linux/sched/rt.h>\\n#include \"rcu/rcu.h\"\\n\\nMODULE_DESCRIPTION(\"Common functions for in-kernel torture tests\");\\nMODULE_LICENSE(\"GPL\");\\nMODULE_AUTHOR(\"Paul E. McKenney <paulmck@linux.ibm.com>\");\\n\\nstatic bool disable_onoff_at_boot;\\nmodule_param(disable_onoff_at_boot, bool, 0444);\\n\\nstatic bool ftrace_dump_at_shutdown;\\nmodule_param(ftrace_dump_at_shutdown, bool, 0444);\\n\\nstatic int verbose_sleep_frequency;\\nmodule_param(verbose_sleep_frequency, int, 0444);\\n\\nstatic int verbose_sleep_duration = 1;\\nmodule_param(verbose_sleep_duration, int, 0444);\\n\\nstatic int random_shuffle;\\nmodule_param(random_shuffle, int, 0444);\\n\\nstatic char *torture_type;\\nstatic int verbose;\\n\\n/* Mediate rmmod and system shutdown.  Concurrent rmmod & shutdown illegal! */\\n#define FULLSTOP_DONTSTOP 0\\t/* Normal operation. */\\n#define FULLSTOP_SHUTDOWN 1\\t/* System shutdown with torture running. */\\n#define FULLSTOP_RMMOD    2\\t/* Normal rmmod of torture. */\\nstatic int fullstop = FULLSTOP_RMMOD;\\nstatic DEFINE_MUTEX(fullstop_mutex);\\n\\nstatic atomic_t verbose_sleep_counter;\\n\\n/*\\n * Sleep if needed from VERBOSE_TOROUT*().\\n */\\nvoid verbose_torout_sleep(void)\\n{\\n\\tif (verbose_sleep_frequency > 0 &&\\n\\t    verbose_sleep_duration > 0 &&\\n\\t    !(atomic_inc_return(&verbose_sleep_counter) % verbose_sleep_frequency))\\n\\t\\tschedule_timeout_uninterruptible(verbose_sleep_duration);\\n}\\nEXPORT_SYMBOL_GPL(verbose_torout_sleep);\\n\\n/*\\n * Schedule a high-resolution-timer sleep in nanoseconds, with a 32-bit\\n * nanosecond random fuzz.  This function and its friends desynchronize\\n * testing from the timer wheel.\\n */\\nint torture_hrtimeout_ns(ktime_t baset_ns, u32 fuzzt_ns, const enum hrtimer_mode mode,\\n\\t\\t\\t struct torture_random_state *trsp)\\n{\\n\\tktime_t hto = baset_ns;\\n\\n\\tif (trsp)\\n\\t\\thto += torture_random(trsp) % fuzzt_ns;\\n\\tset_current_state(TASK_IDLE);\\n\\treturn schedule_hrtimeout(&hto, mode);\\n}\\nEXPORT_SYMBOL_GPL(torture_hrtimeout_ns);\\n\\n/*\\n * Schedule a high-resolution-timer sleep in microseconds, with a 32-bit\\n * nanosecond (not microsecond!) random fuzz.\\n */\\nint torture_hrtimeout_us(u32 baset_us, u32 fuzzt_ns, struct torture_random_state *trsp)\\n{\\n\\tktime_t baset_ns = baset_us * NSEC_PER_USEC;\\n\\n\\treturn torture_hrtimeout_ns(baset_ns, fuzzt_ns, HRTIMER_MODE_REL, trsp);\\n}\\nEXPORT_SYMBOL_GPL(torture_hrtimeout_us);\\n\\n/*\\n * Schedule a high-resolution-timer sleep in milliseconds, with a 32-bit\\n * microsecond (not millisecond!) random fuzz.\\n */\\nint torture_hrtimeout_ms(u32 baset_ms, u32 fuzzt_us, struct torture_random_state *trsp)\\n{\\n\\tktime_t baset_ns = baset_ms * NSEC_PER_MSEC;\\n\\tu32 fuzzt_ns;\\n\\n\\tif ((u32)~0U / NSEC_PER_USEC < fuzzt_us)\\n\\t\\tfuzzt_ns = (u32)~0U;\\n\\telse\\n\\t\\tfuzzt_ns = fuzzt_us * NSEC_PER_USEC;\\n\\treturn torture_hrtimeout_ns(baset_ns, fuzzt_ns, HRTIMER_MODE_REL, trsp);\\n}\\nEXPORT_SYMBOL_GPL(torture_hrtimeout_ms);\\n\\n/*\\n * Schedule a high-resolution-timer sleep in jiffies, with an\\n * implied one-jiffy random fuzz.  This is intended to replace calls to\\n * schedule_timeout_interruptible() and friends.\\n */\\nint torture_hrtimeout_jiffies(u32 baset_j, struct torture_random_state *trsp)\\n{\\n\\tktime_t baset_ns = jiffies_to_nsecs(baset_j);\\n\\n\\treturn torture_hrtimeout_ns(baset_ns, jiffies_to_nsecs(1), HRTIMER_MODE_REL, trsp);\\n}\\nEXPORT_SYMBOL_GPL(torture_hrtimeout_jiffies);\\n\\n/*\\n * Schedule a high-resolution-timer sleep in milliseconds, with a 32-bit\\n * millisecond (not second!) random fuzz.\\n */\\nint torture_hrtimeout_s(u32 baset_s, u32 fuzzt_ms, struct torture_random_state *trsp)\\n{\\n\\tktime_t baset_ns = baset_s * NSEC_PER_SEC;\\n\\tu32 fuzzt_ns;\\n\\n\\tif ((u32)~0U / NSEC_PER_MSEC < fuzzt_ms)\\n\\t\\tfuzzt_ns = (u32)~0U;\\n\\telse\\n\\t\\tfuzzt_ns = fuzzt_ms * NSEC_PER_MSEC;\\n\\treturn torture_hrtimeout_ns(baset_ns, fuzzt_ns, HRTIMER_MODE_REL, trsp);\\n}\\nEXPORT_SYMBOL_GPL(torture_hrtimeout_s);\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\n/*\\n * Variables for online-offline handling.  Only present if CPU hotplug\\n * is enabled, otherwise does nothing.\\n */\\n\\nstatic struct task_struct *onoff_task;\\nstatic long onoff_holdoff;\\nstatic long onoff_interval;\\nstatic torture_ofl_func *onoff_f;\\nstatic long n_offline_attempts;\\nstatic long n_offline_successes;\\nstatic unsigned long sum_offline;\\nstatic int min_offline = -1;\\nstatic int max_offline;\\nstatic long n_online_attempts;\\nstatic long n_online_successes;\\nstatic unsigned long sum_online;\\nstatic int min_online = -1;\\nstatic int max_online;\\n\\nstatic int torture_online_cpus = NR_CPUS;\\n\\n/*\\n * Some torture testing leverages confusion as to the number of online\\n * CPUs.  This function returns the torture-testing view of this number,\\n * which allows torture tests to load-balance appropriately.\\n */\\nint torture_num_online_cpus(void)\\n{\\n\\treturn READ_ONCE(torture_online_cpus);\\n}\\nEXPORT_SYMBOL_GPL(torture_num_online_cpus);\\n\\n/*\\n * Attempt to take a CPU offline.  Return false if the CPU is already\\n * offline or if it is not subject to CPU-hotplug operations.  The\\n * caller can detect other failures by looking at the statistics.\\n */\\nbool torture_offline(int cpu, long *n_offl_attempts, long *n_offl_successes,\\n\\t\\t     unsigned long *sum_offl, int *min_offl, int *max_offl)\\n{\\n\\tunsigned long delta;\\n\\tint ret;\\n\\tchar *s;\\n\\tunsigned long starttime;\\n\\n\\tif (!cpu_online(cpu) || !cpu_is_hotpluggable(cpu))\\n\\t\\treturn false;\\n\\tif (num_online_cpus() <= 1)\\n\\t\\treturn false;  /* Can\\'t offline the last CPU. */\\n\\n\\tif (verbose > 1)\\n\\t\\tpr_alert(\"%s\" TORTURE_FLAG\\n\\t\\t\\t \"torture_onoff task: offlining %d\\\\n\",\\n\\t\\t\\t torture_type, cpu);\\n\\tstarttime = jiffies;\\n\\t(*n_offl_attempts)++;\\n\\tret = remove_cpu(cpu);\\n\\tif (ret) {\\n\\t\\ts = \"\";\\n\\t\\tif (!rcu_inkernel_boot_has_ended() && ret == -EBUSY) {\\n\\t\\t\\t// PCI probe frequently disables hotplug during boot.\\n\\t\\t\\t(*n_offl_attempts)--;\\n\\t\\t\\ts = \" (-EBUSY forgiven during boot)\";\\n\\t\\t}\\n\\t\\tif (verbose)\\n\\t\\t\\tpr_alert(\"%s\" TORTURE_FLAG\\n\\t\\t\\t\\t \"torture_onoff task: offline %d failed%s: errno %d\\\\n\",\\n\\t\\t\\t\\t torture_type, cpu, s, ret);\\n\\t} else {\\n\\t\\tif (verbose > 1)\\n\\t\\t\\tpr_alert(\"%s\" TORTURE_FLAG\\n\\t\\t\\t\\t \"torture_onoff task: offlined %d\\\\n\",\\n\\t\\t\\t\\t torture_type, cpu);\\n\\t\\tif (onoff_f)\\n\\t\\t\\tonoff_f();\\n\\t\\t(*n_offl_successes)++;\\n\\t\\tdelta = jiffies - starttime;\\n\\t\\t*sum_offl += delta;\\n\\t\\tif (*min_offl < 0) {\\n\\t\\t\\t*min_offl = delta;\\n\\t\\t\\t*max_offl = delta;\\n\\t\\t}\\n\\t\\tif (*min_offl > delta)\\n\\t\\t\\t*min_offl = delta;\\n\\t\\tif (*max_offl < delta)\\n\\t\\t\\t*max_offl = delta;\\n\\t\\tWRITE_ONCE(torture_online_cpus, torture_online_cpus - 1);\\n\\t\\tWARN_ON_ONCE(torture_online_cpus <= 0);\\n\\t}\\n\\n\\treturn true;\\n}\\nEXPORT_SYMBOL_GPL(torture_offline);\\n\\n/*\\n * Attempt to bring a CPU online.  Return false if the CPU is already\\n * online or if it is not subject to CPU-hotplug operations.  The\\n * caller can detect other failures by looking at the statistics.\\n */\\nbool torture_online(int cpu, long *n_onl_attempts, long *n_onl_successes,\\n\\t\\t    unsigned long *sum_onl, int *min_onl, int *max_onl)\\n{\\n\\tunsigned long delta;\\n\\tint ret;\\n\\tchar *s;\\n\\tunsigned long starttime;\\n\\n\\tif (cpu_online(cpu) || !cpu_is_hotpluggable(cpu))\\n\\t\\treturn false;\\n\\n\\tif (verbose > 1)\\n\\t\\tpr_alert(\"%s\" TORTURE_FLAG\\n\\t\\t\\t \"torture_onoff task: onlining %d\\\\n\",\\n\\t\\t\\t torture_type, cpu);\\n\\tstarttime = jiffies;\\n\\t(*n_onl_attempts)++;\\n\\tret = add_cpu(cpu);\\n\\tif (ret) {\\n\\t\\ts = \"\";\\n\\t\\tif (!rcu_inkernel_boot_has_ended() && ret == -EBUSY) {\\n\\t\\t\\t// PCI probe frequently disables hotplug during boot.\\n\\t\\t\\t(*n_onl_attempts)--;\\n\\t\\t\\ts = \" (-EBUSY forgiven during boot)\";\\n\\t\\t}\\n\\t\\tif (verbose)\\n\\t\\t\\tpr_alert(\"%s\" TORTURE_FLAG\\n\\t\\t\\t\\t \"torture_onoff task: online %d failed%s: errno %d\\\\n\",\\n\\t\\t\\t\\t torture_type, cpu, s, ret);\\n\\t} else {\\n\\t\\tif (verbose > 1)\\n\\t\\t\\tpr_alert(\"%s\" TORTURE_FLAG\\n\\t\\t\\t\\t \"torture_onoff task: onlined %d\\\\n\",\\n\\t\\t\\t\\t torture_type, cpu);\\n\\t\\t(*n_onl_successes)++;\\n\\t\\tdelta = jiffies - starttime;\\n\\t\\t*sum_onl += delta;\\n\\t\\tif (*min_onl < 0) {\\n\\t\\t\\t*min_onl = delta;\\n\\t\\t\\t*max_onl = delta;\\n\\t\\t}\\n\\t\\tif (*min_onl > delta)\\n\\t\\t\\t*min_onl = delta;\\n\\t\\tif (*max_onl < delta)\\n\\t\\t\\t*max_onl = delta;\\n\\t\\tWRITE_ONCE(torture_online_cpus, torture_online_cpus + 1);\\n\\t}\\n\\n\\treturn true;\\n}\\nEXPORT_SYMBOL_GPL(torture_online);\\n\\n/*\\n * Get everything online at the beginning and ends of tests.\\n */\\nstatic void torture_online_all(char *phase)\\n{\\n\\tint cpu;\\n\\tint ret;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tif (cpu_online(cpu))\\n\\t\\t\\tcontinue;\\n\\t\\tret = add_cpu(cpu);\\n\\t\\tif (ret && verbose) {\\n\\t\\t\\tpr_alert(\"%s\" TORTURE_FLAG\\n\\t\\t\\t\\t \"%s: %s online %d: errno %d\\\\n\",\\n\\t\\t\\t\\t __func__, phase, torture_type, cpu, ret);\\n\\t\\t}\\n\\t}\\n}\\n\\n/*\\n * Execute random CPU-hotplug operations at the interval specified\\n * by the onoff_interval.\\n */\\nstatic int\\ntorture_onoff(void *arg)\\n{\\n\\tint cpu;\\n\\tint maxcpu = -1;\\n\\tDEFINE_TORTURE_RANDOM(rand);\\n\\n\\tVERBOSE_TOROUT_STRING(\"torture_onoff task started\");\\n\\tfor_each_online_cpu(cpu)\\n\\t\\tmaxcpu = cpu;\\n\\tWARN_ON(maxcpu < 0);\\n\\ttorture_online_all(\"Initial\");\\n\\tif (maxcpu == 0) {\\n\\t\\tVERBOSE_TOROUT_STRING(\"Only one CPU, so CPU-hotplug testing is disabled\");\\n\\t\\tgoto stop;\\n\\t}\\n\\n\\tif (onoff_holdoff > 0) {\\n\\t\\tVERBOSE_TOROUT_STRING(\"torture_onoff begin holdoff\");\\n\\t\\ttorture_hrtimeout_jiffies(onoff_holdoff, &rand);\\n\\t\\tVERBOSE_TOROUT_STRING(\"torture_onoff end holdoff\");\\n\\t}\\n\\twhile (!torture_must_stop()) {\\n\\t\\tif (disable_onoff_at_boot && !rcu_inkernel_boot_has_ended()) {\\n\\t\\t\\ttorture_hrtimeout_jiffies(HZ / 10, &rand);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tcpu = torture_random(&rand) % (maxcpu + 1);\\n\\t\\tif (!torture_offline(cpu,\\n\\t\\t\\t\\t     &n_offline_attempts, &n_offline_successes,\\n\\t\\t\\t\\t     &sum_offline, &min_offline, &max_offline))\\n\\t\\t\\ttorture_online(cpu,\\n\\t\\t\\t\\t       &n_online_attempts, &n_online_successes,\\n\\t\\t\\t\\t       &sum_online, &min_online, &max_online);\\n\\t\\ttorture_hrtimeout_jiffies(onoff_interval, &rand);\\n\\t}\\n\\nstop:\\n\\ttorture_kthread_stopping(\"torture_onoff\");\\n\\ttorture_online_all(\"Final\");\\n\\treturn 0;\\n}\\n\\n#endif /* #ifdef CONFIG_HOTPLUG_CPU */\\n\\n/*\\n * Initiate online-offline handling.\\n */\\nint torture_onoff_init(long ooholdoff, long oointerval, torture_ofl_func *f)\\n{\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\tonoff_holdoff = ooholdoff;\\n\\tonoff_interval = oointerval;\\n\\tonoff_f = f;\\n\\tif (onoff_interval <= 0)\\n\\t\\treturn 0;\\n\\treturn torture_create_kthread(torture_onoff, NULL, onoff_task);\\n#else /* #ifdef CONFIG_HOTPLUG_CPU */\\n\\treturn 0;\\n#endif /* #else #ifdef CONFIG_HOTPLUG_CPU */\\n}\\nEXPORT_SYMBOL_GPL(torture_onoff_init);\\n\\n/*\\n * Clean up after online/offline testing.\\n */\\nstatic void torture_onoff_cleanup(void)\\n{\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\tif (onoff_task == NULL)\\n\\t\\treturn;\\n\\tVERBOSE_TOROUT_STRING(\"Stopping torture_onoff task\");\\n\\tkthread_stop(onoff_task);\\n\\tonoff_task = NULL;\\n#endif /* #ifdef CONFIG_HOTPLUG_CPU */\\n}\\n\\n/*\\n * Print online/offline testing statistics.\\n */\\nvoid torture_onoff_stats(void)\\n{\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\tpr_cont(\"onoff: %ld/%ld:%ld/%ld %d,%d:%d,%d %lu:%lu (HZ=%d) \",\\n\\t\\tn_online_successes, n_online_attempts,\\n\\t\\tn_offline_successes, n_offline_attempts,\\n\\t\\tmin_online, max_online,\\n\\t\\tmin_offline, max_offline,\\n\\t\\tsum_online, sum_offline, HZ);\\n#endif /* #ifdef CONFIG_HOTPLUG_CPU */\\n}\\nEXPORT_SYMBOL_GPL(torture_onoff_stats);\\n\\n/*\\n * Were all the online/offline operations successful?\\n */\\nbool torture_onoff_failures(void)\\n{\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\treturn n_online_successes != n_online_attempts ||\\n\\t       n_offline_successes != n_offline_attempts;\\n#else /* #ifdef CONFIG_HOTPLUG_CPU */\\n\\treturn false;\\n#endif /* #else #ifdef CONFIG_HOTPLUG_CPU */\\n}\\nEXPORT_SYMBOL_GPL(torture_onoff_failures);\\n\\n#define TORTURE_RANDOM_MULT\\t39916801  /* prime */\\n#define TORTURE_RANDOM_ADD\\t479001701 /* prime */\\n#define TORTURE_RANDOM_REFRESH\\t10000\\n\\n/*\\n * Crude but fast random-number generator.  Uses a linear congruential\\n * generator, with occasional help from cpu_clock().\\n */\\nunsigned long\\ntorture_random(struct torture_random_state *trsp)\\n{\\n\\tif (--trsp->trs_count < 0) {\\n\\t\\ttrsp->trs_state += (unsigned long)local_clock() + raw_smp_processor_id();\\n\\t\\ttrsp->trs_count = TORTURE_RANDOM_REFRESH;\\n\\t}\\n\\ttrsp->trs_state = trsp->trs_state * TORTURE_RANDOM_MULT +\\n\\t\\tTORTURE_RANDOM_ADD;\\n\\treturn swahw32(trsp->trs_state);\\n}\\nEXPORT_SYMBOL_GPL(torture_random);\\n\\n/*\\n * Variables for shuffling.  The idea is to ensure that each CPU stays\\n * idle for an extended period to test interactions with dyntick idle,\\n * as well as interactions with any per-CPU variables.\\n */\\nstruct shuffle_task {\\n\\tstruct list_head st_l;\\n\\tstruct task_struct *st_t;\\n};\\n\\nstatic long shuffle_interval;\\t/* In jiffies. */\\nstatic struct task_struct *shuffler_task;\\nstatic cpumask_var_t shuffle_tmp_mask;\\nstatic int shuffle_idle_cpu;\\t/* Force all torture tasks off this CPU */\\nstatic struct list_head shuffle_task_list = LIST_HEAD_INIT(shuffle_task_list);\\nstatic DEFINE_MUTEX(shuffle_task_mutex);\\n\\n/*\\n * Register a task to be shuffled.  If there is no memory, just splat\\n * and don\\'t bother registering.\\n */\\nvoid torture_shuffle_task_register(struct task_struct *tp)\\n{\\n\\tstruct shuffle_task *stp;\\n\\n\\tif (WARN_ON_ONCE(tp == NULL))\\n\\t\\treturn;\\n\\tstp = kmalloc(sizeof(*stp), GFP_KERNEL);\\n\\tif (WARN_ON_ONCE(stp == NULL))\\n\\t\\treturn;\\n\\tstp->st_t = tp;\\n\\tmutex_lock(&shuffle_task_mutex);\\n\\tlist_add(&stp->st_l, &shuffle_task_list);\\n\\tmutex_unlock(&shuffle_task_mutex);\\n}\\nEXPORT_SYMBOL_GPL(torture_shuffle_task_register);\\n\\n/*\\n * Unregister all tasks, for example, at the end of the torture run.\\n */\\nstatic void torture_shuffle_task_unregister_all(void)\\n{\\n\\tstruct shuffle_task *stp;\\n\\tstruct shuffle_task *p;\\n\\n\\tmutex_lock(&shuffle_task_mutex);\\n\\tlist_for_each_entry_safe(stp, p, &shuffle_task_list, st_l) {\\n\\t\\tlist_del(&stp->st_l);\\n\\t\\tkfree(stp);\\n\\t}\\n\\tmutex_unlock(&shuffle_task_mutex);\\n}\\n\\n/* Shuffle tasks such that we allow shuffle_idle_cpu to become idle.\\n * A special case is when shuffle_idle_cpu = -1, in which case we allow\\n * the tasks to run on all CPUs.\\n */\\nstatic void torture_shuffle_tasks(struct torture_random_state *trp)\\n{\\n\\tstruct shuffle_task *stp;\\n\\n\\tcpumask_setall(shuffle_tmp_mask);\\n\\tcpus_read_lock();\\n\\n\\t/* No point in shuffling if there is only one online CPU (ex: UP) */\\n\\tif (num_online_cpus() == 1) {\\n\\t\\tcpus_read_unlock();\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* Advance to the next CPU.  Upon overflow, don\\'t idle any CPUs. */\\n\\tshuffle_idle_cpu = cpumask_next(shuffle_idle_cpu, shuffle_tmp_mask);\\n\\tif (shuffle_idle_cpu >= nr_cpu_ids)\\n\\t\\tshuffle_idle_cpu = -1;\\n\\telse\\n\\t\\tcpumask_clear_cpu(shuffle_idle_cpu, shuffle_tmp_mask);\\n\\n\\tmutex_lock(&shuffle_task_mutex);\\n\\tlist_for_each_entry(stp, &shuffle_task_list, st_l) {\\n\\t\\tif (!random_shuffle || torture_random(trp) & 0x1)\\n\\t\\t\\tset_cpus_allowed_ptr(stp->st_t, shuffle_tmp_mask);\\n\\t}\\n\\tmutex_unlock(&shuffle_task_mutex);\\n\\n\\tcpus_read_unlock();\\n}\\n\\n/* Shuffle tasks across CPUs, with the intent of allowing each CPU in the\\n * system to become idle at a time and cut off its timer ticks. This is meant\\n * to test the support for such tickless idle CPU in RCU.\\n */\\nstatic int torture_shuffle(void *arg)\\n{\\n\\tDEFINE_TORTURE_RANDOM(rand);\\n\\n\\tVERBOSE_TOROUT_STRING(\"torture_shuffle task started\");\\n\\tdo {\\n\\t\\ttorture_hrtimeout_jiffies(shuffle_interval, &rand);\\n\\t\\ttorture_shuffle_tasks(&rand);\\n\\t\\ttorture_shutdown_absorb(\"torture_shuffle\");\\n\\t} while (!torture_must_stop());\\n\\ttorture_kthread_stopping(\"torture_shuffle\");\\n\\treturn 0;\\n}\\n\\n/*\\n * Start the shuffler, with shuffint in jiffies.\\n */\\nint torture_shuffle_init(long shuffint)\\n{\\n\\tshuffle_interval = shuffint;\\n\\n\\tshuffle_idle_cpu = -1;\\n\\n\\tif (!alloc_cpumask_var(&shuffle_tmp_mask, GFP_KERNEL)) {\\n\\t\\tTOROUT_ERRSTRING(\"Failed to alloc mask\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\t/* Create the shuffler thread */\\n\\treturn torture_create_kthread(torture_shuffle, NULL, shuffler_task);\\n}\\nEXPORT_SYMBOL_GPL(torture_shuffle_init);\\n\\n/*\\n * Stop the shuffling.\\n */\\nstatic void torture_shuffle_cleanup(void)\\n{\\n\\ttorture_shuffle_task_unregister_all();\\n\\tif (shuffler_task) {\\n\\t\\tVERBOSE_TOROUT_STRING(\"Stopping torture_shuffle task\");\\n\\t\\tkthread_stop(shuffler_task);\\n\\t\\tfree_cpumask_var(shuffle_tmp_mask);\\n\\t}\\n\\tshuffler_task = NULL;\\n}\\n\\n/*\\n * Variables for auto-shutdown.  This allows \"lights out\" torture runs\\n * to be fully scripted.\\n */\\nstatic struct task_struct *shutdown_task;\\nstatic ktime_t shutdown_time;\\t\\t/* time to system shutdown. */\\nstatic void (*torture_shutdown_hook)(void);\\n\\n/*\\n * Absorb kthreads into a kernel function that won\\'t return, so that\\n * they won\\'t ever access module text or data again.\\n */\\nvoid torture_shutdown_absorb(const char *title)\\n{\\n\\twhile (READ_ONCE(fullstop) == FULLSTOP_SHUTDOWN) {\\n\\t\\tpr_notice(\"torture thread %s parking due to system shutdown\\\\n\",\\n\\t\\t\\t  title);\\n\\t\\tschedule_timeout_uninterruptible(MAX_SCHEDULE_TIMEOUT);\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(torture_shutdown_absorb);\\n\\n/*\\n * Cause the torture test to shutdown the system after the test has\\n * run for the time specified by the shutdown_secs parameter.\\n */\\nstatic int torture_shutdown(void *arg)\\n{\\n\\tktime_t ktime_snap;\\n\\n\\tVERBOSE_TOROUT_STRING(\"torture_shutdown task started\");\\n\\tktime_snap = ktime_get();\\n\\twhile (ktime_before(ktime_snap, shutdown_time) &&\\n\\t       !torture_must_stop()) {\\n\\t\\tif (verbose)\\n\\t\\t\\tpr_alert(\"%s\" TORTURE_FLAG\\n\\t\\t\\t\\t \"torture_shutdown task: %llu ms remaining\\\\n\",\\n\\t\\t\\t\\t torture_type,\\n\\t\\t\\t\\t ktime_ms_delta(shutdown_time, ktime_snap));\\n\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\t\\tschedule_hrtimeout(&shutdown_time, HRTIMER_MODE_ABS);\\n\\t\\tktime_snap = ktime_get();\\n\\t}\\n\\tif (torture_must_stop()) {\\n\\t\\ttorture_kthread_stopping(\"torture_shutdown\");\\n\\t\\treturn 0;\\n\\t}\\n\\n\\t/* OK, shut down the system. */\\n\\n\\tVERBOSE_TOROUT_STRING(\"torture_shutdown task shutting down system\");\\n\\tshutdown_task = NULL;\\t/* Avoid self-kill deadlock. */\\n\\tif (torture_shutdown_hook)\\n\\t\\ttorture_shutdown_hook();\\n\\telse\\n\\t\\tVERBOSE_TOROUT_STRING(\"No torture_shutdown_hook(), skipping.\");\\n\\tif (ftrace_dump_at_shutdown)\\n\\t\\trcu_ftrace_dump(DUMP_ALL);\\n\\tkernel_power_off();\\t/* Shut down the system. */\\n\\treturn 0;\\n}\\n\\n/*\\n * Start up the shutdown task.\\n */\\nint torture_shutdown_init(int ssecs, void (*cleanup)(void))\\n{\\n\\ttorture_shutdown_hook = cleanup;\\n\\tif (ssecs > 0) {\\n\\t\\tshutdown_time = ktime_add(ktime_get(), ktime_set(ssecs, 0));\\n\\t\\treturn torture_create_kthread(torture_shutdown, NULL,\\n\\t\\t\\t\\t\\t      shutdown_task);\\n\\t}\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(torture_shutdown_init);\\n\\n/*\\n * Detect and respond to a system shutdown.\\n */\\nstatic int torture_shutdown_notify(struct notifier_block *unused1,\\n\\t\\t\\t\\t   unsigned long unused2, void *unused3)\\n{\\n\\tmutex_lock(&fullstop_mutex);\\n\\tif (READ_ONCE(fullstop) == FULLSTOP_DONTSTOP) {\\n\\t\\tVERBOSE_TOROUT_STRING(\"Unscheduled system shutdown detected\");\\n\\t\\tWRITE_ONCE(fullstop, FULLSTOP_SHUTDOWN);\\n\\t} else {\\n\\t\\tpr_warn(\"Concurrent rmmod and shutdown illegal!\\\\n\");\\n\\t}\\n\\tmutex_unlock(&fullstop_mutex);\\n\\treturn NOTIFY_DONE;\\n}\\n\\nstatic struct notifier_block torture_shutdown_nb = {\\n\\t.notifier_call = torture_shutdown_notify,\\n};\\n\\n/*\\n * Shut down the shutdown task.  Say what???  Heh!  This can happen if\\n * the torture module gets an rmmod before the shutdown time arrives.  ;-)\\n */\\nstatic void torture_shutdown_cleanup(void)\\n{\\n\\tunregister_reboot_notifier(&torture_shutdown_nb);\\n\\tif (shutdown_task != NULL) {\\n\\t\\tVERBOSE_TOROUT_STRING(\"Stopping torture_shutdown task\");\\n\\t\\tkthread_stop(shutdown_task);\\n\\t}\\n\\tshutdown_task = NULL;\\n}\\n\\n/*\\n * Variables for stuttering, which means to periodically pause and\\n * restart testing in order to catch bugs that appear when load is\\n * suddenly applied to or removed from the system.\\n */\\nstatic struct task_struct *stutter_task;\\nstatic ktime_t stutter_till_abs_time;\\nstatic int stutter;\\nstatic int stutter_gap;\\n\\n/*\\n * Block until the stutter interval ends.  This must be called periodically\\n * by all running kthreads that need to be subject to stuttering.\\n */\\nbool stutter_wait(const char *title)\\n{\\n\\tbool ret = false;\\n\\tktime_t till_ns;\\n\\n\\tcond_resched_tasks_rcu_qs();\\n\\ttill_ns = READ_ONCE(stutter_till_abs_time);\\n\\tif (till_ns && ktime_before(ktime_get(), till_ns)) {\\n\\t\\ttorture_hrtimeout_ns(till_ns, 0, HRTIMER_MODE_ABS, NULL);\\n\\t\\tret = true;\\n\\t}\\n\\ttorture_shutdown_absorb(title);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(stutter_wait);\\n\\n/*\\n * Cause the torture test to \"stutter\", starting and stopping all\\n * threads periodically.\\n */\\nstatic int torture_stutter(void *arg)\\n{\\n\\tktime_t till_ns;\\n\\n\\tVERBOSE_TOROUT_STRING(\"torture_stutter task started\");\\n\\tdo {\\n\\t\\tif (!torture_must_stop() && stutter > 1) {\\n\\t\\t\\ttill_ns = ktime_add_ns(ktime_get(),\\n\\t\\t\\t\\t\\t       jiffies_to_nsecs(stutter));\\n\\t\\t\\tWRITE_ONCE(stutter_till_abs_time, till_ns);\\n\\t\\t\\ttorture_hrtimeout_jiffies(stutter - 1, NULL);\\n\\t\\t}\\n\\t\\tif (!torture_must_stop())\\n\\t\\t\\ttorture_hrtimeout_jiffies(stutter_gap, NULL);\\n\\t\\ttorture_shutdown_absorb(\"torture_stutter\");\\n\\t} while (!torture_must_stop());\\n\\ttorture_kthread_stopping(\"torture_stutter\");\\n\\treturn 0;\\n}\\n\\n/*\\n * Initialize and kick off the torture_stutter kthread.\\n */\\nint torture_stutter_init(const int s, const int sgap)\\n{\\n\\tstutter = s;\\n\\tstutter_gap = sgap;\\n\\treturn torture_create_kthread(torture_stutter, NULL, stutter_task);\\n}\\nEXPORT_SYMBOL_GPL(torture_stutter_init);\\n\\n/*\\n * Cleanup after the torture_stutter kthread.\\n */\\nstatic void torture_stutter_cleanup(void)\\n{\\n\\tif (!stutter_task)\\n\\t\\treturn;\\n\\tVERBOSE_TOROUT_STRING(\"Stopping torture_stutter task\");\\n\\tkthread_stop(stutter_task);\\n\\tstutter_task = NULL;\\n}\\n\\nstatic void\\ntorture_print_module_parms(void)\\n{\\n\\tpr_alert(\"torture module --- %s:  disable_onoff_at_boot=%d ftrace_dump_at_shutdown=%d verbose_sleep_frequency=%d verbose_sleep_duration=%d random_shuffle=%d\\\\n\",\\n\\t\\t torture_type, disable_onoff_at_boot, ftrace_dump_at_shutdown, verbose_sleep_frequency, verbose_sleep_duration, random_shuffle);\\n}\\n\\n/*\\n * Initialize torture module.  Please note that this is -not- invoked via\\n * the usual module_init() mechanism, but rather by an explicit call from\\n * the client torture module.  This call must be paired with a later\\n * torture_init_end().\\n *\\n * The runnable parameter points to a flag that controls whether or not\\n * the test is currently runnable.  If there is no such flag, pass in NULL.\\n */\\nbool torture_init_begin(char *ttype, int v)\\n{\\n\\tmutex_lock(&fullstop_mutex);\\n\\tif (torture_type != NULL) {\\n\\t\\tpr_alert(\"%s: Refusing %s init: %s running.\\\\n\",\\n\\t\\t\\t  __func__, ttype, torture_type);\\n\\t\\tpr_alert(\"%s: One torture test at a time!\\\\n\", __func__);\\n\\t\\tmutex_unlock(&fullstop_mutex);\\n\\t\\treturn false;\\n\\t}\\n\\ttorture_type = ttype;\\n\\tverbose = v;\\n\\tfullstop = FULLSTOP_DONTSTOP;\\n\\ttorture_print_module_parms();\\n\\treturn true;\\n}\\nEXPORT_SYMBOL_GPL(torture_init_begin);\\n\\n/*\\n * Tell the torture module that initialization is complete.\\n */\\nvoid torture_init_end(void)\\n{\\n\\tmutex_unlock(&fullstop_mutex);\\n\\tregister_reboot_notifier(&torture_shutdown_nb);\\n}\\nEXPORT_SYMBOL_GPL(torture_init_end);\\n\\n/*\\n * Clean up torture module.  Please note that this is -not- invoked via\\n * the usual module_exit() mechanism, but rather by an explicit call from\\n * the client torture module.  Returns true if a race with system shutdown\\n * is detected, otherwise, all kthreads started by functions in this file\\n * will be shut down.\\n *\\n * This must be called before the caller starts shutting down its own\\n * kthreads.\\n *\\n * Both torture_cleanup_begin() and torture_cleanup_end() must be paired,\\n * in order to correctly perform the cleanup. They are separated because\\n * threads can still need to reference the torture_type type, thus nullify\\n * only after completing all other relevant calls.\\n */\\nbool torture_cleanup_begin(void)\\n{\\n\\tmutex_lock(&fullstop_mutex);\\n\\tif (READ_ONCE(fullstop) == FULLSTOP_SHUTDOWN) {\\n\\t\\tpr_warn(\"Concurrent rmmod and shutdown illegal!\\\\n\");\\n\\t\\tmutex_unlock(&fullstop_mutex);\\n\\t\\tschedule_timeout_uninterruptible(10);\\n\\t\\treturn true;\\n\\t}\\n\\tWRITE_ONCE(fullstop, FULLSTOP_RMMOD);\\n\\tmutex_unlock(&fullstop_mutex);\\n\\ttorture_shutdown_cleanup();\\n\\ttorture_shuffle_cleanup();\\n\\ttorture_stutter_cleanup();\\n\\ttorture_onoff_cleanup();\\n\\treturn false;\\n}\\nEXPORT_SYMBOL_GPL(torture_cleanup_begin);\\n\\nvoid torture_cleanup_end(void)\\n{\\n\\tmutex_lock(&fullstop_mutex);\\n\\ttorture_type = NULL;\\n\\tmutex_unlock(&fullstop_mutex);\\n}\\nEXPORT_SYMBOL_GPL(torture_cleanup_end);\\n\\n/*\\n * Is it time for the current torture test to stop?\\n */\\nbool torture_must_stop(void)\\n{\\n\\treturn torture_must_stop_irq() || kthread_should_stop();\\n}\\nEXPORT_SYMBOL_GPL(torture_must_stop);\\n\\n/*\\n * Is it time for the current torture test to stop?  This is the irq-safe\\n * version, hence no check for kthread_should_stop().\\n */\\nbool torture_must_stop_irq(void)\\n{\\n\\treturn READ_ONCE(fullstop) != FULLSTOP_DONTSTOP;\\n}\\nEXPORT_SYMBOL_GPL(torture_must_stop_irq);\\n\\n/*\\n * Each kthread must wait for kthread_should_stop() before returning from\\n * its top-level function, otherwise segfaults ensue.  This function\\n * prints a \"stopping\" message and waits for kthread_should_stop(), and\\n * should be called from all torture kthreads immediately prior to\\n * returning.\\n */\\nvoid torture_kthread_stopping(char *title)\\n{\\n\\tchar buf[128];\\n\\n\\tsnprintf(buf, sizeof(buf), \"%s is stopping\", title);\\n\\tVERBOSE_TOROUT_STRING(buf);\\n\\twhile (!kthread_should_stop()) {\\n\\t\\ttorture_shutdown_absorb(title);\\n\\t\\tschedule_timeout_uninterruptible(HZ / 20);\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(torture_kthread_stopping);\\n\\n/*\\n * Create a generic torture kthread that is immediately runnable.  If you\\n * need the kthread to be stopped so that you can do something to it before\\n * it starts, you will need to open-code your own.\\n */\\nint _torture_create_kthread(int (*fn)(void *arg), void *arg, char *s, char *m,\\n\\t\\t\\t    char *f, struct task_struct **tp, void (*cbf)(struct task_struct *tp))\\n{\\n\\tint ret = 0;\\n\\n\\tVERBOSE_TOROUT_STRING(m);\\n\\t*tp = kthread_create(fn, arg, \"%s\", s);\\n\\tif (IS_ERR(*tp)) {\\n\\t\\tret = PTR_ERR(*tp);\\n\\t\\tTOROUT_ERRSTRING(f);\\n\\t\\t*tp = NULL;\\n\\t\\treturn ret;\\n\\t}\\n\\n\\tif (cbf)\\n\\t\\tcbf(*tp);\\n\\n\\twake_up_process(*tp);  // Process is sleeping, so ordering provided.\\n\\ttorture_shuffle_task_register(*tp);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(_torture_create_kthread);\\n\\n/*\\n * Stop a generic kthread, emitting a message.\\n */\\nvoid _torture_stop_kthread(char *m, struct task_struct **tp)\\n{\\n\\tif (*tp == NULL)\\n\\t\\treturn;\\n\\tVERBOSE_TOROUT_STRING(m);\\n\\tkthread_stop(*tp);\\n\\t*tp = NULL;\\n}\\nEXPORT_SYMBOL_GPL(_torture_stop_kthread);\\n\\n// SPDX-License-Identifier: GPL-2.0-or-later\\n/*\\n *  Kernel Probes (KProbes)\\n *\\n * Copyright (C) IBM Corporation, 2002, 2004\\n *\\n * 2002-Oct\\tCreated by Vamsi Krishna S <vamsi_krishna@in.ibm.com> Kernel\\n *\\t\\tProbes initial implementation (includes suggestions from\\n *\\t\\tRusty Russell).\\n * 2004-Aug\\tUpdated by Prasanna S Panchamukhi <prasanna@in.ibm.com> with\\n *\\t\\thlists and exceptions notifier as suggested by Andi Kleen.\\n * 2004-July\\tSuparna Bhattacharya <suparna@in.ibm.com> added jumper probes\\n *\\t\\tinterface to access function arguments.\\n * 2004-Sep\\tPrasanna S Panchamukhi <prasanna@in.ibm.com> Changed Kprobes\\n *\\t\\texceptions notifier to be first on the priority list.\\n * 2005-May\\tHien Nguyen <hien@us.ibm.com>, Jim Keniston\\n *\\t\\t<jkenisto@us.ibm.com> and Prasanna S Panchamukhi\\n *\\t\\t<prasanna@in.ibm.com> added function-return probes.\\n */\\n\\n#define pr_fmt(fmt) \"kprobes: \" fmt\\n\\n#include <linux/kprobes.h>\\n#include <linux/hash.h>\\n#include <linux/init.h>\\n#include <linux/slab.h>\\n#include <linux/stddef.h>\\n#include <linux/export.h>\\n#include <linux/kallsyms.h>\\n#include <linux/freezer.h>\\n#include <linux/seq_file.h>\\n#include <linux/debugfs.h>\\n#include <linux/sysctl.h>\\n#include <linux/kdebug.h>\\n#include <linux/memory.h>\\n#include <linux/ftrace.h>\\n#include <linux/cpu.h>\\n#include <linux/jump_label.h>\\n#include <linux/static_call.h>\\n#include <linux/perf_event.h>\\n#include <linux/execmem.h>\\n\\n#include <asm/sections.h>\\n#include <asm/cacheflush.h>\\n#include <asm/errno.h>\\n#include <linux/uaccess.h>\\n\\n#define KPROBE_HASH_BITS 6\\n#define KPROBE_TABLE_SIZE (1 << KPROBE_HASH_BITS)\\n\\n#if !defined(CONFIG_OPTPROBES) || !defined(CONFIG_SYSCTL)\\n#define kprobe_sysctls_init() do { } while (0)\\n#endif\\n\\nstatic int kprobes_initialized;\\n/* kprobe_table can be accessed by\\n * - Normal hlist traversal and RCU add/del under \\'kprobe_mutex\\' is held.\\n * Or\\n * - RCU hlist traversal under disabling preempt (breakpoint handlers)\\n */\\nstatic struct hlist_head kprobe_table[KPROBE_TABLE_SIZE];\\n\\n/* NOTE: change this value only with \\'kprobe_mutex\\' held */\\nstatic bool kprobes_all_disarmed;\\n\\n/* This protects \\'kprobe_table\\' and \\'optimizing_list\\' */\\nstatic DEFINE_MUTEX(kprobe_mutex);\\nstatic DEFINE_PER_CPU(struct kprobe *, kprobe_instance);\\n\\nkprobe_opcode_t * __weak kprobe_lookup_name(const char *name,\\n\\t\\t\\t\\t\\tunsigned int __unused)\\n{\\n\\treturn ((kprobe_opcode_t *)(kallsyms_lookup_name(name)));\\n}\\n\\n/*\\n * Blacklist -- list of \\'struct kprobe_blacklist_entry\\' to store info where\\n * kprobes can not probe.\\n */\\nstatic LIST_HEAD(kprobe_blacklist);\\n\\n#ifdef __ARCH_WANT_KPROBES_INSN_SLOT\\n/*\\n * \\'kprobe::ainsn.insn\\' points to the copy of the instruction to be\\n * single-stepped. x86_64, POWER4 and above have no-exec support and\\n * stepping on the instruction on a vmalloced/kmalloced/data page\\n * is a recipe for disaster\\n */\\nstruct kprobe_insn_page {\\n\\tstruct list_head list;\\n\\tkprobe_opcode_t *insns;\\t\\t/* Page of instruction slots */\\n\\tstruct kprobe_insn_cache *cache;\\n\\tint nused;\\n\\tint ngarbage;\\n\\tchar slot_used[];\\n};\\n\\nstatic int slots_per_page(struct kprobe_insn_cache *c)\\n{\\n\\treturn PAGE_SIZE/(c->insn_size * sizeof(kprobe_opcode_t));\\n}\\n\\nenum kprobe_slot_state {\\n\\tSLOT_CLEAN = 0,\\n\\tSLOT_DIRTY = 1,\\n\\tSLOT_USED = 2,\\n};\\n\\nvoid __weak *alloc_insn_page(void)\\n{\\n\\t/*\\n\\t * Use execmem_alloc() so this page is within +/- 2GB of where the\\n\\t * kernel image and loaded module images reside. This is required\\n\\t * for most of the architectures.\\n\\t * (e.g. x86-64 needs this to handle the %rip-relative fixups.)\\n\\t */\\n\\treturn execmem_alloc(EXECMEM_KPROBES, PAGE_SIZE);\\n}\\n\\nstatic void free_insn_page(void *page)\\n{\\n\\texecmem_free(page);\\n}\\n\\nstruct kprobe_insn_cache kprobe_insn_slots = {\\n\\t.mutex = __MUTEX_INITIALIZER(kprobe_insn_slots.mutex),\\n\\t.alloc = alloc_insn_page,\\n\\t.free = free_insn_page,\\n\\t.sym = KPROBE_INSN_PAGE_SYM,\\n\\t.pages = LIST_HEAD_INIT(kprobe_insn_slots.pages),\\n\\t.insn_size = MAX_INSN_SIZE,\\n\\t.nr_garbage = 0,\\n};\\nstatic int collect_garbage_slots(struct kprobe_insn_cache *c);\\n\\n/**\\n * __get_insn_slot() - Find a slot on an executable page for an instruction.\\n * We allocate an executable page if there\\'s no room on existing ones.\\n */\\nkprobe_opcode_t *__get_insn_slot(struct kprobe_insn_cache *c)\\n{\\n\\tstruct kprobe_insn_page *kip;\\n\\tkprobe_opcode_t *slot = NULL;\\n\\n\\t/* Since the slot array is not protected by rcu, we need a mutex */\\n\\tmutex_lock(&c->mutex);\\n retry:\\n\\trcu_read_lock();\\n\\tlist_for_each_entry_rcu(kip, &c->pages, list) {\\n\\t\\tif (kip->nused < slots_per_page(c)) {\\n\\t\\t\\tint i;\\n\\n\\t\\t\\tfor (i = 0; i < slots_per_page(c); i++) {\\n\\t\\t\\t\\tif (kip->slot_used[i] == SLOT_CLEAN) {\\n\\t\\t\\t\\t\\tkip->slot_used[i] = SLOT_USED;\\n\\t\\t\\t\\t\\tkip->nused++;\\n\\t\\t\\t\\t\\tslot = kip->insns + (i * c->insn_size);\\n\\t\\t\\t\\t\\trcu_read_unlock();\\n\\t\\t\\t\\t\\tgoto out;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\t/* kip->nused is broken. Fix it. */\\n\\t\\t\\tkip->nused = slots_per_page(c);\\n\\t\\t\\tWARN_ON(1);\\n\\t\\t}\\n\\t}\\n\\trcu_read_unlock();\\n\\n\\t/* If there are any garbage slots, collect it and try again. */\\n\\tif (c->nr_garbage && collect_garbage_slots(c) == 0)\\n\\t\\tgoto retry;\\n\\n\\t/* All out of space.  Need to allocate a new page. */\\n\\tkip = kmalloc(struct_size(kip, slot_used, slots_per_page(c)), GFP_KERNEL);\\n\\tif (!kip)\\n\\t\\tgoto out;\\n\\n\\tkip->insns = c->alloc();\\n\\tif (!kip->insns) {\\n\\t\\tkfree(kip);\\n\\t\\tgoto out;\\n\\t}\\n\\tINIT_LIST_HEAD(&kip->list);\\n\\tmemset(kip->slot_used, SLOT_CLEAN, slots_per_page(c));\\n\\tkip->slot_used[0] = SLOT_USED;\\n\\tkip->nused = 1;\\n\\tkip->ngarbage = 0;\\n\\tkip->cache = c;\\n\\tlist_add_rcu(&kip->list, &c->pages);\\n\\tslot = kip->insns;\\n\\n\\t/* Record the perf ksymbol register event after adding the page */\\n\\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_OOL, (unsigned long)kip->insns,\\n\\t\\t\\t   PAGE_SIZE, false, c->sym);\\nout:\\n\\tmutex_unlock(&c->mutex);\\n\\treturn slot;\\n}\\n\\n/* Return true if all garbages are collected, otherwise false. */\\nstatic bool collect_one_slot(struct kprobe_insn_page *kip, int idx)\\n{\\n\\tkip->slot_used[idx] = SLOT_CLEAN;\\n\\tkip->nused--;\\n\\tif (kip->nused != 0)\\n\\t\\treturn false;\\n\\n\\t/*\\n\\t * Page is no longer in use.  Free it unless\\n\\t * it\\'s the last one.  We keep the last one\\n\\t * so as not to have to set it up again the\\n\\t * next time somebody inserts a probe.\\n\\t */\\n\\tif (!list_is_singular(&kip->list)) {\\n\\t\\t/*\\n\\t\\t * Record perf ksymbol unregister event before removing\\n\\t\\t * the page.\\n\\t\\t */\\n\\t\\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_OOL,\\n\\t\\t\\t\\t   (unsigned long)kip->insns, PAGE_SIZE, true,\\n\\t\\t\\t\\t   kip->cache->sym);\\n\\t\\tlist_del_rcu(&kip->list);\\n\\t\\tsynchronize_rcu();\\n\\t\\tkip->cache->free(kip->insns);\\n\\t\\tkfree(kip);\\n\\t}\\n\\treturn true;\\n}\\n\\nstatic int collect_garbage_slots(struct kprobe_insn_cache *c)\\n{\\n\\tstruct kprobe_insn_page *kip, *next;\\n\\n\\t/* Ensure no-one is interrupted on the garbages */\\n\\tsynchronize_rcu();\\n\\n\\tlist_for_each_entry_safe(kip, next, &c->pages, list) {\\n\\t\\tint i;\\n\\n\\t\\tif (kip->ngarbage == 0)\\n\\t\\t\\tcontinue;\\n\\t\\tkip->ngarbage = 0;\\t/* we will collect all garbages */\\n\\t\\tfor (i = 0; i < slots_per_page(c); i++) {\\n\\t\\t\\tif (kip->slot_used[i] == SLOT_DIRTY && collect_one_slot(kip, i))\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\tc->nr_garbage = 0;\\n\\treturn 0;\\n}\\n\\nvoid __free_insn_slot(struct kprobe_insn_cache *c,\\n\\t\\t      kprobe_opcode_t *slot, int dirty)\\n{\\n\\tstruct kprobe_insn_page *kip;\\n\\tlong idx;\\n\\n\\tmutex_lock(&c->mutex);\\n\\trcu_read_lock();\\n\\tlist_for_each_entry_rcu(kip, &c->pages, list) {\\n\\t\\tidx = ((long)slot - (long)kip->insns) /\\n\\t\\t\\t(c->insn_size * sizeof(kprobe_opcode_t));\\n\\t\\tif (idx >= 0 && idx < slots_per_page(c))\\n\\t\\t\\tgoto out;\\n\\t}\\n\\t/* Could not find this slot. */\\n\\tWARN_ON(1);\\n\\tkip = NULL;\\nout:\\n\\trcu_read_unlock();\\n\\t/* Mark and sweep: this may sleep */\\n\\tif (kip) {\\n\\t\\t/* Check double free */\\n\\t\\tWARN_ON(kip->slot_used[idx] != SLOT_USED);\\n\\t\\tif (dirty) {\\n\\t\\t\\tkip->slot_used[idx] = SLOT_DIRTY;\\n\\t\\t\\tkip->ngarbage++;\\n\\t\\t\\tif (++c->nr_garbage > slots_per_page(c))\\n\\t\\t\\t\\tcollect_garbage_slots(c);\\n\\t\\t} else {\\n\\t\\t\\tcollect_one_slot(kip, idx);\\n\\t\\t}\\n\\t}\\n\\tmutex_unlock(&c->mutex);\\n}\\n\\n/*\\n * Check given address is on the page of kprobe instruction slots.\\n * This will be used for checking whether the address on a stack\\n * is on a text area or not.\\n */\\nbool __is_insn_slot_addr(struct kprobe_insn_cache *c, unsigned long addr)\\n{\\n\\tstruct kprobe_insn_page *kip;\\n\\tbool ret = false;\\n\\n\\trcu_read_lock();\\n\\tlist_for_each_entry_rcu(kip, &c->pages, list) {\\n\\t\\tif (addr >= (unsigned long)kip->insns &&\\n\\t\\t    addr < (unsigned long)kip->insns + PAGE_SIZE) {\\n\\t\\t\\tret = true;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\trcu_read_unlock();\\n\\n\\treturn ret;\\n}\\n\\nint kprobe_cache_get_kallsym(struct kprobe_insn_cache *c, unsigned int *symnum,\\n\\t\\t\\t     unsigned long *value, char *type, char *sym)\\n{\\n\\tstruct kprobe_insn_page *kip;\\n\\tint ret = -ERANGE;\\n\\n\\trcu_read_lock();\\n\\tlist_for_each_entry_rcu(kip, &c->pages, list) {\\n\\t\\tif ((*symnum)--)\\n\\t\\t\\tcontinue;\\n\\t\\tstrscpy(sym, c->sym, KSYM_NAME_LEN);\\n\\t\\t*type = \\'t\\';\\n\\t\\t*value = (unsigned long)kip->insns;\\n\\t\\tret = 0;\\n\\t\\tbreak;\\n\\t}\\n\\trcu_read_unlock();\\n\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_OPTPROBES\\nvoid __weak *alloc_optinsn_page(void)\\n{\\n\\treturn alloc_insn_page();\\n}\\n\\nvoid __weak free_optinsn_page(void *page)\\n{\\n\\tfree_insn_page(page);\\n}\\n\\n/* For optimized_kprobe buffer */\\nstruct kprobe_insn_cache kprobe_optinsn_slots = {\\n\\t.mutex = __MUTEX_INITIALIZER(kprobe_optinsn_slots.mutex),\\n\\t.alloc = alloc_optinsn_page,\\n\\t.free = free_optinsn_page,\\n\\t.sym = KPROBE_OPTINSN_PAGE_SYM,\\n\\t.pages = LIST_HEAD_INIT(kprobe_optinsn_slots.pages),\\n\\t/* .insn_size is initialized later */\\n\\t.nr_garbage = 0,\\n};\\n#endif /* CONFIG_OPTPROBES */\\n#endif /* __ARCH_WANT_KPROBES_INSN_SLOT */\\n\\n/* We have preemption disabled.. so it is safe to use __ versions */\\nstatic inline void set_kprobe_instance(struct kprobe *kp)\\n{\\n\\t__this_cpu_write(kprobe_instance, kp);\\n}\\n\\nstatic inline void reset_kprobe_instance(void)\\n{\\n\\t__this_cpu_write(kprobe_instance, NULL);\\n}\\n\\n/*\\n * This routine is called either:\\n *\\t- under the \\'kprobe_mutex\\' - during kprobe_[un]register().\\n *\\t\\t\\t\\tOR\\n *\\t- with preemption disabled - from architecture specific code.\\n */\\nstruct kprobe *get_kprobe(void *addr)\\n{\\n\\tstruct hlist_head *head;\\n\\tstruct kprobe *p;\\n\\n\\thead = &kprobe_table[hash_ptr(addr, KPROBE_HASH_BITS)];\\n\\thlist_for_each_entry_rcu(p, head, hlist,\\n\\t\\t\\t\\t lockdep_is_held(&kprobe_mutex)) {\\n\\t\\tif (p->addr == addr)\\n\\t\\t\\treturn p;\\n\\t}\\n\\n\\treturn NULL;\\n}\\nNOKPROBE_SYMBOL(get_kprobe);\\n\\nstatic int aggr_pre_handler(struct kprobe *p, struct pt_regs *regs);\\n\\n/* Return true if \\'p\\' is an aggregator */\\nstatic inline bool kprobe_aggrprobe(struct kprobe *p)\\n{\\n\\treturn p->pre_handler == aggr_pre_handler;\\n}\\n\\n/* Return true if \\'p\\' is unused */\\nstatic inline bool kprobe_unused(struct kprobe *p)\\n{\\n\\treturn kprobe_aggrprobe(p) && kprobe_disabled(p) &&\\n\\t       list_empty(&p->list);\\n}\\n\\n/* Keep all fields in the kprobe consistent. */\\nstatic inline void copy_kprobe(struct kprobe *ap, struct kprobe *p)\\n{\\n\\tmemcpy(&p->opcode, &ap->opcode, sizeof(kprobe_opcode_t));\\n\\tmemcpy(&p->ainsn, &ap->ainsn, sizeof(struct arch_specific_insn));\\n}\\n\\n#ifdef CONFIG_OPTPROBES\\n/* NOTE: This is protected by \\'kprobe_mutex\\'. */\\nstatic bool kprobes_allow_optimization;\\n\\n/*\\n * Call all \\'kprobe::pre_handler\\' on the list, but ignores its return value.\\n * This must be called from arch-dep optimized caller.\\n */\\nvoid opt_pre_handler(struct kprobe *p, struct pt_regs *regs)\\n{\\n\\tstruct kprobe *kp;\\n\\n\\tlist_for_each_entry_rcu(kp, &p->list, list) {\\n\\t\\tif (kp->pre_handler && likely(!kprobe_disabled(kp))) {\\n\\t\\t\\tset_kprobe_instance(kp);\\n\\t\\t\\tkp->pre_handler(kp, regs);\\n\\t\\t}\\n\\t\\treset_kprobe_instance();\\n\\t}\\n}\\nNOKPROBE_SYMBOL(opt_pre_handler);\\n\\n/* Free optimized instructions and optimized_kprobe */\\nstatic void free_aggr_kprobe(struct kprobe *p)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\top = container_of(p, struct optimized_kprobe, kp);\\n\\tarch_remove_optimized_kprobe(op);\\n\\tarch_remove_kprobe(p);\\n\\tkfree(op);\\n}\\n\\n/* Return true if the kprobe is ready for optimization. */\\nstatic inline int kprobe_optready(struct kprobe *p)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\tif (kprobe_aggrprobe(p)) {\\n\\t\\top = container_of(p, struct optimized_kprobe, kp);\\n\\t\\treturn arch_prepared_optinsn(&op->optinsn);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/* Return true if the kprobe is disarmed. Note: p must be on hash list */\\nbool kprobe_disarmed(struct kprobe *p)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\t/* If kprobe is not aggr/opt probe, just return kprobe is disabled */\\n\\tif (!kprobe_aggrprobe(p))\\n\\t\\treturn kprobe_disabled(p);\\n\\n\\top = container_of(p, struct optimized_kprobe, kp);\\n\\n\\treturn kprobe_disabled(p) && list_empty(&op->list);\\n}\\n\\n/* Return true if the probe is queued on (un)optimizing lists */\\nstatic bool kprobe_queued(struct kprobe *p)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\tif (kprobe_aggrprobe(p)) {\\n\\t\\top = container_of(p, struct optimized_kprobe, kp);\\n\\t\\tif (!list_empty(&op->list))\\n\\t\\t\\treturn true;\\n\\t}\\n\\treturn false;\\n}\\n\\n/*\\n * Return an optimized kprobe whose optimizing code replaces\\n * instructions including \\'addr\\' (exclude breakpoint).\\n */\\nstatic struct kprobe *get_optimized_kprobe(kprobe_opcode_t *addr)\\n{\\n\\tint i;\\n\\tstruct kprobe *p = NULL;\\n\\tstruct optimized_kprobe *op;\\n\\n\\t/* Don\\'t check i == 0, since that is a breakpoint case. */\\n\\tfor (i = 1; !p && i < MAX_OPTIMIZED_LENGTH / sizeof(kprobe_opcode_t); i++)\\n\\t\\tp = get_kprobe(addr - i);\\n\\n\\tif (p && kprobe_optready(p)) {\\n\\t\\top = container_of(p, struct optimized_kprobe, kp);\\n\\t\\tif (arch_within_optimized_kprobe(op, addr))\\n\\t\\t\\treturn p;\\n\\t}\\n\\n\\treturn NULL;\\n}\\n\\n/* Optimization staging list, protected by \\'kprobe_mutex\\' */\\nstatic LIST_HEAD(optimizing_list);\\nstatic LIST_HEAD(unoptimizing_list);\\nstatic LIST_HEAD(freeing_list);\\n\\nstatic void kprobe_optimizer(struct work_struct *work);\\nstatic DECLARE_DELAYED_WORK(optimizing_work, kprobe_optimizer);\\n#define OPTIMIZE_DELAY 5\\n\\n/*\\n * Optimize (replace a breakpoint with a jump) kprobes listed on\\n * \\'optimizing_list\\'.\\n */\\nstatic void do_optimize_kprobes(void)\\n{\\n\\tlockdep_assert_held(&text_mutex);\\n\\t/*\\n\\t * The optimization/unoptimization refers \\'online_cpus\\' via\\n\\t * stop_machine() and cpu-hotplug modifies the \\'online_cpus\\'.\\n\\t * And same time, \\'text_mutex\\' will be held in cpu-hotplug and here.\\n\\t * This combination can cause a deadlock (cpu-hotplug tries to lock\\n\\t * \\'text_mutex\\' but stop_machine() can not be done because\\n\\t * the \\'online_cpus\\' has been changed)\\n\\t * To avoid this deadlock, caller must have locked cpu-hotplug\\n\\t * for preventing cpu-hotplug outside of \\'text_mutex\\' locking.\\n\\t */\\n\\tlockdep_assert_cpus_held();\\n\\n\\t/* Optimization never be done when disarmed */\\n\\tif (kprobes_all_disarmed || !kprobes_allow_optimization ||\\n\\t    list_empty(&optimizing_list))\\n\\t\\treturn;\\n\\n\\tarch_optimize_kprobes(&optimizing_list);\\n}\\n\\n/*\\n * Unoptimize (replace a jump with a breakpoint and remove the breakpoint\\n * if need) kprobes listed on \\'unoptimizing_list\\'.\\n */\\nstatic void do_unoptimize_kprobes(void)\\n{\\n\\tstruct optimized_kprobe *op, *tmp;\\n\\n\\tlockdep_assert_held(&text_mutex);\\n\\t/* See comment in do_optimize_kprobes() */\\n\\tlockdep_assert_cpus_held();\\n\\n\\tif (!list_empty(&unoptimizing_list))\\n\\t\\tarch_unoptimize_kprobes(&unoptimizing_list, &freeing_list);\\n\\n\\t/* Loop on \\'freeing_list\\' for disarming and removing from kprobe hash list */\\n\\tlist_for_each_entry_safe(op, tmp, &freeing_list, list) {\\n\\t\\t/* Switching from detour code to origin */\\n\\t\\top->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;\\n\\t\\t/* Disarm probes if marked disabled and not gone */\\n\\t\\tif (kprobe_disabled(&op->kp) && !kprobe_gone(&op->kp))\\n\\t\\t\\tarch_disarm_kprobe(&op->kp);\\n\\t\\tif (kprobe_unused(&op->kp)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Remove unused probes from hash list. After waiting\\n\\t\\t\\t * for synchronization, these probes are reclaimed.\\n\\t\\t\\t * (reclaiming is done by do_free_cleaned_kprobes().)\\n\\t\\t\\t */\\n\\t\\t\\thlist_del_rcu(&op->kp.hlist);\\n\\t\\t} else\\n\\t\\t\\tlist_del_init(&op->list);\\n\\t}\\n}\\n\\n/* Reclaim all kprobes on the \\'freeing_list\\' */\\nstatic void do_free_cleaned_kprobes(void)\\n{\\n\\tstruct optimized_kprobe *op, *tmp;\\n\\n\\tlist_for_each_entry_safe(op, tmp, &freeing_list, list) {\\n\\t\\tlist_del_init(&op->list);\\n\\t\\tif (WARN_ON_ONCE(!kprobe_unused(&op->kp))) {\\n\\t\\t\\t/*\\n\\t\\t\\t * This must not happen, but if there is a kprobe\\n\\t\\t\\t * still in use, keep it on kprobes hash list.\\n\\t\\t\\t */\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tfree_aggr_kprobe(&op->kp);\\n\\t}\\n}\\n\\n/* Start optimizer after OPTIMIZE_DELAY passed */\\nstatic void kick_kprobe_optimizer(void)\\n{\\n\\tschedule_delayed_work(&optimizing_work, OPTIMIZE_DELAY);\\n}\\n\\n/* Kprobe jump optimizer */\\nstatic void kprobe_optimizer(struct work_struct *work)\\n{\\n\\tmutex_lock(&kprobe_mutex);\\n\\tcpus_read_lock();\\n\\tmutex_lock(&text_mutex);\\n\\n\\t/*\\n\\t * Step 1: Unoptimize kprobes and collect cleaned (unused and disarmed)\\n\\t * kprobes before waiting for quiesence period.\\n\\t */\\n\\tdo_unoptimize_kprobes();\\n\\n\\t/*\\n\\t * Step 2: Wait for quiesence period to ensure all potentially\\n\\t * preempted tasks to have normally scheduled. Because optprobe\\n\\t * may modify multiple instructions, there is a chance that Nth\\n\\t * instruction is preempted. In that case, such tasks can return\\n\\t * to 2nd-Nth byte of jump instruction. This wait is for avoiding it.\\n\\t * Note that on non-preemptive kernel, this is transparently converted\\n\\t * to synchronoze_sched() to wait for all interrupts to have completed.\\n\\t */\\n\\tsynchronize_rcu_tasks();\\n\\n\\t/* Step 3: Optimize kprobes after quiesence period */\\n\\tdo_optimize_kprobes();\\n\\n\\t/* Step 4: Free cleaned kprobes after quiesence period */\\n\\tdo_free_cleaned_kprobes();\\n\\n\\tmutex_unlock(&text_mutex);\\n\\tcpus_read_unlock();\\n\\n\\t/* Step 5: Kick optimizer again if needed */\\n\\tif (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list))\\n\\t\\tkick_kprobe_optimizer();\\n\\n\\tmutex_unlock(&kprobe_mutex);\\n}\\n\\n/* Wait for completing optimization and unoptimization */\\nvoid wait_for_kprobe_optimizer(void)\\n{\\n\\tmutex_lock(&kprobe_mutex);\\n\\n\\twhile (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list)) {\\n\\t\\tmutex_unlock(&kprobe_mutex);\\n\\n\\t\\t/* This will also make \\'optimizing_work\\' execute immmediately */\\n\\t\\tflush_delayed_work(&optimizing_work);\\n\\t\\t/* \\'optimizing_work\\' might not have been queued yet, relax */\\n\\t\\tcpu_relax();\\n\\n\\t\\tmutex_lock(&kprobe_mutex);\\n\\t}\\n\\n\\tmutex_unlock(&kprobe_mutex);\\n}\\n\\nbool optprobe_queued_unopt(struct optimized_kprobe *op)\\n{\\n\\tstruct optimized_kprobe *_op;\\n\\n\\tlist_for_each_entry(_op, &unoptimizing_list, list) {\\n\\t\\tif (op == _op)\\n\\t\\t\\treturn true;\\n\\t}\\n\\n\\treturn false;\\n}\\n\\n/* Optimize kprobe if p is ready to be optimized */\\nstatic void optimize_kprobe(struct kprobe *p)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\t/* Check if the kprobe is disabled or not ready for optimization. */\\n\\tif (!kprobe_optready(p) || !kprobes_allow_optimization ||\\n\\t    (kprobe_disabled(p) || kprobes_all_disarmed))\\n\\t\\treturn;\\n\\n\\t/* kprobes with \\'post_handler\\' can not be optimized */\\n\\tif (p->post_handler)\\n\\t\\treturn;\\n\\n\\top = container_of(p, struct optimized_kprobe, kp);\\n\\n\\t/* Check there is no other kprobes at the optimized instructions */\\n\\tif (arch_check_optimized_kprobe(op) < 0)\\n\\t\\treturn;\\n\\n\\t/* Check if it is already optimized. */\\n\\tif (op->kp.flags & KPROBE_FLAG_OPTIMIZED) {\\n\\t\\tif (optprobe_queued_unopt(op)) {\\n\\t\\t\\t/* This is under unoptimizing. Just dequeue the probe */\\n\\t\\t\\tlist_del_init(&op->list);\\n\\t\\t}\\n\\t\\treturn;\\n\\t}\\n\\top->kp.flags |= KPROBE_FLAG_OPTIMIZED;\\n\\n\\t/*\\n\\t * On the \\'unoptimizing_list\\' and \\'optimizing_list\\',\\n\\t * \\'op\\' must have OPTIMIZED flag\\n\\t */\\n\\tif (WARN_ON_ONCE(!list_empty(&op->list)))\\n\\t\\treturn;\\n\\n\\tlist_add(&op->list, &optimizing_list);\\n\\tkick_kprobe_optimizer();\\n}\\n\\n/* Short cut to direct unoptimizing */\\nstatic void force_unoptimize_kprobe(struct optimized_kprobe *op)\\n{\\n\\tlockdep_assert_cpus_held();\\n\\tarch_unoptimize_kprobe(op);\\n\\top->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;\\n}\\n\\n/* Unoptimize a kprobe if p is optimized */\\nstatic void unoptimize_kprobe(struct kprobe *p, bool force)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\tif (!kprobe_aggrprobe(p) || kprobe_disarmed(p))\\n\\t\\treturn; /* This is not an optprobe nor optimized */\\n\\n\\top = container_of(p, struct optimized_kprobe, kp);\\n\\tif (!kprobe_optimized(p))\\n\\t\\treturn;\\n\\n\\tif (!list_empty(&op->list)) {\\n\\t\\tif (optprobe_queued_unopt(op)) {\\n\\t\\t\\t/* Queued in unoptimizing queue */\\n\\t\\t\\tif (force) {\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * Forcibly unoptimize the kprobe here, and queue it\\n\\t\\t\\t\\t * in the freeing list for release afterwards.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\tforce_unoptimize_kprobe(op);\\n\\t\\t\\t\\tlist_move(&op->list, &freeing_list);\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\t/* Dequeue from the optimizing queue */\\n\\t\\t\\tlist_del_init(&op->list);\\n\\t\\t\\top->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;\\n\\t\\t}\\n\\t\\treturn;\\n\\t}\\n\\n\\t/* Optimized kprobe case */\\n\\tif (force) {\\n\\t\\t/* Forcibly update the code: this is a special case */\\n\\t\\tforce_unoptimize_kprobe(op);\\n\\t} else {\\n\\t\\tlist_add(&op->list, &unoptimizing_list);\\n\\t\\tkick_kprobe_optimizer();\\n\\t}\\n}\\n\\n/* Cancel unoptimizing for reusing */\\nstatic int reuse_unused_kprobe(struct kprobe *ap)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\t/*\\n\\t * Unused kprobe MUST be on the way of delayed unoptimizing (means\\n\\t * there is still a relative jump) and disabled.\\n\\t */\\n\\top = container_of(ap, struct optimized_kprobe, kp);\\n\\tWARN_ON_ONCE(list_empty(&op->list));\\n\\t/* Enable the probe again */\\n\\tap->flags &= ~KPROBE_FLAG_DISABLED;\\n\\t/* Optimize it again. (remove from \\'op->list\\') */\\n\\tif (!kprobe_optready(ap))\\n\\t\\treturn -EINVAL;\\n\\n\\toptimize_kprobe(ap);\\n\\treturn 0;\\n}\\n\\n/* Remove optimized instructions */\\nstatic void kill_optimized_kprobe(struct kprobe *p)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\top = container_of(p, struct optimized_kprobe, kp);\\n\\tif (!list_empty(&op->list))\\n\\t\\t/* Dequeue from the (un)optimization queue */\\n\\t\\tlist_del_init(&op->list);\\n\\top->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;\\n\\n\\tif (kprobe_unused(p)) {\\n\\t\\t/*\\n\\t\\t * Unused kprobe is on unoptimizing or freeing list. We move it\\n\\t\\t * to freeing_list and let the kprobe_optimizer() remove it from\\n\\t\\t * the kprobe hash list and free it.\\n\\t\\t */\\n\\t\\tif (optprobe_queued_unopt(op))\\n\\t\\t\\tlist_move(&op->list, &freeing_list);\\n\\t}\\n\\n\\t/* Don\\'t touch the code, because it is already freed. */\\n\\tarch_remove_optimized_kprobe(op);\\n}\\n\\nstatic inline\\nvoid __prepare_optimized_kprobe(struct optimized_kprobe *op, struct kprobe *p)\\n{\\n\\tif (!kprobe_ftrace(p))\\n\\t\\tarch_prepare_optimized_kprobe(op, p);\\n}\\n\\n/* Try to prepare optimized instructions */\\nstatic void prepare_optimized_kprobe(struct kprobe *p)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\top = container_of(p, struct optimized_kprobe, kp);\\n\\t__prepare_optimized_kprobe(op, p);\\n}\\n\\n/* Allocate new optimized_kprobe and try to prepare optimized instructions. */\\nstatic struct kprobe *alloc_aggr_kprobe(struct kprobe *p)\\n{\\n\\tstruct optimized_kprobe *op;\\n\\n\\top = kzalloc(sizeof(struct optimized_kprobe), GFP_KERNEL);\\n\\tif (!op)\\n\\t\\treturn NULL;\\n\\n\\tINIT_LIST_HEAD(&op->list);\\n\\top->kp.addr = p->addr;\\n\\t__prepare_optimized_kprobe(op, p);\\n\\n\\treturn &op->kp;\\n}\\n\\nstatic void init_aggr_kprobe(struct kprobe *ap, struct kprobe *p);\\n\\n/*\\n * Prepare an optimized_kprobe and optimize it.\\n * NOTE: \\'p\\' must be a normal registered kprobe.\\n */\\nstatic void try_to_optimize_kprobe(struct kprobe *p)\\n{\\n\\tstruct kprobe *ap;\\n\\tstruct optimized_kprobe *op;\\n\\n\\t/* Impossible to optimize ftrace-based kprobe. */\\n\\tif (kprobe_ftrace(p))\\n\\t\\treturn;\\n\\n\\t/* For preparing optimization, jump_label_text_reserved() is called. */\\n\\tcpus_read_lock();\\n\\tjump_label_lock();\\n\\tmutex_lock(&text_mutex);\\n\\n\\tap = alloc_aggr_kprobe(p);\\n\\tif (!ap)\\n\\t\\tgoto out;\\n\\n\\top = container_of(ap, struct optimized_kprobe, kp);\\n\\tif (!arch_prepared_optinsn(&op->optinsn)) {\\n\\t\\t/* If failed to setup optimizing, fallback to kprobe. */\\n\\t\\tarch_remove_optimized_kprobe(op);\\n\\t\\tkfree(op);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tinit_aggr_kprobe(ap, p);\\n\\toptimize_kprobe(ap);\\t/* This just kicks optimizer thread. */\\n\\nout:\\n\\tmutex_unlock(&text_mutex);\\n\\tjump_label_unlock();\\n\\tcpus_read_unlock();\\n}\\n\\nstatic void optimize_all_kprobes(void)\\n{\\n\\tstruct hlist_head *head;\\n\\tstruct kprobe *p;\\n\\tunsigned int i;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\t/* If optimization is already allowed, just return. */\\n\\tif (kprobes_allow_optimization)\\n\\t\\tgoto out;\\n\\n\\tcpus_read_lock();\\n\\tkprobes_allow_optimization = true;\\n\\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\\n\\t\\thead = &kprobe_table[i];\\n\\t\\thlist_for_each_entry(p, head, hlist)\\n\\t\\t\\tif (!kprobe_disabled(p))\\n\\t\\t\\t\\toptimize_kprobe(p);\\n\\t}\\n\\tcpus_read_unlock();\\n\\tpr_info(\"kprobe jump-optimization is enabled. All kprobes are optimized if possible.\\\\n\");\\nout:\\n\\tmutex_unlock(&kprobe_mutex);\\n}\\n\\n#ifdef CONFIG_SYSCTL\\nstatic void unoptimize_all_kprobes(void)\\n{\\n\\tstruct hlist_head *head;\\n\\tstruct kprobe *p;\\n\\tunsigned int i;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\t/* If optimization is already prohibited, just return. */\\n\\tif (!kprobes_allow_optimization) {\\n\\t\\tmutex_unlock(&kprobe_mutex);\\n\\t\\treturn;\\n\\t}\\n\\n\\tcpus_read_lock();\\n\\tkprobes_allow_optimization = false;\\n\\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\\n\\t\\thead = &kprobe_table[i];\\n\\t\\thlist_for_each_entry(p, head, hlist) {\\n\\t\\t\\tif (!kprobe_disabled(p))\\n\\t\\t\\t\\tunoptimize_kprobe(p, false);\\n\\t\\t}\\n\\t}\\n\\tcpus_read_unlock();\\n\\tmutex_unlock(&kprobe_mutex);\\n\\n\\t/* Wait for unoptimizing completion. */\\n\\twait_for_kprobe_optimizer();\\n\\tpr_info(\"kprobe jump-optimization is disabled. All kprobes are based on software breakpoint.\\\\n\");\\n}\\n\\nstatic DEFINE_MUTEX(kprobe_sysctl_mutex);\\nstatic int sysctl_kprobes_optimization;\\nstatic int proc_kprobes_optimization_handler(const struct ctl_table *table,\\n\\t\\t\\t\\t\\t     int write, void *buffer,\\n\\t\\t\\t\\t\\t     size_t *length, loff_t *ppos)\\n{\\n\\tint ret;\\n\\n\\tmutex_lock(&kprobe_sysctl_mutex);\\n\\tsysctl_kprobes_optimization = kprobes_allow_optimization ? 1 : 0;\\n\\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\\n\\n\\tif (sysctl_kprobes_optimization)\\n\\t\\toptimize_all_kprobes();\\n\\telse\\n\\t\\tunoptimize_all_kprobes();\\n\\tmutex_unlock(&kprobe_sysctl_mutex);\\n\\n\\treturn ret;\\n}\\n\\nstatic struct ctl_table kprobe_sysctls[] = {\\n\\t{\\n\\t\\t.procname\\t= \"kprobes-optimization\",\\n\\t\\t.data\\t\\t= &sysctl_kprobes_optimization,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_kprobes_optimization_handler,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n};\\n\\nstatic void __init kprobe_sysctls_init(void)\\n{\\n\\tregister_sysctl_init(\"debug\", kprobe_sysctls);\\n}\\n#endif /* CONFIG_SYSCTL */\\n\\n/* Put a breakpoint for a probe. */\\nstatic void __arm_kprobe(struct kprobe *p)\\n{\\n\\tstruct kprobe *_p;\\n\\n\\tlockdep_assert_held(&text_mutex);\\n\\n\\t/* Find the overlapping optimized kprobes. */\\n\\t_p = get_optimized_kprobe(p->addr);\\n\\tif (unlikely(_p))\\n\\t\\t/* Fallback to unoptimized kprobe */\\n\\t\\tunoptimize_kprobe(_p, true);\\n\\n\\tarch_arm_kprobe(p);\\n\\toptimize_kprobe(p);\\t/* Try to optimize (add kprobe to a list) */\\n}\\n\\n/* Remove the breakpoint of a probe. */\\nstatic void __disarm_kprobe(struct kprobe *p, bool reopt)\\n{\\n\\tstruct kprobe *_p;\\n\\n\\tlockdep_assert_held(&text_mutex);\\n\\n\\t/* Try to unoptimize */\\n\\tunoptimize_kprobe(p, kprobes_all_disarmed);\\n\\n\\tif (!kprobe_queued(p)) {\\n\\t\\tarch_disarm_kprobe(p);\\n\\t\\t/* If another kprobe was blocked, re-optimize it. */\\n\\t\\t_p = get_optimized_kprobe(p->addr);\\n\\t\\tif (unlikely(_p) && reopt)\\n\\t\\t\\toptimize_kprobe(_p);\\n\\t}\\n\\t/*\\n\\t * TODO: Since unoptimization and real disarming will be done by\\n\\t * the worker thread, we can not check whether another probe are\\n\\t * unoptimized because of this probe here. It should be re-optimized\\n\\t * by the worker thread.\\n\\t */\\n}\\n\\n#else /* !CONFIG_OPTPROBES */\\n\\n#define optimize_kprobe(p)\\t\\t\\tdo {} while (0)\\n#define unoptimize_kprobe(p, f)\\t\\t\\tdo {} while (0)\\n#define kill_optimized_kprobe(p)\\t\\tdo {} while (0)\\n#define prepare_optimized_kprobe(p)\\t\\tdo {} while (0)\\n#define try_to_optimize_kprobe(p)\\t\\tdo {} while (0)\\n#define __arm_kprobe(p)\\t\\t\\t\\tarch_arm_kprobe(p)\\n#define __disarm_kprobe(p, o)\\t\\t\\tarch_disarm_kprobe(p)\\n#define kprobe_disarmed(p)\\t\\t\\tkprobe_disabled(p)\\n#define wait_for_kprobe_optimizer()\\t\\tdo {} while (0)\\n\\nstatic int reuse_unused_kprobe(struct kprobe *ap)\\n{\\n\\t/*\\n\\t * If the optimized kprobe is NOT supported, the aggr kprobe is\\n\\t * released at the same time that the last aggregated kprobe is\\n\\t * unregistered.\\n\\t * Thus there should be no chance to reuse unused kprobe.\\n\\t */\\n\\tWARN_ON_ONCE(1);\\n\\treturn -EINVAL;\\n}\\n\\nstatic void free_aggr_kprobe(struct kprobe *p)\\n{\\n\\tarch_remove_kprobe(p);\\n\\tkfree(p);\\n}\\n\\nstatic struct kprobe *alloc_aggr_kprobe(struct kprobe *p)\\n{\\n\\treturn kzalloc(sizeof(struct kprobe), GFP_KERNEL);\\n}\\n#endif /* CONFIG_OPTPROBES */\\n\\n#ifdef CONFIG_KPROBES_ON_FTRACE\\nstatic struct ftrace_ops kprobe_ftrace_ops __read_mostly = {\\n\\t.func = kprobe_ftrace_handler,\\n\\t.flags = FTRACE_OPS_FL_SAVE_REGS,\\n};\\n\\nstatic struct ftrace_ops kprobe_ipmodify_ops __read_mostly = {\\n\\t.func = kprobe_ftrace_handler,\\n\\t.flags = FTRACE_OPS_FL_SAVE_REGS | FTRACE_OPS_FL_IPMODIFY,\\n};\\n\\nstatic int kprobe_ipmodify_enabled;\\nstatic int kprobe_ftrace_enabled;\\nbool kprobe_ftrace_disabled;\\n\\nstatic int __arm_kprobe_ftrace(struct kprobe *p, struct ftrace_ops *ops,\\n\\t\\t\\t       int *cnt)\\n{\\n\\tint ret;\\n\\n\\tlockdep_assert_held(&kprobe_mutex);\\n\\n\\tret = ftrace_set_filter_ip(ops, (unsigned long)p->addr, 0, 0);\\n\\tif (WARN_ONCE(ret < 0, \"Failed to arm kprobe-ftrace at %pS (error %d)\\\\n\", p->addr, ret))\\n\\t\\treturn ret;\\n\\n\\tif (*cnt == 0) {\\n\\t\\tret = register_ftrace_function(ops);\\n\\t\\tif (WARN(ret < 0, \"Failed to register kprobe-ftrace (error %d)\\\\n\", ret))\\n\\t\\t\\tgoto err_ftrace;\\n\\t}\\n\\n\\t(*cnt)++;\\n\\treturn ret;\\n\\nerr_ftrace:\\n\\t/*\\n\\t * At this point, sinec ops is not registered, we should be sefe from\\n\\t * registering empty filter.\\n\\t */\\n\\tftrace_set_filter_ip(ops, (unsigned long)p->addr, 1, 0);\\n\\treturn ret;\\n}\\n\\nstatic int arm_kprobe_ftrace(struct kprobe *p)\\n{\\n\\tbool ipmodify = (p->post_handler != NULL);\\n\\n\\treturn __arm_kprobe_ftrace(p,\\n\\t\\tipmodify ? &kprobe_ipmodify_ops : &kprobe_ftrace_ops,\\n\\t\\tipmodify ? &kprobe_ipmodify_enabled : &kprobe_ftrace_enabled);\\n}\\n\\nstatic int __disarm_kprobe_ftrace(struct kprobe *p, struct ftrace_ops *ops,\\n\\t\\t\\t\\t  int *cnt)\\n{\\n\\tint ret;\\n\\n\\tlockdep_assert_held(&kprobe_mutex);\\n\\n\\tif (*cnt == 1) {\\n\\t\\tret = unregister_ftrace_function(ops);\\n\\t\\tif (WARN(ret < 0, \"Failed to unregister kprobe-ftrace (error %d)\\\\n\", ret))\\n\\t\\t\\treturn ret;\\n\\t}\\n\\n\\t(*cnt)--;\\n\\n\\tret = ftrace_set_filter_ip(ops, (unsigned long)p->addr, 1, 0);\\n\\tWARN_ONCE(ret < 0, \"Failed to disarm kprobe-ftrace at %pS (error %d)\\\\n\",\\n\\t\\t  p->addr, ret);\\n\\treturn ret;\\n}\\n\\nstatic int disarm_kprobe_ftrace(struct kprobe *p)\\n{\\n\\tbool ipmodify = (p->post_handler != NULL);\\n\\n\\treturn __disarm_kprobe_ftrace(p,\\n\\t\\tipmodify ? &kprobe_ipmodify_ops : &kprobe_ftrace_ops,\\n\\t\\tipmodify ? &kprobe_ipmodify_enabled : &kprobe_ftrace_enabled);\\n}\\n\\nvoid kprobe_ftrace_kill(void)\\n{\\n\\tkprobe_ftrace_disabled = true;\\n}\\n#else\\t/* !CONFIG_KPROBES_ON_FTRACE */\\nstatic inline int arm_kprobe_ftrace(struct kprobe *p)\\n{\\n\\treturn -ENODEV;\\n}\\n\\nstatic inline int disarm_kprobe_ftrace(struct kprobe *p)\\n{\\n\\treturn -ENODEV;\\n}\\n#endif\\n\\nstatic int prepare_kprobe(struct kprobe *p)\\n{\\n\\t/* Must ensure p->addr is really on ftrace */\\n\\tif (kprobe_ftrace(p))\\n\\t\\treturn arch_prepare_kprobe_ftrace(p);\\n\\n\\treturn arch_prepare_kprobe(p);\\n}\\n\\nstatic int arm_kprobe(struct kprobe *kp)\\n{\\n\\tif (unlikely(kprobe_ftrace(kp)))\\n\\t\\treturn arm_kprobe_ftrace(kp);\\n\\n\\tcpus_read_lock();\\n\\tmutex_lock(&text_mutex);\\n\\t__arm_kprobe(kp);\\n\\tmutex_unlock(&text_mutex);\\n\\tcpus_read_unlock();\\n\\n\\treturn 0;\\n}\\n\\nstatic int disarm_kprobe(struct kprobe *kp, bool reopt)\\n{\\n\\tif (unlikely(kprobe_ftrace(kp)))\\n\\t\\treturn disarm_kprobe_ftrace(kp);\\n\\n\\tcpus_read_lock();\\n\\tmutex_lock(&text_mutex);\\n\\t__disarm_kprobe(kp, reopt);\\n\\tmutex_unlock(&text_mutex);\\n\\tcpus_read_unlock();\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * Aggregate handlers for multiple kprobes support - these handlers\\n * take care of invoking the individual kprobe handlers on p->list\\n */\\nstatic int aggr_pre_handler(struct kprobe *p, struct pt_regs *regs)\\n{\\n\\tstruct kprobe *kp;\\n\\n\\tlist_for_each_entry_rcu(kp, &p->list, list) {\\n\\t\\tif (kp->pre_handler && likely(!kprobe_disabled(kp))) {\\n\\t\\t\\tset_kprobe_instance(kp);\\n\\t\\t\\tif (kp->pre_handler(kp, regs))\\n\\t\\t\\t\\treturn 1;\\n\\t\\t}\\n\\t\\treset_kprobe_instance();\\n\\t}\\n\\treturn 0;\\n}\\nNOKPROBE_SYMBOL(aggr_pre_handler);\\n\\nstatic void aggr_post_handler(struct kprobe *p, struct pt_regs *regs,\\n\\t\\t\\t      unsigned long flags)\\n{\\n\\tstruct kprobe *kp;\\n\\n\\tlist_for_each_entry_rcu(kp, &p->list, list) {\\n\\t\\tif (kp->post_handler && likely(!kprobe_disabled(kp))) {\\n\\t\\t\\tset_kprobe_instance(kp);\\n\\t\\t\\tkp->post_handler(kp, regs, flags);\\n\\t\\t\\treset_kprobe_instance();\\n\\t\\t}\\n\\t}\\n}\\nNOKPROBE_SYMBOL(aggr_post_handler);\\n\\n/* Walks the list and increments \\'nmissed\\' if \\'p\\' has child probes. */\\nvoid kprobes_inc_nmissed_count(struct kprobe *p)\\n{\\n\\tstruct kprobe *kp;\\n\\n\\tif (!kprobe_aggrprobe(p)) {\\n\\t\\tp->nmissed++;\\n\\t} else {\\n\\t\\tlist_for_each_entry_rcu(kp, &p->list, list)\\n\\t\\t\\tkp->nmissed++;\\n\\t}\\n}\\nNOKPROBE_SYMBOL(kprobes_inc_nmissed_count);\\n\\nstatic struct kprobe kprobe_busy = {\\n\\t.addr = (void *) get_kprobe,\\n};\\n\\nvoid kprobe_busy_begin(void)\\n{\\n\\tstruct kprobe_ctlblk *kcb;\\n\\n\\tpreempt_disable();\\n\\t__this_cpu_write(current_kprobe, &kprobe_busy);\\n\\tkcb = get_kprobe_ctlblk();\\n\\tkcb->kprobe_status = KPROBE_HIT_ACTIVE;\\n}\\n\\nvoid kprobe_busy_end(void)\\n{\\n\\t__this_cpu_write(current_kprobe, NULL);\\n\\tpreempt_enable();\\n}\\n\\n/* Add the new probe to \\'ap->list\\'. */\\nstatic int add_new_kprobe(struct kprobe *ap, struct kprobe *p)\\n{\\n\\tif (p->post_handler)\\n\\t\\tunoptimize_kprobe(ap, true);\\t/* Fall back to normal kprobe */\\n\\n\\tlist_add_rcu(&p->list, &ap->list);\\n\\tif (p->post_handler && !ap->post_handler)\\n\\t\\tap->post_handler = aggr_post_handler;\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * Fill in the required fields of the aggregator kprobe. Replace the\\n * earlier kprobe in the hlist with the aggregator kprobe.\\n */\\nstatic void init_aggr_kprobe(struct kprobe *ap, struct kprobe *p)\\n{\\n\\t/* Copy the insn slot of \\'p\\' to \\'ap\\'. */\\n\\tcopy_kprobe(p, ap);\\n\\tflush_insn_slot(ap);\\n\\tap->addr = p->addr;\\n\\tap->flags = p->flags & ~KPROBE_FLAG_OPTIMIZED;\\n\\tap->pre_handler = aggr_pre_handler;\\n\\t/* We don\\'t care the kprobe which has gone. */\\n\\tif (p->post_handler && !kprobe_gone(p))\\n\\t\\tap->post_handler = aggr_post_handler;\\n\\n\\tINIT_LIST_HEAD(&ap->list);\\n\\tINIT_HLIST_NODE(&ap->hlist);\\n\\n\\tlist_add_rcu(&p->list, &ap->list);\\n\\thlist_replace_rcu(&p->hlist, &ap->hlist);\\n}\\n\\n/*\\n * This registers the second or subsequent kprobe at the same address.\\n */\\nstatic int register_aggr_kprobe(struct kprobe *orig_p, struct kprobe *p)\\n{\\n\\tint ret = 0;\\n\\tstruct kprobe *ap = orig_p;\\n\\n\\tcpus_read_lock();\\n\\n\\t/* For preparing optimization, jump_label_text_reserved() is called */\\n\\tjump_label_lock();\\n\\tmutex_lock(&text_mutex);\\n\\n\\tif (!kprobe_aggrprobe(orig_p)) {\\n\\t\\t/* If \\'orig_p\\' is not an \\'aggr_kprobe\\', create new one. */\\n\\t\\tap = alloc_aggr_kprobe(orig_p);\\n\\t\\tif (!ap) {\\n\\t\\t\\tret = -ENOMEM;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tinit_aggr_kprobe(ap, orig_p);\\n\\t} else if (kprobe_unused(ap)) {\\n\\t\\t/* This probe is going to die. Rescue it */\\n\\t\\tret = reuse_unused_kprobe(ap);\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\tif (kprobe_gone(ap)) {\\n\\t\\t/*\\n\\t\\t * Attempting to insert new probe at the same location that\\n\\t\\t * had a probe in the module vaddr area which already\\n\\t\\t * freed. So, the instruction slot has already been\\n\\t\\t * released. We need a new slot for the new probe.\\n\\t\\t */\\n\\t\\tret = arch_prepare_kprobe(ap);\\n\\t\\tif (ret)\\n\\t\\t\\t/*\\n\\t\\t\\t * Even if fail to allocate new slot, don\\'t need to\\n\\t\\t\\t * free the \\'ap\\'. It will be used next time, or\\n\\t\\t\\t * freed by unregister_kprobe().\\n\\t\\t\\t */\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/* Prepare optimized instructions if possible. */\\n\\t\\tprepare_optimized_kprobe(ap);\\n\\n\\t\\t/*\\n\\t\\t * Clear gone flag to prevent allocating new slot again, and\\n\\t\\t * set disabled flag because it is not armed yet.\\n\\t\\t */\\n\\t\\tap->flags = (ap->flags & ~KPROBE_FLAG_GONE)\\n\\t\\t\\t    | KPROBE_FLAG_DISABLED;\\n\\t}\\n\\n\\t/* Copy the insn slot of \\'p\\' to \\'ap\\'. */\\n\\tcopy_kprobe(ap, p);\\n\\tret = add_new_kprobe(ap, p);\\n\\nout:\\n\\tmutex_unlock(&text_mutex);\\n\\tjump_label_unlock();\\n\\tcpus_read_unlock();\\n\\n\\tif (ret == 0 && kprobe_disabled(ap) && !kprobe_disabled(p)) {\\n\\t\\tap->flags &= ~KPROBE_FLAG_DISABLED;\\n\\t\\tif (!kprobes_all_disarmed) {\\n\\t\\t\\t/* Arm the breakpoint again. */\\n\\t\\t\\tret = arm_kprobe(ap);\\n\\t\\t\\tif (ret) {\\n\\t\\t\\t\\tap->flags |= KPROBE_FLAG_DISABLED;\\n\\t\\t\\t\\tlist_del_rcu(&p->list);\\n\\t\\t\\t\\tsynchronize_rcu();\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\treturn ret;\\n}\\n\\nbool __weak arch_within_kprobe_blacklist(unsigned long addr)\\n{\\n\\t/* The \\'__kprobes\\' functions and entry code must not be probed. */\\n\\treturn addr >= (unsigned long)__kprobes_text_start &&\\n\\t       addr < (unsigned long)__kprobes_text_end;\\n}\\n\\nstatic bool __within_kprobe_blacklist(unsigned long addr)\\n{\\n\\tstruct kprobe_blacklist_entry *ent;\\n\\n\\tif (arch_within_kprobe_blacklist(addr))\\n\\t\\treturn true;\\n\\t/*\\n\\t * If \\'kprobe_blacklist\\' is defined, check the address and\\n\\t * reject any probe registration in the prohibited area.\\n\\t */\\n\\tlist_for_each_entry(ent, &kprobe_blacklist, list) {\\n\\t\\tif (addr >= ent->start_addr && addr < ent->end_addr)\\n\\t\\t\\treturn true;\\n\\t}\\n\\treturn false;\\n}\\n\\nbool within_kprobe_blacklist(unsigned long addr)\\n{\\n\\tchar symname[KSYM_NAME_LEN], *p;\\n\\n\\tif (__within_kprobe_blacklist(addr))\\n\\t\\treturn true;\\n\\n\\t/* Check if the address is on a suffixed-symbol */\\n\\tif (!lookup_symbol_name(addr, symname)) {\\n\\t\\tp = strchr(symname, \\'.\\');\\n\\t\\tif (!p)\\n\\t\\t\\treturn false;\\n\\t\\t*p = \\'\\\\0\\';\\n\\t\\taddr = (unsigned long)kprobe_lookup_name(symname, 0);\\n\\t\\tif (addr)\\n\\t\\t\\treturn __within_kprobe_blacklist(addr);\\n\\t}\\n\\treturn false;\\n}\\n\\n/*\\n * arch_adjust_kprobe_addr - adjust the address\\n * @addr: symbol base address\\n * @offset: offset within the symbol\\n * @on_func_entry: was this @addr+@offset on the function entry\\n *\\n * Typically returns @addr + @offset, except for special cases where the\\n * function might be prefixed by a CFI landing pad, in that case any offset\\n * inside the landing pad is mapped to the first \\'real\\' instruction of the\\n * symbol.\\n *\\n * Specifically, for things like IBT/BTI, skip the resp. ENDBR/BTI.C\\n * instruction at +0.\\n */\\nkprobe_opcode_t *__weak arch_adjust_kprobe_addr(unsigned long addr,\\n\\t\\t\\t\\t\\t\\tunsigned long offset,\\n\\t\\t\\t\\t\\t\\tbool *on_func_entry)\\n{\\n\\t*on_func_entry = !offset;\\n\\treturn (kprobe_opcode_t *)(addr + offset);\\n}\\n\\n/*\\n * If \\'symbol_name\\' is specified, look it up and add the \\'offset\\'\\n * to it. This way, we can specify a relative address to a symbol.\\n * This returns encoded errors if it fails to look up symbol or invalid\\n * combination of parameters.\\n */\\nstatic kprobe_opcode_t *\\n_kprobe_addr(kprobe_opcode_t *addr, const char *symbol_name,\\n\\t     unsigned long offset, bool *on_func_entry)\\n{\\n\\tif ((symbol_name && addr) || (!symbol_name && !addr))\\n\\t\\tgoto invalid;\\n\\n\\tif (symbol_name) {\\n\\t\\t/*\\n\\t\\t * Input: @sym + @offset\\n\\t\\t * Output: @addr + @offset\\n\\t\\t *\\n\\t\\t * NOTE: kprobe_lookup_name() does *NOT* fold the offset\\n\\t\\t *       argument into it\\'s output!\\n\\t\\t */\\n\\t\\taddr = kprobe_lookup_name(symbol_name, offset);\\n\\t\\tif (!addr)\\n\\t\\t\\treturn ERR_PTR(-ENOENT);\\n\\t}\\n\\n\\t/*\\n\\t * So here we have @addr + @offset, displace it into a new\\n\\t * @addr\\' + @offset\\' where @addr\\' is the symbol start address.\\n\\t */\\n\\taddr = (void *)addr + offset;\\n\\tif (!kallsyms_lookup_size_offset((unsigned long)addr, NULL, &offset))\\n\\t\\treturn ERR_PTR(-ENOENT);\\n\\taddr = (void *)addr - offset;\\n\\n\\t/*\\n\\t * Then ask the architecture to re-combine them, taking care of\\n\\t * magical function entry details while telling us if this was indeed\\n\\t * at the start of the function.\\n\\t */\\n\\taddr = arch_adjust_kprobe_addr((unsigned long)addr, offset, on_func_entry);\\n\\tif (addr)\\n\\t\\treturn addr;\\n\\ninvalid:\\n\\treturn ERR_PTR(-EINVAL);\\n}\\n\\nstatic kprobe_opcode_t *kprobe_addr(struct kprobe *p)\\n{\\n\\tbool on_func_entry;\\n\\treturn _kprobe_addr(p->addr, p->symbol_name, p->offset, &on_func_entry);\\n}\\n\\n/*\\n * Check the \\'p\\' is valid and return the aggregator kprobe\\n * at the same address.\\n */\\nstatic struct kprobe *__get_valid_kprobe(struct kprobe *p)\\n{\\n\\tstruct kprobe *ap, *list_p;\\n\\n\\tlockdep_assert_held(&kprobe_mutex);\\n\\n\\tap = get_kprobe(p->addr);\\n\\tif (unlikely(!ap))\\n\\t\\treturn NULL;\\n\\n\\tif (p != ap) {\\n\\t\\tlist_for_each_entry(list_p, &ap->list, list)\\n\\t\\t\\tif (list_p == p)\\n\\t\\t\\t/* kprobe p is a valid probe */\\n\\t\\t\\t\\tgoto valid;\\n\\t\\treturn NULL;\\n\\t}\\nvalid:\\n\\treturn ap;\\n}\\n\\n/*\\n * Warn and return error if the kprobe is being re-registered since\\n * there must be a software bug.\\n */\\nstatic inline int warn_kprobe_rereg(struct kprobe *p)\\n{\\n\\tint ret = 0;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\tif (WARN_ON_ONCE(__get_valid_kprobe(p)))\\n\\t\\tret = -EINVAL;\\n\\tmutex_unlock(&kprobe_mutex);\\n\\n\\treturn ret;\\n}\\n\\nstatic int check_ftrace_location(struct kprobe *p)\\n{\\n\\tunsigned long addr = (unsigned long)p->addr;\\n\\n\\tif (ftrace_location(addr) == addr) {\\n#ifdef CONFIG_KPROBES_ON_FTRACE\\n\\t\\tp->flags |= KPROBE_FLAG_FTRACE;\\n#else\\n\\t\\treturn -EINVAL;\\n#endif\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic bool is_cfi_preamble_symbol(unsigned long addr)\\n{\\n\\tchar symbuf[KSYM_NAME_LEN];\\n\\n\\tif (lookup_symbol_name(addr, symbuf))\\n\\t\\treturn false;\\n\\n\\treturn str_has_prefix(symbuf, \"__cfi_\") ||\\n\\t\\tstr_has_prefix(symbuf, \"__pfx_\");\\n}\\n\\nstatic int check_kprobe_address_safe(struct kprobe *p,\\n\\t\\t\\t\\t     struct module **probed_mod)\\n{\\n\\tint ret;\\n\\n\\tret = check_ftrace_location(p);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\tjump_label_lock();\\n\\tpreempt_disable();\\n\\n\\t/* Ensure the address is in a text area, and find a module if exists. */\\n\\t*probed_mod = NULL;\\n\\tif (!core_kernel_text((unsigned long) p->addr)) {\\n\\t\\t*probed_mod = __module_text_address((unsigned long) p->addr);\\n\\t\\tif (!(*probed_mod)) {\\n\\t\\t\\tret = -EINVAL;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\t/* Ensure it is not in reserved area. */\\n\\tif (in_gate_area_no_mm((unsigned long) p->addr) ||\\n\\t    within_kprobe_blacklist((unsigned long) p->addr) ||\\n\\t    jump_label_text_reserved(p->addr, p->addr) ||\\n\\t    static_call_text_reserved(p->addr, p->addr) ||\\n\\t    find_bug((unsigned long)p->addr) ||\\n\\t    is_cfi_preamble_symbol((unsigned long)p->addr)) {\\n\\t\\tret = -EINVAL;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t/* Get module refcount and reject __init functions for loaded modules. */\\n\\tif (IS_ENABLED(CONFIG_MODULES) && *probed_mod) {\\n\\t\\t/*\\n\\t\\t * We must hold a refcount of the probed module while updating\\n\\t\\t * its code to prohibit unexpected unloading.\\n\\t\\t */\\n\\t\\tif (unlikely(!try_module_get(*probed_mod))) {\\n\\t\\t\\tret = -ENOENT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * If the module freed \\'.init.text\\', we couldn\\'t insert\\n\\t\\t * kprobes in there.\\n\\t\\t */\\n\\t\\tif (within_module_init((unsigned long)p->addr, *probed_mod) &&\\n\\t\\t    !module_is_coming(*probed_mod)) {\\n\\t\\t\\tmodule_put(*probed_mod);\\n\\t\\t\\t*probed_mod = NULL;\\n\\t\\t\\tret = -ENOENT;\\n\\t\\t}\\n\\t}\\n\\nout:\\n\\tpreempt_enable();\\n\\tjump_label_unlock();\\n\\n\\treturn ret;\\n}\\n\\nint register_kprobe(struct kprobe *p)\\n{\\n\\tint ret;\\n\\tstruct kprobe *old_p;\\n\\tstruct module *probed_mod;\\n\\tkprobe_opcode_t *addr;\\n\\tbool on_func_entry;\\n\\n\\t/* Adjust probe address from symbol */\\n\\taddr = _kprobe_addr(p->addr, p->symbol_name, p->offset, &on_func_entry);\\n\\tif (IS_ERR(addr))\\n\\t\\treturn PTR_ERR(addr);\\n\\tp->addr = addr;\\n\\n\\tret = warn_kprobe_rereg(p);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/* User can pass only KPROBE_FLAG_DISABLED to register_kprobe */\\n\\tp->flags &= KPROBE_FLAG_DISABLED;\\n\\tp->nmissed = 0;\\n\\tINIT_LIST_HEAD(&p->list);\\n\\n\\tret = check_kprobe_address_safe(p, &probed_mod);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\n\\tif (on_func_entry)\\n\\t\\tp->flags |= KPROBE_FLAG_ON_FUNC_ENTRY;\\n\\n\\told_p = get_kprobe(p->addr);\\n\\tif (old_p) {\\n\\t\\t/* Since this may unoptimize \\'old_p\\', locking \\'text_mutex\\'. */\\n\\t\\tret = register_aggr_kprobe(old_p, p);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tcpus_read_lock();\\n\\t/* Prevent text modification */\\n\\tmutex_lock(&text_mutex);\\n\\tret = prepare_kprobe(p);\\n\\tmutex_unlock(&text_mutex);\\n\\tcpus_read_unlock();\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\tINIT_HLIST_NODE(&p->hlist);\\n\\thlist_add_head_rcu(&p->hlist,\\n\\t\\t       &kprobe_table[hash_ptr(p->addr, KPROBE_HASH_BITS)]);\\n\\n\\tif (!kprobes_all_disarmed && !kprobe_disabled(p)) {\\n\\t\\tret = arm_kprobe(p);\\n\\t\\tif (ret) {\\n\\t\\t\\thlist_del_rcu(&p->hlist);\\n\\t\\t\\tsynchronize_rcu();\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\n\\t/* Try to optimize kprobe */\\n\\ttry_to_optimize_kprobe(p);\\nout:\\n\\tmutex_unlock(&kprobe_mutex);\\n\\n\\tif (probed_mod)\\n\\t\\tmodule_put(probed_mod);\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(register_kprobe);\\n\\n/* Check if all probes on the \\'ap\\' are disabled. */\\nstatic bool aggr_kprobe_disabled(struct kprobe *ap)\\n{\\n\\tstruct kprobe *kp;\\n\\n\\tlockdep_assert_held(&kprobe_mutex);\\n\\n\\tlist_for_each_entry(kp, &ap->list, list)\\n\\t\\tif (!kprobe_disabled(kp))\\n\\t\\t\\t/*\\n\\t\\t\\t * Since there is an active probe on the list,\\n\\t\\t\\t * we can\\'t disable this \\'ap\\'.\\n\\t\\t\\t */\\n\\t\\t\\treturn false;\\n\\n\\treturn true;\\n}\\n\\nstatic struct kprobe *__disable_kprobe(struct kprobe *p)\\n{\\n\\tstruct kprobe *orig_p;\\n\\tint ret;\\n\\n\\tlockdep_assert_held(&kprobe_mutex);\\n\\n\\t/* Get an original kprobe for return */\\n\\torig_p = __get_valid_kprobe(p);\\n\\tif (unlikely(orig_p == NULL))\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\tif (kprobe_disabled(p))\\n\\t\\treturn orig_p;\\n\\n\\t/* Disable probe if it is a child probe */\\n\\tif (p != orig_p)\\n\\t\\tp->flags |= KPROBE_FLAG_DISABLED;\\n\\n\\t/* Try to disarm and disable this/parent probe */\\n\\tif (p == orig_p || aggr_kprobe_disabled(orig_p)) {\\n\\t\\t/*\\n\\t\\t * Don\\'t be lazy here.  Even if \\'kprobes_all_disarmed\\'\\n\\t\\t * is false, \\'orig_p\\' might not have been armed yet.\\n\\t\\t * Note arm_all_kprobes() __tries__ to arm all kprobes\\n\\t\\t * on the best effort basis.\\n\\t\\t */\\n\\t\\tif (!kprobes_all_disarmed && !kprobe_disabled(orig_p)) {\\n\\t\\t\\tret = disarm_kprobe(orig_p, true);\\n\\t\\t\\tif (ret) {\\n\\t\\t\\t\\tp->flags &= ~KPROBE_FLAG_DISABLED;\\n\\t\\t\\t\\treturn ERR_PTR(ret);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\torig_p->flags |= KPROBE_FLAG_DISABLED;\\n\\t}\\n\\n\\treturn orig_p;\\n}\\n\\n/*\\n * Unregister a kprobe without a scheduler synchronization.\\n */\\nstatic int __unregister_kprobe_top(struct kprobe *p)\\n{\\n\\tstruct kprobe *ap, *list_p;\\n\\n\\t/* Disable kprobe. This will disarm it if needed. */\\n\\tap = __disable_kprobe(p);\\n\\tif (IS_ERR(ap))\\n\\t\\treturn PTR_ERR(ap);\\n\\n\\tif (ap == p)\\n\\t\\t/*\\n\\t\\t * This probe is an independent(and non-optimized) kprobe\\n\\t\\t * (not an aggrprobe). Remove from the hash list.\\n\\t\\t */\\n\\t\\tgoto disarmed;\\n\\n\\t/* Following process expects this probe is an aggrprobe */\\n\\tWARN_ON(!kprobe_aggrprobe(ap));\\n\\n\\tif (list_is_singular(&ap->list) && kprobe_disarmed(ap))\\n\\t\\t/*\\n\\t\\t * !disarmed could be happen if the probe is under delayed\\n\\t\\t * unoptimizing.\\n\\t\\t */\\n\\t\\tgoto disarmed;\\n\\telse {\\n\\t\\t/* If disabling probe has special handlers, update aggrprobe */\\n\\t\\tif (p->post_handler && !kprobe_gone(p)) {\\n\\t\\t\\tlist_for_each_entry(list_p, &ap->list, list) {\\n\\t\\t\\t\\tif ((list_p != p) && (list_p->post_handler))\\n\\t\\t\\t\\t\\tgoto noclean;\\n\\t\\t\\t}\\n\\t\\t\\t/*\\n\\t\\t\\t * For the kprobe-on-ftrace case, we keep the\\n\\t\\t\\t * post_handler setting to identify this aggrprobe\\n\\t\\t\\t * armed with kprobe_ipmodify_ops.\\n\\t\\t\\t */\\n\\t\\t\\tif (!kprobe_ftrace(ap))\\n\\t\\t\\t\\tap->post_handler = NULL;\\n\\t\\t}\\nnoclean:\\n\\t\\t/*\\n\\t\\t * Remove from the aggrprobe: this path will do nothing in\\n\\t\\t * __unregister_kprobe_bottom().\\n\\t\\t */\\n\\t\\tlist_del_rcu(&p->list);\\n\\t\\tif (!kprobe_disabled(ap) && !kprobes_all_disarmed)\\n\\t\\t\\t/*\\n\\t\\t\\t * Try to optimize this probe again, because post\\n\\t\\t\\t * handler may have been changed.\\n\\t\\t\\t */\\n\\t\\t\\toptimize_kprobe(ap);\\n\\t}\\n\\treturn 0;\\n\\ndisarmed:\\n\\thlist_del_rcu(&ap->hlist);\\n\\treturn 0;\\n}\\n\\nstatic void __unregister_kprobe_bottom(struct kprobe *p)\\n{\\n\\tstruct kprobe *ap;\\n\\n\\tif (list_empty(&p->list))\\n\\t\\t/* This is an independent kprobe */\\n\\t\\tarch_remove_kprobe(p);\\n\\telse if (list_is_singular(&p->list)) {\\n\\t\\t/* This is the last child of an aggrprobe */\\n\\t\\tap = list_entry(p->list.next, struct kprobe, list);\\n\\t\\tlist_del(&p->list);\\n\\t\\tfree_aggr_kprobe(ap);\\n\\t}\\n\\t/* Otherwise, do nothing. */\\n}\\n\\nint register_kprobes(struct kprobe **kps, int num)\\n{\\n\\tint i, ret = 0;\\n\\n\\tif (num <= 0)\\n\\t\\treturn -EINVAL;\\n\\tfor (i = 0; i < num; i++) {\\n\\t\\tret = register_kprobe(kps[i]);\\n\\t\\tif (ret < 0) {\\n\\t\\t\\tif (i > 0)\\n\\t\\t\\t\\tunregister_kprobes(kps, i);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(register_kprobes);\\n\\nvoid unregister_kprobe(struct kprobe *p)\\n{\\n\\tunregister_kprobes(&p, 1);\\n}\\nEXPORT_SYMBOL_GPL(unregister_kprobe);\\n\\nvoid unregister_kprobes(struct kprobe **kps, int num)\\n{\\n\\tint i;\\n\\n\\tif (num <= 0)\\n\\t\\treturn;\\n\\tmutex_lock(&kprobe_mutex);\\n\\tfor (i = 0; i < num; i++)\\n\\t\\tif (__unregister_kprobe_top(kps[i]) < 0)\\n\\t\\t\\tkps[i]->addr = NULL;\\n\\tmutex_unlock(&kprobe_mutex);\\n\\n\\tsynchronize_rcu();\\n\\tfor (i = 0; i < num; i++)\\n\\t\\tif (kps[i]->addr)\\n\\t\\t\\t__unregister_kprobe_bottom(kps[i]);\\n}\\nEXPORT_SYMBOL_GPL(unregister_kprobes);\\n\\nint __weak kprobe_exceptions_notify(struct notifier_block *self,\\n\\t\\t\\t\\t\\tunsigned long val, void *data)\\n{\\n\\treturn NOTIFY_DONE;\\n}\\nNOKPROBE_SYMBOL(kprobe_exceptions_notify);\\n\\nstatic struct notifier_block kprobe_exceptions_nb = {\\n\\t.notifier_call = kprobe_exceptions_notify,\\n\\t.priority = 0x7fffffff /* we need to be notified first */\\n};\\n\\n#ifdef CONFIG_KRETPROBES\\n\\n#if !defined(CONFIG_KRETPROBE_ON_RETHOOK)\\n\\n/* callbacks for objpool of kretprobe instances */\\nstatic int kretprobe_init_inst(void *nod, void *context)\\n{\\n\\tstruct kretprobe_instance *ri = nod;\\n\\n\\tri->rph = context;\\n\\treturn 0;\\n}\\nstatic int kretprobe_fini_pool(struct objpool_head *head, void *context)\\n{\\n\\tkfree(context);\\n\\treturn 0;\\n}\\n\\nstatic void free_rp_inst_rcu(struct rcu_head *head)\\n{\\n\\tstruct kretprobe_instance *ri = container_of(head, struct kretprobe_instance, rcu);\\n\\tstruct kretprobe_holder *rph = ri->rph;\\n\\n\\tobjpool_drop(ri, &rph->pool);\\n}\\nNOKPROBE_SYMBOL(free_rp_inst_rcu);\\n\\nstatic void recycle_rp_inst(struct kretprobe_instance *ri)\\n{\\n\\tstruct kretprobe *rp = get_kretprobe(ri);\\n\\n\\tif (likely(rp))\\n\\t\\tobjpool_push(ri, &rp->rph->pool);\\n\\telse\\n\\t\\tcall_rcu(&ri->rcu, free_rp_inst_rcu);\\n}\\nNOKPROBE_SYMBOL(recycle_rp_inst);\\n\\n/*\\n * This function is called from delayed_put_task_struct() when a task is\\n * dead and cleaned up to recycle any kretprobe instances associated with\\n * this task. These left over instances represent probed functions that\\n * have been called but will never return.\\n */\\nvoid kprobe_flush_task(struct task_struct *tk)\\n{\\n\\tstruct kretprobe_instance *ri;\\n\\tstruct llist_node *node;\\n\\n\\t/* Early boot, not yet initialized. */\\n\\tif (unlikely(!kprobes_initialized))\\n\\t\\treturn;\\n\\n\\tkprobe_busy_begin();\\n\\n\\tnode = __llist_del_all(&tk->kretprobe_instances);\\n\\twhile (node) {\\n\\t\\tri = container_of(node, struct kretprobe_instance, llist);\\n\\t\\tnode = node->next;\\n\\n\\t\\trecycle_rp_inst(ri);\\n\\t}\\n\\n\\tkprobe_busy_end();\\n}\\nNOKPROBE_SYMBOL(kprobe_flush_task);\\n\\nstatic inline void free_rp_inst(struct kretprobe *rp)\\n{\\n\\tstruct kretprobe_holder *rph = rp->rph;\\n\\n\\tif (!rph)\\n\\t\\treturn;\\n\\trp->rph = NULL;\\n\\tobjpool_fini(&rph->pool);\\n}\\n\\n/* This assumes the \\'tsk\\' is the current task or the is not running. */\\nstatic kprobe_opcode_t *__kretprobe_find_ret_addr(struct task_struct *tsk,\\n\\t\\t\\t\\t\\t\\t  struct llist_node **cur)\\n{\\n\\tstruct kretprobe_instance *ri = NULL;\\n\\tstruct llist_node *node = *cur;\\n\\n\\tif (!node)\\n\\t\\tnode = tsk->kretprobe_instances.first;\\n\\telse\\n\\t\\tnode = node->next;\\n\\n\\twhile (node) {\\n\\t\\tri = container_of(node, struct kretprobe_instance, llist);\\n\\t\\tif (ri->ret_addr != kretprobe_trampoline_addr()) {\\n\\t\\t\\t*cur = node;\\n\\t\\t\\treturn ri->ret_addr;\\n\\t\\t}\\n\\t\\tnode = node->next;\\n\\t}\\n\\treturn NULL;\\n}\\nNOKPROBE_SYMBOL(__kretprobe_find_ret_addr);\\n\\n/**\\n * kretprobe_find_ret_addr -- Find correct return address modified by kretprobe\\n * @tsk: Target task\\n * @fp: A frame pointer\\n * @cur: a storage of the loop cursor llist_node pointer for next call\\n *\\n * Find the correct return address modified by a kretprobe on @tsk in unsigned\\n * long type. If it finds the return address, this returns that address value,\\n * or this returns 0.\\n * The @tsk must be \\'current\\' or a task which is not running. @fp is a hint\\n * to get the currect return address - which is compared with the\\n * kretprobe_instance::fp field. The @cur is a loop cursor for searching the\\n * kretprobe return addresses on the @tsk. The \\'*@cur\\' should be NULL at the\\n * first call, but \\'@cur\\' itself must NOT NULL.\\n */\\nunsigned long kretprobe_find_ret_addr(struct task_struct *tsk, void *fp,\\n\\t\\t\\t\\t      struct llist_node **cur)\\n{\\n\\tstruct kretprobe_instance *ri;\\n\\tkprobe_opcode_t *ret;\\n\\n\\tif (WARN_ON_ONCE(!cur))\\n\\t\\treturn 0;\\n\\n\\tdo {\\n\\t\\tret = __kretprobe_find_ret_addr(tsk, cur);\\n\\t\\tif (!ret)\\n\\t\\t\\tbreak;\\n\\t\\tri = container_of(*cur, struct kretprobe_instance, llist);\\n\\t} while (ri->fp != fp);\\n\\n\\treturn (unsigned long)ret;\\n}\\nNOKPROBE_SYMBOL(kretprobe_find_ret_addr);\\n\\nvoid __weak arch_kretprobe_fixup_return(struct pt_regs *regs,\\n\\t\\t\\t\\t\\tkprobe_opcode_t *correct_ret_addr)\\n{\\n\\t/*\\n\\t * Do nothing by default. Please fill this to update the fake return\\n\\t * address on the stack with the correct one on each arch if possible.\\n\\t */\\n}\\n\\nunsigned long __kretprobe_trampoline_handler(struct pt_regs *regs,\\n\\t\\t\\t\\t\\t     void *frame_pointer)\\n{\\n\\tstruct kretprobe_instance *ri = NULL;\\n\\tstruct llist_node *first, *node = NULL;\\n\\tkprobe_opcode_t *correct_ret_addr;\\n\\tstruct kretprobe *rp;\\n\\n\\t/* Find correct address and all nodes for this frame. */\\n\\tcorrect_ret_addr = __kretprobe_find_ret_addr(current, &node);\\n\\tif (!correct_ret_addr) {\\n\\t\\tpr_err(\"kretprobe: Return address not found, not execute handler. Maybe there is a bug in the kernel.\\\\n\");\\n\\t\\tBUG_ON(1);\\n\\t}\\n\\n\\t/*\\n\\t * Set the return address as the instruction pointer, because if the\\n\\t * user handler calls stack_trace_save_regs() with this \\'regs\\',\\n\\t * the stack trace will start from the instruction pointer.\\n\\t */\\n\\tinstruction_pointer_set(regs, (unsigned long)correct_ret_addr);\\n\\n\\t/* Run the user handler of the nodes. */\\n\\tfirst = current->kretprobe_instances.first;\\n\\twhile (first) {\\n\\t\\tri = container_of(first, struct kretprobe_instance, llist);\\n\\n\\t\\tif (WARN_ON_ONCE(ri->fp != frame_pointer))\\n\\t\\t\\tbreak;\\n\\n\\t\\trp = get_kretprobe(ri);\\n\\t\\tif (rp && rp->handler) {\\n\\t\\t\\tstruct kprobe *prev = kprobe_running();\\n\\n\\t\\t\\t__this_cpu_write(current_kprobe, &rp->kp);\\n\\t\\t\\tri->ret_addr = correct_ret_addr;\\n\\t\\t\\trp->handler(ri, regs);\\n\\t\\t\\t__this_cpu_write(current_kprobe, prev);\\n\\t\\t}\\n\\t\\tif (first == node)\\n\\t\\t\\tbreak;\\n\\n\\t\\tfirst = first->next;\\n\\t}\\n\\n\\tarch_kretprobe_fixup_return(regs, correct_ret_addr);\\n\\n\\t/* Unlink all nodes for this frame. */\\n\\tfirst = current->kretprobe_instances.first;\\n\\tcurrent->kretprobe_instances.first = node->next;\\n\\tnode->next = NULL;\\n\\n\\t/* Recycle free instances. */\\n\\twhile (first) {\\n\\t\\tri = container_of(first, struct kretprobe_instance, llist);\\n\\t\\tfirst = first->next;\\n\\n\\t\\trecycle_rp_inst(ri);\\n\\t}\\n\\n\\treturn (unsigned long)correct_ret_addr;\\n}\\nNOKPROBE_SYMBOL(__kretprobe_trampoline_handler)\\n\\n/*\\n * This kprobe pre_handler is registered with every kretprobe. When probe\\n * hits it will set up the return probe.\\n */\\nstatic int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)\\n{\\n\\tstruct kretprobe *rp = container_of(p, struct kretprobe, kp);\\n\\tstruct kretprobe_holder *rph = rp->rph;\\n\\tstruct kretprobe_instance *ri;\\n\\n\\tri = objpool_pop(&rph->pool);\\n\\tif (!ri) {\\n\\t\\trp->nmissed++;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (rp->entry_handler && rp->entry_handler(ri, regs)) {\\n\\t\\tobjpool_push(ri, &rph->pool);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tarch_prepare_kretprobe(ri, regs);\\n\\n\\t__llist_add(&ri->llist, &current->kretprobe_instances);\\n\\n\\treturn 0;\\n}\\nNOKPROBE_SYMBOL(pre_handler_kretprobe);\\n#else /* CONFIG_KRETPROBE_ON_RETHOOK */\\n/*\\n * This kprobe pre_handler is registered with every kretprobe. When probe\\n * hits it will set up the return probe.\\n */\\nstatic int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)\\n{\\n\\tstruct kretprobe *rp = container_of(p, struct kretprobe, kp);\\n\\tstruct kretprobe_instance *ri;\\n\\tstruct rethook_node *rhn;\\n\\n\\trhn = rethook_try_get(rp->rh);\\n\\tif (!rhn) {\\n\\t\\trp->nmissed++;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tri = container_of(rhn, struct kretprobe_instance, node);\\n\\n\\tif (rp->entry_handler && rp->entry_handler(ri, regs))\\n\\t\\trethook_recycle(rhn);\\n\\telse\\n\\t\\trethook_hook(rhn, regs, kprobe_ftrace(p));\\n\\n\\treturn 0;\\n}\\nNOKPROBE_SYMBOL(pre_handler_kretprobe);\\n\\nstatic void kretprobe_rethook_handler(struct rethook_node *rh, void *data,\\n\\t\\t\\t\\t      unsigned long ret_addr,\\n\\t\\t\\t\\t      struct pt_regs *regs)\\n{\\n\\tstruct kretprobe *rp = (struct kretprobe *)data;\\n\\tstruct kretprobe_instance *ri;\\n\\tstruct kprobe_ctlblk *kcb;\\n\\n\\t/* The data must NOT be null. This means rethook data structure is broken. */\\n\\tif (WARN_ON_ONCE(!data) || !rp->handler)\\n\\t\\treturn;\\n\\n\\t__this_cpu_write(current_kprobe, &rp->kp);\\n\\tkcb = get_kprobe_ctlblk();\\n\\tkcb->kprobe_status = KPROBE_HIT_ACTIVE;\\n\\n\\tri = container_of(rh, struct kretprobe_instance, node);\\n\\trp->handler(ri, regs);\\n\\n\\t__this_cpu_write(current_kprobe, NULL);\\n}\\nNOKPROBE_SYMBOL(kretprobe_rethook_handler);\\n\\n#endif /* !CONFIG_KRETPROBE_ON_RETHOOK */\\n\\n/**\\n * kprobe_on_func_entry() -- check whether given address is function entry\\n * @addr: Target address\\n * @sym:  Target symbol name\\n * @offset: The offset from the symbol or the address\\n *\\n * This checks whether the given @addr+@offset or @sym+@offset is on the\\n * function entry address or not.\\n * This returns 0 if it is the function entry, or -EINVAL if it is not.\\n * And also it returns -ENOENT if it fails the symbol or address lookup.\\n * Caller must pass @addr or @sym (either one must be NULL), or this\\n * returns -EINVAL.\\n */\\nint kprobe_on_func_entry(kprobe_opcode_t *addr, const char *sym, unsigned long offset)\\n{\\n\\tbool on_func_entry;\\n\\tkprobe_opcode_t *kp_addr = _kprobe_addr(addr, sym, offset, &on_func_entry);\\n\\n\\tif (IS_ERR(kp_addr))\\n\\t\\treturn PTR_ERR(kp_addr);\\n\\n\\tif (!on_func_entry)\\n\\t\\treturn -EINVAL;\\n\\n\\treturn 0;\\n}\\n\\nint register_kretprobe(struct kretprobe *rp)\\n{\\n\\tint ret;\\n\\tint i;\\n\\tvoid *addr;\\n\\n\\tret = kprobe_on_func_entry(rp->kp.addr, rp->kp.symbol_name, rp->kp.offset);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/* If only \\'rp->kp.addr\\' is specified, check reregistering kprobes */\\n\\tif (rp->kp.addr && warn_kprobe_rereg(&rp->kp))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (kretprobe_blacklist_size) {\\n\\t\\taddr = kprobe_addr(&rp->kp);\\n\\t\\tif (IS_ERR(addr))\\n\\t\\t\\treturn PTR_ERR(addr);\\n\\n\\t\\tfor (i = 0; kretprobe_blacklist[i].name != NULL; i++) {\\n\\t\\t\\tif (kretprobe_blacklist[i].addr == addr)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t}\\n\\n\\tif (rp->data_size > KRETPROBE_MAX_DATA_SIZE)\\n\\t\\treturn -E2BIG;\\n\\n\\trp->kp.pre_handler = pre_handler_kretprobe;\\n\\trp->kp.post_handler = NULL;\\n\\n\\t/* Pre-allocate memory for max kretprobe instances */\\n\\tif (rp->maxactive <= 0)\\n\\t\\trp->maxactive = max_t(unsigned int, 10, 2*num_possible_cpus());\\n\\n#ifdef CONFIG_KRETPROBE_ON_RETHOOK\\n\\trp->rh = rethook_alloc((void *)rp, kretprobe_rethook_handler,\\n\\t\\t\\t\\tsizeof(struct kretprobe_instance) +\\n\\t\\t\\t\\trp->data_size, rp->maxactive);\\n\\tif (IS_ERR(rp->rh))\\n\\t\\treturn PTR_ERR(rp->rh);\\n\\n\\trp->nmissed = 0;\\n\\t/* Establish function entry probe point */\\n\\tret = register_kprobe(&rp->kp);\\n\\tif (ret != 0) {\\n\\t\\trethook_free(rp->rh);\\n\\t\\trp->rh = NULL;\\n\\t}\\n#else\\t/* !CONFIG_KRETPROBE_ON_RETHOOK */\\n\\trp->rph = kzalloc(sizeof(struct kretprobe_holder), GFP_KERNEL);\\n\\tif (!rp->rph)\\n\\t\\treturn -ENOMEM;\\n\\n\\tif (objpool_init(&rp->rph->pool, rp->maxactive, rp->data_size +\\n\\t\\t\\tsizeof(struct kretprobe_instance), GFP_KERNEL,\\n\\t\\t\\trp->rph, kretprobe_init_inst, kretprobe_fini_pool)) {\\n\\t\\tkfree(rp->rph);\\n\\t\\trp->rph = NULL;\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\trcu_assign_pointer(rp->rph->rp, rp);\\n\\trp->nmissed = 0;\\n\\t/* Establish function entry probe point */\\n\\tret = register_kprobe(&rp->kp);\\n\\tif (ret != 0)\\n\\t\\tfree_rp_inst(rp);\\n#endif\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(register_kretprobe);\\n\\nint register_kretprobes(struct kretprobe **rps, int num)\\n{\\n\\tint ret = 0, i;\\n\\n\\tif (num <= 0)\\n\\t\\treturn -EINVAL;\\n\\tfor (i = 0; i < num; i++) {\\n\\t\\tret = register_kretprobe(rps[i]);\\n\\t\\tif (ret < 0) {\\n\\t\\t\\tif (i > 0)\\n\\t\\t\\t\\tunregister_kretprobes(rps, i);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(register_kretprobes);\\n\\nvoid unregister_kretprobe(struct kretprobe *rp)\\n{\\n\\tunregister_kretprobes(&rp, 1);\\n}\\nEXPORT_SYMBOL_GPL(unregister_kretprobe);\\n\\nvoid unregister_kretprobes(struct kretprobe **rps, int num)\\n{\\n\\tint i;\\n\\n\\tif (num <= 0)\\n\\t\\treturn;\\n\\tmutex_lock(&kprobe_mutex);\\n\\tfor (i = 0; i < num; i++) {\\n\\t\\tif (__unregister_kprobe_top(&rps[i]->kp) < 0)\\n\\t\\t\\trps[i]->kp.addr = NULL;\\n#ifdef CONFIG_KRETPROBE_ON_RETHOOK\\n\\t\\trethook_free(rps[i]->rh);\\n#else\\n\\t\\trcu_assign_pointer(rps[i]->rph->rp, NULL);\\n#endif\\n\\t}\\n\\tmutex_unlock(&kprobe_mutex);\\n\\n\\tsynchronize_rcu();\\n\\tfor (i = 0; i < num; i++) {\\n\\t\\tif (rps[i]->kp.addr) {\\n\\t\\t\\t__unregister_kprobe_bottom(&rps[i]->kp);\\n#ifndef CONFIG_KRETPROBE_ON_RETHOOK\\n\\t\\t\\tfree_rp_inst(rps[i]);\\n#endif\\n\\t\\t}\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(unregister_kretprobes);\\n\\n#else /* CONFIG_KRETPROBES */\\nint register_kretprobe(struct kretprobe *rp)\\n{\\n\\treturn -EOPNOTSUPP;\\n}\\nEXPORT_SYMBOL_GPL(register_kretprobe);\\n\\nint register_kretprobes(struct kretprobe **rps, int num)\\n{\\n\\treturn -EOPNOTSUPP;\\n}\\nEXPORT_SYMBOL_GPL(register_kretprobes);\\n\\nvoid unregister_kretprobe(struct kretprobe *rp)\\n{\\n}\\nEXPORT_SYMBOL_GPL(unregister_kretprobe);\\n\\nvoid unregister_kretprobes(struct kretprobe **rps, int num)\\n{\\n}\\nEXPORT_SYMBOL_GPL(unregister_kretprobes);\\n\\nstatic int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)\\n{\\n\\treturn 0;\\n}\\nNOKPROBE_SYMBOL(pre_handler_kretprobe);\\n\\n#endif /* CONFIG_KRETPROBES */\\n\\n/* Set the kprobe gone and remove its instruction buffer. */\\nstatic void kill_kprobe(struct kprobe *p)\\n{\\n\\tstruct kprobe *kp;\\n\\n\\tlockdep_assert_held(&kprobe_mutex);\\n\\n\\t/*\\n\\t * The module is going away. We should disarm the kprobe which\\n\\t * is using ftrace, because ftrace framework is still available at\\n\\t * \\'MODULE_STATE_GOING\\' notification.\\n\\t */\\n\\tif (kprobe_ftrace(p) && !kprobe_disabled(p) && !kprobes_all_disarmed)\\n\\t\\tdisarm_kprobe_ftrace(p);\\n\\n\\tp->flags |= KPROBE_FLAG_GONE;\\n\\tif (kprobe_aggrprobe(p)) {\\n\\t\\t/*\\n\\t\\t * If this is an aggr_kprobe, we have to list all the\\n\\t\\t * chained probes and mark them GONE.\\n\\t\\t */\\n\\t\\tlist_for_each_entry(kp, &p->list, list)\\n\\t\\t\\tkp->flags |= KPROBE_FLAG_GONE;\\n\\t\\tp->post_handler = NULL;\\n\\t\\tkill_optimized_kprobe(p);\\n\\t}\\n\\t/*\\n\\t * Here, we can remove insn_slot safely, because no thread calls\\n\\t * the original probed function (which will be freed soon) any more.\\n\\t */\\n\\tarch_remove_kprobe(p);\\n}\\n\\n/* Disable one kprobe */\\nint disable_kprobe(struct kprobe *kp)\\n{\\n\\tint ret = 0;\\n\\tstruct kprobe *p;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\n\\t/* Disable this kprobe */\\n\\tp = __disable_kprobe(kp);\\n\\tif (IS_ERR(p))\\n\\t\\tret = PTR_ERR(p);\\n\\n\\tmutex_unlock(&kprobe_mutex);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(disable_kprobe);\\n\\n/* Enable one kprobe */\\nint enable_kprobe(struct kprobe *kp)\\n{\\n\\tint ret = 0;\\n\\tstruct kprobe *p;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\n\\t/* Check whether specified probe is valid. */\\n\\tp = __get_valid_kprobe(kp);\\n\\tif (unlikely(p == NULL)) {\\n\\t\\tret = -EINVAL;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (kprobe_gone(kp)) {\\n\\t\\t/* This kprobe has gone, we couldn\\'t enable it. */\\n\\t\\tret = -EINVAL;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (p != kp)\\n\\t\\tkp->flags &= ~KPROBE_FLAG_DISABLED;\\n\\n\\tif (!kprobes_all_disarmed && kprobe_disabled(p)) {\\n\\t\\tp->flags &= ~KPROBE_FLAG_DISABLED;\\n\\t\\tret = arm_kprobe(p);\\n\\t\\tif (ret) {\\n\\t\\t\\tp->flags |= KPROBE_FLAG_DISABLED;\\n\\t\\t\\tif (p != kp)\\n\\t\\t\\t\\tkp->flags |= KPROBE_FLAG_DISABLED;\\n\\t\\t}\\n\\t}\\nout:\\n\\tmutex_unlock(&kprobe_mutex);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(enable_kprobe);\\n\\n/* Caller must NOT call this in usual path. This is only for critical case */\\nvoid dump_kprobe(struct kprobe *kp)\\n{\\n\\tpr_err(\"Dump kprobe:\\\\n.symbol_name = %s, .offset = %x, .addr = %pS\\\\n\",\\n\\t       kp->symbol_name, kp->offset, kp->addr);\\n}\\nNOKPROBE_SYMBOL(dump_kprobe);\\n\\nint kprobe_add_ksym_blacklist(unsigned long entry)\\n{\\n\\tstruct kprobe_blacklist_entry *ent;\\n\\tunsigned long offset = 0, size = 0;\\n\\n\\tif (!kernel_text_address(entry) ||\\n\\t    !kallsyms_lookup_size_offset(entry, &size, &offset))\\n\\t\\treturn -EINVAL;\\n\\n\\tent = kmalloc(sizeof(*ent), GFP_KERNEL);\\n\\tif (!ent)\\n\\t\\treturn -ENOMEM;\\n\\tent->start_addr = entry;\\n\\tent->end_addr = entry + size;\\n\\tINIT_LIST_HEAD(&ent->list);\\n\\tlist_add_tail(&ent->list, &kprobe_blacklist);\\n\\n\\treturn (int)size;\\n}\\n\\n/* Add all symbols in given area into kprobe blacklist */\\nint kprobe_add_area_blacklist(unsigned long start, unsigned long end)\\n{\\n\\tunsigned long entry;\\n\\tint ret = 0;\\n\\n\\tfor (entry = start; entry < end; entry += ret) {\\n\\t\\tret = kprobe_add_ksym_blacklist(entry);\\n\\t\\tif (ret < 0)\\n\\t\\t\\treturn ret;\\n\\t\\tif (ret == 0)\\t/* In case of alias symbol */\\n\\t\\t\\tret = 1;\\n\\t}\\n\\treturn 0;\\n}\\n\\nint __weak arch_kprobe_get_kallsym(unsigned int *symnum, unsigned long *value,\\n\\t\\t\\t\\t   char *type, char *sym)\\n{\\n\\treturn -ERANGE;\\n}\\n\\nint kprobe_get_kallsym(unsigned int symnum, unsigned long *value, char *type,\\n\\t\\t       char *sym)\\n{\\n#ifdef __ARCH_WANT_KPROBES_INSN_SLOT\\n\\tif (!kprobe_cache_get_kallsym(&kprobe_insn_slots, &symnum, value, type, sym))\\n\\t\\treturn 0;\\n#ifdef CONFIG_OPTPROBES\\n\\tif (!kprobe_cache_get_kallsym(&kprobe_optinsn_slots, &symnum, value, type, sym))\\n\\t\\treturn 0;\\n#endif\\n#endif\\n\\tif (!arch_kprobe_get_kallsym(&symnum, value, type, sym))\\n\\t\\treturn 0;\\n\\treturn -ERANGE;\\n}\\n\\nint __init __weak arch_populate_kprobe_blacklist(void)\\n{\\n\\treturn 0;\\n}\\n\\n/*\\n * Lookup and populate the kprobe_blacklist.\\n *\\n * Unlike the kretprobe blacklist, we\\'ll need to determine\\n * the range of addresses that belong to the said functions,\\n * since a kprobe need not necessarily be at the beginning\\n * of a function.\\n */\\nstatic int __init populate_kprobe_blacklist(unsigned long *start,\\n\\t\\t\\t\\t\\t     unsigned long *end)\\n{\\n\\tunsigned long entry;\\n\\tunsigned long *iter;\\n\\tint ret;\\n\\n\\tfor (iter = start; iter < end; iter++) {\\n\\t\\tentry = (unsigned long)dereference_symbol_descriptor((void *)*iter);\\n\\t\\tret = kprobe_add_ksym_blacklist(entry);\\n\\t\\tif (ret == -EINVAL)\\n\\t\\t\\tcontinue;\\n\\t\\tif (ret < 0)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\n\\t/* Symbols in \\'__kprobes_text\\' are blacklisted */\\n\\tret = kprobe_add_area_blacklist((unsigned long)__kprobes_text_start,\\n\\t\\t\\t\\t\\t(unsigned long)__kprobes_text_end);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/* Symbols in \\'noinstr\\' section are blacklisted */\\n\\tret = kprobe_add_area_blacklist((unsigned long)__noinstr_text_start,\\n\\t\\t\\t\\t\\t(unsigned long)__noinstr_text_end);\\n\\n\\treturn ret ? : arch_populate_kprobe_blacklist();\\n}\\n\\n#ifdef CONFIG_MODULES\\n/* Remove all symbols in given area from kprobe blacklist */\\nstatic void kprobe_remove_area_blacklist(unsigned long start, unsigned long end)\\n{\\n\\tstruct kprobe_blacklist_entry *ent, *n;\\n\\n\\tlist_for_each_entry_safe(ent, n, &kprobe_blacklist, list) {\\n\\t\\tif (ent->start_addr < start || ent->start_addr >= end)\\n\\t\\t\\tcontinue;\\n\\t\\tlist_del(&ent->list);\\n\\t\\tkfree(ent);\\n\\t}\\n}\\n\\nstatic void kprobe_remove_ksym_blacklist(unsigned long entry)\\n{\\n\\tkprobe_remove_area_blacklist(entry, entry + 1);\\n}\\n\\nstatic void add_module_kprobe_blacklist(struct module *mod)\\n{\\n\\tunsigned long start, end;\\n\\tint i;\\n\\n\\tif (mod->kprobe_blacklist) {\\n\\t\\tfor (i = 0; i < mod->num_kprobe_blacklist; i++)\\n\\t\\t\\tkprobe_add_ksym_blacklist(mod->kprobe_blacklist[i]);\\n\\t}\\n\\n\\tstart = (unsigned long)mod->kprobes_text_start;\\n\\tif (start) {\\n\\t\\tend = start + mod->kprobes_text_size;\\n\\t\\tkprobe_add_area_blacklist(start, end);\\n\\t}\\n\\n\\tstart = (unsigned long)mod->noinstr_text_start;\\n\\tif (start) {\\n\\t\\tend = start + mod->noinstr_text_size;\\n\\t\\tkprobe_add_area_blacklist(start, end);\\n\\t}\\n}\\n\\nstatic void remove_module_kprobe_blacklist(struct module *mod)\\n{\\n\\tunsigned long start, end;\\n\\tint i;\\n\\n\\tif (mod->kprobe_blacklist) {\\n\\t\\tfor (i = 0; i < mod->num_kprobe_blacklist; i++)\\n\\t\\t\\tkprobe_remove_ksym_blacklist(mod->kprobe_blacklist[i]);\\n\\t}\\n\\n\\tstart = (unsigned long)mod->kprobes_text_start;\\n\\tif (start) {\\n\\t\\tend = start + mod->kprobes_text_size;\\n\\t\\tkprobe_remove_area_blacklist(start, end);\\n\\t}\\n\\n\\tstart = (unsigned long)mod->noinstr_text_start;\\n\\tif (start) {\\n\\t\\tend = start + mod->noinstr_text_size;\\n\\t\\tkprobe_remove_area_blacklist(start, end);\\n\\t}\\n}\\n\\n/* Module notifier call back, checking kprobes on the module */\\nstatic int kprobes_module_callback(struct notifier_block *nb,\\n\\t\\t\\t\\t   unsigned long val, void *data)\\n{\\n\\tstruct module *mod = data;\\n\\tstruct hlist_head *head;\\n\\tstruct kprobe *p;\\n\\tunsigned int i;\\n\\tint checkcore = (val == MODULE_STATE_GOING);\\n\\n\\tif (val == MODULE_STATE_COMING) {\\n\\t\\tmutex_lock(&kprobe_mutex);\\n\\t\\tadd_module_kprobe_blacklist(mod);\\n\\t\\tmutex_unlock(&kprobe_mutex);\\n\\t}\\n\\tif (val != MODULE_STATE_GOING && val != MODULE_STATE_LIVE)\\n\\t\\treturn NOTIFY_DONE;\\n\\n\\t/*\\n\\t * When \\'MODULE_STATE_GOING\\' was notified, both of module \\'.text\\' and\\n\\t * \\'.init.text\\' sections would be freed. When \\'MODULE_STATE_LIVE\\' was\\n\\t * notified, only \\'.init.text\\' section would be freed. We need to\\n\\t * disable kprobes which have been inserted in the sections.\\n\\t */\\n\\tmutex_lock(&kprobe_mutex);\\n\\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\\n\\t\\thead = &kprobe_table[i];\\n\\t\\thlist_for_each_entry(p, head, hlist)\\n\\t\\t\\tif (within_module_init((unsigned long)p->addr, mod) ||\\n\\t\\t\\t    (checkcore &&\\n\\t\\t\\t     within_module_core((unsigned long)p->addr, mod))) {\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * The vaddr this probe is installed will soon\\n\\t\\t\\t\\t * be vfreed buy not synced to disk. Hence,\\n\\t\\t\\t\\t * disarming the breakpoint isn\\'t needed.\\n\\t\\t\\t\\t *\\n\\t\\t\\t\\t * Note, this will also move any optimized probes\\n\\t\\t\\t\\t * that are pending to be removed from their\\n\\t\\t\\t\\t * corresponding lists to the \\'freeing_list\\' and\\n\\t\\t\\t\\t * will not be touched by the delayed\\n\\t\\t\\t\\t * kprobe_optimizer() work handler.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\tkill_kprobe(p);\\n\\t\\t\\t}\\n\\t}\\n\\tif (val == MODULE_STATE_GOING)\\n\\t\\tremove_module_kprobe_blacklist(mod);\\n\\tmutex_unlock(&kprobe_mutex);\\n\\treturn NOTIFY_DONE;\\n}\\n\\nstatic struct notifier_block kprobe_module_nb = {\\n\\t.notifier_call = kprobes_module_callback,\\n\\t.priority = 0\\n};\\n\\nstatic int kprobe_register_module_notifier(void)\\n{\\n\\treturn register_module_notifier(&kprobe_module_nb);\\n}\\n#else\\nstatic int kprobe_register_module_notifier(void)\\n{\\n\\treturn 0;\\n}\\n#endif /* CONFIG_MODULES */\\n\\nvoid kprobe_free_init_mem(void)\\n{\\n\\tvoid *start = (void *)(&__init_begin);\\n\\tvoid *end = (void *)(&__init_end);\\n\\tstruct hlist_head *head;\\n\\tstruct kprobe *p;\\n\\tint i;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\n\\t/* Kill all kprobes on initmem because the target code has been freed. */\\n\\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\\n\\t\\thead = &kprobe_table[i];\\n\\t\\thlist_for_each_entry(p, head, hlist) {\\n\\t\\t\\tif (start <= (void *)p->addr && (void *)p->addr < end)\\n\\t\\t\\t\\tkill_kprobe(p);\\n\\t\\t}\\n\\t}\\n\\n\\tmutex_unlock(&kprobe_mutex);\\n}\\n\\nstatic int __init init_kprobes(void)\\n{\\n\\tint i, err;\\n\\n\\t/* FIXME allocate the probe table, currently defined statically */\\n\\t/* initialize all list heads */\\n\\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++)\\n\\t\\tINIT_HLIST_HEAD(&kprobe_table[i]);\\n\\n\\terr = populate_kprobe_blacklist(__start_kprobe_blacklist,\\n\\t\\t\\t\\t\\t__stop_kprobe_blacklist);\\n\\tif (err)\\n\\t\\tpr_err(\"Failed to populate blacklist (error %d), kprobes not restricted, be careful using them!\\\\n\", err);\\n\\n\\tif (kretprobe_blacklist_size) {\\n\\t\\t/* lookup the function address from its name */\\n\\t\\tfor (i = 0; kretprobe_blacklist[i].name != NULL; i++) {\\n\\t\\t\\tkretprobe_blacklist[i].addr =\\n\\t\\t\\t\\tkprobe_lookup_name(kretprobe_blacklist[i].name, 0);\\n\\t\\t\\tif (!kretprobe_blacklist[i].addr)\\n\\t\\t\\t\\tpr_err(\"Failed to lookup symbol \\'%s\\' for kretprobe blacklist. Maybe the target function is removed or renamed.\\\\n\",\\n\\t\\t\\t\\t       kretprobe_blacklist[i].name);\\n\\t\\t}\\n\\t}\\n\\n\\t/* By default, kprobes are armed */\\n\\tkprobes_all_disarmed = false;\\n\\n#if defined(CONFIG_OPTPROBES) && defined(__ARCH_WANT_KPROBES_INSN_SLOT)\\n\\t/* Init \\'kprobe_optinsn_slots\\' for allocation */\\n\\tkprobe_optinsn_slots.insn_size = MAX_OPTINSN_SIZE;\\n#endif\\n\\n\\terr = arch_init_kprobes();\\n\\tif (!err)\\n\\t\\terr = register_die_notifier(&kprobe_exceptions_nb);\\n\\tif (!err)\\n\\t\\terr = kprobe_register_module_notifier();\\n\\n\\tkprobes_initialized = (err == 0);\\n\\tkprobe_sysctls_init();\\n\\treturn err;\\n}\\nearly_initcall(init_kprobes);\\n\\n#if defined(CONFIG_OPTPROBES)\\nstatic int __init init_optprobes(void)\\n{\\n\\t/*\\n\\t * Enable kprobe optimization - this kicks the optimizer which\\n\\t * depends on synchronize_rcu_tasks() and ksoftirqd, that is\\n\\t * not spawned in early initcall. So delay the optimization.\\n\\t */\\n\\toptimize_all_kprobes();\\n\\n\\treturn 0;\\n}\\nsubsys_initcall(init_optprobes);\\n#endif\\n\\n#ifdef CONFIG_DEBUG_FS\\nstatic void report_probe(struct seq_file *pi, struct kprobe *p,\\n\\t\\tconst char *sym, int offset, char *modname, struct kprobe *pp)\\n{\\n\\tchar *kprobe_type;\\n\\tvoid *addr = p->addr;\\n\\n\\tif (p->pre_handler == pre_handler_kretprobe)\\n\\t\\tkprobe_type = \"r\";\\n\\telse\\n\\t\\tkprobe_type = \"k\";\\n\\n\\tif (!kallsyms_show_value(pi->file->f_cred))\\n\\t\\taddr = NULL;\\n\\n\\tif (sym)\\n\\t\\tseq_printf(pi, \"%px  %s  %s+0x%x  %s \",\\n\\t\\t\\taddr, kprobe_type, sym, offset,\\n\\t\\t\\t(modname ? modname : \" \"));\\n\\telse\\t/* try to use %pS */\\n\\t\\tseq_printf(pi, \"%px  %s  %pS \",\\n\\t\\t\\taddr, kprobe_type, p->addr);\\n\\n\\tif (!pp)\\n\\t\\tpp = p;\\n\\tseq_printf(pi, \"%s%s%s%s\\\\n\",\\n\\t\\t(kprobe_gone(p) ? \"[GONE]\" : \"\"),\\n\\t\\t((kprobe_disabled(p) && !kprobe_gone(p)) ?  \"[DISABLED]\" : \"\"),\\n\\t\\t(kprobe_optimized(pp) ? \"[OPTIMIZED]\" : \"\"),\\n\\t\\t(kprobe_ftrace(pp) ? \"[FTRACE]\" : \"\"));\\n}\\n\\nstatic void *kprobe_seq_start(struct seq_file *f, loff_t *pos)\\n{\\n\\treturn (*pos < KPROBE_TABLE_SIZE) ? pos : NULL;\\n}\\n\\nstatic void *kprobe_seq_next(struct seq_file *f, void *v, loff_t *pos)\\n{\\n\\t(*pos)++;\\n\\tif (*pos >= KPROBE_TABLE_SIZE)\\n\\t\\treturn NULL;\\n\\treturn pos;\\n}\\n\\nstatic void kprobe_seq_stop(struct seq_file *f, void *v)\\n{\\n\\t/* Nothing to do */\\n}\\n\\nstatic int show_kprobe_addr(struct seq_file *pi, void *v)\\n{\\n\\tstruct hlist_head *head;\\n\\tstruct kprobe *p, *kp;\\n\\tconst char *sym;\\n\\tunsigned int i = *(loff_t *) v;\\n\\tunsigned long offset = 0;\\n\\tchar *modname, namebuf[KSYM_NAME_LEN];\\n\\n\\thead = &kprobe_table[i];\\n\\tpreempt_disable();\\n\\thlist_for_each_entry_rcu(p, head, hlist) {\\n\\t\\tsym = kallsyms_lookup((unsigned long)p->addr, NULL,\\n\\t\\t\\t\\t\\t&offset, &modname, namebuf);\\n\\t\\tif (kprobe_aggrprobe(p)) {\\n\\t\\t\\tlist_for_each_entry_rcu(kp, &p->list, list)\\n\\t\\t\\t\\treport_probe(pi, kp, sym, offset, modname, p);\\n\\t\\t} else\\n\\t\\t\\treport_probe(pi, p, sym, offset, modname, NULL);\\n\\t}\\n\\tpreempt_enable();\\n\\treturn 0;\\n}\\n\\nstatic const struct seq_operations kprobes_sops = {\\n\\t.start = kprobe_seq_start,\\n\\t.next  = kprobe_seq_next,\\n\\t.stop  = kprobe_seq_stop,\\n\\t.show  = show_kprobe_addr\\n};\\n\\nDEFINE_SEQ_ATTRIBUTE(kprobes);\\n\\n/* kprobes/blacklist -- shows which functions can not be probed */\\nstatic void *kprobe_blacklist_seq_start(struct seq_file *m, loff_t *pos)\\n{\\n\\tmutex_lock(&kprobe_mutex);\\n\\treturn seq_list_start(&kprobe_blacklist, *pos);\\n}\\n\\nstatic void *kprobe_blacklist_seq_next(struct seq_file *m, void *v, loff_t *pos)\\n{\\n\\treturn seq_list_next(v, &kprobe_blacklist, pos);\\n}\\n\\nstatic int kprobe_blacklist_seq_show(struct seq_file *m, void *v)\\n{\\n\\tstruct kprobe_blacklist_entry *ent =\\n\\t\\tlist_entry(v, struct kprobe_blacklist_entry, list);\\n\\n\\t/*\\n\\t * If \\'/proc/kallsyms\\' is not showing kernel address, we won\\'t\\n\\t * show them here either.\\n\\t */\\n\\tif (!kallsyms_show_value(m->file->f_cred))\\n\\t\\tseq_printf(m, \"0x%px-0x%px\\\\t%ps\\\\n\", NULL, NULL,\\n\\t\\t\\t   (void *)ent->start_addr);\\n\\telse\\n\\t\\tseq_printf(m, \"0x%px-0x%px\\\\t%ps\\\\n\", (void *)ent->start_addr,\\n\\t\\t\\t   (void *)ent->end_addr, (void *)ent->start_addr);\\n\\treturn 0;\\n}\\n\\nstatic void kprobe_blacklist_seq_stop(struct seq_file *f, void *v)\\n{\\n\\tmutex_unlock(&kprobe_mutex);\\n}\\n\\nstatic const struct seq_operations kprobe_blacklist_sops = {\\n\\t.start = kprobe_blacklist_seq_start,\\n\\t.next  = kprobe_blacklist_seq_next,\\n\\t.stop  = kprobe_blacklist_seq_stop,\\n\\t.show  = kprobe_blacklist_seq_show,\\n};\\nDEFINE_SEQ_ATTRIBUTE(kprobe_blacklist);\\n\\nstatic int arm_all_kprobes(void)\\n{\\n\\tstruct hlist_head *head;\\n\\tstruct kprobe *p;\\n\\tunsigned int i, total = 0, errors = 0;\\n\\tint err, ret = 0;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\n\\t/* If kprobes are armed, just return */\\n\\tif (!kprobes_all_disarmed)\\n\\t\\tgoto already_enabled;\\n\\n\\t/*\\n\\t * optimize_kprobe() called by arm_kprobe() checks\\n\\t * kprobes_all_disarmed, so set kprobes_all_disarmed before\\n\\t * arm_kprobe.\\n\\t */\\n\\tkprobes_all_disarmed = false;\\n\\t/* Arming kprobes doesn\\'t optimize kprobe itself */\\n\\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\\n\\t\\thead = &kprobe_table[i];\\n\\t\\t/* Arm all kprobes on a best-effort basis */\\n\\t\\thlist_for_each_entry(p, head, hlist) {\\n\\t\\t\\tif (!kprobe_disabled(p)) {\\n\\t\\t\\t\\terr = arm_kprobe(p);\\n\\t\\t\\t\\tif (err)  {\\n\\t\\t\\t\\t\\terrors++;\\n\\t\\t\\t\\t\\tret = err;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\ttotal++;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\tif (errors)\\n\\t\\tpr_warn(\"Kprobes globally enabled, but failed to enable %d out of %d probes. Please check which kprobes are kept disabled via debugfs.\\\\n\",\\n\\t\\t\\terrors, total);\\n\\telse\\n\\t\\tpr_info(\"Kprobes globally enabled\\\\n\");\\n\\nalready_enabled:\\n\\tmutex_unlock(&kprobe_mutex);\\n\\treturn ret;\\n}\\n\\nstatic int disarm_all_kprobes(void)\\n{\\n\\tstruct hlist_head *head;\\n\\tstruct kprobe *p;\\n\\tunsigned int i, total = 0, errors = 0;\\n\\tint err, ret = 0;\\n\\n\\tmutex_lock(&kprobe_mutex);\\n\\n\\t/* If kprobes are already disarmed, just return */\\n\\tif (kprobes_all_disarmed) {\\n\\t\\tmutex_unlock(&kprobe_mutex);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tkprobes_all_disarmed = true;\\n\\n\\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\\n\\t\\thead = &kprobe_table[i];\\n\\t\\t/* Disarm all kprobes on a best-effort basis */\\n\\t\\thlist_for_each_entry(p, head, hlist) {\\n\\t\\t\\tif (!arch_trampoline_kprobe(p) && !kprobe_disabled(p)) {\\n\\t\\t\\t\\terr = disarm_kprobe(p, false);\\n\\t\\t\\t\\tif (err) {\\n\\t\\t\\t\\t\\terrors++;\\n\\t\\t\\t\\t\\tret = err;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\ttotal++;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\tif (errors)\\n\\t\\tpr_warn(\"Kprobes globally disabled, but failed to disable %d out of %d probes. Please check which kprobes are kept enabled via debugfs.\\\\n\",\\n\\t\\t\\terrors, total);\\n\\telse\\n\\t\\tpr_info(\"Kprobes globally disabled\\\\n\");\\n\\n\\tmutex_unlock(&kprobe_mutex);\\n\\n\\t/* Wait for disarming all kprobes by optimizer */\\n\\twait_for_kprobe_optimizer();\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * XXX: The debugfs bool file interface doesn\\'t allow for callbacks\\n * when the bool state is switched. We can reuse that facility when\\n * available\\n */\\nstatic ssize_t read_enabled_file_bool(struct file *file,\\n\\t       char __user *user_buf, size_t count, loff_t *ppos)\\n{\\n\\tchar buf[3];\\n\\n\\tif (!kprobes_all_disarmed)\\n\\t\\tbuf[0] = \\'1\\';\\n\\telse\\n\\t\\tbuf[0] = \\'0\\';\\n\\tbuf[1] = \\'\\\\n\\';\\n\\tbuf[2] = 0x00;\\n\\treturn simple_read_from_buffer(user_buf, count, ppos, buf, 2);\\n}\\n\\nstatic ssize_t write_enabled_file_bool(struct file *file,\\n\\t       const char __user *user_buf, size_t count, loff_t *ppos)\\n{\\n\\tbool enable;\\n\\tint ret;\\n\\n\\tret = kstrtobool_from_user(user_buf, count, &enable);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tret = enable ? arm_all_kprobes() : disarm_all_kprobes();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\treturn count;\\n}\\n\\nstatic const struct file_operations fops_kp = {\\n\\t.read =         read_enabled_file_bool,\\n\\t.write =        write_enabled_file_bool,\\n\\t.llseek =\\tdefault_llseek,\\n};\\n\\nstatic int __init debugfs_kprobe_init(void)\\n{\\n\\tstruct dentry *dir;\\n\\n\\tdir = debugfs_create_dir(\"kprobes\", NULL);\\n\\n\\tdebugfs_create_file(\"list\", 0400, dir, NULL, &kprobes_fops);\\n\\n\\tdebugfs_create_file(\"enabled\", 0600, dir, NULL, &fops_kp);\\n\\n\\tdebugfs_create_file(\"blacklist\", 0400, dir, NULL,\\n\\t\\t\\t    &kprobe_blacklist_fops);\\n\\n\\treturn 0;\\n}\\n\\nlate_initcall(debugfs_kprobe_init);\\n#endif /* CONFIG_DEBUG_FS */\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#include <linux/irq_work.h>\\n#include <linux/spinlock.h>\\n#include <linux/task_work.h>\\n#include <linux/resume_user_mode.h>\\n\\nstatic struct callback_head work_exited; /* all we need is ->next == NULL */\\n\\n#ifdef CONFIG_IRQ_WORK\\nstatic void task_work_set_notify_irq(struct irq_work *entry)\\n{\\n\\ttest_and_set_tsk_thread_flag(current, TIF_NOTIFY_RESUME);\\n}\\nstatic DEFINE_PER_CPU(struct irq_work, irq_work_NMI_resume) =\\n\\tIRQ_WORK_INIT_HARD(task_work_set_notify_irq);\\n#endif\\n\\n/**\\n * task_work_add - ask the @task to execute @work->func()\\n * @task: the task which should run the callback\\n * @work: the callback to run\\n * @notify: how to notify the targeted task\\n *\\n * Queue @work for task_work_run() below and notify the @task if @notify\\n * is @TWA_RESUME, @TWA_SIGNAL, @TWA_SIGNAL_NO_IPI or @TWA_NMI_CURRENT.\\n *\\n * @TWA_SIGNAL works like signals, in that the it will interrupt the targeted\\n * task and run the task_work, regardless of whether the task is currently\\n * running in the kernel or userspace.\\n * @TWA_SIGNAL_NO_IPI works like @TWA_SIGNAL, except it doesn\\'t send a\\n * reschedule IPI to force the targeted task to reschedule and run task_work.\\n * This can be advantageous if there\\'s no strict requirement that the\\n * task_work be run as soon as possible, just whenever the task enters the\\n * kernel anyway.\\n * @TWA_RESUME work is run only when the task exits the kernel and returns to\\n * user mode, or before entering guest mode.\\n * @TWA_NMI_CURRENT works like @TWA_RESUME, except it can only be used for the\\n * current @task and if the current context is NMI.\\n *\\n * Fails if the @task is exiting/exited and thus it can\\'t process this @work.\\n * Otherwise @work->func() will be called when the @task goes through one of\\n * the aforementioned transitions, or exits.\\n *\\n * If the targeted task is exiting, then an error is returned and the work item\\n * is not queued. It\\'s up to the caller to arrange for an alternative mechanism\\n * in that case.\\n *\\n * Note: there is no ordering guarantee on works queued here. The task_work\\n * list is LIFO.\\n *\\n * RETURNS:\\n * 0 if succeeds or -ESRCH.\\n */\\nint task_work_add(struct task_struct *task, struct callback_head *work,\\n\\t\\t  enum task_work_notify_mode notify)\\n{\\n\\tstruct callback_head *head;\\n\\tint flags = notify & TWA_FLAGS;\\n\\n\\tnotify &= ~TWA_FLAGS;\\n\\tif (notify == TWA_NMI_CURRENT) {\\n\\t\\tif (WARN_ON_ONCE(task != current))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (!IS_ENABLED(CONFIG_IRQ_WORK))\\n\\t\\t\\treturn -EINVAL;\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * Record the work call stack in order to print it in KASAN\\n\\t\\t * reports.\\n\\t\\t *\\n\\t\\t * Note that stack allocation can fail if TWAF_NO_ALLOC flag\\n\\t\\t * is set and new page is needed to expand the stack buffer.\\n\\t\\t */\\n\\t\\tif (flags & TWAF_NO_ALLOC)\\n\\t\\t\\tkasan_record_aux_stack_noalloc(work);\\n\\t\\telse\\n\\t\\t\\tkasan_record_aux_stack(work);\\n\\t}\\n\\n\\thead = READ_ONCE(task->task_works);\\n\\tdo {\\n\\t\\tif (unlikely(head == &work_exited))\\n\\t\\t\\treturn -ESRCH;\\n\\t\\twork->next = head;\\n\\t} while (!try_cmpxchg(&task->task_works, &head, work));\\n\\n\\tswitch (notify) {\\n\\tcase TWA_NONE:\\n\\t\\tbreak;\\n\\tcase TWA_RESUME:\\n\\t\\tset_notify_resume(task);\\n\\t\\tbreak;\\n\\tcase TWA_SIGNAL:\\n\\t\\tset_notify_signal(task);\\n\\t\\tbreak;\\n\\tcase TWA_SIGNAL_NO_IPI:\\n\\t\\t__set_notify_signal(task);\\n\\t\\tbreak;\\n#ifdef CONFIG_IRQ_WORK\\n\\tcase TWA_NMI_CURRENT:\\n\\t\\tirq_work_queue(this_cpu_ptr(&irq_work_NMI_resume));\\n\\t\\tbreak;\\n#endif\\n\\tdefault:\\n\\t\\tWARN_ON_ONCE(1);\\n\\t\\tbreak;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * task_work_cancel_match - cancel a pending work added by task_work_add()\\n * @task: the task which should execute the work\\n * @match: match function to call\\n * @data: data to be passed in to match function\\n *\\n * RETURNS:\\n * The found work or NULL if not found.\\n */\\nstruct callback_head *\\ntask_work_cancel_match(struct task_struct *task,\\n\\t\\t       bool (*match)(struct callback_head *, void *data),\\n\\t\\t       void *data)\\n{\\n\\tstruct callback_head **pprev = &task->task_works;\\n\\tstruct callback_head *work;\\n\\tunsigned long flags;\\n\\n\\tif (likely(!task_work_pending(task)))\\n\\t\\treturn NULL;\\n\\t/*\\n\\t * If cmpxchg() fails we continue without updating pprev.\\n\\t * Either we raced with task_work_add() which added the\\n\\t * new entry before this work, we will find it again. Or\\n\\t * we raced with task_work_run(), *pprev == NULL/exited.\\n\\t */\\n\\traw_spin_lock_irqsave(&task->pi_lock, flags);\\n\\twork = READ_ONCE(*pprev);\\n\\twhile (work) {\\n\\t\\tif (!match(work, data)) {\\n\\t\\t\\tpprev = &work->next;\\n\\t\\t\\twork = READ_ONCE(*pprev);\\n\\t\\t} else if (try_cmpxchg(pprev, &work, work->next))\\n\\t\\t\\tbreak;\\n\\t}\\n\\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\\n\\n\\treturn work;\\n}\\n\\nstatic bool task_work_func_match(struct callback_head *cb, void *data)\\n{\\n\\treturn cb->func == data;\\n}\\n\\n/**\\n * task_work_cancel_func - cancel a pending work matching a function added by task_work_add()\\n * @task: the task which should execute the func\\'s work\\n * @func: identifies the func to match with a work to remove\\n *\\n * Find the last queued pending work with ->func == @func and remove\\n * it from queue.\\n *\\n * RETURNS:\\n * The found work or NULL if not found.\\n */\\nstruct callback_head *\\ntask_work_cancel_func(struct task_struct *task, task_work_func_t func)\\n{\\n\\treturn task_work_cancel_match(task, task_work_func_match, func);\\n}\\n\\nstatic bool task_work_match(struct callback_head *cb, void *data)\\n{\\n\\treturn cb == data;\\n}\\n\\n/**\\n * task_work_cancel - cancel a pending work added by task_work_add()\\n * @task: the task which should execute the work\\n * @cb: the callback to remove if queued\\n *\\n * Remove a callback from a task\\'s queue if queued.\\n *\\n * RETURNS:\\n * True if the callback was queued and got cancelled, false otherwise.\\n */\\nbool task_work_cancel(struct task_struct *task, struct callback_head *cb)\\n{\\n\\tstruct callback_head *ret;\\n\\n\\tret = task_work_cancel_match(task, task_work_match, cb);\\n\\n\\treturn ret == cb;\\n}\\n\\n/**\\n * task_work_run - execute the works added by task_work_add()\\n *\\n * Flush the pending works. Should be used by the core kernel code.\\n * Called before the task returns to the user-mode or stops, or when\\n * it exits. In the latter case task_work_add() can no longer add the\\n * new work after task_work_run() returns.\\n */\\nvoid task_work_run(void)\\n{\\n\\tstruct task_struct *task = current;\\n\\tstruct callback_head *work, *head, *next;\\n\\n\\tfor (;;) {\\n\\t\\t/*\\n\\t\\t * work->func() can do task_work_add(), do not set\\n\\t\\t * work_exited unless the list is empty.\\n\\t\\t */\\n\\t\\twork = READ_ONCE(task->task_works);\\n\\t\\tdo {\\n\\t\\t\\thead = NULL;\\n\\t\\t\\tif (!work) {\\n\\t\\t\\t\\tif (task->flags & PF_EXITING)\\n\\t\\t\\t\\t\\thead = &work_exited;\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t} while (!try_cmpxchg(&task->task_works, &work, head));\\n\\n\\t\\tif (!work)\\n\\t\\t\\tbreak;\\n\\t\\t/*\\n\\t\\t * Synchronize with task_work_cancel_match(). It can not remove\\n\\t\\t * the first entry == work, cmpxchg(task_works) must fail.\\n\\t\\t * But it can remove another entry from the ->next list.\\n\\t\\t */\\n\\t\\traw_spin_lock_irq(&task->pi_lock);\\n\\t\\traw_spin_unlock_irq(&task->pi_lock);\\n\\n\\t\\tdo {\\n\\t\\t\\tnext = work->next;\\n\\t\\t\\twork->func(work);\\n\\t\\t\\twork = next;\\n\\t\\t\\tcond_resched();\\n\\t\\t} while (work);\\n\\t}\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n\\n#include <linux/linkage.h>\\n#include <linux/errno.h>\\n\\n#include <asm/unistd.h>\\n\\n#ifdef CONFIG_ARCH_HAS_SYSCALL_WRAPPER\\n/* Architectures may override COND_SYSCALL and COND_SYSCALL_COMPAT */\\n#include <asm/syscall_wrapper.h>\\n#endif /* CONFIG_ARCH_HAS_SYSCALL_WRAPPER */\\n\\n/*  we can\\'t #include <linux/syscalls.h> here,\\n    but tell gcc to not warn with -Wmissing-prototypes  */\\nasmlinkage long sys_ni_syscall(void);\\n\\n/*\\n * Non-implemented system calls get redirected here.\\n */\\nasmlinkage long sys_ni_syscall(void)\\n{\\n\\treturn -ENOSYS;\\n}\\n\\n#ifndef COND_SYSCALL\\n#define COND_SYSCALL(name) cond_syscall(sys_##name)\\n#endif /* COND_SYSCALL */\\n\\n#ifndef COND_SYSCALL_COMPAT\\n#define COND_SYSCALL_COMPAT(name) cond_syscall(compat_sys_##name)\\n#endif /* COND_SYSCALL_COMPAT */\\n\\n/*\\n * This list is kept in the same order as include/uapi/asm-generic/unistd.h.\\n * Architecture specific entries go below, followed by deprecated or obsolete\\n * system calls.\\n */\\n\\nCOND_SYSCALL(io_setup);\\nCOND_SYSCALL_COMPAT(io_setup);\\nCOND_SYSCALL(io_destroy);\\nCOND_SYSCALL(io_submit);\\nCOND_SYSCALL_COMPAT(io_submit);\\nCOND_SYSCALL(io_cancel);\\nCOND_SYSCALL(io_getevents_time32);\\nCOND_SYSCALL(io_getevents);\\nCOND_SYSCALL(io_pgetevents_time32);\\nCOND_SYSCALL(io_pgetevents);\\nCOND_SYSCALL_COMPAT(io_pgetevents);\\nCOND_SYSCALL_COMPAT(io_pgetevents_time64);\\nCOND_SYSCALL(io_uring_setup);\\nCOND_SYSCALL(io_uring_enter);\\nCOND_SYSCALL(io_uring_register);\\nCOND_SYSCALL(eventfd2);\\nCOND_SYSCALL(epoll_create1);\\nCOND_SYSCALL(epoll_ctl);\\nCOND_SYSCALL(epoll_pwait);\\nCOND_SYSCALL_COMPAT(epoll_pwait);\\nCOND_SYSCALL(epoll_pwait2);\\nCOND_SYSCALL_COMPAT(epoll_pwait2);\\nCOND_SYSCALL(inotify_init1);\\nCOND_SYSCALL(inotify_add_watch);\\nCOND_SYSCALL(inotify_rm_watch);\\nCOND_SYSCALL(ioprio_set);\\nCOND_SYSCALL(ioprio_get);\\nCOND_SYSCALL(flock);\\nCOND_SYSCALL(quotactl);\\nCOND_SYSCALL(quotactl_fd);\\nCOND_SYSCALL(signalfd4);\\nCOND_SYSCALL_COMPAT(signalfd4);\\nCOND_SYSCALL(timerfd_create);\\nCOND_SYSCALL(timerfd_settime);\\nCOND_SYSCALL(timerfd_settime32);\\nCOND_SYSCALL(timerfd_gettime);\\nCOND_SYSCALL(timerfd_gettime32);\\nCOND_SYSCALL(acct);\\nCOND_SYSCALL(capget);\\nCOND_SYSCALL(capset);\\nCOND_SYSCALL(futex);\\nCOND_SYSCALL(futex_time32);\\nCOND_SYSCALL(set_robust_list);\\nCOND_SYSCALL_COMPAT(set_robust_list);\\nCOND_SYSCALL(get_robust_list);\\nCOND_SYSCALL_COMPAT(get_robust_list);\\nCOND_SYSCALL(futex_waitv);\\nCOND_SYSCALL(futex_wake);\\nCOND_SYSCALL(futex_wait);\\nCOND_SYSCALL(futex_requeue);\\nCOND_SYSCALL(kexec_load);\\nCOND_SYSCALL_COMPAT(kexec_load);\\nCOND_SYSCALL(init_module);\\nCOND_SYSCALL(delete_module);\\nCOND_SYSCALL(syslog);\\nCOND_SYSCALL(setregid);\\nCOND_SYSCALL(setgid);\\nCOND_SYSCALL(setreuid);\\nCOND_SYSCALL(setuid);\\nCOND_SYSCALL(setresuid);\\nCOND_SYSCALL(getresuid);\\nCOND_SYSCALL(setresgid);\\nCOND_SYSCALL(getresgid);\\nCOND_SYSCALL(setfsuid);\\nCOND_SYSCALL(setfsgid);\\nCOND_SYSCALL(setgroups);\\nCOND_SYSCALL(getgroups);\\nCOND_SYSCALL(mq_open);\\nCOND_SYSCALL_COMPAT(mq_open);\\nCOND_SYSCALL(mq_unlink);\\nCOND_SYSCALL(mq_timedsend);\\nCOND_SYSCALL(mq_timedsend_time32);\\nCOND_SYSCALL(mq_timedreceive);\\nCOND_SYSCALL(mq_timedreceive_time32);\\nCOND_SYSCALL(mq_notify);\\nCOND_SYSCALL_COMPAT(mq_notify);\\nCOND_SYSCALL(mq_getsetattr);\\nCOND_SYSCALL_COMPAT(mq_getsetattr);\\nCOND_SYSCALL(msgget);\\nCOND_SYSCALL(old_msgctl);\\nCOND_SYSCALL(msgctl);\\nCOND_SYSCALL_COMPAT(msgctl);\\nCOND_SYSCALL_COMPAT(old_msgctl);\\nCOND_SYSCALL(msgrcv);\\nCOND_SYSCALL_COMPAT(msgrcv);\\nCOND_SYSCALL(msgsnd);\\nCOND_SYSCALL_COMPAT(msgsnd);\\nCOND_SYSCALL(semget);\\nCOND_SYSCALL(old_semctl);\\nCOND_SYSCALL(semctl);\\nCOND_SYSCALL_COMPAT(semctl);\\nCOND_SYSCALL_COMPAT(old_semctl);\\nCOND_SYSCALL(semtimedop);\\nCOND_SYSCALL(semtimedop_time32);\\nCOND_SYSCALL(semop);\\nCOND_SYSCALL(shmget);\\nCOND_SYSCALL(old_shmctl);\\nCOND_SYSCALL(shmctl);\\nCOND_SYSCALL_COMPAT(shmctl);\\nCOND_SYSCALL_COMPAT(old_shmctl);\\nCOND_SYSCALL(shmat);\\nCOND_SYSCALL_COMPAT(shmat);\\nCOND_SYSCALL(shmdt);\\nCOND_SYSCALL(socket);\\nCOND_SYSCALL(socketpair);\\nCOND_SYSCALL(bind);\\nCOND_SYSCALL(listen);\\nCOND_SYSCALL(accept);\\nCOND_SYSCALL(connect);\\nCOND_SYSCALL(getsockname);\\nCOND_SYSCALL(getpeername);\\nCOND_SYSCALL(setsockopt);\\nCOND_SYSCALL_COMPAT(setsockopt);\\nCOND_SYSCALL(getsockopt);\\nCOND_SYSCALL_COMPAT(getsockopt);\\nCOND_SYSCALL(sendto);\\nCOND_SYSCALL(shutdown);\\nCOND_SYSCALL(recvfrom);\\nCOND_SYSCALL_COMPAT(recvfrom);\\nCOND_SYSCALL(sendmsg);\\nCOND_SYSCALL_COMPAT(sendmsg);\\nCOND_SYSCALL(recvmsg);\\nCOND_SYSCALL_COMPAT(recvmsg);\\nCOND_SYSCALL(mremap);\\nCOND_SYSCALL(add_key);\\nCOND_SYSCALL(request_key);\\nCOND_SYSCALL(keyctl);\\nCOND_SYSCALL_COMPAT(keyctl);\\nCOND_SYSCALL(landlock_create_ruleset);\\nCOND_SYSCALL(landlock_add_rule);\\nCOND_SYSCALL(landlock_restrict_self);\\nCOND_SYSCALL(fadvise64_64);\\nCOND_SYSCALL_COMPAT(fadvise64_64);\\nCOND_SYSCALL(lsm_get_self_attr);\\nCOND_SYSCALL(lsm_set_self_attr);\\nCOND_SYSCALL(lsm_list_modules);\\n\\n/* CONFIG_MMU only */\\nCOND_SYSCALL(swapon);\\nCOND_SYSCALL(swapoff);\\nCOND_SYSCALL(mprotect);\\nCOND_SYSCALL(msync);\\nCOND_SYSCALL(mlock);\\nCOND_SYSCALL(munlock);\\nCOND_SYSCALL(mlockall);\\nCOND_SYSCALL(munlockall);\\nCOND_SYSCALL(mincore);\\nCOND_SYSCALL(madvise);\\nCOND_SYSCALL(process_madvise);\\nCOND_SYSCALL(process_mrelease);\\nCOND_SYSCALL(remap_file_pages);\\nCOND_SYSCALL(mbind);\\nCOND_SYSCALL(get_mempolicy);\\nCOND_SYSCALL(set_mempolicy);\\nCOND_SYSCALL(migrate_pages);\\nCOND_SYSCALL(move_pages);\\nCOND_SYSCALL(set_mempolicy_home_node);\\nCOND_SYSCALL(cachestat);\\nCOND_SYSCALL(mseal);\\n\\nCOND_SYSCALL(perf_event_open);\\nCOND_SYSCALL(accept4);\\nCOND_SYSCALL(recvmmsg);\\nCOND_SYSCALL(recvmmsg_time32);\\nCOND_SYSCALL_COMPAT(recvmmsg_time32);\\nCOND_SYSCALL_COMPAT(recvmmsg_time64);\\n\\n/* Posix timer syscalls may be configured out */\\nCOND_SYSCALL(timer_create);\\nCOND_SYSCALL(timer_gettime);\\nCOND_SYSCALL(timer_getoverrun);\\nCOND_SYSCALL(timer_settime);\\nCOND_SYSCALL(timer_delete);\\nCOND_SYSCALL(clock_adjtime);\\nCOND_SYSCALL(getitimer);\\nCOND_SYSCALL(setitimer);\\nCOND_SYSCALL(alarm);\\nCOND_SYSCALL_COMPAT(timer_create);\\nCOND_SYSCALL_COMPAT(getitimer);\\nCOND_SYSCALL_COMPAT(setitimer);\\n\\n/*\\n * Architecture specific syscalls: see further below\\n */\\n\\n/* fanotify */\\nCOND_SYSCALL(fanotify_init);\\nCOND_SYSCALL(fanotify_mark);\\n\\n/* open by handle */\\nCOND_SYSCALL(name_to_handle_at);\\nCOND_SYSCALL(open_by_handle_at);\\nCOND_SYSCALL_COMPAT(open_by_handle_at);\\n\\nCOND_SYSCALL(sendmmsg);\\nCOND_SYSCALL_COMPAT(sendmmsg);\\nCOND_SYSCALL(process_vm_readv);\\nCOND_SYSCALL_COMPAT(process_vm_readv);\\nCOND_SYSCALL(process_vm_writev);\\nCOND_SYSCALL_COMPAT(process_vm_writev);\\n\\n/* compare kernel pointers */\\nCOND_SYSCALL(kcmp);\\n\\nCOND_SYSCALL(finit_module);\\n\\n/* operate on Secure Computing state */\\nCOND_SYSCALL(seccomp);\\n\\nCOND_SYSCALL(memfd_create);\\n\\n/* access BPF programs and maps */\\nCOND_SYSCALL(bpf);\\n\\n/* execveat */\\nCOND_SYSCALL(execveat);\\n\\nCOND_SYSCALL(userfaultfd);\\n\\n/* membarrier */\\nCOND_SYSCALL(membarrier);\\n\\nCOND_SYSCALL(mlock2);\\n\\nCOND_SYSCALL(copy_file_range);\\n\\n/* memory protection keys */\\nCOND_SYSCALL(pkey_mprotect);\\nCOND_SYSCALL(pkey_alloc);\\nCOND_SYSCALL(pkey_free);\\n\\n/* memfd_secret */\\nCOND_SYSCALL(memfd_secret);\\n\\n/*\\n * Architecture specific weak syscall entries.\\n */\\n\\n/* pciconfig: alpha, arm, arm64, ia64, sparc */\\nCOND_SYSCALL(pciconfig_read);\\nCOND_SYSCALL(pciconfig_write);\\nCOND_SYSCALL(pciconfig_iobase);\\n\\n/* sys_socketcall: arm, mips, x86, ... */\\nCOND_SYSCALL(socketcall);\\nCOND_SYSCALL_COMPAT(socketcall);\\n\\n/* compat syscalls for arm64, x86, ... */\\nCOND_SYSCALL_COMPAT(fanotify_mark);\\n\\n/* x86 */\\nCOND_SYSCALL(vm86old);\\nCOND_SYSCALL(modify_ldt);\\nCOND_SYSCALL(vm86);\\nCOND_SYSCALL(kexec_file_load);\\nCOND_SYSCALL(map_shadow_stack);\\n\\n/* s390 */\\nCOND_SYSCALL(s390_pci_mmio_read);\\nCOND_SYSCALL(s390_pci_mmio_write);\\nCOND_SYSCALL(s390_ipc);\\nCOND_SYSCALL_COMPAT(s390_ipc);\\n\\n/* powerpc */\\nCOND_SYSCALL(rtas);\\nCOND_SYSCALL(spu_run);\\nCOND_SYSCALL(spu_create);\\nCOND_SYSCALL(subpage_prot);\\n\\n\\n/*\\n * Deprecated system calls which are still defined in\\n * include/uapi/asm-generic/unistd.h and wanted by >= 1 arch\\n */\\n\\n/* __ARCH_WANT_SYSCALL_NO_FLAGS */\\nCOND_SYSCALL(epoll_create);\\nCOND_SYSCALL(inotify_init);\\nCOND_SYSCALL(eventfd);\\nCOND_SYSCALL(signalfd);\\nCOND_SYSCALL_COMPAT(signalfd);\\n\\n/* __ARCH_WANT_SYSCALL_OFF_T */\\nCOND_SYSCALL(fadvise64);\\n\\n/* __ARCH_WANT_SYSCALL_DEPRECATED */\\nCOND_SYSCALL(epoll_wait);\\nCOND_SYSCALL(recv);\\nCOND_SYSCALL_COMPAT(recv);\\nCOND_SYSCALL(send);\\nCOND_SYSCALL(uselib);\\n\\n/* optional: time32 */\\nCOND_SYSCALL(time32);\\nCOND_SYSCALL(stime32);\\nCOND_SYSCALL(utime32);\\nCOND_SYSCALL(adjtimex_time32);\\nCOND_SYSCALL(sched_rr_get_interval_time32);\\nCOND_SYSCALL(nanosleep_time32);\\nCOND_SYSCALL(rt_sigtimedwait_time32);\\nCOND_SYSCALL_COMPAT(rt_sigtimedwait_time32);\\nCOND_SYSCALL(timer_settime32);\\nCOND_SYSCALL(timer_gettime32);\\nCOND_SYSCALL(clock_settime32);\\nCOND_SYSCALL(clock_gettime32);\\nCOND_SYSCALL(clock_getres_time32);\\nCOND_SYSCALL(clock_nanosleep_time32);\\nCOND_SYSCALL(utimes_time32);\\nCOND_SYSCALL(futimesat_time32);\\nCOND_SYSCALL(pselect6_time32);\\nCOND_SYSCALL_COMPAT(pselect6_time32);\\nCOND_SYSCALL(ppoll_time32);\\nCOND_SYSCALL_COMPAT(ppoll_time32);\\nCOND_SYSCALL(utimensat_time32);\\nCOND_SYSCALL(clock_adjtime32);\\n\\n/*\\n * The syscalls below are not found in include/uapi/asm-generic/unistd.h\\n */\\n\\n/* obsolete: SGETMASK_SYSCALL */\\nCOND_SYSCALL(sgetmask);\\nCOND_SYSCALL(ssetmask);\\n\\n/* obsolete: SYSFS_SYSCALL */\\nCOND_SYSCALL(sysfs);\\n\\n/* obsolete: __ARCH_WANT_SYS_IPC */\\nCOND_SYSCALL(ipc);\\nCOND_SYSCALL_COMPAT(ipc);\\n\\n/* obsolete: UID16 */\\nCOND_SYSCALL(chown16);\\nCOND_SYSCALL(fchown16);\\nCOND_SYSCALL(getegid16);\\nCOND_SYSCALL(geteuid16);\\nCOND_SYSCALL(getgid16);\\nCOND_SYSCALL(getgroups16);\\nCOND_SYSCALL(getresgid16);\\nCOND_SYSCALL(getresuid16);\\nCOND_SYSCALL(getuid16);\\nCOND_SYSCALL(lchown16);\\nCOND_SYSCALL(setfsgid16);\\nCOND_SYSCALL(setfsuid16);\\nCOND_SYSCALL(setgid16);\\nCOND_SYSCALL(setgroups16);\\nCOND_SYSCALL(setregid16);\\nCOND_SYSCALL(setresgid16);\\nCOND_SYSCALL(setresuid16);\\nCOND_SYSCALL(setreuid16);\\nCOND_SYSCALL(setuid16);\\n\\n/* restartable sequence */\\nCOND_SYSCALL(rseq);\\n\\nCOND_SYSCALL(uretprobe);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  Copyright (C) 2007\\n *\\n *  Author: Eric Biederman <ebiederm@xmision.com>\\n */\\n\\n#include <linux/export.h>\\n#include <linux/uts.h>\\n#include <linux/utsname.h>\\n#include <linux/random.h>\\n#include <linux/sysctl.h>\\n#include <linux/wait.h>\\n#include <linux/rwsem.h>\\n\\n#ifdef CONFIG_PROC_SYSCTL\\n\\nstatic void *get_uts(const struct ctl_table *table)\\n{\\n\\tchar *which = table->data;\\n\\tstruct uts_namespace *uts_ns;\\n\\n\\tuts_ns = current->nsproxy->uts_ns;\\n\\twhich = (which - (char *)&init_uts_ns) + (char *)uts_ns;\\n\\n\\treturn which;\\n}\\n\\n/*\\n *\\tSpecial case of dostring for the UTS structure. This has locks\\n *\\tto observe. Should this be in kernel/sys.c ????\\n */\\nstatic int proc_do_uts_string(const struct ctl_table *table, int write,\\n\\t\\t  void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct ctl_table uts_table;\\n\\tint r;\\n\\tchar tmp_data[__NEW_UTS_LEN + 1];\\n\\n\\tmemcpy(&uts_table, table, sizeof(uts_table));\\n\\tuts_table.data = tmp_data;\\n\\n\\t/*\\n\\t * Buffer the value in tmp_data so that proc_dostring() can be called\\n\\t * without holding any locks.\\n\\t * We also need to read the original value in the write==1 case to\\n\\t * support partial writes.\\n\\t */\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(tmp_data, get_uts(table), sizeof(tmp_data));\\n\\tup_read(&uts_sem);\\n\\tr = proc_dostring(&uts_table, write, buffer, lenp, ppos);\\n\\n\\tif (write) {\\n\\t\\t/*\\n\\t\\t * Write back the new value.\\n\\t\\t * Note that, since we dropped uts_sem, the result can\\n\\t\\t * theoretically be incorrect if there are two parallel writes\\n\\t\\t * at non-zero offsets to the same sysctl.\\n\\t\\t */\\n\\t\\tadd_device_randomness(tmp_data, sizeof(tmp_data));\\n\\t\\tdown_write(&uts_sem);\\n\\t\\tmemcpy(get_uts(table), tmp_data, sizeof(tmp_data));\\n\\t\\tup_write(&uts_sem);\\n\\t\\tproc_sys_poll_notify(table->poll);\\n\\t}\\n\\n\\treturn r;\\n}\\n#else\\n#define proc_do_uts_string NULL\\n#endif\\n\\nstatic DEFINE_CTL_TABLE_POLL(hostname_poll);\\nstatic DEFINE_CTL_TABLE_POLL(domainname_poll);\\n\\n// Note: update \\'enum uts_proc\\' to match any changes to this table\\nstatic struct ctl_table uts_kern_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"arch\",\\n\\t\\t.data\\t\\t= init_uts_ns.name.machine,\\n\\t\\t.maxlen\\t\\t= sizeof(init_uts_ns.name.machine),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_do_uts_string,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"ostype\",\\n\\t\\t.data\\t\\t= init_uts_ns.name.sysname,\\n\\t\\t.maxlen\\t\\t= sizeof(init_uts_ns.name.sysname),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_do_uts_string,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"osrelease\",\\n\\t\\t.data\\t\\t= init_uts_ns.name.release,\\n\\t\\t.maxlen\\t\\t= sizeof(init_uts_ns.name.release),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_do_uts_string,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"version\",\\n\\t\\t.data\\t\\t= init_uts_ns.name.version,\\n\\t\\t.maxlen\\t\\t= sizeof(init_uts_ns.name.version),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_do_uts_string,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hostname\",\\n\\t\\t.data\\t\\t= init_uts_ns.name.nodename,\\n\\t\\t.maxlen\\t\\t= sizeof(init_uts_ns.name.nodename),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_do_uts_string,\\n\\t\\t.poll\\t\\t= &hostname_poll,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"domainname\",\\n\\t\\t.data\\t\\t= init_uts_ns.name.domainname,\\n\\t\\t.maxlen\\t\\t= sizeof(init_uts_ns.name.domainname),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_do_uts_string,\\n\\t\\t.poll\\t\\t= &domainname_poll,\\n\\t},\\n};\\n\\n#ifdef CONFIG_PROC_SYSCTL\\n/*\\n * Notify userspace about a change in a certain entry of uts_kern_table,\\n * identified by the parameter proc.\\n */\\nvoid uts_proc_notify(enum uts_proc proc)\\n{\\n\\tstruct ctl_table *table = &uts_kern_table[proc];\\n\\n\\tproc_sys_poll_notify(table->poll);\\n}\\n#endif\\n\\nstatic int __init utsname_sysctl_init(void)\\n{\\n\\tregister_sysctl(\"kernel\", uts_kern_table);\\n\\treturn 0;\\n}\\n\\ndevice_initcall(utsname_sysctl_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * The \"user cache\".\\n *\\n * (C) Copyright 1991-2000 Linus Torvalds\\n *\\n * We have a per-user structure to keep track of how many\\n * processes, files etc the user has claimed, in order to be\\n * able to have per-user limits for system resources. \\n */\\n\\n#include <linux/init.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/bitops.h>\\n#include <linux/key.h>\\n#include <linux/sched/user.h>\\n#include <linux/interrupt.h>\\n#include <linux/export.h>\\n#include <linux/user_namespace.h>\\n#include <linux/binfmts.h>\\n#include <linux/proc_ns.h>\\n\\n#if IS_ENABLED(CONFIG_BINFMT_MISC)\\nstruct binfmt_misc init_binfmt_misc = {\\n\\t.entries = LIST_HEAD_INIT(init_binfmt_misc.entries),\\n\\t.enabled = true,\\n\\t.entries_lock = __RW_LOCK_UNLOCKED(init_binfmt_misc.entries_lock),\\n};\\nEXPORT_SYMBOL_GPL(init_binfmt_misc);\\n#endif\\n\\n/*\\n * userns count is 1 for root user, 1 for init_uts_ns,\\n * and 1 for... ?\\n */\\nstruct user_namespace init_user_ns = {\\n\\t.uid_map = {\\n\\t\\t{\\n\\t\\t\\t.extent[0] = {\\n\\t\\t\\t\\t.first = 0,\\n\\t\\t\\t\\t.lower_first = 0,\\n\\t\\t\\t\\t.count = 4294967295U,\\n\\t\\t\\t},\\n\\t\\t\\t.nr_extents = 1,\\n\\t\\t},\\n\\t},\\n\\t.gid_map = {\\n\\t\\t{\\n\\t\\t\\t.extent[0] = {\\n\\t\\t\\t\\t.first = 0,\\n\\t\\t\\t\\t.lower_first = 0,\\n\\t\\t\\t\\t.count = 4294967295U,\\n\\t\\t\\t},\\n\\t\\t\\t.nr_extents = 1,\\n\\t\\t},\\n\\t},\\n\\t.projid_map = {\\n\\t\\t{\\n\\t\\t\\t.extent[0] = {\\n\\t\\t\\t\\t.first = 0,\\n\\t\\t\\t\\t.lower_first = 0,\\n\\t\\t\\t\\t.count = 4294967295U,\\n\\t\\t\\t},\\n\\t\\t\\t.nr_extents = 1,\\n\\t\\t},\\n\\t},\\n\\t.ns.count = REFCOUNT_INIT(3),\\n\\t.owner = GLOBAL_ROOT_UID,\\n\\t.group = GLOBAL_ROOT_GID,\\n\\t.ns.inum = PROC_USER_INIT_INO,\\n#ifdef CONFIG_USER_NS\\n\\t.ns.ops = &userns_operations,\\n#endif\\n\\t.flags = USERNS_INIT_FLAGS,\\n#ifdef CONFIG_KEYS\\n\\t.keyring_name_list = LIST_HEAD_INIT(init_user_ns.keyring_name_list),\\n\\t.keyring_sem = __RWSEM_INITIALIZER(init_user_ns.keyring_sem),\\n#endif\\n#if IS_ENABLED(CONFIG_BINFMT_MISC)\\n\\t.binfmt_misc = &init_binfmt_misc,\\n#endif\\n};\\nEXPORT_SYMBOL_GPL(init_user_ns);\\n\\n/*\\n * UID task count cache, to get fast user lookup in \"alloc_uid\"\\n * when changing user ID\\'s (ie setuid() and friends).\\n */\\n\\n#define UIDHASH_BITS\\t(IS_ENABLED(CONFIG_BASE_SMALL) ? 3 : 7)\\n#define UIDHASH_SZ\\t(1 << UIDHASH_BITS)\\n#define UIDHASH_MASK\\t\\t(UIDHASH_SZ - 1)\\n#define __uidhashfn(uid)\\t(((uid >> UIDHASH_BITS) + uid) & UIDHASH_MASK)\\n#define uidhashentry(uid)\\t(uidhash_table + __uidhashfn((__kuid_val(uid))))\\n\\nstatic struct kmem_cache *uid_cachep;\\nstatic struct hlist_head uidhash_table[UIDHASH_SZ];\\n\\n/*\\n * The uidhash_lock is mostly taken from process context, but it is\\n * occasionally also taken from softirq/tasklet context, when\\n * task-structs get RCU-freed. Hence all locking must be softirq-safe.\\n * But free_uid() is also called with local interrupts disabled, and running\\n * local_bh_enable() with local interrupts disabled is an error - we\\'ll run\\n * softirq callbacks, and they can unconditionally enable interrupts, and\\n * the caller of free_uid() didn\\'t expect that..\\n */\\nstatic DEFINE_SPINLOCK(uidhash_lock);\\n\\n/* root_user.__count is 1, for init task cred */\\nstruct user_struct root_user = {\\n\\t.__count\\t= REFCOUNT_INIT(1),\\n\\t.uid\\t\\t= GLOBAL_ROOT_UID,\\n\\t.ratelimit\\t= RATELIMIT_STATE_INIT(root_user.ratelimit, 0, 0),\\n};\\n\\n/*\\n * These routines must be called with the uidhash spinlock held!\\n */\\nstatic void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)\\n{\\n\\thlist_add_head(&up->uidhash_node, hashent);\\n}\\n\\nstatic void uid_hash_remove(struct user_struct *up)\\n{\\n\\thlist_del_init(&up->uidhash_node);\\n}\\n\\nstatic struct user_struct *uid_hash_find(kuid_t uid, struct hlist_head *hashent)\\n{\\n\\tstruct user_struct *user;\\n\\n\\thlist_for_each_entry(user, hashent, uidhash_node) {\\n\\t\\tif (uid_eq(user->uid, uid)) {\\n\\t\\t\\trefcount_inc(&user->__count);\\n\\t\\t\\treturn user;\\n\\t\\t}\\n\\t}\\n\\n\\treturn NULL;\\n}\\n\\nstatic int user_epoll_alloc(struct user_struct *up)\\n{\\n#ifdef CONFIG_EPOLL\\n\\treturn percpu_counter_init(&up->epoll_watches, 0, GFP_KERNEL);\\n#else\\n\\treturn 0;\\n#endif\\n}\\n\\nstatic void user_epoll_free(struct user_struct *up)\\n{\\n#ifdef CONFIG_EPOLL\\n\\tpercpu_counter_destroy(&up->epoll_watches);\\n#endif\\n}\\n\\n/* IRQs are disabled and uidhash_lock is held upon function entry.\\n * IRQ state (as stored in flags) is restored and uidhash_lock released\\n * upon function exit.\\n */\\nstatic void free_user(struct user_struct *up, unsigned long flags)\\n\\t__releases(&uidhash_lock)\\n{\\n\\tuid_hash_remove(up);\\n\\tspin_unlock_irqrestore(&uidhash_lock, flags);\\n\\tuser_epoll_free(up);\\n\\tkmem_cache_free(uid_cachep, up);\\n}\\n\\n/*\\n * Locate the user_struct for the passed UID.  If found, take a ref on it.  The\\n * caller must undo that ref with free_uid().\\n *\\n * If the user_struct could not be found, return NULL.\\n */\\nstruct user_struct *find_user(kuid_t uid)\\n{\\n\\tstruct user_struct *ret;\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&uidhash_lock, flags);\\n\\tret = uid_hash_find(uid, uidhashentry(uid));\\n\\tspin_unlock_irqrestore(&uidhash_lock, flags);\\n\\treturn ret;\\n}\\n\\nvoid free_uid(struct user_struct *up)\\n{\\n\\tunsigned long flags;\\n\\n\\tif (!up)\\n\\t\\treturn;\\n\\n\\tif (refcount_dec_and_lock_irqsave(&up->__count, &uidhash_lock, &flags))\\n\\t\\tfree_user(up, flags);\\n}\\nEXPORT_SYMBOL_GPL(free_uid);\\n\\nstruct user_struct *alloc_uid(kuid_t uid)\\n{\\n\\tstruct hlist_head *hashent = uidhashentry(uid);\\n\\tstruct user_struct *up, *new;\\n\\n\\tspin_lock_irq(&uidhash_lock);\\n\\tup = uid_hash_find(uid, hashent);\\n\\tspin_unlock_irq(&uidhash_lock);\\n\\n\\tif (!up) {\\n\\t\\tnew = kmem_cache_zalloc(uid_cachep, GFP_KERNEL);\\n\\t\\tif (!new)\\n\\t\\t\\treturn NULL;\\n\\n\\t\\tnew->uid = uid;\\n\\t\\trefcount_set(&new->__count, 1);\\n\\t\\tif (user_epoll_alloc(new)) {\\n\\t\\t\\tkmem_cache_free(uid_cachep, new);\\n\\t\\t\\treturn NULL;\\n\\t\\t}\\n\\t\\tratelimit_state_init(&new->ratelimit, HZ, 100);\\n\\t\\tratelimit_set_flags(&new->ratelimit, RATELIMIT_MSG_ON_RELEASE);\\n\\n\\t\\t/*\\n\\t\\t * Before adding this, check whether we raced\\n\\t\\t * on adding the same user already..\\n\\t\\t */\\n\\t\\tspin_lock_irq(&uidhash_lock);\\n\\t\\tup = uid_hash_find(uid, hashent);\\n\\t\\tif (up) {\\n\\t\\t\\tuser_epoll_free(new);\\n\\t\\t\\tkmem_cache_free(uid_cachep, new);\\n\\t\\t} else {\\n\\t\\t\\tuid_hash_insert(new, hashent);\\n\\t\\t\\tup = new;\\n\\t\\t}\\n\\t\\tspin_unlock_irq(&uidhash_lock);\\n\\t}\\n\\n\\treturn up;\\n}\\n\\nstatic int __init uid_cache_init(void)\\n{\\n\\tint n;\\n\\n\\tuid_cachep = kmem_cache_create(\"uid_cache\", sizeof(struct user_struct),\\n\\t\\t\\t0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\\n\\n\\tfor(n = 0; n < UIDHASH_SZ; ++n)\\n\\t\\tINIT_HLIST_HEAD(uidhash_table + n);\\n\\n\\tif (user_epoll_alloc(&root_user))\\n\\t\\tpanic(\"root_user epoll percpu counter alloc failed\");\\n\\n\\t/* Insert the root user immediately (init already runs as root) */\\n\\tspin_lock_irq(&uidhash_lock);\\n\\tuid_hash_insert(&root_user, uidhashentry(GLOBAL_ROOT_UID));\\n\\tspin_unlock_irq(&uidhash_lock);\\n\\n\\treturn 0;\\n}\\nsubsys_initcall(uid_cache_init);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Detect hard lockups on a system using perf\\n *\\n * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.\\n *\\n * Note: Most of this code is borrowed heavily from the original softlockup\\n * detector, so thanks to Ingo for the initial implementation.\\n * Some chunks also taken from the old x86-specific nmi watchdog code, thanks\\n * to those contributors as well.\\n */\\n\\n#define pr_fmt(fmt) \"NMI watchdog: \" fmt\\n\\n#include <linux/nmi.h>\\n#include <linux/atomic.h>\\n#include <linux/module.h>\\n#include <linux/sched/debug.h>\\n\\n#include <asm/irq_regs.h>\\n#include <linux/perf_event.h>\\n\\nstatic DEFINE_PER_CPU(struct perf_event *, watchdog_ev);\\nstatic DEFINE_PER_CPU(struct perf_event *, dead_event);\\nstatic struct cpumask dead_events_mask;\\n\\nstatic atomic_t watchdog_cpus = ATOMIC_INIT(0);\\n\\n#ifdef CONFIG_HARDLOCKUP_CHECK_TIMESTAMP\\nstatic DEFINE_PER_CPU(ktime_t, last_timestamp);\\nstatic DEFINE_PER_CPU(unsigned int, nmi_rearmed);\\nstatic ktime_t watchdog_hrtimer_sample_threshold __read_mostly;\\n\\nvoid watchdog_update_hrtimer_threshold(u64 period)\\n{\\n\\t/*\\n\\t * The hrtimer runs with a period of (watchdog_threshold * 2) / 5\\n\\t *\\n\\t * So it runs effectively with 2.5 times the rate of the NMI\\n\\t * watchdog. That means the hrtimer should fire 2-3 times before\\n\\t * the NMI watchdog expires. The NMI watchdog on x86 is based on\\n\\t * unhalted CPU cycles, so if Turbo-Mode is enabled the CPU cycles\\n\\t * might run way faster than expected and the NMI fires in a\\n\\t * smaller period than the one deduced from the nominal CPU\\n\\t * frequency. Depending on the Turbo-Mode factor this might be fast\\n\\t * enough to get the NMI period smaller than the hrtimer watchdog\\n\\t * period and trigger false positives.\\n\\t *\\n\\t * The sample threshold is used to check in the NMI handler whether\\n\\t * the minimum time between two NMI samples has elapsed. That\\n\\t * prevents false positives.\\n\\t *\\n\\t * Set this to 4/5 of the actual watchdog threshold period so the\\n\\t * hrtimer is guaranteed to fire at least once within the real\\n\\t * watchdog threshold.\\n\\t */\\n\\twatchdog_hrtimer_sample_threshold = period * 2;\\n}\\n\\nstatic bool watchdog_check_timestamp(void)\\n{\\n\\tktime_t delta, now = ktime_get_mono_fast_ns();\\n\\n\\tdelta = now - __this_cpu_read(last_timestamp);\\n\\tif (delta < watchdog_hrtimer_sample_threshold) {\\n\\t\\t/*\\n\\t\\t * If ktime is jiffies based, a stalled timer would prevent\\n\\t\\t * jiffies from being incremented and the filter would look\\n\\t\\t * at a stale timestamp and never trigger.\\n\\t\\t */\\n\\t\\tif (__this_cpu_inc_return(nmi_rearmed) < 10)\\n\\t\\t\\treturn false;\\n\\t}\\n\\t__this_cpu_write(nmi_rearmed, 0);\\n\\t__this_cpu_write(last_timestamp, now);\\n\\treturn true;\\n}\\n\\nstatic void watchdog_init_timestamp(void)\\n{\\n\\t__this_cpu_write(nmi_rearmed, 0);\\n\\t__this_cpu_write(last_timestamp, ktime_get_mono_fast_ns());\\n}\\n#else\\nstatic inline bool watchdog_check_timestamp(void) { return true; }\\nstatic inline void watchdog_init_timestamp(void) { }\\n#endif\\n\\nstatic struct perf_event_attr wd_hw_attr = {\\n\\t.type\\t\\t= PERF_TYPE_HARDWARE,\\n\\t.config\\t\\t= PERF_COUNT_HW_CPU_CYCLES,\\n\\t.size\\t\\t= sizeof(struct perf_event_attr),\\n\\t.pinned\\t\\t= 1,\\n\\t.disabled\\t= 1,\\n};\\n\\nstatic struct perf_event_attr fallback_wd_hw_attr = {\\n\\t.type\\t\\t= PERF_TYPE_HARDWARE,\\n\\t.config\\t\\t= PERF_COUNT_HW_CPU_CYCLES,\\n\\t.size\\t\\t= sizeof(struct perf_event_attr),\\n\\t.pinned\\t\\t= 1,\\n\\t.disabled\\t= 1,\\n};\\n\\n/* Callback function for perf event subsystem */\\nstatic void watchdog_overflow_callback(struct perf_event *event,\\n\\t\\t\\t\\t       struct perf_sample_data *data,\\n\\t\\t\\t\\t       struct pt_regs *regs)\\n{\\n\\t/* Ensure the watchdog never gets throttled */\\n\\tevent->hw.interrupts = 0;\\n\\n\\tif (!watchdog_check_timestamp())\\n\\t\\treturn;\\n\\n\\twatchdog_hardlockup_check(smp_processor_id(), regs);\\n}\\n\\nstatic int hardlockup_detector_event_create(void)\\n{\\n\\tunsigned int cpu;\\n\\tstruct perf_event_attr *wd_attr;\\n\\tstruct perf_event *evt;\\n\\n\\t/*\\n\\t * Preemption is not disabled because memory will be allocated.\\n\\t * Ensure CPU-locality by calling this in per-CPU kthread.\\n\\t */\\n\\tWARN_ON(!is_percpu_thread());\\n\\tcpu = raw_smp_processor_id();\\n\\twd_attr = &wd_hw_attr;\\n\\twd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);\\n\\n\\t/* Try to register using hardware perf events */\\n\\tevt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,\\n\\t\\t\\t\\t\\t       watchdog_overflow_callback, NULL);\\n\\tif (IS_ERR(evt)) {\\n\\t\\twd_attr = &fallback_wd_hw_attr;\\n\\t\\twd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);\\n\\t\\tevt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,\\n\\t\\t\\t\\t\\t\\t       watchdog_overflow_callback, NULL);\\n\\t}\\n\\n\\tif (IS_ERR(evt)) {\\n\\t\\tpr_debug(\"Perf event create on CPU %d failed with %ld\\\\n\", cpu,\\n\\t\\t\\t PTR_ERR(evt));\\n\\t\\treturn PTR_ERR(evt);\\n\\t}\\n\\tthis_cpu_write(watchdog_ev, evt);\\n\\treturn 0;\\n}\\n\\n/**\\n * watchdog_hardlockup_enable - Enable the local event\\n * @cpu: The CPU to enable hard lockup on.\\n */\\nvoid watchdog_hardlockup_enable(unsigned int cpu)\\n{\\n\\tWARN_ON_ONCE(cpu != smp_processor_id());\\n\\n\\tif (hardlockup_detector_event_create())\\n\\t\\treturn;\\n\\n\\t/* use original value for check */\\n\\tif (!atomic_fetch_inc(&watchdog_cpus))\\n\\t\\tpr_info(\"Enabled. Permanently consumes one hw-PMU counter.\\\\n\");\\n\\n\\twatchdog_init_timestamp();\\n\\tperf_event_enable(this_cpu_read(watchdog_ev));\\n}\\n\\n/**\\n * watchdog_hardlockup_disable - Disable the local event\\n * @cpu: The CPU to enable hard lockup on.\\n */\\nvoid watchdog_hardlockup_disable(unsigned int cpu)\\n{\\n\\tstruct perf_event *event = this_cpu_read(watchdog_ev);\\n\\n\\tWARN_ON_ONCE(cpu != smp_processor_id());\\n\\n\\tif (event) {\\n\\t\\tperf_event_disable(event);\\n\\t\\tthis_cpu_write(watchdog_ev, NULL);\\n\\t\\tthis_cpu_write(dead_event, event);\\n\\t\\tcpumask_set_cpu(smp_processor_id(), &dead_events_mask);\\n\\t\\tatomic_dec(&watchdog_cpus);\\n\\t}\\n}\\n\\n/**\\n * hardlockup_detector_perf_cleanup - Cleanup disabled events and destroy them\\n *\\n * Called from lockup_detector_cleanup(). Serialized by the caller.\\n */\\nvoid hardlockup_detector_perf_cleanup(void)\\n{\\n\\tint cpu;\\n\\n\\tfor_each_cpu(cpu, &dead_events_mask) {\\n\\t\\tstruct perf_event *event = per_cpu(dead_event, cpu);\\n\\n\\t\\t/*\\n\\t\\t * Required because for_each_cpu() reports  unconditionally\\n\\t\\t * CPU0 as set on UP kernels. Sigh.\\n\\t\\t */\\n\\t\\tif (event)\\n\\t\\t\\tperf_event_release_kernel(event);\\n\\t\\tper_cpu(dead_event, cpu) = NULL;\\n\\t}\\n\\tcpumask_clear(&dead_events_mask);\\n}\\n\\n/**\\n * hardlockup_detector_perf_stop - Globally stop watchdog events\\n *\\n * Special interface for x86 to handle the perf HT bug.\\n */\\nvoid __init hardlockup_detector_perf_stop(void)\\n{\\n\\tint cpu;\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tstruct perf_event *event = per_cpu(watchdog_ev, cpu);\\n\\n\\t\\tif (event)\\n\\t\\t\\tperf_event_disable(event);\\n\\t}\\n}\\n\\n/**\\n * hardlockup_detector_perf_restart - Globally restart watchdog events\\n *\\n * Special interface for x86 to handle the perf HT bug.\\n */\\nvoid __init hardlockup_detector_perf_restart(void)\\n{\\n\\tint cpu;\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tif (!(watchdog_enabled & WATCHDOG_HARDLOCKUP_ENABLED))\\n\\t\\treturn;\\n\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tstruct perf_event *event = per_cpu(watchdog_ev, cpu);\\n\\n\\t\\tif (event)\\n\\t\\t\\tperf_event_enable(event);\\n\\t}\\n}\\n\\nbool __weak __init arch_perf_nmi_is_available(void)\\n{\\n\\treturn true;\\n}\\n\\n/**\\n * watchdog_hardlockup_probe - Probe whether NMI event is available at all\\n */\\nint __init watchdog_hardlockup_probe(void)\\n{\\n\\tint ret;\\n\\n\\tif (!arch_perf_nmi_is_available())\\n\\t\\treturn -ENODEV;\\n\\n\\tret = hardlockup_detector_event_create();\\n\\n\\tif (ret) {\\n\\t\\tpr_info(\"Perf NMI watchdog permanently disabled\\\\n\");\\n\\t} else {\\n\\t\\tperf_event_release_kernel(this_cpu_read(watchdog_ev));\\n\\t\\tthis_cpu_write(watchdog_ev, NULL);\\n\\t}\\n\\treturn ret;\\n}\\n\\n/**\\n * hardlockup_config_perf_event - Overwrite config of wd_hw_attr.\\n * @str: number which identifies the raw perf event to use\\n */\\nvoid __init hardlockup_config_perf_event(const char *str)\\n{\\n\\tu64 config;\\n\\tchar buf[24];\\n\\tchar *comma = strchr(str, \\',\\');\\n\\n\\tif (!comma) {\\n\\t\\tif (kstrtoull(str, 16, &config))\\n\\t\\t\\treturn;\\n\\t} else {\\n\\t\\tunsigned int len = comma - str;\\n\\n\\t\\tif (len >= sizeof(buf))\\n\\t\\t\\treturn;\\n\\n\\t\\tif (strscpy(buf, str, sizeof(buf)) < 0)\\n\\t\\t\\treturn;\\n\\t\\tbuf[len] = 0;\\n\\t\\tif (kstrtoull(buf, 16, &config))\\n\\t\\t\\treturn;\\n\\t}\\n\\n\\twd_hw_attr.type = PERF_TYPE_RAW;\\n\\twd_hw_attr.config = config;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * umd - User mode driver support\\n */\\n#include <linux/shmem_fs.h>\\n#include <linux/pipe_fs_i.h>\\n#include <linux/mount.h>\\n#include <linux/fs_struct.h>\\n#include <linux/task_work.h>\\n#include <linux/usermode_driver.h>\\n\\nstatic struct vfsmount *blob_to_mnt(const void *data, size_t len, const char *name)\\n{\\n\\tstruct file_system_type *type;\\n\\tstruct vfsmount *mnt;\\n\\tstruct file *file;\\n\\tssize_t written;\\n\\tloff_t pos = 0;\\n\\n\\ttype = get_fs_type(\"tmpfs\");\\n\\tif (!type)\\n\\t\\treturn ERR_PTR(-ENODEV);\\n\\n\\tmnt = kern_mount(type);\\n\\tput_filesystem(type);\\n\\tif (IS_ERR(mnt))\\n\\t\\treturn mnt;\\n\\n\\tfile = file_open_root_mnt(mnt, name, O_CREAT | O_WRONLY, 0700);\\n\\tif (IS_ERR(file)) {\\n\\t\\tkern_unmount(mnt);\\n\\t\\treturn ERR_CAST(file);\\n\\t}\\n\\n\\twritten = kernel_write(file, data, len, &pos);\\n\\tif (written != len) {\\n\\t\\tint err = written;\\n\\t\\tif (err >= 0)\\n\\t\\t\\terr = -ENOMEM;\\n\\t\\tfilp_close(file, NULL);\\n\\t\\tkern_unmount(mnt);\\n\\t\\treturn ERR_PTR(err);\\n\\t}\\n\\n\\tfput(file);\\n\\n\\t/* Flush delayed fput so exec can open the file read-only */\\n\\tflush_delayed_fput();\\n\\ttask_work_run();\\n\\treturn mnt;\\n}\\n\\n/**\\n * umd_load_blob - Remember a blob of bytes for fork_usermode_driver\\n * @info: information about usermode driver\\n * @data: a blob of bytes that can be executed as a file\\n * @len:  The lentgh of the blob\\n *\\n */\\nint umd_load_blob(struct umd_info *info, const void *data, size_t len)\\n{\\n\\tstruct vfsmount *mnt;\\n\\n\\tif (WARN_ON_ONCE(info->wd.dentry || info->wd.mnt))\\n\\t\\treturn -EBUSY;\\n\\n\\tmnt = blob_to_mnt(data, len, info->driver_name);\\n\\tif (IS_ERR(mnt))\\n\\t\\treturn PTR_ERR(mnt);\\n\\n\\tinfo->wd.mnt = mnt;\\n\\tinfo->wd.dentry = mnt->mnt_root;\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(umd_load_blob);\\n\\n/**\\n * umd_unload_blob - Disassociate @info from a previously loaded blob\\n * @info: information about usermode driver\\n *\\n */\\nint umd_unload_blob(struct umd_info *info)\\n{\\n\\tif (WARN_ON_ONCE(!info->wd.mnt ||\\n\\t\\t\\t !info->wd.dentry ||\\n\\t\\t\\t info->wd.mnt->mnt_root != info->wd.dentry))\\n\\t\\treturn -EINVAL;\\n\\n\\tkern_unmount(info->wd.mnt);\\n\\tinfo->wd.mnt = NULL;\\n\\tinfo->wd.dentry = NULL;\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(umd_unload_blob);\\n\\nstatic int umd_setup(struct subprocess_info *info, struct cred *new)\\n{\\n\\tstruct umd_info *umd_info = info->data;\\n\\tstruct file *from_umh[2];\\n\\tstruct file *to_umh[2];\\n\\tint err;\\n\\n\\t/* create pipe to send data to umh */\\n\\terr = create_pipe_files(to_umh, 0);\\n\\tif (err)\\n\\t\\treturn err;\\n\\terr = replace_fd(0, to_umh[0], 0);\\n\\tfput(to_umh[0]);\\n\\tif (err < 0) {\\n\\t\\tfput(to_umh[1]);\\n\\t\\treturn err;\\n\\t}\\n\\n\\t/* create pipe to receive data from umh */\\n\\terr = create_pipe_files(from_umh, 0);\\n\\tif (err) {\\n\\t\\tfput(to_umh[1]);\\n\\t\\treplace_fd(0, NULL, 0);\\n\\t\\treturn err;\\n\\t}\\n\\terr = replace_fd(1, from_umh[1], 0);\\n\\tfput(from_umh[1]);\\n\\tif (err < 0) {\\n\\t\\tfput(to_umh[1]);\\n\\t\\treplace_fd(0, NULL, 0);\\n\\t\\tfput(from_umh[0]);\\n\\t\\treturn err;\\n\\t}\\n\\n\\tset_fs_pwd(current->fs, &umd_info->wd);\\n\\tumd_info->pipe_to_umh = to_umh[1];\\n\\tumd_info->pipe_from_umh = from_umh[0];\\n\\tumd_info->tgid = get_pid(task_tgid(current));\\n\\treturn 0;\\n}\\n\\nstatic void umd_cleanup(struct subprocess_info *info)\\n{\\n\\tstruct umd_info *umd_info = info->data;\\n\\n\\t/* cleanup if umh_setup() was successful but exec failed */\\n\\tif (info->retval)\\n\\t\\tumd_cleanup_helper(umd_info);\\n}\\n\\n/**\\n * umd_cleanup_helper - release the resources which were allocated in umd_setup\\n * @info: information about usermode driver\\n */\\nvoid umd_cleanup_helper(struct umd_info *info)\\n{\\n\\tfput(info->pipe_to_umh);\\n\\tfput(info->pipe_from_umh);\\n\\tput_pid(info->tgid);\\n\\tinfo->tgid = NULL;\\n}\\nEXPORT_SYMBOL_GPL(umd_cleanup_helper);\\n\\n/**\\n * fork_usermode_driver - fork a usermode driver\\n * @info: information about usermode driver (shouldn\\'t be NULL)\\n *\\n * Returns either negative error or zero which indicates success in\\n * executing a usermode driver. In such case \\'struct umd_info *info\\'\\n * is populated with two pipes and a tgid of the process. The caller is\\n * responsible for health check of the user process, killing it via\\n * tgid, and closing the pipes when user process is no longer needed.\\n */\\nint fork_usermode_driver(struct umd_info *info)\\n{\\n\\tstruct subprocess_info *sub_info;\\n\\tconst char *argv[] = { info->driver_name, NULL };\\n\\tint err;\\n\\n\\tif (WARN_ON_ONCE(info->tgid))\\n\\t\\treturn -EBUSY;\\n\\n\\terr = -ENOMEM;\\n\\tsub_info = call_usermodehelper_setup(info->driver_name,\\n\\t\\t\\t\\t\\t     (char **)argv, NULL, GFP_KERNEL,\\n\\t\\t\\t\\t\\t     umd_setup, umd_cleanup, info);\\n\\tif (!sub_info)\\n\\t\\tgoto out;\\n\\n\\terr = call_usermodehelper_exec(sub_info, UMH_WAIT_EXEC);\\nout:\\n\\treturn err;\\n}\\nEXPORT_SYMBOL_GPL(fork_usermode_driver);\\n\\n\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n *\\tWrapper functions for 16bit uid back compatibility. All nicely tied\\n *\\ttogether in the faint hope we can take the out in five years time.\\n */\\n\\n#include <linux/mm.h>\\n#include <linux/mman.h>\\n#include <linux/notifier.h>\\n#include <linux/reboot.h>\\n#include <linux/prctl.h>\\n#include <linux/capability.h>\\n#include <linux/init.h>\\n#include <linux/highuid.h>\\n#include <linux/security.h>\\n#include <linux/cred.h>\\n#include <linux/syscalls.h>\\n\\n#include <linux/uaccess.h>\\n\\n#include \"uid16.h\"\\n\\nSYSCALL_DEFINE3(chown16, const char __user *, filename, old_uid_t, user, old_gid_t, group)\\n{\\n\\treturn ksys_chown(filename, low2highuid(user), low2highgid(group));\\n}\\n\\nSYSCALL_DEFINE3(lchown16, const char __user *, filename, old_uid_t, user, old_gid_t, group)\\n{\\n\\treturn ksys_lchown(filename, low2highuid(user), low2highgid(group));\\n}\\n\\nSYSCALL_DEFINE3(fchown16, unsigned int, fd, old_uid_t, user, old_gid_t, group)\\n{\\n\\treturn ksys_fchown(fd, low2highuid(user), low2highgid(group));\\n}\\n\\nSYSCALL_DEFINE2(setregid16, old_gid_t, rgid, old_gid_t, egid)\\n{\\n\\treturn __sys_setregid(low2highgid(rgid), low2highgid(egid));\\n}\\n\\nSYSCALL_DEFINE1(setgid16, old_gid_t, gid)\\n{\\n\\treturn __sys_setgid(low2highgid(gid));\\n}\\n\\nSYSCALL_DEFINE2(setreuid16, old_uid_t, ruid, old_uid_t, euid)\\n{\\n\\treturn __sys_setreuid(low2highuid(ruid), low2highuid(euid));\\n}\\n\\nSYSCALL_DEFINE1(setuid16, old_uid_t, uid)\\n{\\n\\treturn __sys_setuid(low2highuid(uid));\\n}\\n\\nSYSCALL_DEFINE3(setresuid16, old_uid_t, ruid, old_uid_t, euid, old_uid_t, suid)\\n{\\n\\treturn __sys_setresuid(low2highuid(ruid), low2highuid(euid),\\n\\t\\t\\t\\t low2highuid(suid));\\n}\\n\\nSYSCALL_DEFINE3(getresuid16, old_uid_t __user *, ruidp, old_uid_t __user *, euidp, old_uid_t __user *, suidp)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint retval;\\n\\told_uid_t ruid, euid, suid;\\n\\n\\truid = high2lowuid(from_kuid_munged(cred->user_ns, cred->uid));\\n\\teuid = high2lowuid(from_kuid_munged(cred->user_ns, cred->euid));\\n\\tsuid = high2lowuid(from_kuid_munged(cred->user_ns, cred->suid));\\n\\n\\tif (!(retval   = put_user(ruid, ruidp)) &&\\n\\t    !(retval   = put_user(euid, euidp)))\\n\\t\\tretval = put_user(suid, suidp);\\n\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE3(setresgid16, old_gid_t, rgid, old_gid_t, egid, old_gid_t, sgid)\\n{\\n\\treturn __sys_setresgid(low2highgid(rgid), low2highgid(egid),\\n\\t\\t\\t\\t low2highgid(sgid));\\n}\\n\\nSYSCALL_DEFINE3(getresgid16, old_gid_t __user *, rgidp, old_gid_t __user *, egidp, old_gid_t __user *, sgidp)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint retval;\\n\\told_gid_t rgid, egid, sgid;\\n\\n\\trgid = high2lowgid(from_kgid_munged(cred->user_ns, cred->gid));\\n\\tegid = high2lowgid(from_kgid_munged(cred->user_ns, cred->egid));\\n\\tsgid = high2lowgid(from_kgid_munged(cred->user_ns, cred->sgid));\\n\\n\\tif (!(retval   = put_user(rgid, rgidp)) &&\\n\\t    !(retval   = put_user(egid, egidp)))\\n\\t\\tretval = put_user(sgid, sgidp);\\n\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE1(setfsuid16, old_uid_t, uid)\\n{\\n\\treturn __sys_setfsuid(low2highuid(uid));\\n}\\n\\nSYSCALL_DEFINE1(setfsgid16, old_gid_t, gid)\\n{\\n\\treturn __sys_setfsgid(low2highgid(gid));\\n}\\n\\nstatic int groups16_to_user(old_gid_t __user *grouplist,\\n    struct group_info *group_info)\\n{\\n\\tstruct user_namespace *user_ns = current_user_ns();\\n\\tint i;\\n\\told_gid_t group;\\n\\tkgid_t kgid;\\n\\n\\tfor (i = 0; i < group_info->ngroups; i++) {\\n\\t\\tkgid = group_info->gid[i];\\n\\t\\tgroup = high2lowgid(from_kgid_munged(user_ns, kgid));\\n\\t\\tif (put_user(group, grouplist+i))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic int groups16_from_user(struct group_info *group_info,\\n    old_gid_t __user *grouplist)\\n{\\n\\tstruct user_namespace *user_ns = current_user_ns();\\n\\tint i;\\n\\told_gid_t group;\\n\\tkgid_t kgid;\\n\\n\\tfor (i = 0; i < group_info->ngroups; i++) {\\n\\t\\tif (get_user(group, grouplist+i))\\n\\t\\t\\treturn  -EFAULT;\\n\\n\\t\\tkgid = make_kgid(user_ns, low2highgid(group));\\n\\t\\tif (!gid_valid(kgid))\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\tgroup_info->gid[i] = kgid;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nSYSCALL_DEFINE2(getgroups16, int, gidsetsize, old_gid_t __user *, grouplist)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint i;\\n\\n\\tif (gidsetsize < 0)\\n\\t\\treturn -EINVAL;\\n\\n\\ti = cred->group_info->ngroups;\\n\\tif (gidsetsize) {\\n\\t\\tif (i > gidsetsize) {\\n\\t\\t\\ti = -EINVAL;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tif (groups16_to_user(grouplist, cred->group_info)) {\\n\\t\\t\\ti = -EFAULT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\nout:\\n\\treturn i;\\n}\\n\\nSYSCALL_DEFINE2(setgroups16, int, gidsetsize, old_gid_t __user *, grouplist)\\n{\\n\\tstruct group_info *group_info;\\n\\tint retval;\\n\\n\\tif (!may_setgroups())\\n\\t\\treturn -EPERM;\\n\\tif ((unsigned)gidsetsize > NGROUPS_MAX)\\n\\t\\treturn -EINVAL;\\n\\n\\tgroup_info = groups_alloc(gidsetsize);\\n\\tif (!group_info)\\n\\t\\treturn -ENOMEM;\\n\\tretval = groups16_from_user(group_info, grouplist);\\n\\tif (retval) {\\n\\t\\tput_group_info(group_info);\\n\\t\\treturn retval;\\n\\t}\\n\\n\\tgroups_sort(group_info);\\n\\tretval = set_current_groups(group_info);\\n\\tput_group_info(group_info);\\n\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE0(getuid16)\\n{\\n\\treturn high2lowuid(from_kuid_munged(current_user_ns(), current_uid()));\\n}\\n\\nSYSCALL_DEFINE0(geteuid16)\\n{\\n\\treturn high2lowuid(from_kuid_munged(current_user_ns(), current_euid()));\\n}\\n\\nSYSCALL_DEFINE0(getgid16)\\n{\\n\\treturn high2lowgid(from_kgid_munged(current_user_ns(), current_gid()));\\n}\\n\\nSYSCALL_DEFINE0(getegid16)\\n{\\n\\treturn high2lowgid(from_kgid_munged(current_user_ns(), current_egid()));\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n\\n#include <linux/user-return-notifier.h>\\n#include <linux/percpu.h>\\n#include <linux/sched.h>\\n#include <linux/export.h>\\n\\nstatic DEFINE_PER_CPU(struct hlist_head, return_notifier_list);\\n\\n/*\\n * Request a notification when the current cpu returns to userspace.  Must be\\n * called in atomic context.  The notifier will also be called in atomic\\n * context.\\n */\\nvoid user_return_notifier_register(struct user_return_notifier *urn)\\n{\\n\\tset_tsk_thread_flag(current, TIF_USER_RETURN_NOTIFY);\\n\\thlist_add_head(&urn->link, this_cpu_ptr(&return_notifier_list));\\n}\\nEXPORT_SYMBOL_GPL(user_return_notifier_register);\\n\\n/*\\n * Removes a registered user return notifier.  Must be called from atomic\\n * context, and from the same cpu registration occurred in.\\n */\\nvoid user_return_notifier_unregister(struct user_return_notifier *urn)\\n{\\n\\thlist_del(&urn->link);\\n\\tif (hlist_empty(this_cpu_ptr(&return_notifier_list)))\\n\\t\\tclear_tsk_thread_flag(current, TIF_USER_RETURN_NOTIFY);\\n}\\nEXPORT_SYMBOL_GPL(user_return_notifier_unregister);\\n\\n/* Calls registered user return notifiers */\\nvoid fire_user_return_notifiers(void)\\n{\\n\\tstruct user_return_notifier *urn;\\n\\tstruct hlist_node *tmp2;\\n\\tstruct hlist_head *head;\\n\\n\\thead = &get_cpu_var(return_notifier_list);\\n\\thlist_for_each_entry_safe(urn, tmp2, head, link)\\n\\t\\turn->on_user_return(urn);\\n\\tput_cpu_var(return_notifier_list);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n\\n#include <linux/cpu.h>\\n#include <linux/cpumask.h>\\n#include <linux/kernel.h>\\n#include <linux/nmi.h>\\n#include <linux/percpu-defs.h>\\n\\nstatic cpumask_t __read_mostly watchdog_cpus;\\n\\nstatic unsigned int watchdog_next_cpu(unsigned int cpu)\\n{\\n\\tunsigned int next_cpu;\\n\\n\\tnext_cpu = cpumask_next(cpu, &watchdog_cpus);\\n\\tif (next_cpu >= nr_cpu_ids)\\n\\t\\tnext_cpu = cpumask_first(&watchdog_cpus);\\n\\n\\tif (next_cpu == cpu)\\n\\t\\treturn nr_cpu_ids;\\n\\n\\treturn next_cpu;\\n}\\n\\nint __init watchdog_hardlockup_probe(void)\\n{\\n\\treturn 0;\\n}\\n\\nvoid watchdog_hardlockup_enable(unsigned int cpu)\\n{\\n\\tunsigned int next_cpu;\\n\\n\\t/*\\n\\t * The new CPU will be marked online before the hrtimer interrupt\\n\\t * gets a chance to run on it. If another CPU tests for a\\n\\t * hardlockup on the new CPU before it has run its the hrtimer\\n\\t * interrupt, it will get a false positive. Touch the watchdog on\\n\\t * the new CPU to delay the check for at least 3 sampling periods\\n\\t * to guarantee one hrtimer has run on the new CPU.\\n\\t */\\n\\twatchdog_hardlockup_touch_cpu(cpu);\\n\\n\\t/*\\n\\t * We are going to check the next CPU. Our watchdog_hrtimer\\n\\t * need not be zero if the CPU has already been online earlier.\\n\\t * Touch the watchdog on the next CPU to avoid false positive\\n\\t * if we try to check it in less then 3 interrupts.\\n\\t */\\n\\tnext_cpu = watchdog_next_cpu(cpu);\\n\\tif (next_cpu < nr_cpu_ids)\\n\\t\\twatchdog_hardlockup_touch_cpu(next_cpu);\\n\\n\\t/*\\n\\t * Makes sure that watchdog is touched on this CPU before\\n\\t * other CPUs could see it in watchdog_cpus. The counter\\n\\t * part is in watchdog_buddy_check_hardlockup().\\n\\t */\\n\\tsmp_wmb();\\n\\n\\tcpumask_set_cpu(cpu, &watchdog_cpus);\\n}\\n\\nvoid watchdog_hardlockup_disable(unsigned int cpu)\\n{\\n\\tunsigned int next_cpu = watchdog_next_cpu(cpu);\\n\\n\\t/*\\n\\t * Offlining this CPU will cause the CPU before this one to start\\n\\t * checking the one after this one. If this CPU just finished checking\\n\\t * the next CPU and updating hrtimer_interrupts_saved, and then the\\n\\t * previous CPU checks it within one sample period, it will trigger a\\n\\t * false positive. Touch the watchdog on the next CPU to prevent it.\\n\\t */\\n\\tif (next_cpu < nr_cpu_ids)\\n\\t\\twatchdog_hardlockup_touch_cpu(next_cpu);\\n\\n\\t/*\\n\\t * Makes sure that watchdog is touched on the next CPU before\\n\\t * this CPU disappear in watchdog_cpus. The counter part is in\\n\\t * watchdog_buddy_check_hardlockup().\\n\\t */\\n\\tsmp_wmb();\\n\\n\\tcpumask_clear_cpu(cpu, &watchdog_cpus);\\n}\\n\\nvoid watchdog_buddy_check_hardlockup(int hrtimer_interrupts)\\n{\\n\\tunsigned int next_cpu;\\n\\n\\t/*\\n\\t * Test for hardlockups every 3 samples. The sample period is\\n\\t *  watchdog_thresh * 2 / 5, so 3 samples gets us back to slightly over\\n\\t *  watchdog_thresh (over by 20%).\\n\\t */\\n\\tif (hrtimer_interrupts % 3 != 0)\\n\\t\\treturn;\\n\\n\\t/* check for a hardlockup on the next CPU */\\n\\tnext_cpu = watchdog_next_cpu(smp_processor_id());\\n\\tif (next_cpu >= nr_cpu_ids)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Make sure that the watchdog was touched on next CPU when\\n\\t * watchdog_next_cpu() returned another one because of\\n\\t * a change in watchdog_hardlockup_enable()/disable().\\n\\t */\\n\\tsmp_rmb();\\n\\n\\twatchdog_hardlockup_check(next_cpu, NULL);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Copyright (C) 2021 Oracle Corporation\\n */\\n#include <linux/slab.h>\\n#include <linux/completion.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/vhost_task.h>\\n#include <linux/sched/signal.h>\\n\\nenum vhost_task_flags {\\n\\tVHOST_TASK_FLAGS_STOP,\\n\\tVHOST_TASK_FLAGS_KILLED,\\n};\\n\\nstruct vhost_task {\\n\\tbool (*fn)(void *data);\\n\\tvoid (*handle_sigkill)(void *data);\\n\\tvoid *data;\\n\\tstruct completion exited;\\n\\tunsigned long flags;\\n\\tstruct task_struct *task;\\n\\t/* serialize SIGKILL and vhost_task_stop calls */\\n\\tstruct mutex exit_mutex;\\n};\\n\\nstatic int vhost_task_fn(void *data)\\n{\\n\\tstruct vhost_task *vtsk = data;\\n\\n\\tfor (;;) {\\n\\t\\tbool did_work;\\n\\n\\t\\tif (signal_pending(current)) {\\n\\t\\t\\tstruct ksignal ksig;\\n\\n\\t\\t\\tif (get_signal(&ksig))\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\t/* mb paired w/ vhost_task_stop */\\n\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\n\\t\\tif (test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tdid_work = vtsk->fn(vtsk->data);\\n\\t\\tif (!did_work)\\n\\t\\t\\tschedule();\\n\\t}\\n\\n\\tmutex_lock(&vtsk->exit_mutex);\\n\\t/*\\n\\t * If a vhost_task_stop and SIGKILL race, we can ignore the SIGKILL.\\n\\t * When the vhost layer has called vhost_task_stop it\\'s already stopped\\n\\t * new work and flushed.\\n\\t */\\n\\tif (!test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {\\n\\t\\tset_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags);\\n\\t\\tvtsk->handle_sigkill(vtsk->data);\\n\\t}\\n\\tmutex_unlock(&vtsk->exit_mutex);\\n\\tcomplete(&vtsk->exited);\\n\\n\\tdo_exit(0);\\n}\\n\\n/**\\n * vhost_task_wake - wakeup the vhost_task\\n * @vtsk: vhost_task to wake\\n *\\n * wake up the vhost_task worker thread\\n */\\nvoid vhost_task_wake(struct vhost_task *vtsk)\\n{\\n\\twake_up_process(vtsk->task);\\n}\\nEXPORT_SYMBOL_GPL(vhost_task_wake);\\n\\n/**\\n * vhost_task_stop - stop a vhost_task\\n * @vtsk: vhost_task to stop\\n *\\n * vhost_task_fn ensures the worker thread exits after\\n * VHOST_TASK_FLAGS_STOP becomes true.\\n */\\nvoid vhost_task_stop(struct vhost_task *vtsk)\\n{\\n\\tmutex_lock(&vtsk->exit_mutex);\\n\\tif (!test_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags)) {\\n\\t\\tset_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags);\\n\\t\\tvhost_task_wake(vtsk);\\n\\t}\\n\\tmutex_unlock(&vtsk->exit_mutex);\\n\\n\\t/*\\n\\t * Make sure vhost_task_fn is no longer accessing the vhost_task before\\n\\t * freeing it below.\\n\\t */\\n\\twait_for_completion(&vtsk->exited);\\n\\tkfree(vtsk);\\n}\\nEXPORT_SYMBOL_GPL(vhost_task_stop);\\n\\n/**\\n * vhost_task_create - create a copy of a task to be used by the kernel\\n * @fn: vhost worker function\\n * @handle_sigkill: vhost function to handle when we are killed\\n * @arg: data to be passed to fn and handled_kill\\n * @name: the thread\\'s name\\n *\\n * This returns a specialized task for use by the vhost layer or NULL on\\n * failure. The returned task is inactive, and the caller must fire it up\\n * through vhost_task_start().\\n */\\nstruct vhost_task *vhost_task_create(bool (*fn)(void *),\\n\\t\\t\\t\\t     void (*handle_sigkill)(void *), void *arg,\\n\\t\\t\\t\\t     const char *name)\\n{\\n\\tstruct kernel_clone_args args = {\\n\\t\\t.flags\\t\\t= CLONE_FS | CLONE_UNTRACED | CLONE_VM |\\n\\t\\t\\t\\t  CLONE_THREAD | CLONE_SIGHAND,\\n\\t\\t.exit_signal\\t= 0,\\n\\t\\t.fn\\t\\t= vhost_task_fn,\\n\\t\\t.name\\t\\t= name,\\n\\t\\t.user_worker\\t= 1,\\n\\t\\t.no_files\\t= 1,\\n\\t};\\n\\tstruct vhost_task *vtsk;\\n\\tstruct task_struct *tsk;\\n\\n\\tvtsk = kzalloc(sizeof(*vtsk), GFP_KERNEL);\\n\\tif (!vtsk)\\n\\t\\treturn NULL;\\n\\tinit_completion(&vtsk->exited);\\n\\tmutex_init(&vtsk->exit_mutex);\\n\\tvtsk->data = arg;\\n\\tvtsk->fn = fn;\\n\\tvtsk->handle_sigkill = handle_sigkill;\\n\\n\\targs.fn_arg = vtsk;\\n\\n\\ttsk = copy_process(NULL, 0, NUMA_NO_NODE, &args);\\n\\tif (IS_ERR(tsk)) {\\n\\t\\tkfree(vtsk);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tvtsk->task = tsk;\\n\\treturn vtsk;\\n}\\nEXPORT_SYMBOL_GPL(vhost_task_create);\\n\\n/**\\n * vhost_task_start - start a vhost_task created with vhost_task_create\\n * @vtsk: vhost_task to wake up\\n */\\nvoid vhost_task_start(struct vhost_task *vtsk)\\n{\\n\\twake_up_new_task(vtsk->task);\\n}\\nEXPORT_SYMBOL_GPL(vhost_task_start);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * crash.c - kernel crash support code.\\n * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>\\n */\\n\\n#include <linux/buildid.h>\\n#include <linux/init.h>\\n#include <linux/utsname.h>\\n#include <linux/vmalloc.h>\\n#include <linux/sizes.h>\\n#include <linux/kexec.h>\\n#include <linux/memory.h>\\n#include <linux/cpuhotplug.h>\\n#include <linux/memblock.h>\\n#include <linux/kmemleak.h>\\n\\n#include <asm/page.h>\\n#include <asm/sections.h>\\n\\n#include <crypto/sha1.h>\\n\\n#include \"kallsyms_internal.h\"\\n#include \"kexec_internal.h\"\\n\\n/* vmcoreinfo stuff */\\nunsigned char *vmcoreinfo_data;\\nsize_t vmcoreinfo_size;\\nu32 *vmcoreinfo_note;\\n\\n/* trusted vmcoreinfo, e.g. we can make a copy in the crash memory */\\nstatic unsigned char *vmcoreinfo_data_safecopy;\\n\\nElf_Word *append_elf_note(Elf_Word *buf, char *name, unsigned int type,\\n\\t\\t\\t  void *data, size_t data_len)\\n{\\n\\tstruct elf_note *note = (struct elf_note *)buf;\\n\\n\\tnote->n_namesz = strlen(name) + 1;\\n\\tnote->n_descsz = data_len;\\n\\tnote->n_type   = type;\\n\\tbuf += DIV_ROUND_UP(sizeof(*note), sizeof(Elf_Word));\\n\\tmemcpy(buf, name, note->n_namesz);\\n\\tbuf += DIV_ROUND_UP(note->n_namesz, sizeof(Elf_Word));\\n\\tmemcpy(buf, data, data_len);\\n\\tbuf += DIV_ROUND_UP(data_len, sizeof(Elf_Word));\\n\\n\\treturn buf;\\n}\\n\\nvoid final_note(Elf_Word *buf)\\n{\\n\\tmemset(buf, 0, sizeof(struct elf_note));\\n}\\n\\nstatic void update_vmcoreinfo_note(void)\\n{\\n\\tu32 *buf = vmcoreinfo_note;\\n\\n\\tif (!vmcoreinfo_size)\\n\\t\\treturn;\\n\\tbuf = append_elf_note(buf, VMCOREINFO_NOTE_NAME, 0, vmcoreinfo_data,\\n\\t\\t\\t      vmcoreinfo_size);\\n\\tfinal_note(buf);\\n}\\n\\nvoid crash_update_vmcoreinfo_safecopy(void *ptr)\\n{\\n\\tif (ptr)\\n\\t\\tmemcpy(ptr, vmcoreinfo_data, vmcoreinfo_size);\\n\\n\\tvmcoreinfo_data_safecopy = ptr;\\n}\\n\\nvoid crash_save_vmcoreinfo(void)\\n{\\n\\tif (!vmcoreinfo_note)\\n\\t\\treturn;\\n\\n\\t/* Use the safe copy to generate vmcoreinfo note if have */\\n\\tif (vmcoreinfo_data_safecopy)\\n\\t\\tvmcoreinfo_data = vmcoreinfo_data_safecopy;\\n\\n\\tvmcoreinfo_append_str(\"CRASHTIME=%lld\\\\n\", ktime_get_real_seconds());\\n\\tupdate_vmcoreinfo_note();\\n}\\n\\nvoid vmcoreinfo_append_str(const char *fmt, ...)\\n{\\n\\tva_list args;\\n\\tchar buf[0x50];\\n\\tsize_t r;\\n\\n\\tva_start(args, fmt);\\n\\tr = vscnprintf(buf, sizeof(buf), fmt, args);\\n\\tva_end(args);\\n\\n\\tr = min(r, (size_t)VMCOREINFO_BYTES - vmcoreinfo_size);\\n\\n\\tmemcpy(&vmcoreinfo_data[vmcoreinfo_size], buf, r);\\n\\n\\tvmcoreinfo_size += r;\\n\\n\\tWARN_ONCE(vmcoreinfo_size == VMCOREINFO_BYTES,\\n\\t\\t  \"vmcoreinfo data exceeds allocated size, truncating\");\\n}\\n\\n/*\\n * provide an empty default implementation here -- architecture\\n * code may override this\\n */\\nvoid __weak arch_crash_save_vmcoreinfo(void)\\n{}\\n\\nphys_addr_t __weak paddr_vmcoreinfo_note(void)\\n{\\n\\treturn __pa(vmcoreinfo_note);\\n}\\nEXPORT_SYMBOL(paddr_vmcoreinfo_note);\\n\\nstatic int __init crash_save_vmcoreinfo_init(void)\\n{\\n\\tvmcoreinfo_data = (unsigned char *)get_zeroed_page(GFP_KERNEL);\\n\\tif (!vmcoreinfo_data) {\\n\\t\\tpr_warn(\"Memory allocation for vmcoreinfo_data failed\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tvmcoreinfo_note = alloc_pages_exact(VMCOREINFO_NOTE_SIZE,\\n\\t\\t\\t\\t\\t\\tGFP_KERNEL | __GFP_ZERO);\\n\\tif (!vmcoreinfo_note) {\\n\\t\\tfree_page((unsigned long)vmcoreinfo_data);\\n\\t\\tvmcoreinfo_data = NULL;\\n\\t\\tpr_warn(\"Memory allocation for vmcoreinfo_note failed\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tVMCOREINFO_OSRELEASE(init_uts_ns.name.release);\\n\\tVMCOREINFO_BUILD_ID();\\n\\tVMCOREINFO_PAGESIZE(PAGE_SIZE);\\n\\n\\tVMCOREINFO_SYMBOL(init_uts_ns);\\n\\tVMCOREINFO_OFFSET(uts_namespace, name);\\n\\tVMCOREINFO_SYMBOL(node_online_map);\\n#ifdef CONFIG_MMU\\n\\tVMCOREINFO_SYMBOL_ARRAY(swapper_pg_dir);\\n#endif\\n\\tVMCOREINFO_SYMBOL(_stext);\\n\\tvmcoreinfo_append_str(\"NUMBER(VMALLOC_START)=0x%lx\\\\n\", (unsigned long) VMALLOC_START);\\n\\n#ifndef CONFIG_NUMA\\n\\tVMCOREINFO_SYMBOL(mem_map);\\n\\tVMCOREINFO_SYMBOL(contig_page_data);\\n#endif\\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\\n\\tVMCOREINFO_SYMBOL_ARRAY(vmemmap);\\n#endif\\n#ifdef CONFIG_SPARSEMEM\\n\\tVMCOREINFO_SYMBOL_ARRAY(mem_section);\\n\\tVMCOREINFO_LENGTH(mem_section, NR_SECTION_ROOTS);\\n\\tVMCOREINFO_STRUCT_SIZE(mem_section);\\n\\tVMCOREINFO_OFFSET(mem_section, section_mem_map);\\n\\tVMCOREINFO_NUMBER(SECTION_SIZE_BITS);\\n\\tVMCOREINFO_NUMBER(MAX_PHYSMEM_BITS);\\n#endif\\n\\tVMCOREINFO_STRUCT_SIZE(page);\\n\\tVMCOREINFO_STRUCT_SIZE(pglist_data);\\n\\tVMCOREINFO_STRUCT_SIZE(zone);\\n\\tVMCOREINFO_STRUCT_SIZE(free_area);\\n\\tVMCOREINFO_STRUCT_SIZE(list_head);\\n\\tVMCOREINFO_SIZE(nodemask_t);\\n\\tVMCOREINFO_OFFSET(page, flags);\\n\\tVMCOREINFO_OFFSET(page, _refcount);\\n\\tVMCOREINFO_OFFSET(page, mapping);\\n\\tVMCOREINFO_OFFSET(page, lru);\\n\\tVMCOREINFO_OFFSET(page, _mapcount);\\n\\tVMCOREINFO_OFFSET(page, private);\\n\\tVMCOREINFO_OFFSET(page, compound_head);\\n\\tVMCOREINFO_OFFSET(pglist_data, node_zones);\\n\\tVMCOREINFO_OFFSET(pglist_data, nr_zones);\\n#ifdef CONFIG_FLATMEM\\n\\tVMCOREINFO_OFFSET(pglist_data, node_mem_map);\\n#endif\\n\\tVMCOREINFO_OFFSET(pglist_data, node_start_pfn);\\n\\tVMCOREINFO_OFFSET(pglist_data, node_spanned_pages);\\n\\tVMCOREINFO_OFFSET(pglist_data, node_id);\\n\\tVMCOREINFO_OFFSET(zone, free_area);\\n\\tVMCOREINFO_OFFSET(zone, vm_stat);\\n\\tVMCOREINFO_OFFSET(zone, spanned_pages);\\n\\tVMCOREINFO_OFFSET(free_area, free_list);\\n\\tVMCOREINFO_OFFSET(list_head, next);\\n\\tVMCOREINFO_OFFSET(list_head, prev);\\n\\tVMCOREINFO_LENGTH(zone.free_area, NR_PAGE_ORDERS);\\n\\tlog_buf_vmcoreinfo_setup();\\n\\tVMCOREINFO_LENGTH(free_area.free_list, MIGRATE_TYPES);\\n\\tVMCOREINFO_NUMBER(NR_FREE_PAGES);\\n\\tVMCOREINFO_NUMBER(PG_lru);\\n\\tVMCOREINFO_NUMBER(PG_private);\\n\\tVMCOREINFO_NUMBER(PG_swapcache);\\n\\tVMCOREINFO_NUMBER(PG_swapbacked);\\n#define PAGE_SLAB_MAPCOUNT_VALUE\\t(PGTY_slab << 24)\\n\\tVMCOREINFO_NUMBER(PAGE_SLAB_MAPCOUNT_VALUE);\\n#ifdef CONFIG_MEMORY_FAILURE\\n\\tVMCOREINFO_NUMBER(PG_hwpoison);\\n#endif\\n\\tVMCOREINFO_NUMBER(PG_head_mask);\\n#define PAGE_BUDDY_MAPCOUNT_VALUE\\t(PGTY_buddy << 24)\\n\\tVMCOREINFO_NUMBER(PAGE_BUDDY_MAPCOUNT_VALUE);\\n#define PAGE_HUGETLB_MAPCOUNT_VALUE\\t(PGTY_hugetlb << 24)\\n\\tVMCOREINFO_NUMBER(PAGE_HUGETLB_MAPCOUNT_VALUE);\\n#define PAGE_OFFLINE_MAPCOUNT_VALUE\\t(PGTY_offline << 24)\\n\\tVMCOREINFO_NUMBER(PAGE_OFFLINE_MAPCOUNT_VALUE);\\n\\n#ifdef CONFIG_KALLSYMS\\n\\tVMCOREINFO_SYMBOL(kallsyms_names);\\n\\tVMCOREINFO_SYMBOL(kallsyms_num_syms);\\n\\tVMCOREINFO_SYMBOL(kallsyms_token_table);\\n\\tVMCOREINFO_SYMBOL(kallsyms_token_index);\\n\\tVMCOREINFO_SYMBOL(kallsyms_offsets);\\n\\tVMCOREINFO_SYMBOL(kallsyms_relative_base);\\n#endif /* CONFIG_KALLSYMS */\\n\\n\\tarch_crash_save_vmcoreinfo();\\n\\tupdate_vmcoreinfo_note();\\n\\n\\treturn 0;\\n}\\n\\nsubsys_initcall(crash_save_vmcoreinfo_init);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * umh - the kernel usermode helper\\n */\\n#include <linux/module.h>\\n#include <linux/sched.h>\\n#include <linux/sched/task.h>\\n#include <linux/binfmts.h>\\n#include <linux/syscalls.h>\\n#include <linux/unistd.h>\\n#include <linux/kmod.h>\\n#include <linux/slab.h>\\n#include <linux/completion.h>\\n#include <linux/cred.h>\\n#include <linux/file.h>\\n#include <linux/fs_struct.h>\\n#include <linux/workqueue.h>\\n#include <linux/security.h>\\n#include <linux/mount.h>\\n#include <linux/kernel.h>\\n#include <linux/init.h>\\n#include <linux/resource.h>\\n#include <linux/notifier.h>\\n#include <linux/suspend.h>\\n#include <linux/rwsem.h>\\n#include <linux/ptrace.h>\\n#include <linux/async.h>\\n#include <linux/uaccess.h>\\n#include <linux/initrd.h>\\n#include <linux/freezer.h>\\n\\n#include <trace/events/module.h>\\n\\nstatic kernel_cap_t usermodehelper_bset = CAP_FULL_SET;\\nstatic kernel_cap_t usermodehelper_inheritable = CAP_FULL_SET;\\nstatic DEFINE_SPINLOCK(umh_sysctl_lock);\\nstatic DECLARE_RWSEM(umhelper_sem);\\n\\nstatic void call_usermodehelper_freeinfo(struct subprocess_info *info)\\n{\\n\\tif (info->cleanup)\\n\\t\\t(*info->cleanup)(info);\\n\\tkfree(info);\\n}\\n\\nstatic void umh_complete(struct subprocess_info *sub_info)\\n{\\n\\tstruct completion *comp = xchg(&sub_info->complete, NULL);\\n\\t/*\\n\\t * See call_usermodehelper_exec(). If xchg() returns NULL\\n\\t * we own sub_info, the UMH_KILLABLE caller has gone away\\n\\t * or the caller used UMH_NO_WAIT.\\n\\t */\\n\\tif (comp)\\n\\t\\tcomplete(comp);\\n\\telse\\n\\t\\tcall_usermodehelper_freeinfo(sub_info);\\n}\\n\\n/*\\n * This is the task which runs the usermode application\\n */\\nstatic int call_usermodehelper_exec_async(void *data)\\n{\\n\\tstruct subprocess_info *sub_info = data;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\n\\tspin_lock_irq(&current->sighand->siglock);\\n\\tflush_signal_handlers(current, 1);\\n\\tspin_unlock_irq(&current->sighand->siglock);\\n\\n\\t/*\\n\\t * Initial kernel threads share ther FS with init, in order to\\n\\t * get the init root directory. But we\\'ve now created a new\\n\\t * thread that is going to execve a user process and has its own\\n\\t * \\'struct fs_struct\\'. Reset umask to the default.\\n\\t */\\n\\tcurrent->fs->umask = 0022;\\n\\n\\t/*\\n\\t * Our parent (unbound workqueue) runs with elevated scheduling\\n\\t * priority. Avoid propagating that into the userspace child.\\n\\t */\\n\\tset_user_nice(current, 0);\\n\\n\\tretval = -ENOMEM;\\n\\tnew = prepare_kernel_cred(current);\\n\\tif (!new)\\n\\t\\tgoto out;\\n\\n\\tspin_lock(&umh_sysctl_lock);\\n\\tnew->cap_bset = cap_intersect(usermodehelper_bset, new->cap_bset);\\n\\tnew->cap_inheritable = cap_intersect(usermodehelper_inheritable,\\n\\t\\t\\t\\t\\t     new->cap_inheritable);\\n\\tspin_unlock(&umh_sysctl_lock);\\n\\n\\tif (sub_info->init) {\\n\\t\\tretval = sub_info->init(sub_info, new);\\n\\t\\tif (retval) {\\n\\t\\t\\tabort_creds(new);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\n\\tcommit_creds(new);\\n\\n\\twait_for_initramfs();\\n\\tretval = kernel_execve(sub_info->path,\\n\\t\\t\\t       (const char *const *)sub_info->argv,\\n\\t\\t\\t       (const char *const *)sub_info->envp);\\nout:\\n\\tsub_info->retval = retval;\\n\\t/*\\n\\t * call_usermodehelper_exec_sync() will call umh_complete\\n\\t * if UHM_WAIT_PROC.\\n\\t */\\n\\tif (!(sub_info->wait & UMH_WAIT_PROC))\\n\\t\\tumh_complete(sub_info);\\n\\tif (!retval)\\n\\t\\treturn 0;\\n\\tdo_exit(0);\\n}\\n\\n/* Handles UMH_WAIT_PROC.  */\\nstatic void call_usermodehelper_exec_sync(struct subprocess_info *sub_info)\\n{\\n\\tpid_t pid;\\n\\n\\t/* If SIGCLD is ignored do_wait won\\'t populate the status. */\\n\\tkernel_sigaction(SIGCHLD, SIG_DFL);\\n\\tpid = user_mode_thread(call_usermodehelper_exec_async, sub_info, SIGCHLD);\\n\\tif (pid < 0)\\n\\t\\tsub_info->retval = pid;\\n\\telse\\n\\t\\tkernel_wait(pid, &sub_info->retval);\\n\\n\\t/* Restore default kernel sig handler */\\n\\tkernel_sigaction(SIGCHLD, SIG_IGN);\\n\\tumh_complete(sub_info);\\n}\\n\\n/*\\n * We need to create the usermodehelper kernel thread from a task that is affine\\n * to an optimized set of CPUs (or nohz housekeeping ones) such that they\\n * inherit a widest affinity irrespective of call_usermodehelper() callers with\\n * possibly reduced affinity (eg: per-cpu workqueues). We don\\'t want\\n * usermodehelper targets to contend a busy CPU.\\n *\\n * Unbound workqueues provide such wide affinity and allow to block on\\n * UMH_WAIT_PROC requests without blocking pending request (up to some limit).\\n *\\n * Besides, workqueues provide the privilege level that caller might not have\\n * to perform the usermodehelper request.\\n *\\n */\\nstatic void call_usermodehelper_exec_work(struct work_struct *work)\\n{\\n\\tstruct subprocess_info *sub_info =\\n\\t\\tcontainer_of(work, struct subprocess_info, work);\\n\\n\\tif (sub_info->wait & UMH_WAIT_PROC) {\\n\\t\\tcall_usermodehelper_exec_sync(sub_info);\\n\\t} else {\\n\\t\\tpid_t pid;\\n\\t\\t/*\\n\\t\\t * Use CLONE_PARENT to reparent it to kthreadd; we do not\\n\\t\\t * want to pollute current->children, and we need a parent\\n\\t\\t * that always ignores SIGCHLD to ensure auto-reaping.\\n\\t\\t */\\n\\t\\tpid = user_mode_thread(call_usermodehelper_exec_async, sub_info,\\n\\t\\t\\t\\t       CLONE_PARENT | SIGCHLD);\\n\\t\\tif (pid < 0) {\\n\\t\\t\\tsub_info->retval = pid;\\n\\t\\t\\tumh_complete(sub_info);\\n\\t\\t}\\n\\t}\\n}\\n\\n/*\\n * If set, call_usermodehelper_exec() will exit immediately returning -EBUSY\\n * (used for preventing user land processes from being created after the user\\n * land has been frozen during a system-wide hibernation or suspend operation).\\n * Should always be manipulated under umhelper_sem acquired for write.\\n */\\nstatic enum umh_disable_depth usermodehelper_disabled = UMH_DISABLED;\\n\\n/* Number of helpers running */\\nstatic atomic_t running_helpers = ATOMIC_INIT(0);\\n\\n/*\\n * Wait queue head used by usermodehelper_disable() to wait for all running\\n * helpers to finish.\\n */\\nstatic DECLARE_WAIT_QUEUE_HEAD(running_helpers_waitq);\\n\\n/*\\n * Used by usermodehelper_read_lock_wait() to wait for usermodehelper_disabled\\n * to become \\'false\\'.\\n */\\nstatic DECLARE_WAIT_QUEUE_HEAD(usermodehelper_disabled_waitq);\\n\\n/*\\n * Time to wait for running_helpers to become zero before the setting of\\n * usermodehelper_disabled in usermodehelper_disable() fails\\n */\\n#define RUNNING_HELPERS_TIMEOUT\\t(5 * HZ)\\n\\nint usermodehelper_read_trylock(void)\\n{\\n\\tDEFINE_WAIT(wait);\\n\\tint ret = 0;\\n\\n\\tdown_read(&umhelper_sem);\\n\\tfor (;;) {\\n\\t\\tprepare_to_wait(&usermodehelper_disabled_waitq, &wait,\\n\\t\\t\\t\\tTASK_INTERRUPTIBLE);\\n\\t\\tif (!usermodehelper_disabled)\\n\\t\\t\\tbreak;\\n\\n\\t\\tif (usermodehelper_disabled == UMH_DISABLED)\\n\\t\\t\\tret = -EAGAIN;\\n\\n\\t\\tup_read(&umhelper_sem);\\n\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\n\\t\\tschedule();\\n\\t\\ttry_to_freeze();\\n\\n\\t\\tdown_read(&umhelper_sem);\\n\\t}\\n\\tfinish_wait(&usermodehelper_disabled_waitq, &wait);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(usermodehelper_read_trylock);\\n\\nlong usermodehelper_read_lock_wait(long timeout)\\n{\\n\\tDEFINE_WAIT(wait);\\n\\n\\tif (timeout < 0)\\n\\t\\treturn -EINVAL;\\n\\n\\tdown_read(&umhelper_sem);\\n\\tfor (;;) {\\n\\t\\tprepare_to_wait(&usermodehelper_disabled_waitq, &wait,\\n\\t\\t\\t\\tTASK_UNINTERRUPTIBLE);\\n\\t\\tif (!usermodehelper_disabled)\\n\\t\\t\\tbreak;\\n\\n\\t\\tup_read(&umhelper_sem);\\n\\n\\t\\ttimeout = schedule_timeout(timeout);\\n\\t\\tif (!timeout)\\n\\t\\t\\tbreak;\\n\\n\\t\\tdown_read(&umhelper_sem);\\n\\t}\\n\\tfinish_wait(&usermodehelper_disabled_waitq, &wait);\\n\\treturn timeout;\\n}\\nEXPORT_SYMBOL_GPL(usermodehelper_read_lock_wait);\\n\\nvoid usermodehelper_read_unlock(void)\\n{\\n\\tup_read(&umhelper_sem);\\n}\\nEXPORT_SYMBOL_GPL(usermodehelper_read_unlock);\\n\\n/**\\n * __usermodehelper_set_disable_depth - Modify usermodehelper_disabled.\\n * @depth: New value to assign to usermodehelper_disabled.\\n *\\n * Change the value of usermodehelper_disabled (under umhelper_sem locked for\\n * writing) and wakeup tasks waiting for it to change.\\n */\\nvoid __usermodehelper_set_disable_depth(enum umh_disable_depth depth)\\n{\\n\\tdown_write(&umhelper_sem);\\n\\tusermodehelper_disabled = depth;\\n\\twake_up(&usermodehelper_disabled_waitq);\\n\\tup_write(&umhelper_sem);\\n}\\n\\n/**\\n * __usermodehelper_disable - Prevent new helpers from being started.\\n * @depth: New value to assign to usermodehelper_disabled.\\n *\\n * Set usermodehelper_disabled to @depth and wait for running helpers to exit.\\n */\\nint __usermodehelper_disable(enum umh_disable_depth depth)\\n{\\n\\tlong retval;\\n\\n\\tif (!depth)\\n\\t\\treturn -EINVAL;\\n\\n\\tdown_write(&umhelper_sem);\\n\\tusermodehelper_disabled = depth;\\n\\tup_write(&umhelper_sem);\\n\\n\\t/*\\n\\t * From now on call_usermodehelper_exec() won\\'t start any new\\n\\t * helpers, so it is sufficient if running_helpers turns out to\\n\\t * be zero at one point (it may be increased later, but that\\n\\t * doesn\\'t matter).\\n\\t */\\n\\tretval = wait_event_timeout(running_helpers_waitq,\\n\\t\\t\\t\\t\\tatomic_read(&running_helpers) == 0,\\n\\t\\t\\t\\t\\tRUNNING_HELPERS_TIMEOUT);\\n\\tif (retval)\\n\\t\\treturn 0;\\n\\n\\t__usermodehelper_set_disable_depth(UMH_ENABLED);\\n\\treturn -EAGAIN;\\n}\\n\\nstatic void helper_lock(void)\\n{\\n\\tatomic_inc(&running_helpers);\\n\\tsmp_mb__after_atomic();\\n}\\n\\nstatic void helper_unlock(void)\\n{\\n\\tif (atomic_dec_and_test(&running_helpers))\\n\\t\\twake_up(&running_helpers_waitq);\\n}\\n\\n/**\\n * call_usermodehelper_setup - prepare to call a usermode helper\\n * @path: path to usermode executable\\n * @argv: arg vector for process\\n * @envp: environment for process\\n * @gfp_mask: gfp mask for memory allocation\\n * @init: an init function\\n * @cleanup: a cleanup function\\n * @data: arbitrary context sensitive data\\n *\\n * Returns either %NULL on allocation failure, or a subprocess_info\\n * structure.  This should be passed to call_usermodehelper_exec to\\n * exec the process and free the structure.\\n *\\n * The init function is used to customize the helper process prior to\\n * exec.  A non-zero return code causes the process to error out, exit,\\n * and return the failure to the calling process\\n *\\n * The cleanup function is just before the subprocess_info is about to\\n * be freed.  This can be used for freeing the argv and envp.  The\\n * Function must be runnable in either a process context or the\\n * context in which call_usermodehelper_exec is called.\\n */\\nstruct subprocess_info *call_usermodehelper_setup(const char *path, char **argv,\\n\\t\\tchar **envp, gfp_t gfp_mask,\\n\\t\\tint (*init)(struct subprocess_info *info, struct cred *new),\\n\\t\\tvoid (*cleanup)(struct subprocess_info *info),\\n\\t\\tvoid *data)\\n{\\n\\tstruct subprocess_info *sub_info;\\n\\tsub_info = kzalloc(sizeof(struct subprocess_info), gfp_mask);\\n\\tif (!sub_info)\\n\\t\\tgoto out;\\n\\n\\tINIT_WORK(&sub_info->work, call_usermodehelper_exec_work);\\n\\n#ifdef CONFIG_STATIC_USERMODEHELPER\\n\\tsub_info->path = CONFIG_STATIC_USERMODEHELPER_PATH;\\n#else\\n\\tsub_info->path = path;\\n#endif\\n\\tsub_info->argv = argv;\\n\\tsub_info->envp = envp;\\n\\n\\tsub_info->cleanup = cleanup;\\n\\tsub_info->init = init;\\n\\tsub_info->data = data;\\n  out:\\n\\treturn sub_info;\\n}\\nEXPORT_SYMBOL(call_usermodehelper_setup);\\n\\n/**\\n * call_usermodehelper_exec - start a usermode application\\n * @sub_info: information about the subprocess\\n * @wait: wait for the application to finish and return status.\\n *        when UMH_NO_WAIT don\\'t wait at all, but you get no useful error back\\n *        when the program couldn\\'t be exec\\'ed. This makes it safe to call\\n *        from interrupt context.\\n *\\n * Runs a user-space application.  The application is started\\n * asynchronously if wait is not set, and runs as a child of system workqueues.\\n * (ie. it runs with full root capabilities and optimized affinity).\\n *\\n * Note: successful return value does not guarantee the helper was called at\\n * all. You can\\'t rely on sub_info->{init,cleanup} being called even for\\n * UMH_WAIT_* wait modes as STATIC_USERMODEHELPER_PATH=\"\" turns all helpers\\n * into a successful no-op.\\n */\\nint call_usermodehelper_exec(struct subprocess_info *sub_info, int wait)\\n{\\n\\tunsigned int state = TASK_UNINTERRUPTIBLE;\\n\\tDECLARE_COMPLETION_ONSTACK(done);\\n\\tint retval = 0;\\n\\n\\tif (!sub_info->path) {\\n\\t\\tcall_usermodehelper_freeinfo(sub_info);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\thelper_lock();\\n\\tif (usermodehelper_disabled) {\\n\\t\\tretval = -EBUSY;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * If there is no binary for us to call, then just return and get out of\\n\\t * here.  This allows us to set STATIC_USERMODEHELPER_PATH to \"\" and\\n\\t * disable all call_usermodehelper() calls.\\n\\t */\\n\\tif (strlen(sub_info->path) == 0)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Set the completion pointer only if there is a waiter.\\n\\t * This makes it possible to use umh_complete to free\\n\\t * the data structure in case of UMH_NO_WAIT.\\n\\t */\\n\\tsub_info->complete = (wait == UMH_NO_WAIT) ? NULL : &done;\\n\\tsub_info->wait = wait;\\n\\n\\tqueue_work(system_unbound_wq, &sub_info->work);\\n\\tif (wait == UMH_NO_WAIT)\\t/* task has freed sub_info */\\n\\t\\tgoto unlock;\\n\\n\\tif (wait & UMH_FREEZABLE)\\n\\t\\tstate |= TASK_FREEZABLE;\\n\\n\\tif (wait & UMH_KILLABLE) {\\n\\t\\tretval = wait_for_completion_state(&done, state | TASK_KILLABLE);\\n\\t\\tif (!retval)\\n\\t\\t\\tgoto wait_done;\\n\\n\\t\\t/* umh_complete() will see NULL and free sub_info */\\n\\t\\tif (xchg(&sub_info->complete, NULL))\\n\\t\\t\\tgoto unlock;\\n\\n\\t\\t/*\\n\\t\\t * fallthrough; in case of -ERESTARTSYS now do uninterruptible\\n\\t\\t * wait_for_completion_state(). Since umh_complete() shall call\\n\\t\\t * complete() in a moment if xchg() above returned NULL, this\\n\\t\\t * uninterruptible wait_for_completion_state() will not block\\n\\t\\t * SIGKILL\\'ed processes for long.\\n\\t\\t */\\n\\t}\\n\\twait_for_completion_state(&done, state);\\n\\nwait_done:\\n\\tretval = sub_info->retval;\\nout:\\n\\tcall_usermodehelper_freeinfo(sub_info);\\nunlock:\\n\\thelper_unlock();\\n\\treturn retval;\\n}\\nEXPORT_SYMBOL(call_usermodehelper_exec);\\n\\n/**\\n * call_usermodehelper() - prepare and start a usermode application\\n * @path: path to usermode executable\\n * @argv: arg vector for process\\n * @envp: environment for process\\n * @wait: wait for the application to finish and return status.\\n *        when UMH_NO_WAIT don\\'t wait at all, but you get no useful error back\\n *        when the program couldn\\'t be exec\\'ed. This makes it safe to call\\n *        from interrupt context.\\n *\\n * This function is the equivalent to use call_usermodehelper_setup() and\\n * call_usermodehelper_exec().\\n */\\nint call_usermodehelper(const char *path, char **argv, char **envp, int wait)\\n{\\n\\tstruct subprocess_info *info;\\n\\tgfp_t gfp_mask = (wait == UMH_NO_WAIT) ? GFP_ATOMIC : GFP_KERNEL;\\n\\n\\tinfo = call_usermodehelper_setup(path, argv, envp, gfp_mask,\\n\\t\\t\\t\\t\\t NULL, NULL, NULL);\\n\\tif (info == NULL)\\n\\t\\treturn -ENOMEM;\\n\\n\\treturn call_usermodehelper_exec(info, wait);\\n}\\nEXPORT_SYMBOL(call_usermodehelper);\\n\\n#if defined(CONFIG_SYSCTL)\\nstatic int proc_cap_handler(const struct ctl_table *table, int write,\\n\\t\\t\\t void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct ctl_table t;\\n\\tunsigned long cap_array[2];\\n\\tkernel_cap_t new_cap, *cap;\\n\\tint err;\\n\\n\\tif (write && (!capable(CAP_SETPCAP) ||\\n\\t\\t      !capable(CAP_SYS_MODULE)))\\n\\t\\treturn -EPERM;\\n\\n\\t/*\\n\\t * convert from the global kernel_cap_t to the ulong array to print to\\n\\t * userspace if this is a read.\\n\\t *\\n\\t * Legacy format: capabilities are exposed as two 32-bit values\\n\\t */\\n\\tcap = table->data;\\n\\tspin_lock(&umh_sysctl_lock);\\n\\tcap_array[0] = (u32) cap->val;\\n\\tcap_array[1] = cap->val >> 32;\\n\\tspin_unlock(&umh_sysctl_lock);\\n\\n\\tt = *table;\\n\\tt.data = &cap_array;\\n\\n\\t/*\\n\\t * actually read or write and array of ulongs from userspace.  Remember\\n\\t * these are least significant 32 bits first\\n\\t */\\n\\terr = proc_doulongvec_minmax(&t, write, buffer, lenp, ppos);\\n\\tif (err < 0)\\n\\t\\treturn err;\\n\\n\\tnew_cap.val = (u32)cap_array[0];\\n\\tnew_cap.val += (u64)cap_array[1] << 32;\\n\\n\\t/*\\n\\t * Drop everything not in the new_cap (but don\\'t add things)\\n\\t */\\n\\tif (write) {\\n\\t\\tspin_lock(&umh_sysctl_lock);\\n\\t\\t*cap = cap_intersect(*cap, new_cap);\\n\\t\\tspin_unlock(&umh_sysctl_lock);\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic struct ctl_table usermodehelper_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"bset\",\\n\\t\\t.data\\t\\t= &usermodehelper_bset,\\n\\t\\t.maxlen\\t\\t= 2 * sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_cap_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"inheritable\",\\n\\t\\t.data\\t\\t= &usermodehelper_inheritable,\\n\\t\\t.maxlen\\t\\t= 2 * sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_cap_handler,\\n\\t},\\n};\\n\\nstatic int __init init_umh_sysctls(void)\\n{\\n\\tregister_sysctl_init(\"kernel/usermodehelper\", usermodehelper_table);\\n\\treturn 0;\\n}\\nearly_initcall(init_umh_sysctls);\\n#endif /* CONFIG_SYSCTL */\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * kernel/workqueue.c - generic async execution with shared worker pool\\n *\\n * Copyright (C) 2002\\t\\tIngo Molnar\\n *\\n *   Derived from the taskqueue/keventd code by:\\n *     David Woodhouse <dwmw2@infradead.org>\\n *     Andrew Morton\\n *     Kai Petzke <wpp@marie.physik.tu-berlin.de>\\n *     Theodore Ts\\'o <tytso@mit.edu>\\n *\\n * Made to use alloc_percpu by Christoph Lameter.\\n *\\n * Copyright (C) 2010\\t\\tSUSE Linux Products GmbH\\n * Copyright (C) 2010\\t\\tTejun Heo <tj@kernel.org>\\n *\\n * This is the generic async execution mechanism.  Work items as are\\n * executed in process context.  The worker pool is shared and\\n * automatically managed.  There are two worker pools for each CPU (one for\\n * normal work items and the other for high priority ones) and some extra\\n * pools for workqueues which are not bound to any specific CPU - the\\n * number of these backing pools is dynamic.\\n *\\n * Please read Documentation/core-api/workqueue.rst for details.\\n */\\n\\n#include <linux/export.h>\\n#include <linux/kernel.h>\\n#include <linux/sched.h>\\n#include <linux/init.h>\\n#include <linux/interrupt.h>\\n#include <linux/signal.h>\\n#include <linux/completion.h>\\n#include <linux/workqueue.h>\\n#include <linux/slab.h>\\n#include <linux/cpu.h>\\n#include <linux/notifier.h>\\n#include <linux/kthread.h>\\n#include <linux/hardirq.h>\\n#include <linux/mempolicy.h>\\n#include <linux/freezer.h>\\n#include <linux/debug_locks.h>\\n#include <linux/lockdep.h>\\n#include <linux/idr.h>\\n#include <linux/jhash.h>\\n#include <linux/hashtable.h>\\n#include <linux/rculist.h>\\n#include <linux/nodemask.h>\\n#include <linux/moduleparam.h>\\n#include <linux/uaccess.h>\\n#include <linux/sched/isolation.h>\\n#include <linux/sched/debug.h>\\n#include <linux/nmi.h>\\n#include <linux/kvm_para.h>\\n#include <linux/delay.h>\\n#include <linux/irq_work.h>\\n\\n#include \"workqueue_internal.h\"\\n\\nenum worker_pool_flags {\\n\\t/*\\n\\t * worker_pool flags\\n\\t *\\n\\t * A bound pool is either associated or disassociated with its CPU.\\n\\t * While associated (!DISASSOCIATED), all workers are bound to the\\n\\t * CPU and none has %WORKER_UNBOUND set and concurrency management\\n\\t * is in effect.\\n\\t *\\n\\t * While DISASSOCIATED, the cpu may be offline and all workers have\\n\\t * %WORKER_UNBOUND set and concurrency management disabled, and may\\n\\t * be executing on any CPU.  The pool behaves as an unbound one.\\n\\t *\\n\\t * Note that DISASSOCIATED should be flipped only while holding\\n\\t * wq_pool_attach_mutex to avoid changing binding state while\\n\\t * worker_attach_to_pool() is in progress.\\n\\t *\\n\\t * As there can only be one concurrent BH execution context per CPU, a\\n\\t * BH pool is per-CPU and always DISASSOCIATED.\\n\\t */\\n\\tPOOL_BH\\t\\t\\t= 1 << 0,\\t/* is a BH pool */\\n\\tPOOL_MANAGER_ACTIVE\\t= 1 << 1,\\t/* being managed */\\n\\tPOOL_DISASSOCIATED\\t= 1 << 2,\\t/* cpu can\\'t serve workers */\\n\\tPOOL_BH_DRAINING\\t= 1 << 3,\\t/* draining after CPU offline */\\n};\\n\\nenum worker_flags {\\n\\t/* worker flags */\\n\\tWORKER_DIE\\t\\t= 1 << 1,\\t/* die die die */\\n\\tWORKER_IDLE\\t\\t= 1 << 2,\\t/* is idle */\\n\\tWORKER_PREP\\t\\t= 1 << 3,\\t/* preparing to run works */\\n\\tWORKER_CPU_INTENSIVE\\t= 1 << 6,\\t/* cpu intensive */\\n\\tWORKER_UNBOUND\\t\\t= 1 << 7,\\t/* worker is unbound */\\n\\tWORKER_REBOUND\\t\\t= 1 << 8,\\t/* worker was rebound */\\n\\n\\tWORKER_NOT_RUNNING\\t= WORKER_PREP | WORKER_CPU_INTENSIVE |\\n\\t\\t\\t\\t  WORKER_UNBOUND | WORKER_REBOUND,\\n};\\n\\nenum work_cancel_flags {\\n\\tWORK_CANCEL_DELAYED\\t= 1 << 0,\\t/* canceling a delayed_work */\\n\\tWORK_CANCEL_DISABLE\\t= 1 << 1,\\t/* canceling to disable */\\n};\\n\\nenum wq_internal_consts {\\n\\tNR_STD_WORKER_POOLS\\t= 2,\\t\\t/* # standard pools per cpu */\\n\\n\\tUNBOUND_POOL_HASH_ORDER\\t= 6,\\t\\t/* hashed by pool->attrs */\\n\\tBUSY_WORKER_HASH_ORDER\\t= 6,\\t\\t/* 64 pointers */\\n\\n\\tMAX_IDLE_WORKERS_RATIO\\t= 4,\\t\\t/* 1/4 of busy can be idle */\\n\\tIDLE_WORKER_TIMEOUT\\t= 300 * HZ,\\t/* keep idle ones for 5 mins */\\n\\n\\tMAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,\\n\\t\\t\\t\\t\\t\\t/* call for help after 10ms\\n\\t\\t\\t\\t\\t\\t   (min two ticks) */\\n\\tMAYDAY_INTERVAL\\t\\t= HZ / 10,\\t/* and then every 100ms */\\n\\tCREATE_COOLDOWN\\t\\t= HZ,\\t\\t/* time to breath after fail */\\n\\n\\t/*\\n\\t * Rescue workers are used only on emergencies and shared by\\n\\t * all cpus.  Give MIN_NICE.\\n\\t */\\n\\tRESCUER_NICE_LEVEL\\t= MIN_NICE,\\n\\tHIGHPRI_NICE_LEVEL\\t= MIN_NICE,\\n\\n\\tWQ_NAME_LEN\\t\\t= 32,\\n\\tWORKER_ID_LEN\\t\\t= 10 + WQ_NAME_LEN, /* \"kworker/R-\" + WQ_NAME_LEN */\\n};\\n\\n/*\\n * We don\\'t want to trap softirq for too long. See MAX_SOFTIRQ_TIME and\\n * MAX_SOFTIRQ_RESTART in kernel/softirq.c. These are macros because\\n * msecs_to_jiffies() can\\'t be an initializer.\\n */\\n#define BH_WORKER_JIFFIES\\tmsecs_to_jiffies(2)\\n#define BH_WORKER_RESTARTS\\t10\\n\\n/*\\n * Structure fields follow one of the following exclusion rules.\\n *\\n * I: Modifiable by initialization/destruction paths and read-only for\\n *    everyone else.\\n *\\n * P: Preemption protected.  Disabling preemption is enough and should\\n *    only be modified and accessed from the local cpu.\\n *\\n * L: pool->lock protected.  Access with pool->lock held.\\n *\\n * LN: pool->lock and wq_node_nr_active->lock protected for writes. Either for\\n *     reads.\\n *\\n * K: Only modified by worker while holding pool->lock. Can be safely read by\\n *    self, while holding pool->lock or from IRQ context if %current is the\\n *    kworker.\\n *\\n * S: Only modified by worker self.\\n *\\n * A: wq_pool_attach_mutex protected.\\n *\\n * PL: wq_pool_mutex protected.\\n *\\n * PR: wq_pool_mutex protected for writes.  RCU protected for reads.\\n *\\n * PW: wq_pool_mutex and wq->mutex protected for writes.  Either for reads.\\n *\\n * PWR: wq_pool_mutex and wq->mutex protected for writes.  Either or\\n *      RCU for reads.\\n *\\n * WQ: wq->mutex protected.\\n *\\n * WR: wq->mutex protected for writes.  RCU protected for reads.\\n *\\n * WO: wq->mutex protected for writes. Updated with WRITE_ONCE() and can be read\\n *     with READ_ONCE() without locking.\\n *\\n * MD: wq_mayday_lock protected.\\n *\\n * WD: Used internally by the watchdog.\\n */\\n\\n/* struct worker is defined in workqueue_internal.h */\\n\\nstruct worker_pool {\\n\\traw_spinlock_t\\t\\tlock;\\t\\t/* the pool lock */\\n\\tint\\t\\t\\tcpu;\\t\\t/* I: the associated cpu */\\n\\tint\\t\\t\\tnode;\\t\\t/* I: the associated node ID */\\n\\tint\\t\\t\\tid;\\t\\t/* I: pool ID */\\n\\tunsigned int\\t\\tflags;\\t\\t/* L: flags */\\n\\n\\tunsigned long\\t\\twatchdog_ts;\\t/* L: watchdog timestamp */\\n\\tbool\\t\\t\\tcpu_stall;\\t/* WD: stalled cpu bound pool */\\n\\n\\t/*\\n\\t * The counter is incremented in a process context on the associated CPU\\n\\t * w/ preemption disabled, and decremented or reset in the same context\\n\\t * but w/ pool->lock held. The readers grab pool->lock and are\\n\\t * guaranteed to see if the counter reached zero.\\n\\t */\\n\\tint\\t\\t\\tnr_running;\\n\\n\\tstruct list_head\\tworklist;\\t/* L: list of pending works */\\n\\n\\tint\\t\\t\\tnr_workers;\\t/* L: total number of workers */\\n\\tint\\t\\t\\tnr_idle;\\t/* L: currently idle workers */\\n\\n\\tstruct list_head\\tidle_list;\\t/* L: list of idle workers */\\n\\tstruct timer_list\\tidle_timer;\\t/* L: worker idle timeout */\\n\\tstruct work_struct      idle_cull_work; /* L: worker idle cleanup */\\n\\n\\tstruct timer_list\\tmayday_timer;\\t  /* L: SOS timer for workers */\\n\\n\\t/* a workers is either on busy_hash or idle_list, or the manager */\\n\\tDECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);\\n\\t\\t\\t\\t\\t\\t/* L: hash of busy workers */\\n\\n\\tstruct worker\\t\\t*manager;\\t/* L: purely informational */\\n\\tstruct list_head\\tworkers;\\t/* A: attached workers */\\n\\n\\tstruct ida\\t\\tworker_ida;\\t/* worker IDs for task name */\\n\\n\\tstruct workqueue_attrs\\t*attrs;\\t\\t/* I: worker attributes */\\n\\tstruct hlist_node\\thash_node;\\t/* PL: unbound_pool_hash node */\\n\\tint\\t\\t\\trefcnt;\\t\\t/* PL: refcnt for unbound pools */\\n\\n\\t/*\\n\\t * Destruction of pool is RCU protected to allow dereferences\\n\\t * from get_work_pool().\\n\\t */\\n\\tstruct rcu_head\\t\\trcu;\\n};\\n\\n/*\\n * Per-pool_workqueue statistics. These can be monitored using\\n * tools/workqueue/wq_monitor.py.\\n */\\nenum pool_workqueue_stats {\\n\\tPWQ_STAT_STARTED,\\t/* work items started execution */\\n\\tPWQ_STAT_COMPLETED,\\t/* work items completed execution */\\n\\tPWQ_STAT_CPU_TIME,\\t/* total CPU time consumed */\\n\\tPWQ_STAT_CPU_INTENSIVE,\\t/* wq_cpu_intensive_thresh_us violations */\\n\\tPWQ_STAT_CM_WAKEUP,\\t/* concurrency-management worker wakeups */\\n\\tPWQ_STAT_REPATRIATED,\\t/* unbound workers brought back into scope */\\n\\tPWQ_STAT_MAYDAY,\\t/* maydays to rescuer */\\n\\tPWQ_STAT_RESCUED,\\t/* linked work items executed by rescuer */\\n\\n\\tPWQ_NR_STATS,\\n};\\n\\n/*\\n * The per-pool workqueue.  While queued, bits below WORK_PWQ_SHIFT\\n * of work_struct->data are used for flags and the remaining high bits\\n * point to the pwq; thus, pwqs need to be aligned at two\\'s power of the\\n * number of flag bits.\\n */\\nstruct pool_workqueue {\\n\\tstruct worker_pool\\t*pool;\\t\\t/* I: the associated pool */\\n\\tstruct workqueue_struct *wq;\\t\\t/* I: the owning workqueue */\\n\\tint\\t\\t\\twork_color;\\t/* L: current color */\\n\\tint\\t\\t\\tflush_color;\\t/* L: flushing color */\\n\\tint\\t\\t\\trefcnt;\\t\\t/* L: reference count */\\n\\tint\\t\\t\\tnr_in_flight[WORK_NR_COLORS];\\n\\t\\t\\t\\t\\t\\t/* L: nr of in_flight works */\\n\\tbool\\t\\t\\tplugged;\\t/* L: execution suspended */\\n\\n\\t/*\\n\\t * nr_active management and WORK_STRUCT_INACTIVE:\\n\\t *\\n\\t * When pwq->nr_active >= max_active, new work item is queued to\\n\\t * pwq->inactive_works instead of pool->worklist and marked with\\n\\t * WORK_STRUCT_INACTIVE.\\n\\t *\\n\\t * All work items marked with WORK_STRUCT_INACTIVE do not participate in\\n\\t * nr_active and all work items in pwq->inactive_works are marked with\\n\\t * WORK_STRUCT_INACTIVE. But not all WORK_STRUCT_INACTIVE work items are\\n\\t * in pwq->inactive_works. Some of them are ready to run in\\n\\t * pool->worklist or worker->scheduled. Those work itmes are only struct\\n\\t * wq_barrier which is used for flush_work() and should not participate\\n\\t * in nr_active. For non-barrier work item, it is marked with\\n\\t * WORK_STRUCT_INACTIVE iff it is in pwq->inactive_works.\\n\\t */\\n\\tint\\t\\t\\tnr_active;\\t/* L: nr of active works */\\n\\tstruct list_head\\tinactive_works;\\t/* L: inactive works */\\n\\tstruct list_head\\tpending_node;\\t/* LN: node on wq_node_nr_active->pending_pwqs */\\n\\tstruct list_head\\tpwqs_node;\\t/* WR: node on wq->pwqs */\\n\\tstruct list_head\\tmayday_node;\\t/* MD: node on wq->maydays */\\n\\n\\tu64\\t\\t\\tstats[PWQ_NR_STATS];\\n\\n\\t/*\\n\\t * Release of unbound pwq is punted to a kthread_worker. See put_pwq()\\n\\t * and pwq_release_workfn() for details. pool_workqueue itself is also\\n\\t * RCU protected so that the first pwq can be determined without\\n\\t * grabbing wq->mutex.\\n\\t */\\n\\tstruct kthread_work\\trelease_work;\\n\\tstruct rcu_head\\t\\trcu;\\n} __aligned(1 << WORK_STRUCT_PWQ_SHIFT);\\n\\n/*\\n * Structure used to wait for workqueue flush.\\n */\\nstruct wq_flusher {\\n\\tstruct list_head\\tlist;\\t\\t/* WQ: list of flushers */\\n\\tint\\t\\t\\tflush_color;\\t/* WQ: flush color waiting for */\\n\\tstruct completion\\tdone;\\t\\t/* flush completion */\\n};\\n\\nstruct wq_device;\\n\\n/*\\n * Unlike in a per-cpu workqueue where max_active limits its concurrency level\\n * on each CPU, in an unbound workqueue, max_active applies to the whole system.\\n * As sharing a single nr_active across multiple sockets can be very expensive,\\n * the counting and enforcement is per NUMA node.\\n *\\n * The following struct is used to enforce per-node max_active. When a pwq wants\\n * to start executing a work item, it should increment ->nr using\\n * tryinc_node_nr_active(). If acquisition fails due to ->nr already being over\\n * ->max, the pwq is queued on ->pending_pwqs. As in-flight work items finish\\n * and decrement ->nr, node_activate_pending_pwq() activates the pending pwqs in\\n * round-robin order.\\n */\\nstruct wq_node_nr_active {\\n\\tint\\t\\t\\tmax;\\t\\t/* per-node max_active */\\n\\tatomic_t\\t\\tnr;\\t\\t/* per-node nr_active */\\n\\traw_spinlock_t\\t\\tlock;\\t\\t/* nests inside pool locks */\\n\\tstruct list_head\\tpending_pwqs;\\t/* LN: pwqs with inactive works */\\n};\\n\\n/*\\n * The externally visible workqueue.  It relays the issued work items to\\n * the appropriate worker_pool through its pool_workqueues.\\n */\\nstruct workqueue_struct {\\n\\tstruct list_head\\tpwqs;\\t\\t/* WR: all pwqs of this wq */\\n\\tstruct list_head\\tlist;\\t\\t/* PR: list of all workqueues */\\n\\n\\tstruct mutex\\t\\tmutex;\\t\\t/* protects this wq */\\n\\tint\\t\\t\\twork_color;\\t/* WQ: current work color */\\n\\tint\\t\\t\\tflush_color;\\t/* WQ: current flush color */\\n\\tatomic_t\\t\\tnr_pwqs_to_flush; /* flush in progress */\\n\\tstruct wq_flusher\\t*first_flusher;\\t/* WQ: first flusher */\\n\\tstruct list_head\\tflusher_queue;\\t/* WQ: flush waiters */\\n\\tstruct list_head\\tflusher_overflow; /* WQ: flush overflow list */\\n\\n\\tstruct list_head\\tmaydays;\\t/* MD: pwqs requesting rescue */\\n\\tstruct worker\\t\\t*rescuer;\\t/* MD: rescue worker */\\n\\n\\tint\\t\\t\\tnr_drainers;\\t/* WQ: drain in progress */\\n\\n\\t/* See alloc_workqueue() function comment for info on min/max_active */\\n\\tint\\t\\t\\tmax_active;\\t/* WO: max active works */\\n\\tint\\t\\t\\tmin_active;\\t/* WO: min active works */\\n\\tint\\t\\t\\tsaved_max_active; /* WQ: saved max_active */\\n\\tint\\t\\t\\tsaved_min_active; /* WQ: saved min_active */\\n\\n\\tstruct workqueue_attrs\\t*unbound_attrs;\\t/* PW: only for unbound wqs */\\n\\tstruct pool_workqueue __rcu *dfl_pwq;   /* PW: only for unbound wqs */\\n\\n#ifdef CONFIG_SYSFS\\n\\tstruct wq_device\\t*wq_dev;\\t/* I: for sysfs interface */\\n#endif\\n#ifdef CONFIG_LOCKDEP\\n\\tchar\\t\\t\\t*lock_name;\\n\\tstruct lock_class_key\\tkey;\\n\\tstruct lockdep_map\\t__lockdep_map;\\n\\tstruct lockdep_map\\t*lockdep_map;\\n#endif\\n\\tchar\\t\\t\\tname[WQ_NAME_LEN]; /* I: workqueue name */\\n\\n\\t/*\\n\\t * Destruction of workqueue_struct is RCU protected to allow walking\\n\\t * the workqueues list without grabbing wq_pool_mutex.\\n\\t * This is used to dump all workqueues from sysrq.\\n\\t */\\n\\tstruct rcu_head\\t\\trcu;\\n\\n\\t/* hot fields used during command issue, aligned to cacheline */\\n\\tunsigned int\\t\\tflags ____cacheline_aligned; /* WQ: WQ_* flags */\\n\\tstruct pool_workqueue __rcu * __percpu *cpu_pwq; /* I: per-cpu pwqs */\\n\\tstruct wq_node_nr_active *node_nr_active[]; /* I: per-node nr_active */\\n};\\n\\n/*\\n * Each pod type describes how CPUs should be grouped for unbound workqueues.\\n * See the comment above workqueue_attrs->affn_scope.\\n */\\nstruct wq_pod_type {\\n\\tint\\t\\t\\tnr_pods;\\t/* number of pods */\\n\\tcpumask_var_t\\t\\t*pod_cpus;\\t/* pod -> cpus */\\n\\tint\\t\\t\\t*pod_node;\\t/* pod -> node */\\n\\tint\\t\\t\\t*cpu_pod;\\t/* cpu -> pod */\\n};\\n\\nstruct work_offq_data {\\n\\tu32\\t\\t\\tpool_id;\\n\\tu32\\t\\t\\tdisable;\\n\\tu32\\t\\t\\tflags;\\n};\\n\\nstatic const char *wq_affn_names[WQ_AFFN_NR_TYPES] = {\\n\\t[WQ_AFFN_DFL]\\t\\t= \"default\",\\n\\t[WQ_AFFN_CPU]\\t\\t= \"cpu\",\\n\\t[WQ_AFFN_SMT]\\t\\t= \"smt\",\\n\\t[WQ_AFFN_CACHE]\\t\\t= \"cache\",\\n\\t[WQ_AFFN_NUMA]\\t\\t= \"numa\",\\n\\t[WQ_AFFN_SYSTEM]\\t= \"system\",\\n};\\n\\n/*\\n * Per-cpu work items which run for longer than the following threshold are\\n * automatically considered CPU intensive and excluded from concurrency\\n * management to prevent them from noticeably delaying other per-cpu work items.\\n * ULONG_MAX indicates that the user hasn\\'t overridden it with a boot parameter.\\n * The actual value is initialized in wq_cpu_intensive_thresh_init().\\n */\\nstatic unsigned long wq_cpu_intensive_thresh_us = ULONG_MAX;\\nmodule_param_named(cpu_intensive_thresh_us, wq_cpu_intensive_thresh_us, ulong, 0644);\\n#ifdef CONFIG_WQ_CPU_INTENSIVE_REPORT\\nstatic unsigned int wq_cpu_intensive_warning_thresh = 4;\\nmodule_param_named(cpu_intensive_warning_thresh, wq_cpu_intensive_warning_thresh, uint, 0644);\\n#endif\\n\\n/* see the comment above the definition of WQ_POWER_EFFICIENT */\\nstatic bool wq_power_efficient = IS_ENABLED(CONFIG_WQ_POWER_EFFICIENT_DEFAULT);\\nmodule_param_named(power_efficient, wq_power_efficient, bool, 0444);\\n\\nstatic bool wq_online;\\t\\t\\t/* can kworkers be created yet? */\\nstatic bool wq_topo_initialized __read_mostly = false;\\n\\nstatic struct kmem_cache *pwq_cache;\\n\\nstatic struct wq_pod_type wq_pod_types[WQ_AFFN_NR_TYPES];\\nstatic enum wq_affn_scope wq_affn_dfl = WQ_AFFN_CACHE;\\n\\n/* buf for wq_update_unbound_pod_attrs(), protected by CPU hotplug exclusion */\\nstatic struct workqueue_attrs *unbound_wq_update_pwq_attrs_buf;\\n\\nstatic DEFINE_MUTEX(wq_pool_mutex);\\t/* protects pools and workqueues list */\\nstatic DEFINE_MUTEX(wq_pool_attach_mutex); /* protects worker attach/detach */\\nstatic DEFINE_RAW_SPINLOCK(wq_mayday_lock);\\t/* protects wq->maydays list */\\n/* wait for manager to go away */\\nstatic struct rcuwait manager_wait = __RCUWAIT_INITIALIZER(manager_wait);\\n\\nstatic LIST_HEAD(workqueues);\\t\\t/* PR: list of all workqueues */\\nstatic bool workqueue_freezing;\\t\\t/* PL: have wqs started freezing? */\\n\\n/* PL: mirror the cpu_online_mask excluding the CPU in the midst of hotplugging */\\nstatic cpumask_var_t wq_online_cpumask;\\n\\n/* PL&A: allowable cpus for unbound wqs and work items */\\nstatic cpumask_var_t wq_unbound_cpumask;\\n\\n/* PL: user requested unbound cpumask via sysfs */\\nstatic cpumask_var_t wq_requested_unbound_cpumask;\\n\\n/* PL: isolated cpumask to be excluded from unbound cpumask */\\nstatic cpumask_var_t wq_isolated_cpumask;\\n\\n/* for further constrain wq_unbound_cpumask by cmdline parameter*/\\nstatic struct cpumask wq_cmdline_cpumask __initdata;\\n\\n/* CPU where unbound work was last round robin scheduled from this CPU */\\nstatic DEFINE_PER_CPU(int, wq_rr_cpu_last);\\n\\n/*\\n * Local execution of unbound work items is no longer guaranteed.  The\\n * following always forces round-robin CPU selection on unbound work items\\n * to uncover usages which depend on it.\\n */\\n#ifdef CONFIG_DEBUG_WQ_FORCE_RR_CPU\\nstatic bool wq_debug_force_rr_cpu = true;\\n#else\\nstatic bool wq_debug_force_rr_cpu = false;\\n#endif\\nmodule_param_named(debug_force_rr_cpu, wq_debug_force_rr_cpu, bool, 0644);\\n\\n/* to raise softirq for the BH worker pools on other CPUs */\\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct irq_work [NR_STD_WORKER_POOLS], bh_pool_irq_works);\\n\\n/* the BH worker pools */\\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], bh_worker_pools);\\n\\n/* the per-cpu worker pools */\\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);\\n\\nstatic DEFINE_IDR(worker_pool_idr);\\t/* PR: idr of all pools */\\n\\n/* PL: hash of all unbound pools keyed by pool->attrs */\\nstatic DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);\\n\\n/* I: attributes used when instantiating standard unbound pools on demand */\\nstatic struct workqueue_attrs *unbound_std_wq_attrs[NR_STD_WORKER_POOLS];\\n\\n/* I: attributes used when instantiating ordered pools on demand */\\nstatic struct workqueue_attrs *ordered_wq_attrs[NR_STD_WORKER_POOLS];\\n\\n/*\\n * I: kthread_worker to release pwq\\'s. pwq release needs to be bounced to a\\n * process context while holding a pool lock. Bounce to a dedicated kthread\\n * worker to avoid A-A deadlocks.\\n */\\nstatic struct kthread_worker *pwq_release_worker __ro_after_init;\\n\\nstruct workqueue_struct *system_wq __ro_after_init;\\nEXPORT_SYMBOL(system_wq);\\nstruct workqueue_struct *system_highpri_wq __ro_after_init;\\nEXPORT_SYMBOL_GPL(system_highpri_wq);\\nstruct workqueue_struct *system_long_wq __ro_after_init;\\nEXPORT_SYMBOL_GPL(system_long_wq);\\nstruct workqueue_struct *system_unbound_wq __ro_after_init;\\nEXPORT_SYMBOL_GPL(system_unbound_wq);\\nstruct workqueue_struct *system_freezable_wq __ro_after_init;\\nEXPORT_SYMBOL_GPL(system_freezable_wq);\\nstruct workqueue_struct *system_power_efficient_wq __ro_after_init;\\nEXPORT_SYMBOL_GPL(system_power_efficient_wq);\\nstruct workqueue_struct *system_freezable_power_efficient_wq __ro_after_init;\\nEXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);\\nstruct workqueue_struct *system_bh_wq;\\nEXPORT_SYMBOL_GPL(system_bh_wq);\\nstruct workqueue_struct *system_bh_highpri_wq;\\nEXPORT_SYMBOL_GPL(system_bh_highpri_wq);\\n\\nstatic int worker_thread(void *__worker);\\nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq);\\nstatic void show_pwq(struct pool_workqueue *pwq);\\nstatic void show_one_worker_pool(struct worker_pool *pool);\\n\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/workqueue.h>\\n\\n#define assert_rcu_or_pool_mutex()\\t\\t\\t\\t\\t\\\\\\n\\tRCU_LOCKDEP_WARN(!rcu_read_lock_any_held() &&\\t\\t\\t\\\\\\n\\t\\t\\t !lockdep_is_held(&wq_pool_mutex),\\t\\t\\\\\\n\\t\\t\\t \"RCU or wq_pool_mutex should be held\")\\n\\n#define assert_rcu_or_wq_mutex_or_pool_mutex(wq)\\t\\t\\t\\\\\\n\\tRCU_LOCKDEP_WARN(!rcu_read_lock_any_held() &&\\t\\t\\t\\\\\\n\\t\\t\\t !lockdep_is_held(&wq->mutex) &&\\t\\t\\\\\\n\\t\\t\\t !lockdep_is_held(&wq_pool_mutex),\\t\\t\\\\\\n\\t\\t\\t \"RCU, wq->mutex or wq_pool_mutex should be held\")\\n\\n#define for_each_bh_worker_pool(pool, cpu)\\t\\t\\t\\t\\\\\\n\\tfor ((pool) = &per_cpu(bh_worker_pools, cpu)[0];\\t\\t\\\\\\n\\t     (pool) < &per_cpu(bh_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\\\\\n\\t     (pool)++)\\n\\n#define for_each_cpu_worker_pool(pool, cpu)\\t\\t\\t\\t\\\\\\n\\tfor ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];\\t\\t\\\\\\n\\t     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\\\\\n\\t     (pool)++)\\n\\n/**\\n * for_each_pool - iterate through all worker_pools in the system\\n * @pool: iteration cursor\\n * @pi: integer used for iteration\\n *\\n * This must be called either with wq_pool_mutex held or RCU read\\n * locked.  If the pool needs to be used beyond the locking in effect, the\\n * caller is responsible for guaranteeing that the pool stays online.\\n *\\n * The if/else clause exists only for the lockdep assertion and can be\\n * ignored.\\n */\\n#define for_each_pool(pool, pi)\\t\\t\\t\\t\\t\\t\\\\\\n\\tidr_for_each_entry(&worker_pool_idr, pool, pi)\\t\\t\\t\\\\\\n\\t\\tif (({ assert_rcu_or_pool_mutex(); false; })) { }\\t\\\\\\n\\t\\telse\\n\\n/**\\n * for_each_pool_worker - iterate through all workers of a worker_pool\\n * @worker: iteration cursor\\n * @pool: worker_pool to iterate workers of\\n *\\n * This must be called with wq_pool_attach_mutex.\\n *\\n * The if/else clause exists only for the lockdep assertion and can be\\n * ignored.\\n */\\n#define for_each_pool_worker(worker, pool)\\t\\t\\t\\t\\\\\\n\\tlist_for_each_entry((worker), &(pool)->workers, node)\\t\\t\\\\\\n\\t\\tif (({ lockdep_assert_held(&wq_pool_attach_mutex); false; })) { } \\\\\\n\\t\\telse\\n\\n/**\\n * for_each_pwq - iterate through all pool_workqueues of the specified workqueue\\n * @pwq: iteration cursor\\n * @wq: the target workqueue\\n *\\n * This must be called either with wq->mutex held or RCU read locked.\\n * If the pwq needs to be used beyond the locking in effect, the caller is\\n * responsible for guaranteeing that the pwq stays online.\\n *\\n * The if/else clause exists only for the lockdep assertion and can be\\n * ignored.\\n */\\n#define for_each_pwq(pwq, wq)\\t\\t\\t\\t\\t\\t\\\\\\n\\tlist_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node,\\t\\t\\\\\\n\\t\\t\\t\\t lockdep_is_held(&(wq->mutex)))\\n\\n#ifdef CONFIG_DEBUG_OBJECTS_WORK\\n\\nstatic const struct debug_obj_descr work_debug_descr;\\n\\nstatic void *work_debug_hint(void *addr)\\n{\\n\\treturn ((struct work_struct *) addr)->func;\\n}\\n\\nstatic bool work_is_static_object(void *addr)\\n{\\n\\tstruct work_struct *work = addr;\\n\\n\\treturn test_bit(WORK_STRUCT_STATIC_BIT, work_data_bits(work));\\n}\\n\\n/*\\n * fixup_init is called when:\\n * - an active object is initialized\\n */\\nstatic bool work_fixup_init(void *addr, enum debug_obj_state state)\\n{\\n\\tstruct work_struct *work = addr;\\n\\n\\tswitch (state) {\\n\\tcase ODEBUG_STATE_ACTIVE:\\n\\t\\tcancel_work_sync(work);\\n\\t\\tdebug_object_init(work, &work_debug_descr);\\n\\t\\treturn true;\\n\\tdefault:\\n\\t\\treturn false;\\n\\t}\\n}\\n\\n/*\\n * fixup_free is called when:\\n * - an active object is freed\\n */\\nstatic bool work_fixup_free(void *addr, enum debug_obj_state state)\\n{\\n\\tstruct work_struct *work = addr;\\n\\n\\tswitch (state) {\\n\\tcase ODEBUG_STATE_ACTIVE:\\n\\t\\tcancel_work_sync(work);\\n\\t\\tdebug_object_free(work, &work_debug_descr);\\n\\t\\treturn true;\\n\\tdefault:\\n\\t\\treturn false;\\n\\t}\\n}\\n\\nstatic const struct debug_obj_descr work_debug_descr = {\\n\\t.name\\t\\t= \"work_struct\",\\n\\t.debug_hint\\t= work_debug_hint,\\n\\t.is_static_object = work_is_static_object,\\n\\t.fixup_init\\t= work_fixup_init,\\n\\t.fixup_free\\t= work_fixup_free,\\n};\\n\\nstatic inline void debug_work_activate(struct work_struct *work)\\n{\\n\\tdebug_object_activate(work, &work_debug_descr);\\n}\\n\\nstatic inline void debug_work_deactivate(struct work_struct *work)\\n{\\n\\tdebug_object_deactivate(work, &work_debug_descr);\\n}\\n\\nvoid __init_work(struct work_struct *work, int onstack)\\n{\\n\\tif (onstack)\\n\\t\\tdebug_object_init_on_stack(work, &work_debug_descr);\\n\\telse\\n\\t\\tdebug_object_init(work, &work_debug_descr);\\n}\\nEXPORT_SYMBOL_GPL(__init_work);\\n\\nvoid destroy_work_on_stack(struct work_struct *work)\\n{\\n\\tdebug_object_free(work, &work_debug_descr);\\n}\\nEXPORT_SYMBOL_GPL(destroy_work_on_stack);\\n\\nvoid destroy_delayed_work_on_stack(struct delayed_work *work)\\n{\\n\\tdestroy_timer_on_stack(&work->timer);\\n\\tdebug_object_free(&work->work, &work_debug_descr);\\n}\\nEXPORT_SYMBOL_GPL(destroy_delayed_work_on_stack);\\n\\n#else\\nstatic inline void debug_work_activate(struct work_struct *work) { }\\nstatic inline void debug_work_deactivate(struct work_struct *work) { }\\n#endif\\n\\n/**\\n * worker_pool_assign_id - allocate ID and assign it to @pool\\n * @pool: the pool pointer of interest\\n *\\n * Returns 0 if ID in [0, WORK_OFFQ_POOL_NONE) is allocated and assigned\\n * successfully, -errno on failure.\\n */\\nstatic int worker_pool_assign_id(struct worker_pool *pool)\\n{\\n\\tint ret;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tret = idr_alloc(&worker_pool_idr, pool, 0, WORK_OFFQ_POOL_NONE,\\n\\t\\t\\tGFP_KERNEL);\\n\\tif (ret >= 0) {\\n\\t\\tpool->id = ret;\\n\\t\\treturn 0;\\n\\t}\\n\\treturn ret;\\n}\\n\\nstatic struct pool_workqueue __rcu **\\nunbound_pwq_slot(struct workqueue_struct *wq, int cpu)\\n{\\n       if (cpu >= 0)\\n               return per_cpu_ptr(wq->cpu_pwq, cpu);\\n       else\\n               return &wq->dfl_pwq;\\n}\\n\\n/* @cpu < 0 for dfl_pwq */\\nstatic struct pool_workqueue *unbound_pwq(struct workqueue_struct *wq, int cpu)\\n{\\n\\treturn rcu_dereference_check(*unbound_pwq_slot(wq, cpu),\\n\\t\\t\\t\\t     lockdep_is_held(&wq_pool_mutex) ||\\n\\t\\t\\t\\t     lockdep_is_held(&wq->mutex));\\n}\\n\\n/**\\n * unbound_effective_cpumask - effective cpumask of an unbound workqueue\\n * @wq: workqueue of interest\\n *\\n * @wq->unbound_attrs->cpumask contains the cpumask requested by the user which\\n * is masked with wq_unbound_cpumask to determine the effective cpumask. The\\n * default pwq is always mapped to the pool with the current effective cpumask.\\n */\\nstatic struct cpumask *unbound_effective_cpumask(struct workqueue_struct *wq)\\n{\\n\\treturn unbound_pwq(wq, -1)->pool->attrs->__pod_cpumask;\\n}\\n\\nstatic unsigned int work_color_to_flags(int color)\\n{\\n\\treturn color << WORK_STRUCT_COLOR_SHIFT;\\n}\\n\\nstatic int get_work_color(unsigned long work_data)\\n{\\n\\treturn (work_data >> WORK_STRUCT_COLOR_SHIFT) &\\n\\t\\t((1 << WORK_STRUCT_COLOR_BITS) - 1);\\n}\\n\\nstatic int work_next_color(int color)\\n{\\n\\treturn (color + 1) % WORK_NR_COLORS;\\n}\\n\\nstatic unsigned long pool_offq_flags(struct worker_pool *pool)\\n{\\n\\treturn (pool->flags & POOL_BH) ? WORK_OFFQ_BH : 0;\\n}\\n\\n/*\\n * While queued, %WORK_STRUCT_PWQ is set and non flag bits of a work\\'s data\\n * contain the pointer to the queued pwq.  Once execution starts, the flag\\n * is cleared and the high bits contain OFFQ flags and pool ID.\\n *\\n * set_work_pwq(), set_work_pool_and_clear_pending() and mark_work_canceling()\\n * can be used to set the pwq, pool or clear work->data. These functions should\\n * only be called while the work is owned - ie. while the PENDING bit is set.\\n *\\n * get_work_pool() and get_work_pwq() can be used to obtain the pool or pwq\\n * corresponding to a work.  Pool is available once the work has been\\n * queued anywhere after initialization until it is sync canceled.  pwq is\\n * available only while the work item is queued.\\n */\\nstatic inline void set_work_data(struct work_struct *work, unsigned long data)\\n{\\n\\tWARN_ON_ONCE(!work_pending(work));\\n\\tatomic_long_set(&work->data, data | work_static(work));\\n}\\n\\nstatic void set_work_pwq(struct work_struct *work, struct pool_workqueue *pwq,\\n\\t\\t\\t unsigned long flags)\\n{\\n\\tset_work_data(work, (unsigned long)pwq | WORK_STRUCT_PENDING |\\n\\t\\t      WORK_STRUCT_PWQ | flags);\\n}\\n\\nstatic void set_work_pool_and_keep_pending(struct work_struct *work,\\n\\t\\t\\t\\t\\t   int pool_id, unsigned long flags)\\n{\\n\\tset_work_data(work, ((unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT) |\\n\\t\\t      WORK_STRUCT_PENDING | flags);\\n}\\n\\nstatic void set_work_pool_and_clear_pending(struct work_struct *work,\\n\\t\\t\\t\\t\\t    int pool_id, unsigned long flags)\\n{\\n\\t/*\\n\\t * The following wmb is paired with the implied mb in\\n\\t * test_and_set_bit(PENDING) and ensures all updates to @work made\\n\\t * here are visible to and precede any updates by the next PENDING\\n\\t * owner.\\n\\t */\\n\\tsmp_wmb();\\n\\tset_work_data(work, ((unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT) |\\n\\t\\t      flags);\\n\\t/*\\n\\t * The following mb guarantees that previous clear of a PENDING bit\\n\\t * will not be reordered with any speculative LOADS or STORES from\\n\\t * work->current_func, which is executed afterwards.  This possible\\n\\t * reordering can lead to a missed execution on attempt to queue\\n\\t * the same @work.  E.g. consider this case:\\n\\t *\\n\\t *   CPU#0                         CPU#1\\n\\t *   ----------------------------  --------------------------------\\n\\t *\\n\\t * 1  STORE event_indicated\\n\\t * 2  queue_work_on() {\\n\\t * 3    test_and_set_bit(PENDING)\\n\\t * 4 }                             set_..._and_clear_pending() {\\n\\t * 5                                 set_work_data() # clear bit\\n\\t * 6                                 smp_mb()\\n\\t * 7                               work->current_func() {\\n\\t * 8\\t\\t\\t\\t      LOAD event_indicated\\n\\t *\\t\\t\\t\\t   }\\n\\t *\\n\\t * Without an explicit full barrier speculative LOAD on line 8 can\\n\\t * be executed before CPU#0 does STORE on line 1.  If that happens,\\n\\t * CPU#0 observes the PENDING bit is still set and new execution of\\n\\t * a @work is not queued in a hope, that CPU#1 will eventually\\n\\t * finish the queued @work.  Meanwhile CPU#1 does not see\\n\\t * event_indicated is set, because speculative LOAD was executed\\n\\t * before actual STORE.\\n\\t */\\n\\tsmp_mb();\\n}\\n\\nstatic inline struct pool_workqueue *work_struct_pwq(unsigned long data)\\n{\\n\\treturn (struct pool_workqueue *)(data & WORK_STRUCT_PWQ_MASK);\\n}\\n\\nstatic struct pool_workqueue *get_work_pwq(struct work_struct *work)\\n{\\n\\tunsigned long data = atomic_long_read(&work->data);\\n\\n\\tif (data & WORK_STRUCT_PWQ)\\n\\t\\treturn work_struct_pwq(data);\\n\\telse\\n\\t\\treturn NULL;\\n}\\n\\n/**\\n * get_work_pool - return the worker_pool a given work was associated with\\n * @work: the work item of interest\\n *\\n * Pools are created and destroyed under wq_pool_mutex, and allows read\\n * access under RCU read lock.  As such, this function should be\\n * called under wq_pool_mutex or inside of a rcu_read_lock() region.\\n *\\n * All fields of the returned pool are accessible as long as the above\\n * mentioned locking is in effect.  If the returned pool needs to be used\\n * beyond the critical section, the caller is responsible for ensuring the\\n * returned pool is and stays online.\\n *\\n * Return: The worker_pool @work was last associated with.  %NULL if none.\\n */\\nstatic struct worker_pool *get_work_pool(struct work_struct *work)\\n{\\n\\tunsigned long data = atomic_long_read(&work->data);\\n\\tint pool_id;\\n\\n\\tassert_rcu_or_pool_mutex();\\n\\n\\tif (data & WORK_STRUCT_PWQ)\\n\\t\\treturn work_struct_pwq(data)->pool;\\n\\n\\tpool_id = data >> WORK_OFFQ_POOL_SHIFT;\\n\\tif (pool_id == WORK_OFFQ_POOL_NONE)\\n\\t\\treturn NULL;\\n\\n\\treturn idr_find(&worker_pool_idr, pool_id);\\n}\\n\\nstatic unsigned long shift_and_mask(unsigned long v, u32 shift, u32 bits)\\n{\\n\\treturn (v >> shift) & ((1U << bits) - 1);\\n}\\n\\nstatic void work_offqd_unpack(struct work_offq_data *offqd, unsigned long data)\\n{\\n\\tWARN_ON_ONCE(data & WORK_STRUCT_PWQ);\\n\\n\\toffqd->pool_id = shift_and_mask(data, WORK_OFFQ_POOL_SHIFT,\\n\\t\\t\\t\\t\\tWORK_OFFQ_POOL_BITS);\\n\\toffqd->disable = shift_and_mask(data, WORK_OFFQ_DISABLE_SHIFT,\\n\\t\\t\\t\\t\\tWORK_OFFQ_DISABLE_BITS);\\n\\toffqd->flags = data & WORK_OFFQ_FLAG_MASK;\\n}\\n\\nstatic unsigned long work_offqd_pack_flags(struct work_offq_data *offqd)\\n{\\n\\treturn ((unsigned long)offqd->disable << WORK_OFFQ_DISABLE_SHIFT) |\\n\\t\\t((unsigned long)offqd->flags);\\n}\\n\\n/*\\n * Policy functions.  These define the policies on how the global worker\\n * pools are managed.  Unless noted otherwise, these functions assume that\\n * they\\'re being called with pool->lock held.\\n */\\n\\n/*\\n * Need to wake up a worker?  Called from anything but currently\\n * running workers.\\n *\\n * Note that, because unbound workers never contribute to nr_running, this\\n * function will always return %true for unbound pools as long as the\\n * worklist isn\\'t empty.\\n */\\nstatic bool need_more_worker(struct worker_pool *pool)\\n{\\n\\treturn !list_empty(&pool->worklist) && !pool->nr_running;\\n}\\n\\n/* Can I start working?  Called from busy but !running workers. */\\nstatic bool may_start_working(struct worker_pool *pool)\\n{\\n\\treturn pool->nr_idle;\\n}\\n\\n/* Do I need to keep working?  Called from currently running workers. */\\nstatic bool keep_working(struct worker_pool *pool)\\n{\\n\\treturn !list_empty(&pool->worklist) && (pool->nr_running <= 1);\\n}\\n\\n/* Do we need a new worker?  Called from manager. */\\nstatic bool need_to_create_worker(struct worker_pool *pool)\\n{\\n\\treturn need_more_worker(pool) && !may_start_working(pool);\\n}\\n\\n/* Do we have too many workers and should some go away? */\\nstatic bool too_many_workers(struct worker_pool *pool)\\n{\\n\\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;\\n\\tint nr_idle = pool->nr_idle + managing; /* manager is considered idle */\\n\\tint nr_busy = pool->nr_workers - nr_idle;\\n\\n\\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;\\n}\\n\\n/**\\n * worker_set_flags - set worker flags and adjust nr_running accordingly\\n * @worker: self\\n * @flags: flags to set\\n *\\n * Set @flags in @worker->flags and adjust nr_running accordingly.\\n */\\nstatic inline void worker_set_flags(struct worker *worker, unsigned int flags)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\tlockdep_assert_held(&pool->lock);\\n\\n\\t/* If transitioning into NOT_RUNNING, adjust nr_running. */\\n\\tif ((flags & WORKER_NOT_RUNNING) &&\\n\\t    !(worker->flags & WORKER_NOT_RUNNING)) {\\n\\t\\tpool->nr_running--;\\n\\t}\\n\\n\\tworker->flags |= flags;\\n}\\n\\n/**\\n * worker_clr_flags - clear worker flags and adjust nr_running accordingly\\n * @worker: self\\n * @flags: flags to clear\\n *\\n * Clear @flags in @worker->flags and adjust nr_running accordingly.\\n */\\nstatic inline void worker_clr_flags(struct worker *worker, unsigned int flags)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\tunsigned int oflags = worker->flags;\\n\\n\\tlockdep_assert_held(&pool->lock);\\n\\n\\tworker->flags &= ~flags;\\n\\n\\t/*\\n\\t * If transitioning out of NOT_RUNNING, increment nr_running.  Note\\n\\t * that the nested NOT_RUNNING is not a noop.  NOT_RUNNING is mask\\n\\t * of multiple flags, not a single flag.\\n\\t */\\n\\tif ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))\\n\\t\\tif (!(worker->flags & WORKER_NOT_RUNNING))\\n\\t\\t\\tpool->nr_running++;\\n}\\n\\n/* Return the first idle worker.  Called with pool->lock held. */\\nstatic struct worker *first_idle_worker(struct worker_pool *pool)\\n{\\n\\tif (unlikely(list_empty(&pool->idle_list)))\\n\\t\\treturn NULL;\\n\\n\\treturn list_first_entry(&pool->idle_list, struct worker, entry);\\n}\\n\\n/**\\n * worker_enter_idle - enter idle state\\n * @worker: worker which is entering idle state\\n *\\n * @worker is entering idle state.  Update stats and idle timer if\\n * necessary.\\n *\\n * LOCKING:\\n * raw_spin_lock_irq(pool->lock).\\n */\\nstatic void worker_enter_idle(struct worker *worker)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\tif (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||\\n\\t    WARN_ON_ONCE(!list_empty(&worker->entry) &&\\n\\t\\t\\t (worker->hentry.next || worker->hentry.pprev)))\\n\\t\\treturn;\\n\\n\\t/* can\\'t use worker_set_flags(), also called from create_worker() */\\n\\tworker->flags |= WORKER_IDLE;\\n\\tpool->nr_idle++;\\n\\tworker->last_active = jiffies;\\n\\n\\t/* idle_list is LIFO */\\n\\tlist_add(&worker->entry, &pool->idle_list);\\n\\n\\tif (too_many_workers(pool) && !timer_pending(&pool->idle_timer))\\n\\t\\tmod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);\\n\\n\\t/* Sanity check nr_running. */\\n\\tWARN_ON_ONCE(pool->nr_workers == pool->nr_idle && pool->nr_running);\\n}\\n\\n/**\\n * worker_leave_idle - leave idle state\\n * @worker: worker which is leaving idle state\\n *\\n * @worker is leaving idle state.  Update stats.\\n *\\n * LOCKING:\\n * raw_spin_lock_irq(pool->lock).\\n */\\nstatic void worker_leave_idle(struct worker *worker)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\tif (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))\\n\\t\\treturn;\\n\\tworker_clr_flags(worker, WORKER_IDLE);\\n\\tpool->nr_idle--;\\n\\tlist_del_init(&worker->entry);\\n}\\n\\n/**\\n * find_worker_executing_work - find worker which is executing a work\\n * @pool: pool of interest\\n * @work: work to find worker for\\n *\\n * Find a worker which is executing @work on @pool by searching\\n * @pool->busy_hash which is keyed by the address of @work.  For a worker\\n * to match, its current execution should match the address of @work and\\n * its work function.  This is to avoid unwanted dependency between\\n * unrelated work executions through a work item being recycled while still\\n * being executed.\\n *\\n * This is a bit tricky.  A work item may be freed once its execution\\n * starts and nothing prevents the freed area from being recycled for\\n * another work item.  If the same work item address ends up being reused\\n * before the original execution finishes, workqueue will identify the\\n * recycled work item as currently executing and make it wait until the\\n * current execution finishes, introducing an unwanted dependency.\\n *\\n * This function checks the work item address and work function to avoid\\n * false positives.  Note that this isn\\'t complete as one may construct a\\n * work function which can introduce dependency onto itself through a\\n * recycled work item.  Well, if somebody wants to shoot oneself in the\\n * foot that badly, there\\'s only so much we can do, and if such deadlock\\n * actually occurs, it should be easy to locate the culprit work function.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock).\\n *\\n * Return:\\n * Pointer to worker which is executing @work if found, %NULL\\n * otherwise.\\n */\\nstatic struct worker *find_worker_executing_work(struct worker_pool *pool,\\n\\t\\t\\t\\t\\t\\t struct work_struct *work)\\n{\\n\\tstruct worker *worker;\\n\\n\\thash_for_each_possible(pool->busy_hash, worker, hentry,\\n\\t\\t\\t       (unsigned long)work)\\n\\t\\tif (worker->current_work == work &&\\n\\t\\t    worker->current_func == work->func)\\n\\t\\t\\treturn worker;\\n\\n\\treturn NULL;\\n}\\n\\n/**\\n * move_linked_works - move linked works to a list\\n * @work: start of series of works to be scheduled\\n * @head: target list to append @work to\\n * @nextp: out parameter for nested worklist walking\\n *\\n * Schedule linked works starting from @work to @head. Work series to be\\n * scheduled starts at @work and includes any consecutive work with\\n * WORK_STRUCT_LINKED set in its predecessor. See assign_work() for details on\\n * @nextp.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock).\\n */\\nstatic void move_linked_works(struct work_struct *work, struct list_head *head,\\n\\t\\t\\t      struct work_struct **nextp)\\n{\\n\\tstruct work_struct *n;\\n\\n\\t/*\\n\\t * Linked worklist will always end before the end of the list,\\n\\t * use NULL for list head.\\n\\t */\\n\\tlist_for_each_entry_safe_from(work, n, NULL, entry) {\\n\\t\\tlist_move_tail(&work->entry, head);\\n\\t\\tif (!(*work_data_bits(work) & WORK_STRUCT_LINKED))\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\t/*\\n\\t * If we\\'re already inside safe list traversal and have moved\\n\\t * multiple works to the scheduled queue, the next position\\n\\t * needs to be updated.\\n\\t */\\n\\tif (nextp)\\n\\t\\t*nextp = n;\\n}\\n\\n/**\\n * assign_work - assign a work item and its linked work items to a worker\\n * @work: work to assign\\n * @worker: worker to assign to\\n * @nextp: out parameter for nested worklist walking\\n *\\n * Assign @work and its linked work items to @worker. If @work is already being\\n * executed by another worker in the same pool, it\\'ll be punted there.\\n *\\n * If @nextp is not NULL, it\\'s updated to point to the next work of the last\\n * scheduled work. This allows assign_work() to be nested inside\\n * list_for_each_entry_safe().\\n *\\n * Returns %true if @work was successfully assigned to @worker. %false if @work\\n * was punted to another worker already executing it.\\n */\\nstatic bool assign_work(struct work_struct *work, struct worker *worker,\\n\\t\\t\\tstruct work_struct **nextp)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\tstruct worker *collision;\\n\\n\\tlockdep_assert_held(&pool->lock);\\n\\n\\t/*\\n\\t * A single work shouldn\\'t be executed concurrently by multiple workers.\\n\\t * __queue_work() ensures that @work doesn\\'t jump to a different pool\\n\\t * while still running in the previous pool. Here, we should ensure that\\n\\t * @work is not executed concurrently by multiple workers from the same\\n\\t * pool. Check whether anyone is already processing the work. If so,\\n\\t * defer the work to the currently executing one.\\n\\t */\\n\\tcollision = find_worker_executing_work(pool, work);\\n\\tif (unlikely(collision)) {\\n\\t\\tmove_linked_works(work, &collision->scheduled, nextp);\\n\\t\\treturn false;\\n\\t}\\n\\n\\tmove_linked_works(work, &worker->scheduled, nextp);\\n\\treturn true;\\n}\\n\\nstatic struct irq_work *bh_pool_irq_work(struct worker_pool *pool)\\n{\\n\\tint high = pool->attrs->nice == HIGHPRI_NICE_LEVEL ? 1 : 0;\\n\\n\\treturn &per_cpu(bh_pool_irq_works, pool->cpu)[high];\\n}\\n\\nstatic void kick_bh_pool(struct worker_pool *pool)\\n{\\n#ifdef CONFIG_SMP\\n\\t/* see drain_dead_softirq_workfn() for BH_DRAINING */\\n\\tif (unlikely(pool->cpu != smp_processor_id() &&\\n\\t\\t     !(pool->flags & POOL_BH_DRAINING))) {\\n\\t\\tirq_work_queue_on(bh_pool_irq_work(pool), pool->cpu);\\n\\t\\treturn;\\n\\t}\\n#endif\\n\\tif (pool->attrs->nice == HIGHPRI_NICE_LEVEL)\\n\\t\\traise_softirq_irqoff(HI_SOFTIRQ);\\n\\telse\\n\\t\\traise_softirq_irqoff(TASKLET_SOFTIRQ);\\n}\\n\\n/**\\n * kick_pool - wake up an idle worker if necessary\\n * @pool: pool to kick\\n *\\n * @pool may have pending work items. Wake up worker if necessary. Returns\\n * whether a worker was woken up.\\n */\\nstatic bool kick_pool(struct worker_pool *pool)\\n{\\n\\tstruct worker *worker = first_idle_worker(pool);\\n\\tstruct task_struct *p;\\n\\n\\tlockdep_assert_held(&pool->lock);\\n\\n\\tif (!need_more_worker(pool) || !worker)\\n\\t\\treturn false;\\n\\n\\tif (pool->flags & POOL_BH) {\\n\\t\\tkick_bh_pool(pool);\\n\\t\\treturn true;\\n\\t}\\n\\n\\tp = worker->task;\\n\\n#ifdef CONFIG_SMP\\n\\t/*\\n\\t * Idle @worker is about to execute @work and waking up provides an\\n\\t * opportunity to migrate @worker at a lower cost by setting the task\\'s\\n\\t * wake_cpu field. Let\\'s see if we want to move @worker to improve\\n\\t * execution locality.\\n\\t *\\n\\t * We\\'re waking the worker that went idle the latest and there\\'s some\\n\\t * chance that @worker is marked idle but hasn\\'t gone off CPU yet. If\\n\\t * so, setting the wake_cpu won\\'t do anything. As this is a best-effort\\n\\t * optimization and the race window is narrow, let\\'s leave as-is for\\n\\t * now. If this becomes pronounced, we can skip over workers which are\\n\\t * still on cpu when picking an idle worker.\\n\\t *\\n\\t * If @pool has non-strict affinity, @worker might have ended up outside\\n\\t * its affinity scope. Repatriate.\\n\\t */\\n\\tif (!pool->attrs->affn_strict &&\\n\\t    !cpumask_test_cpu(p->wake_cpu, pool->attrs->__pod_cpumask)) {\\n\\t\\tstruct work_struct *work = list_first_entry(&pool->worklist,\\n\\t\\t\\t\\t\\t\\tstruct work_struct, entry);\\n\\t\\tint wake_cpu = cpumask_any_and_distribute(pool->attrs->__pod_cpumask,\\n\\t\\t\\t\\t\\t\\t\\t  cpu_online_mask);\\n\\t\\tif (wake_cpu < nr_cpu_ids) {\\n\\t\\t\\tp->wake_cpu = wake_cpu;\\n\\t\\t\\tget_work_pwq(work)->stats[PWQ_STAT_REPATRIATED]++;\\n\\t\\t}\\n\\t}\\n#endif\\n\\twake_up_process(p);\\n\\treturn true;\\n}\\n\\n#ifdef CONFIG_WQ_CPU_INTENSIVE_REPORT\\n\\n/*\\n * Concurrency-managed per-cpu work items that hog CPU for longer than\\n * wq_cpu_intensive_thresh_us trigger the automatic CPU_INTENSIVE mechanism,\\n * which prevents them from stalling other concurrency-managed work items. If a\\n * work function keeps triggering this mechanism, it\\'s likely that the work item\\n * should be using an unbound workqueue instead.\\n *\\n * wq_cpu_intensive_report() tracks work functions which trigger such conditions\\n * and report them so that they can be examined and converted to use unbound\\n * workqueues as appropriate. To avoid flooding the console, each violating work\\n * function is tracked and reported with exponential backoff.\\n */\\n#define WCI_MAX_ENTS 128\\n\\nstruct wci_ent {\\n\\twork_func_t\\t\\tfunc;\\n\\tatomic64_t\\t\\tcnt;\\n\\tstruct hlist_node\\thash_node;\\n};\\n\\nstatic struct wci_ent wci_ents[WCI_MAX_ENTS];\\nstatic int wci_nr_ents;\\nstatic DEFINE_RAW_SPINLOCK(wci_lock);\\nstatic DEFINE_HASHTABLE(wci_hash, ilog2(WCI_MAX_ENTS));\\n\\nstatic struct wci_ent *wci_find_ent(work_func_t func)\\n{\\n\\tstruct wci_ent *ent;\\n\\n\\thash_for_each_possible_rcu(wci_hash, ent, hash_node,\\n\\t\\t\\t\\t   (unsigned long)func) {\\n\\t\\tif (ent->func == func)\\n\\t\\t\\treturn ent;\\n\\t}\\n\\treturn NULL;\\n}\\n\\nstatic void wq_cpu_intensive_report(work_func_t func)\\n{\\n\\tstruct wci_ent *ent;\\n\\nrestart:\\n\\tent = wci_find_ent(func);\\n\\tif (ent) {\\n\\t\\tu64 cnt;\\n\\n\\t\\t/*\\n\\t\\t * Start reporting from the warning_thresh and back off\\n\\t\\t * exponentially.\\n\\t\\t */\\n\\t\\tcnt = atomic64_inc_return_relaxed(&ent->cnt);\\n\\t\\tif (wq_cpu_intensive_warning_thresh &&\\n\\t\\t    cnt >= wq_cpu_intensive_warning_thresh &&\\n\\t\\t    is_power_of_2(cnt + 1 - wq_cpu_intensive_warning_thresh))\\n\\t\\t\\tprintk_deferred(KERN_WARNING \"workqueue: %ps hogged CPU for >%luus %llu times, consider switching to WQ_UNBOUND\\\\n\",\\n\\t\\t\\t\\t\\tent->func, wq_cpu_intensive_thresh_us,\\n\\t\\t\\t\\t\\tatomic64_read(&ent->cnt));\\n\\t\\treturn;\\n\\t}\\n\\n\\t/*\\n\\t * @func is a new violation. Allocate a new entry for it. If wcn_ents[]\\n\\t * is exhausted, something went really wrong and we probably made enough\\n\\t * noise already.\\n\\t */\\n\\tif (wci_nr_ents >= WCI_MAX_ENTS)\\n\\t\\treturn;\\n\\n\\traw_spin_lock(&wci_lock);\\n\\n\\tif (wci_nr_ents >= WCI_MAX_ENTS) {\\n\\t\\traw_spin_unlock(&wci_lock);\\n\\t\\treturn;\\n\\t}\\n\\n\\tif (wci_find_ent(func)) {\\n\\t\\traw_spin_unlock(&wci_lock);\\n\\t\\tgoto restart;\\n\\t}\\n\\n\\tent = &wci_ents[wci_nr_ents++];\\n\\tent->func = func;\\n\\tatomic64_set(&ent->cnt, 0);\\n\\thash_add_rcu(wci_hash, &ent->hash_node, (unsigned long)func);\\n\\n\\traw_spin_unlock(&wci_lock);\\n\\n\\tgoto restart;\\n}\\n\\n#else\\t/* CONFIG_WQ_CPU_INTENSIVE_REPORT */\\nstatic void wq_cpu_intensive_report(work_func_t func) {}\\n#endif\\t/* CONFIG_WQ_CPU_INTENSIVE_REPORT */\\n\\n/**\\n * wq_worker_running - a worker is running again\\n * @task: task waking up\\n *\\n * This function is called when a worker returns from schedule()\\n */\\nvoid wq_worker_running(struct task_struct *task)\\n{\\n\\tstruct worker *worker = kthread_data(task);\\n\\n\\tif (!READ_ONCE(worker->sleeping))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * If preempted by unbind_workers() between the WORKER_NOT_RUNNING check\\n\\t * and the nr_running increment below, we may ruin the nr_running reset\\n\\t * and leave with an unexpected pool->nr_running == 1 on the newly unbound\\n\\t * pool. Protect against such race.\\n\\t */\\n\\tpreempt_disable();\\n\\tif (!(worker->flags & WORKER_NOT_RUNNING))\\n\\t\\tworker->pool->nr_running++;\\n\\tpreempt_enable();\\n\\n\\t/*\\n\\t * CPU intensive auto-detection cares about how long a work item hogged\\n\\t * CPU without sleeping. Reset the starting timestamp on wakeup.\\n\\t */\\n\\tworker->current_at = worker->task->se.sum_exec_runtime;\\n\\n\\tWRITE_ONCE(worker->sleeping, 0);\\n}\\n\\n/**\\n * wq_worker_sleeping - a worker is going to sleep\\n * @task: task going to sleep\\n *\\n * This function is called from schedule() when a busy worker is\\n * going to sleep.\\n */\\nvoid wq_worker_sleeping(struct task_struct *task)\\n{\\n\\tstruct worker *worker = kthread_data(task);\\n\\tstruct worker_pool *pool;\\n\\n\\t/*\\n\\t * Rescuers, which may not have all the fields set up like normal\\n\\t * workers, also reach here, let\\'s not access anything before\\n\\t * checking NOT_RUNNING.\\n\\t */\\n\\tif (worker->flags & WORKER_NOT_RUNNING)\\n\\t\\treturn;\\n\\n\\tpool = worker->pool;\\n\\n\\t/* Return if preempted before wq_worker_running() was reached */\\n\\tif (READ_ONCE(worker->sleeping))\\n\\t\\treturn;\\n\\n\\tWRITE_ONCE(worker->sleeping, 1);\\n\\traw_spin_lock_irq(&pool->lock);\\n\\n\\t/*\\n\\t * Recheck in case unbind_workers() preempted us. We don\\'t\\n\\t * want to decrement nr_running after the worker is unbound\\n\\t * and nr_running has been reset.\\n\\t */\\n\\tif (worker->flags & WORKER_NOT_RUNNING) {\\n\\t\\traw_spin_unlock_irq(&pool->lock);\\n\\t\\treturn;\\n\\t}\\n\\n\\tpool->nr_running--;\\n\\tif (kick_pool(pool))\\n\\t\\tworker->current_pwq->stats[PWQ_STAT_CM_WAKEUP]++;\\n\\n\\traw_spin_unlock_irq(&pool->lock);\\n}\\n\\n/**\\n * wq_worker_tick - a scheduler tick occurred while a kworker is running\\n * @task: task currently running\\n *\\n * Called from sched_tick(). We\\'re in the IRQ context and the current\\n * worker\\'s fields which follow the \\'K\\' locking rule can be accessed safely.\\n */\\nvoid wq_worker_tick(struct task_struct *task)\\n{\\n\\tstruct worker *worker = kthread_data(task);\\n\\tstruct pool_workqueue *pwq = worker->current_pwq;\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\tif (!pwq)\\n\\t\\treturn;\\n\\n\\tpwq->stats[PWQ_STAT_CPU_TIME] += TICK_USEC;\\n\\n\\tif (!wq_cpu_intensive_thresh_us)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * If the current worker is concurrency managed and hogged the CPU for\\n\\t * longer than wq_cpu_intensive_thresh_us, it\\'s automatically marked\\n\\t * CPU_INTENSIVE to avoid stalling other concurrency-managed work items.\\n\\t *\\n\\t * Set @worker->sleeping means that @worker is in the process of\\n\\t * switching out voluntarily and won\\'t be contributing to\\n\\t * @pool->nr_running until it wakes up. As wq_worker_sleeping() also\\n\\t * decrements ->nr_running, setting CPU_INTENSIVE here can lead to\\n\\t * double decrements. The task is releasing the CPU anyway. Let\\'s skip.\\n\\t * We probably want to make this prettier in the future.\\n\\t */\\n\\tif ((worker->flags & WORKER_NOT_RUNNING) || READ_ONCE(worker->sleeping) ||\\n\\t    worker->task->se.sum_exec_runtime - worker->current_at <\\n\\t    wq_cpu_intensive_thresh_us * NSEC_PER_USEC)\\n\\t\\treturn;\\n\\n\\traw_spin_lock(&pool->lock);\\n\\n\\tworker_set_flags(worker, WORKER_CPU_INTENSIVE);\\n\\twq_cpu_intensive_report(worker->current_func);\\n\\tpwq->stats[PWQ_STAT_CPU_INTENSIVE]++;\\n\\n\\tif (kick_pool(pool))\\n\\t\\tpwq->stats[PWQ_STAT_CM_WAKEUP]++;\\n\\n\\traw_spin_unlock(&pool->lock);\\n}\\n\\n/**\\n * wq_worker_last_func - retrieve worker\\'s last work function\\n * @task: Task to retrieve last work function of.\\n *\\n * Determine the last function a worker executed. This is called from\\n * the scheduler to get a worker\\'s last known identity.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(rq->lock)\\n *\\n * This function is called during schedule() when a kworker is going\\n * to sleep. It\\'s used by psi to identify aggregation workers during\\n * dequeuing, to allow periodic aggregation to shut-off when that\\n * worker is the last task in the system or cgroup to go to sleep.\\n *\\n * As this function doesn\\'t involve any workqueue-related locking, it\\n * only returns stable values when called from inside the scheduler\\'s\\n * queuing and dequeuing paths, when @task, which must be a kworker,\\n * is guaranteed to not be processing any works.\\n *\\n * Return:\\n * The last work function %current executed as a worker, NULL if it\\n * hasn\\'t executed any work yet.\\n */\\nwork_func_t wq_worker_last_func(struct task_struct *task)\\n{\\n\\tstruct worker *worker = kthread_data(task);\\n\\n\\treturn worker->last_func;\\n}\\n\\n/**\\n * wq_node_nr_active - Determine wq_node_nr_active to use\\n * @wq: workqueue of interest\\n * @node: NUMA node, can be %NUMA_NO_NODE\\n *\\n * Determine wq_node_nr_active to use for @wq on @node. Returns:\\n *\\n * - %NULL for per-cpu workqueues as they don\\'t need to use shared nr_active.\\n *\\n * - node_nr_active[nr_node_ids] if @node is %NUMA_NO_NODE.\\n *\\n * - Otherwise, node_nr_active[@node].\\n */\\nstatic struct wq_node_nr_active *wq_node_nr_active(struct workqueue_struct *wq,\\n\\t\\t\\t\\t\\t\\t   int node)\\n{\\n\\tif (!(wq->flags & WQ_UNBOUND))\\n\\t\\treturn NULL;\\n\\n\\tif (node == NUMA_NO_NODE)\\n\\t\\tnode = nr_node_ids;\\n\\n\\treturn wq->node_nr_active[node];\\n}\\n\\n/**\\n * wq_update_node_max_active - Update per-node max_actives to use\\n * @wq: workqueue to update\\n * @off_cpu: CPU that\\'s going down, -1 if a CPU is not going down\\n *\\n * Update @wq->node_nr_active[]->max. @wq must be unbound. max_active is\\n * distributed among nodes according to the proportions of numbers of online\\n * cpus. The result is always between @wq->min_active and max_active.\\n */\\nstatic void wq_update_node_max_active(struct workqueue_struct *wq, int off_cpu)\\n{\\n\\tstruct cpumask *effective = unbound_effective_cpumask(wq);\\n\\tint min_active = READ_ONCE(wq->min_active);\\n\\tint max_active = READ_ONCE(wq->max_active);\\n\\tint total_cpus, node;\\n\\n\\tlockdep_assert_held(&wq->mutex);\\n\\n\\tif (!wq_topo_initialized)\\n\\t\\treturn;\\n\\n\\tif (off_cpu >= 0 && !cpumask_test_cpu(off_cpu, effective))\\n\\t\\toff_cpu = -1;\\n\\n\\ttotal_cpus = cpumask_weight_and(effective, cpu_online_mask);\\n\\tif (off_cpu >= 0)\\n\\t\\ttotal_cpus--;\\n\\n\\t/* If all CPUs of the wq get offline, use the default values */\\n\\tif (unlikely(!total_cpus)) {\\n\\t\\tfor_each_node(node)\\n\\t\\t\\twq_node_nr_active(wq, node)->max = min_active;\\n\\n\\t\\twq_node_nr_active(wq, NUMA_NO_NODE)->max = max_active;\\n\\t\\treturn;\\n\\t}\\n\\n\\tfor_each_node(node) {\\n\\t\\tint node_cpus;\\n\\n\\t\\tnode_cpus = cpumask_weight_and(effective, cpumask_of_node(node));\\n\\t\\tif (off_cpu >= 0 && cpu_to_node(off_cpu) == node)\\n\\t\\t\\tnode_cpus--;\\n\\n\\t\\twq_node_nr_active(wq, node)->max =\\n\\t\\t\\tclamp(DIV_ROUND_UP(max_active * node_cpus, total_cpus),\\n\\t\\t\\t      min_active, max_active);\\n\\t}\\n\\n\\twq_node_nr_active(wq, NUMA_NO_NODE)->max = max_active;\\n}\\n\\n/**\\n * get_pwq - get an extra reference on the specified pool_workqueue\\n * @pwq: pool_workqueue to get\\n *\\n * Obtain an extra reference on @pwq.  The caller should guarantee that\\n * @pwq has positive refcnt and be holding the matching pool->lock.\\n */\\nstatic void get_pwq(struct pool_workqueue *pwq)\\n{\\n\\tlockdep_assert_held(&pwq->pool->lock);\\n\\tWARN_ON_ONCE(pwq->refcnt <= 0);\\n\\tpwq->refcnt++;\\n}\\n\\n/**\\n * put_pwq - put a pool_workqueue reference\\n * @pwq: pool_workqueue to put\\n *\\n * Drop a reference of @pwq.  If its refcnt reaches zero, schedule its\\n * destruction.  The caller should be holding the matching pool->lock.\\n */\\nstatic void put_pwq(struct pool_workqueue *pwq)\\n{\\n\\tlockdep_assert_held(&pwq->pool->lock);\\n\\tif (likely(--pwq->refcnt))\\n\\t\\treturn;\\n\\t/*\\n\\t * @pwq can\\'t be released under pool->lock, bounce to a dedicated\\n\\t * kthread_worker to avoid A-A deadlocks.\\n\\t */\\n\\tkthread_queue_work(pwq_release_worker, &pwq->release_work);\\n}\\n\\n/**\\n * put_pwq_unlocked - put_pwq() with surrounding pool lock/unlock\\n * @pwq: pool_workqueue to put (can be %NULL)\\n *\\n * put_pwq() with locking.  This function also allows %NULL @pwq.\\n */\\nstatic void put_pwq_unlocked(struct pool_workqueue *pwq)\\n{\\n\\tif (pwq) {\\n\\t\\t/*\\n\\t\\t * As both pwqs and pools are RCU protected, the\\n\\t\\t * following lock operations are safe.\\n\\t\\t */\\n\\t\\traw_spin_lock_irq(&pwq->pool->lock);\\n\\t\\tput_pwq(pwq);\\n\\t\\traw_spin_unlock_irq(&pwq->pool->lock);\\n\\t}\\n}\\n\\nstatic bool pwq_is_empty(struct pool_workqueue *pwq)\\n{\\n\\treturn !pwq->nr_active && list_empty(&pwq->inactive_works);\\n}\\n\\nstatic void __pwq_activate_work(struct pool_workqueue *pwq,\\n\\t\\t\\t\\tstruct work_struct *work)\\n{\\n\\tunsigned long *wdb = work_data_bits(work);\\n\\n\\tWARN_ON_ONCE(!(*wdb & WORK_STRUCT_INACTIVE));\\n\\ttrace_workqueue_activate_work(work);\\n\\tif (list_empty(&pwq->pool->worklist))\\n\\t\\tpwq->pool->watchdog_ts = jiffies;\\n\\tmove_linked_works(work, &pwq->pool->worklist, NULL);\\n\\t__clear_bit(WORK_STRUCT_INACTIVE_BIT, wdb);\\n}\\n\\nstatic bool tryinc_node_nr_active(struct wq_node_nr_active *nna)\\n{\\n\\tint max = READ_ONCE(nna->max);\\n\\n\\twhile (true) {\\n\\t\\tint old, tmp;\\n\\n\\t\\told = atomic_read(&nna->nr);\\n\\t\\tif (old >= max)\\n\\t\\t\\treturn false;\\n\\t\\ttmp = atomic_cmpxchg_relaxed(&nna->nr, old, old + 1);\\n\\t\\tif (tmp == old)\\n\\t\\t\\treturn true;\\n\\t}\\n}\\n\\n/**\\n * pwq_tryinc_nr_active - Try to increment nr_active for a pwq\\n * @pwq: pool_workqueue of interest\\n * @fill: max_active may have increased, try to increase concurrency level\\n *\\n * Try to increment nr_active for @pwq. Returns %true if an nr_active count is\\n * successfully obtained. %false otherwise.\\n */\\nstatic bool pwq_tryinc_nr_active(struct pool_workqueue *pwq, bool fill)\\n{\\n\\tstruct workqueue_struct *wq = pwq->wq;\\n\\tstruct worker_pool *pool = pwq->pool;\\n\\tstruct wq_node_nr_active *nna = wq_node_nr_active(wq, pool->node);\\n\\tbool obtained = false;\\n\\n\\tlockdep_assert_held(&pool->lock);\\n\\n\\tif (!nna) {\\n\\t\\t/* BH or per-cpu workqueue, pwq->nr_active is sufficient */\\n\\t\\tobtained = pwq->nr_active < READ_ONCE(wq->max_active);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (unlikely(pwq->plugged))\\n\\t\\treturn false;\\n\\n\\t/*\\n\\t * Unbound workqueue uses per-node shared nr_active $nna. If @pwq is\\n\\t * already waiting on $nna, pwq_dec_nr_active() will maintain the\\n\\t * concurrency level. Don\\'t jump the line.\\n\\t *\\n\\t * We need to ignore the pending test after max_active has increased as\\n\\t * pwq_dec_nr_active() can only maintain the concurrency level but not\\n\\t * increase it. This is indicated by @fill.\\n\\t */\\n\\tif (!list_empty(&pwq->pending_node) && likely(!fill))\\n\\t\\tgoto out;\\n\\n\\tobtained = tryinc_node_nr_active(nna);\\n\\tif (obtained)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Lockless acquisition failed. Lock, add ourself to $nna->pending_pwqs\\n\\t * and try again. The smp_mb() is paired with the implied memory barrier\\n\\t * of atomic_dec_return() in pwq_dec_nr_active() to ensure that either\\n\\t * we see the decremented $nna->nr or they see non-empty\\n\\t * $nna->pending_pwqs.\\n\\t */\\n\\traw_spin_lock(&nna->lock);\\n\\n\\tif (list_empty(&pwq->pending_node))\\n\\t\\tlist_add_tail(&pwq->pending_node, &nna->pending_pwqs);\\n\\telse if (likely(!fill))\\n\\t\\tgoto out_unlock;\\n\\n\\tsmp_mb();\\n\\n\\tobtained = tryinc_node_nr_active(nna);\\n\\n\\t/*\\n\\t * If @fill, @pwq might have already been pending. Being spuriously\\n\\t * pending in cold paths doesn\\'t affect anything. Let\\'s leave it be.\\n\\t */\\n\\tif (obtained && likely(!fill))\\n\\t\\tlist_del_init(&pwq->pending_node);\\n\\nout_unlock:\\n\\traw_spin_unlock(&nna->lock);\\nout:\\n\\tif (obtained)\\n\\t\\tpwq->nr_active++;\\n\\treturn obtained;\\n}\\n\\n/**\\n * pwq_activate_first_inactive - Activate the first inactive work item on a pwq\\n * @pwq: pool_workqueue of interest\\n * @fill: max_active may have increased, try to increase concurrency level\\n *\\n * Activate the first inactive work item of @pwq if available and allowed by\\n * max_active limit.\\n *\\n * Returns %true if an inactive work item has been activated. %false if no\\n * inactive work item is found or max_active limit is reached.\\n */\\nstatic bool pwq_activate_first_inactive(struct pool_workqueue *pwq, bool fill)\\n{\\n\\tstruct work_struct *work =\\n\\t\\tlist_first_entry_or_null(&pwq->inactive_works,\\n\\t\\t\\t\\t\\t struct work_struct, entry);\\n\\n\\tif (work && pwq_tryinc_nr_active(pwq, fill)) {\\n\\t\\t__pwq_activate_work(pwq, work);\\n\\t\\treturn true;\\n\\t} else {\\n\\t\\treturn false;\\n\\t}\\n}\\n\\n/**\\n * unplug_oldest_pwq - unplug the oldest pool_workqueue\\n * @wq: workqueue_struct where its oldest pwq is to be unplugged\\n *\\n * This function should only be called for ordered workqueues where only the\\n * oldest pwq is unplugged, the others are plugged to suspend execution to\\n * ensure proper work item ordering::\\n *\\n *    dfl_pwq --------------+     [P] - plugged\\n *                          |\\n *                          v\\n *    pwqs -> A -> B [P] -> C [P] (newest)\\n *            |    |        |\\n *            1    3        5\\n *            |    |        |\\n *            2    4        6\\n *\\n * When the oldest pwq is drained and removed, this function should be called\\n * to unplug the next oldest one to start its work item execution. Note that\\n * pwq\\'s are linked into wq->pwqs with the oldest first, so the first one in\\n * the list is the oldest.\\n */\\nstatic void unplug_oldest_pwq(struct workqueue_struct *wq)\\n{\\n\\tstruct pool_workqueue *pwq;\\n\\n\\tlockdep_assert_held(&wq->mutex);\\n\\n\\t/* Caller should make sure that pwqs isn\\'t empty before calling */\\n\\tpwq = list_first_entry_or_null(&wq->pwqs, struct pool_workqueue,\\n\\t\\t\\t\\t       pwqs_node);\\n\\traw_spin_lock_irq(&pwq->pool->lock);\\n\\tif (pwq->plugged) {\\n\\t\\tpwq->plugged = false;\\n\\t\\tif (pwq_activate_first_inactive(pwq, true))\\n\\t\\t\\tkick_pool(pwq->pool);\\n\\t}\\n\\traw_spin_unlock_irq(&pwq->pool->lock);\\n}\\n\\n/**\\n * node_activate_pending_pwq - Activate a pending pwq on a wq_node_nr_active\\n * @nna: wq_node_nr_active to activate a pending pwq for\\n * @caller_pool: worker_pool the caller is locking\\n *\\n * Activate a pwq in @nna->pending_pwqs. Called with @caller_pool locked.\\n * @caller_pool may be unlocked and relocked to lock other worker_pools.\\n */\\nstatic void node_activate_pending_pwq(struct wq_node_nr_active *nna,\\n\\t\\t\\t\\t      struct worker_pool *caller_pool)\\n{\\n\\tstruct worker_pool *locked_pool = caller_pool;\\n\\tstruct pool_workqueue *pwq;\\n\\tstruct work_struct *work;\\n\\n\\tlockdep_assert_held(&caller_pool->lock);\\n\\n\\traw_spin_lock(&nna->lock);\\nretry:\\n\\tpwq = list_first_entry_or_null(&nna->pending_pwqs,\\n\\t\\t\\t\\t       struct pool_workqueue, pending_node);\\n\\tif (!pwq)\\n\\t\\tgoto out_unlock;\\n\\n\\t/*\\n\\t * If @pwq is for a different pool than @locked_pool, we need to lock\\n\\t * @pwq->pool->lock. Let\\'s trylock first. If unsuccessful, do the unlock\\n\\t * / lock dance. For that, we also need to release @nna->lock as it\\'s\\n\\t * nested inside pool locks.\\n\\t */\\n\\tif (pwq->pool != locked_pool) {\\n\\t\\traw_spin_unlock(&locked_pool->lock);\\n\\t\\tlocked_pool = pwq->pool;\\n\\t\\tif (!raw_spin_trylock(&locked_pool->lock)) {\\n\\t\\t\\traw_spin_unlock(&nna->lock);\\n\\t\\t\\traw_spin_lock(&locked_pool->lock);\\n\\t\\t\\traw_spin_lock(&nna->lock);\\n\\t\\t\\tgoto retry;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * $pwq may not have any inactive work items due to e.g. cancellations.\\n\\t * Drop it from pending_pwqs and see if there\\'s another one.\\n\\t */\\n\\twork = list_first_entry_or_null(&pwq->inactive_works,\\n\\t\\t\\t\\t\\tstruct work_struct, entry);\\n\\tif (!work) {\\n\\t\\tlist_del_init(&pwq->pending_node);\\n\\t\\tgoto retry;\\n\\t}\\n\\n\\t/*\\n\\t * Acquire an nr_active count and activate the inactive work item. If\\n\\t * $pwq still has inactive work items, rotate it to the end of the\\n\\t * pending_pwqs so that we round-robin through them. This means that\\n\\t * inactive work items are not activated in queueing order which is fine\\n\\t * given that there has never been any ordering across different pwqs.\\n\\t */\\n\\tif (likely(tryinc_node_nr_active(nna))) {\\n\\t\\tpwq->nr_active++;\\n\\t\\t__pwq_activate_work(pwq, work);\\n\\n\\t\\tif (list_empty(&pwq->inactive_works))\\n\\t\\t\\tlist_del_init(&pwq->pending_node);\\n\\t\\telse\\n\\t\\t\\tlist_move_tail(&pwq->pending_node, &nna->pending_pwqs);\\n\\n\\t\\t/* if activating a foreign pool, make sure it\\'s running */\\n\\t\\tif (pwq->pool != caller_pool)\\n\\t\\t\\tkick_pool(pwq->pool);\\n\\t}\\n\\nout_unlock:\\n\\traw_spin_unlock(&nna->lock);\\n\\tif (locked_pool != caller_pool) {\\n\\t\\traw_spin_unlock(&locked_pool->lock);\\n\\t\\traw_spin_lock(&caller_pool->lock);\\n\\t}\\n}\\n\\n/**\\n * pwq_dec_nr_active - Retire an active count\\n * @pwq: pool_workqueue of interest\\n *\\n * Decrement @pwq\\'s nr_active and try to activate the first inactive work item.\\n * For unbound workqueues, this function may temporarily drop @pwq->pool->lock.\\n */\\nstatic void pwq_dec_nr_active(struct pool_workqueue *pwq)\\n{\\n\\tstruct worker_pool *pool = pwq->pool;\\n\\tstruct wq_node_nr_active *nna = wq_node_nr_active(pwq->wq, pool->node);\\n\\n\\tlockdep_assert_held(&pool->lock);\\n\\n\\t/*\\n\\t * @pwq->nr_active should be decremented for both percpu and unbound\\n\\t * workqueues.\\n\\t */\\n\\tpwq->nr_active--;\\n\\n\\t/*\\n\\t * For a percpu workqueue, it\\'s simple. Just need to kick the first\\n\\t * inactive work item on @pwq itself.\\n\\t */\\n\\tif (!nna) {\\n\\t\\tpwq_activate_first_inactive(pwq, false);\\n\\t\\treturn;\\n\\t}\\n\\n\\t/*\\n\\t * If @pwq is for an unbound workqueue, it\\'s more complicated because\\n\\t * multiple pwqs and pools may be sharing the nr_active count. When a\\n\\t * pwq needs to wait for an nr_active count, it puts itself on\\n\\t * $nna->pending_pwqs. The following atomic_dec_return()\\'s implied\\n\\t * memory barrier is paired with smp_mb() in pwq_tryinc_nr_active() to\\n\\t * guarantee that either we see non-empty pending_pwqs or they see\\n\\t * decremented $nna->nr.\\n\\t *\\n\\t * $nna->max may change as CPUs come online/offline and @pwq->wq\\'s\\n\\t * max_active gets updated. However, it is guaranteed to be equal to or\\n\\t * larger than @pwq->wq->min_active which is above zero unless freezing.\\n\\t * This maintains the forward progress guarantee.\\n\\t */\\n\\tif (atomic_dec_return(&nna->nr) >= READ_ONCE(nna->max))\\n\\t\\treturn;\\n\\n\\tif (!list_empty(&nna->pending_pwqs))\\n\\t\\tnode_activate_pending_pwq(nna, pool);\\n}\\n\\n/**\\n * pwq_dec_nr_in_flight - decrement pwq\\'s nr_in_flight\\n * @pwq: pwq of interest\\n * @work_data: work_data of work which left the queue\\n *\\n * A work either has completed or is removed from pending queue,\\n * decrement nr_in_flight of its pwq and handle workqueue flushing.\\n *\\n * NOTE:\\n * For unbound workqueues, this function may temporarily drop @pwq->pool->lock\\n * and thus should be called after all other state updates for the in-flight\\n * work item is complete.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock).\\n */\\nstatic void pwq_dec_nr_in_flight(struct pool_workqueue *pwq, unsigned long work_data)\\n{\\n\\tint color = get_work_color(work_data);\\n\\n\\tif (!(work_data & WORK_STRUCT_INACTIVE))\\n\\t\\tpwq_dec_nr_active(pwq);\\n\\n\\tpwq->nr_in_flight[color]--;\\n\\n\\t/* is flush in progress and are we at the flushing tip? */\\n\\tif (likely(pwq->flush_color != color))\\n\\t\\tgoto out_put;\\n\\n\\t/* are there still in-flight works? */\\n\\tif (pwq->nr_in_flight[color])\\n\\t\\tgoto out_put;\\n\\n\\t/* this pwq is done, clear flush_color */\\n\\tpwq->flush_color = -1;\\n\\n\\t/*\\n\\t * If this was the last pwq, wake up the first flusher.  It\\n\\t * will handle the rest.\\n\\t */\\n\\tif (atomic_dec_and_test(&pwq->wq->nr_pwqs_to_flush))\\n\\t\\tcomplete(&pwq->wq->first_flusher->done);\\nout_put:\\n\\tput_pwq(pwq);\\n}\\n\\n/**\\n * try_to_grab_pending - steal work item from worklist and disable irq\\n * @work: work item to steal\\n * @cflags: %WORK_CANCEL_ flags\\n * @irq_flags: place to store irq state\\n *\\n * Try to grab PENDING bit of @work.  This function can handle @work in any\\n * stable state - idle, on timer or on worklist.\\n *\\n * Return:\\n *\\n *  ========\\t================================================================\\n *  1\\t\\tif @work was pending and we successfully stole PENDING\\n *  0\\t\\tif @work was idle and we claimed PENDING\\n *  -EAGAIN\\tif PENDING couldn\\'t be grabbed at the moment, safe to busy-retry\\n *  ========\\t================================================================\\n *\\n * Note:\\n * On >= 0 return, the caller owns @work\\'s PENDING bit.  To avoid getting\\n * interrupted while holding PENDING and @work off queue, irq must be\\n * disabled on entry.  This, combined with delayed_work->timer being\\n * irqsafe, ensures that we return -EAGAIN for finite short period of time.\\n *\\n * On successful return, >= 0, irq is disabled and the caller is\\n * responsible for releasing it using local_irq_restore(*@irq_flags).\\n *\\n * This function is safe to call from any context including IRQ handler.\\n */\\nstatic int try_to_grab_pending(struct work_struct *work, u32 cflags,\\n\\t\\t\\t       unsigned long *irq_flags)\\n{\\n\\tstruct worker_pool *pool;\\n\\tstruct pool_workqueue *pwq;\\n\\n\\tlocal_irq_save(*irq_flags);\\n\\n\\t/* try to steal the timer if it exists */\\n\\tif (cflags & WORK_CANCEL_DELAYED) {\\n\\t\\tstruct delayed_work *dwork = to_delayed_work(work);\\n\\n\\t\\t/*\\n\\t\\t * dwork->timer is irqsafe.  If del_timer() fails, it\\'s\\n\\t\\t * guaranteed that the timer is not queued anywhere and not\\n\\t\\t * running on the local CPU.\\n\\t\\t */\\n\\t\\tif (likely(del_timer(&dwork->timer)))\\n\\t\\t\\treturn 1;\\n\\t}\\n\\n\\t/* try to claim PENDING the normal way */\\n\\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)))\\n\\t\\treturn 0;\\n\\n\\trcu_read_lock();\\n\\t/*\\n\\t * The queueing is in progress, or it is already queued. Try to\\n\\t * steal it from ->worklist without clearing WORK_STRUCT_PENDING.\\n\\t */\\n\\tpool = get_work_pool(work);\\n\\tif (!pool)\\n\\t\\tgoto fail;\\n\\n\\traw_spin_lock(&pool->lock);\\n\\t/*\\n\\t * work->data is guaranteed to point to pwq only while the work\\n\\t * item is queued on pwq->wq, and both updating work->data to point\\n\\t * to pwq on queueing and to pool on dequeueing are done under\\n\\t * pwq->pool->lock.  This in turn guarantees that, if work->data\\n\\t * points to pwq which is associated with a locked pool, the work\\n\\t * item is currently queued on that pool.\\n\\t */\\n\\tpwq = get_work_pwq(work);\\n\\tif (pwq && pwq->pool == pool) {\\n\\t\\tunsigned long work_data = *work_data_bits(work);\\n\\n\\t\\tdebug_work_deactivate(work);\\n\\n\\t\\t/*\\n\\t\\t * A cancelable inactive work item must be in the\\n\\t\\t * pwq->inactive_works since a queued barrier can\\'t be\\n\\t\\t * canceled (see the comments in insert_wq_barrier()).\\n\\t\\t *\\n\\t\\t * An inactive work item cannot be deleted directly because\\n\\t\\t * it might have linked barrier work items which, if left\\n\\t\\t * on the inactive_works list, will confuse pwq->nr_active\\n\\t\\t * management later on and cause stall.  Move the linked\\n\\t\\t * barrier work items to the worklist when deleting the grabbed\\n\\t\\t * item. Also keep WORK_STRUCT_INACTIVE in work_data, so that\\n\\t\\t * it doesn\\'t participate in nr_active management in later\\n\\t\\t * pwq_dec_nr_in_flight().\\n\\t\\t */\\n\\t\\tif (work_data & WORK_STRUCT_INACTIVE)\\n\\t\\t\\tmove_linked_works(work, &pwq->pool->worklist, NULL);\\n\\n\\t\\tlist_del_init(&work->entry);\\n\\n\\t\\t/*\\n\\t\\t * work->data points to pwq iff queued. Let\\'s point to pool. As\\n\\t\\t * this destroys work->data needed by the next step, stash it.\\n\\t\\t */\\n\\t\\tset_work_pool_and_keep_pending(work, pool->id,\\n\\t\\t\\t\\t\\t       pool_offq_flags(pool));\\n\\n\\t\\t/* must be the last step, see the function comment */\\n\\t\\tpwq_dec_nr_in_flight(pwq, work_data);\\n\\n\\t\\traw_spin_unlock(&pool->lock);\\n\\t\\trcu_read_unlock();\\n\\t\\treturn 1;\\n\\t}\\n\\traw_spin_unlock(&pool->lock);\\nfail:\\n\\trcu_read_unlock();\\n\\tlocal_irq_restore(*irq_flags);\\n\\treturn -EAGAIN;\\n}\\n\\n/**\\n * work_grab_pending - steal work item from worklist and disable irq\\n * @work: work item to steal\\n * @cflags: %WORK_CANCEL_ flags\\n * @irq_flags: place to store IRQ state\\n *\\n * Grab PENDING bit of @work. @work can be in any stable state - idle, on timer\\n * or on worklist.\\n *\\n * Can be called from any context. IRQ is disabled on return with IRQ state\\n * stored in *@irq_flags. The caller is responsible for re-enabling it using\\n * local_irq_restore().\\n *\\n * Returns %true if @work was pending. %false if idle.\\n */\\nstatic bool work_grab_pending(struct work_struct *work, u32 cflags,\\n\\t\\t\\t      unsigned long *irq_flags)\\n{\\n\\tint ret;\\n\\n\\twhile (true) {\\n\\t\\tret = try_to_grab_pending(work, cflags, irq_flags);\\n\\t\\tif (ret >= 0)\\n\\t\\t\\treturn ret;\\n\\t\\tcpu_relax();\\n\\t}\\n}\\n\\n/**\\n * insert_work - insert a work into a pool\\n * @pwq: pwq @work belongs to\\n * @work: work to insert\\n * @head: insertion point\\n * @extra_flags: extra WORK_STRUCT_* flags to set\\n *\\n * Insert @work which belongs to @pwq after @head.  @extra_flags is or\\'d to\\n * work_struct flags.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock).\\n */\\nstatic void insert_work(struct pool_workqueue *pwq, struct work_struct *work,\\n\\t\\t\\tstruct list_head *head, unsigned int extra_flags)\\n{\\n\\tdebug_work_activate(work);\\n\\n\\t/* record the work call stack in order to print it in KASAN reports */\\n\\tkasan_record_aux_stack_noalloc(work);\\n\\n\\t/* we own @work, set data and link */\\n\\tset_work_pwq(work, pwq, extra_flags);\\n\\tlist_add_tail(&work->entry, head);\\n\\tget_pwq(pwq);\\n}\\n\\n/*\\n * Test whether @work is being queued from another work executing on the\\n * same workqueue.\\n */\\nstatic bool is_chained_work(struct workqueue_struct *wq)\\n{\\n\\tstruct worker *worker;\\n\\n\\tworker = current_wq_worker();\\n\\t/*\\n\\t * Return %true iff I\\'m a worker executing a work item on @wq.  If\\n\\t * I\\'m @worker, it\\'s safe to dereference it without locking.\\n\\t */\\n\\treturn worker && worker->current_pwq->wq == wq;\\n}\\n\\n/*\\n * When queueing an unbound work item to a wq, prefer local CPU if allowed\\n * by wq_unbound_cpumask.  Otherwise, round robin among the allowed ones to\\n * avoid perturbing sensitive tasks.\\n */\\nstatic int wq_select_unbound_cpu(int cpu)\\n{\\n\\tint new_cpu;\\n\\n\\tif (likely(!wq_debug_force_rr_cpu)) {\\n\\t\\tif (cpumask_test_cpu(cpu, wq_unbound_cpumask))\\n\\t\\t\\treturn cpu;\\n\\t} else {\\n\\t\\tpr_warn_once(\"workqueue: round-robin CPU selection forced, expect performance impact\\\\n\");\\n\\t}\\n\\n\\tnew_cpu = __this_cpu_read(wq_rr_cpu_last);\\n\\tnew_cpu = cpumask_next_and(new_cpu, wq_unbound_cpumask, cpu_online_mask);\\n\\tif (unlikely(new_cpu >= nr_cpu_ids)) {\\n\\t\\tnew_cpu = cpumask_first_and(wq_unbound_cpumask, cpu_online_mask);\\n\\t\\tif (unlikely(new_cpu >= nr_cpu_ids))\\n\\t\\t\\treturn cpu;\\n\\t}\\n\\t__this_cpu_write(wq_rr_cpu_last, new_cpu);\\n\\n\\treturn new_cpu;\\n}\\n\\nstatic void __queue_work(int cpu, struct workqueue_struct *wq,\\n\\t\\t\\t struct work_struct *work)\\n{\\n\\tstruct pool_workqueue *pwq;\\n\\tstruct worker_pool *last_pool, *pool;\\n\\tunsigned int work_flags;\\n\\tunsigned int req_cpu = cpu;\\n\\n\\t/*\\n\\t * While a work item is PENDING && off queue, a task trying to\\n\\t * steal the PENDING will busy-loop waiting for it to either get\\n\\t * queued or lose PENDING.  Grabbing PENDING and queueing should\\n\\t * happen with IRQ disabled.\\n\\t */\\n\\tlockdep_assert_irqs_disabled();\\n\\n\\t/*\\n\\t * For a draining wq, only works from the same workqueue are\\n\\t * allowed. The __WQ_DESTROYING helps to spot the issue that\\n\\t * queues a new work item to a wq after destroy_workqueue(wq).\\n\\t */\\n\\tif (unlikely(wq->flags & (__WQ_DESTROYING | __WQ_DRAINING) &&\\n\\t\\t     WARN_ON_ONCE(!is_chained_work(wq))))\\n\\t\\treturn;\\n\\trcu_read_lock();\\nretry:\\n\\t/* pwq which will be used unless @work is executing elsewhere */\\n\\tif (req_cpu == WORK_CPU_UNBOUND) {\\n\\t\\tif (wq->flags & WQ_UNBOUND)\\n\\t\\t\\tcpu = wq_select_unbound_cpu(raw_smp_processor_id());\\n\\t\\telse\\n\\t\\t\\tcpu = raw_smp_processor_id();\\n\\t}\\n\\n\\tpwq = rcu_dereference(*per_cpu_ptr(wq->cpu_pwq, cpu));\\n\\tpool = pwq->pool;\\n\\n\\t/*\\n\\t * If @work was previously on a different pool, it might still be\\n\\t * running there, in which case the work needs to be queued on that\\n\\t * pool to guarantee non-reentrancy.\\n\\t *\\n\\t * For ordered workqueue, work items must be queued on the newest pwq\\n\\t * for accurate order management.  Guaranteed order also guarantees\\n\\t * non-reentrancy.  See the comments above unplug_oldest_pwq().\\n\\t */\\n\\tlast_pool = get_work_pool(work);\\n\\tif (last_pool && last_pool != pool && !(wq->flags & __WQ_ORDERED)) {\\n\\t\\tstruct worker *worker;\\n\\n\\t\\traw_spin_lock(&last_pool->lock);\\n\\n\\t\\tworker = find_worker_executing_work(last_pool, work);\\n\\n\\t\\tif (worker && worker->current_pwq->wq == wq) {\\n\\t\\t\\tpwq = worker->current_pwq;\\n\\t\\t\\tpool = pwq->pool;\\n\\t\\t\\tWARN_ON_ONCE(pool != last_pool);\\n\\t\\t} else {\\n\\t\\t\\t/* meh... not running there, queue here */\\n\\t\\t\\traw_spin_unlock(&last_pool->lock);\\n\\t\\t\\traw_spin_lock(&pool->lock);\\n\\t\\t}\\n\\t} else {\\n\\t\\traw_spin_lock(&pool->lock);\\n\\t}\\n\\n\\t/*\\n\\t * pwq is determined and locked. For unbound pools, we could have raced\\n\\t * with pwq release and it could already be dead. If its refcnt is zero,\\n\\t * repeat pwq selection. Note that unbound pwqs never die without\\n\\t * another pwq replacing it in cpu_pwq or while work items are executing\\n\\t * on it, so the retrying is guaranteed to make forward-progress.\\n\\t */\\n\\tif (unlikely(!pwq->refcnt)) {\\n\\t\\tif (wq->flags & WQ_UNBOUND) {\\n\\t\\t\\traw_spin_unlock(&pool->lock);\\n\\t\\t\\tcpu_relax();\\n\\t\\t\\tgoto retry;\\n\\t\\t}\\n\\t\\t/* oops */\\n\\t\\tWARN_ONCE(true, \"workqueue: per-cpu pwq for %s on cpu%d has 0 refcnt\",\\n\\t\\t\\t  wq->name, cpu);\\n\\t}\\n\\n\\t/* pwq determined, queue */\\n\\ttrace_workqueue_queue_work(req_cpu, pwq, work);\\n\\n\\tif (WARN_ON(!list_empty(&work->entry)))\\n\\t\\tgoto out;\\n\\n\\tpwq->nr_in_flight[pwq->work_color]++;\\n\\twork_flags = work_color_to_flags(pwq->work_color);\\n\\n\\t/*\\n\\t * Limit the number of concurrently active work items to max_active.\\n\\t * @work must also queue behind existing inactive work items to maintain\\n\\t * ordering when max_active changes. See wq_adjust_max_active().\\n\\t */\\n\\tif (list_empty(&pwq->inactive_works) && pwq_tryinc_nr_active(pwq, false)) {\\n\\t\\tif (list_empty(&pool->worklist))\\n\\t\\t\\tpool->watchdog_ts = jiffies;\\n\\n\\t\\ttrace_workqueue_activate_work(work);\\n\\t\\tinsert_work(pwq, work, &pool->worklist, work_flags);\\n\\t\\tkick_pool(pool);\\n\\t} else {\\n\\t\\twork_flags |= WORK_STRUCT_INACTIVE;\\n\\t\\tinsert_work(pwq, work, &pwq->inactive_works, work_flags);\\n\\t}\\n\\nout:\\n\\traw_spin_unlock(&pool->lock);\\n\\trcu_read_unlock();\\n}\\n\\nstatic bool clear_pending_if_disabled(struct work_struct *work)\\n{\\n\\tunsigned long data = *work_data_bits(work);\\n\\tstruct work_offq_data offqd;\\n\\n\\tif (likely((data & WORK_STRUCT_PWQ) ||\\n\\t\\t   !(data & WORK_OFFQ_DISABLE_MASK)))\\n\\t\\treturn false;\\n\\n\\twork_offqd_unpack(&offqd, data);\\n\\tset_work_pool_and_clear_pending(work, offqd.pool_id,\\n\\t\\t\\t\\t\\twork_offqd_pack_flags(&offqd));\\n\\treturn true;\\n}\\n\\n/**\\n * queue_work_on - queue work on specific cpu\\n * @cpu: CPU number to execute work on\\n * @wq: workqueue to use\\n * @work: work to queue\\n *\\n * We queue the work to a specific CPU, the caller must ensure it\\n * can\\'t go away.  Callers that fail to ensure that the specified\\n * CPU cannot go away will execute on a randomly chosen CPU.\\n * But note well that callers specifying a CPU that never has been\\n * online will get a splat.\\n *\\n * Return: %false if @work was already on a queue, %true otherwise.\\n */\\nbool queue_work_on(int cpu, struct workqueue_struct *wq,\\n\\t\\t   struct work_struct *work)\\n{\\n\\tbool ret = false;\\n\\tunsigned long irq_flags;\\n\\n\\tlocal_irq_save(irq_flags);\\n\\n\\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)) &&\\n\\t    !clear_pending_if_disabled(work)) {\\n\\t\\t__queue_work(cpu, wq, work);\\n\\t\\tret = true;\\n\\t}\\n\\n\\tlocal_irq_restore(irq_flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(queue_work_on);\\n\\n/**\\n * select_numa_node_cpu - Select a CPU based on NUMA node\\n * @node: NUMA node ID that we want to select a CPU from\\n *\\n * This function will attempt to find a \"random\" cpu available on a given\\n * node. If there are no CPUs available on the given node it will return\\n * WORK_CPU_UNBOUND indicating that we should just schedule to any\\n * available CPU if we need to schedule this work.\\n */\\nstatic int select_numa_node_cpu(int node)\\n{\\n\\tint cpu;\\n\\n\\t/* Delay binding to CPU if node is not valid or online */\\n\\tif (node < 0 || node >= MAX_NUMNODES || !node_online(node))\\n\\t\\treturn WORK_CPU_UNBOUND;\\n\\n\\t/* Use local node/cpu if we are already there */\\n\\tcpu = raw_smp_processor_id();\\n\\tif (node == cpu_to_node(cpu))\\n\\t\\treturn cpu;\\n\\n\\t/* Use \"random\" otherwise know as \"first\" online CPU of node */\\n\\tcpu = cpumask_any_and(cpumask_of_node(node), cpu_online_mask);\\n\\n\\t/* If CPU is valid return that, otherwise just defer */\\n\\treturn cpu < nr_cpu_ids ? cpu : WORK_CPU_UNBOUND;\\n}\\n\\n/**\\n * queue_work_node - queue work on a \"random\" cpu for a given NUMA node\\n * @node: NUMA node that we are targeting the work for\\n * @wq: workqueue to use\\n * @work: work to queue\\n *\\n * We queue the work to a \"random\" CPU within a given NUMA node. The basic\\n * idea here is to provide a way to somehow associate work with a given\\n * NUMA node.\\n *\\n * This function will only make a best effort attempt at getting this onto\\n * the right NUMA node. If no node is requested or the requested node is\\n * offline then we just fall back to standard queue_work behavior.\\n *\\n * Currently the \"random\" CPU ends up being the first available CPU in the\\n * intersection of cpu_online_mask and the cpumask of the node, unless we\\n * are running on the node. In that case we just use the current CPU.\\n *\\n * Return: %false if @work was already on a queue, %true otherwise.\\n */\\nbool queue_work_node(int node, struct workqueue_struct *wq,\\n\\t\\t     struct work_struct *work)\\n{\\n\\tunsigned long irq_flags;\\n\\tbool ret = false;\\n\\n\\t/*\\n\\t * This current implementation is specific to unbound workqueues.\\n\\t * Specifically we only return the first available CPU for a given\\n\\t * node instead of cycling through individual CPUs within the node.\\n\\t *\\n\\t * If this is used with a per-cpu workqueue then the logic in\\n\\t * workqueue_select_cpu_near would need to be updated to allow for\\n\\t * some round robin type logic.\\n\\t */\\n\\tWARN_ON_ONCE(!(wq->flags & WQ_UNBOUND));\\n\\n\\tlocal_irq_save(irq_flags);\\n\\n\\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)) &&\\n\\t    !clear_pending_if_disabled(work)) {\\n\\t\\tint cpu = select_numa_node_cpu(node);\\n\\n\\t\\t__queue_work(cpu, wq, work);\\n\\t\\tret = true;\\n\\t}\\n\\n\\tlocal_irq_restore(irq_flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(queue_work_node);\\n\\nvoid delayed_work_timer_fn(struct timer_list *t)\\n{\\n\\tstruct delayed_work *dwork = from_timer(dwork, t, timer);\\n\\n\\t/* should have been called from irqsafe timer with irq already off */\\n\\t__queue_work(dwork->cpu, dwork->wq, &dwork->work);\\n}\\nEXPORT_SYMBOL(delayed_work_timer_fn);\\n\\nstatic void __queue_delayed_work(int cpu, struct workqueue_struct *wq,\\n\\t\\t\\t\\tstruct delayed_work *dwork, unsigned long delay)\\n{\\n\\tstruct timer_list *timer = &dwork->timer;\\n\\tstruct work_struct *work = &dwork->work;\\n\\n\\tWARN_ON_ONCE(!wq);\\n\\tWARN_ON_ONCE(timer->function != delayed_work_timer_fn);\\n\\tWARN_ON_ONCE(timer_pending(timer));\\n\\tWARN_ON_ONCE(!list_empty(&work->entry));\\n\\n\\t/*\\n\\t * If @delay is 0, queue @dwork->work immediately.  This is for\\n\\t * both optimization and correctness.  The earliest @timer can\\n\\t * expire is on the closest next tick and delayed_work users depend\\n\\t * on that there\\'s no such delay when @delay is 0.\\n\\t */\\n\\tif (!delay) {\\n\\t\\t__queue_work(cpu, wq, &dwork->work);\\n\\t\\treturn;\\n\\t}\\n\\n\\tdwork->wq = wq;\\n\\tdwork->cpu = cpu;\\n\\ttimer->expires = jiffies + delay;\\n\\n\\tif (housekeeping_enabled(HK_TYPE_TIMER)) {\\n\\t\\t/* If the current cpu is a housekeeping cpu, use it. */\\n\\t\\tcpu = smp_processor_id();\\n\\t\\tif (!housekeeping_test_cpu(cpu, HK_TYPE_TIMER))\\n\\t\\t\\tcpu = housekeeping_any_cpu(HK_TYPE_TIMER);\\n\\t\\tadd_timer_on(timer, cpu);\\n\\t} else {\\n\\t\\tif (likely(cpu == WORK_CPU_UNBOUND))\\n\\t\\t\\tadd_timer_global(timer);\\n\\t\\telse\\n\\t\\t\\tadd_timer_on(timer, cpu);\\n\\t}\\n}\\n\\n/**\\n * queue_delayed_work_on - queue work on specific CPU after delay\\n * @cpu: CPU number to execute work on\\n * @wq: workqueue to use\\n * @dwork: work to queue\\n * @delay: number of jiffies to wait before queueing\\n *\\n * Return: %false if @work was already on a queue, %true otherwise.  If\\n * @delay is zero and @dwork is idle, it will be scheduled for immediate\\n * execution.\\n */\\nbool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,\\n\\t\\t\\t   struct delayed_work *dwork, unsigned long delay)\\n{\\n\\tstruct work_struct *work = &dwork->work;\\n\\tbool ret = false;\\n\\tunsigned long irq_flags;\\n\\n\\t/* read the comment in __queue_work() */\\n\\tlocal_irq_save(irq_flags);\\n\\n\\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)) &&\\n\\t    !clear_pending_if_disabled(work)) {\\n\\t\\t__queue_delayed_work(cpu, wq, dwork, delay);\\n\\t\\tret = true;\\n\\t}\\n\\n\\tlocal_irq_restore(irq_flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(queue_delayed_work_on);\\n\\n/**\\n * mod_delayed_work_on - modify delay of or queue a delayed work on specific CPU\\n * @cpu: CPU number to execute work on\\n * @wq: workqueue to use\\n * @dwork: work to queue\\n * @delay: number of jiffies to wait before queueing\\n *\\n * If @dwork is idle, equivalent to queue_delayed_work_on(); otherwise,\\n * modify @dwork\\'s timer so that it expires after @delay.  If @delay is\\n * zero, @work is guaranteed to be scheduled immediately regardless of its\\n * current state.\\n *\\n * Return: %false if @dwork was idle and queued, %true if @dwork was\\n * pending and its timer was modified.\\n *\\n * This function is safe to call from any context including IRQ handler.\\n * See try_to_grab_pending() for details.\\n */\\nbool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,\\n\\t\\t\\t struct delayed_work *dwork, unsigned long delay)\\n{\\n\\tunsigned long irq_flags;\\n\\tbool ret;\\n\\n\\tret = work_grab_pending(&dwork->work, WORK_CANCEL_DELAYED, &irq_flags);\\n\\n\\tif (!clear_pending_if_disabled(&dwork->work))\\n\\t\\t__queue_delayed_work(cpu, wq, dwork, delay);\\n\\n\\tlocal_irq_restore(irq_flags);\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(mod_delayed_work_on);\\n\\nstatic void rcu_work_rcufn(struct rcu_head *rcu)\\n{\\n\\tstruct rcu_work *rwork = container_of(rcu, struct rcu_work, rcu);\\n\\n\\t/* read the comment in __queue_work() */\\n\\tlocal_irq_disable();\\n\\t__queue_work(WORK_CPU_UNBOUND, rwork->wq, &rwork->work);\\n\\tlocal_irq_enable();\\n}\\n\\n/**\\n * queue_rcu_work - queue work after a RCU grace period\\n * @wq: workqueue to use\\n * @rwork: work to queue\\n *\\n * Return: %false if @rwork was already pending, %true otherwise.  Note\\n * that a full RCU grace period is guaranteed only after a %true return.\\n * While @rwork is guaranteed to be executed after a %false return, the\\n * execution may happen before a full RCU grace period has passed.\\n */\\nbool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork)\\n{\\n\\tstruct work_struct *work = &rwork->work;\\n\\n\\t/*\\n\\t * rcu_work can\\'t be canceled or disabled. Warn if the user reached\\n\\t * inside @rwork and disabled the inner work.\\n\\t */\\n\\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)) &&\\n\\t    !WARN_ON_ONCE(clear_pending_if_disabled(work))) {\\n\\t\\trwork->wq = wq;\\n\\t\\tcall_rcu_hurry(&rwork->rcu, rcu_work_rcufn);\\n\\t\\treturn true;\\n\\t}\\n\\n\\treturn false;\\n}\\nEXPORT_SYMBOL(queue_rcu_work);\\n\\nstatic struct worker *alloc_worker(int node)\\n{\\n\\tstruct worker *worker;\\n\\n\\tworker = kzalloc_node(sizeof(*worker), GFP_KERNEL, node);\\n\\tif (worker) {\\n\\t\\tINIT_LIST_HEAD(&worker->entry);\\n\\t\\tINIT_LIST_HEAD(&worker->scheduled);\\n\\t\\tINIT_LIST_HEAD(&worker->node);\\n\\t\\t/* on creation a worker is in !idle && prep state */\\n\\t\\tworker->flags = WORKER_PREP;\\n\\t}\\n\\treturn worker;\\n}\\n\\nstatic cpumask_t *pool_allowed_cpus(struct worker_pool *pool)\\n{\\n\\tif (pool->cpu < 0 && pool->attrs->affn_strict)\\n\\t\\treturn pool->attrs->__pod_cpumask;\\n\\telse\\n\\t\\treturn pool->attrs->cpumask;\\n}\\n\\n/**\\n * worker_attach_to_pool() - attach a worker to a pool\\n * @worker: worker to be attached\\n * @pool: the target pool\\n *\\n * Attach @worker to @pool.  Once attached, the %WORKER_UNBOUND flag and\\n * cpu-binding of @worker are kept coordinated with the pool across\\n * cpu-[un]hotplugs.\\n */\\nstatic void worker_attach_to_pool(struct worker *worker,\\n\\t\\t\\t\\t  struct worker_pool *pool)\\n{\\n\\tmutex_lock(&wq_pool_attach_mutex);\\n\\n\\t/*\\n\\t * The wq_pool_attach_mutex ensures %POOL_DISASSOCIATED remains stable\\n\\t * across this function. See the comments above the flag definition for\\n\\t * details. BH workers are, while per-CPU, always DISASSOCIATED.\\n\\t */\\n\\tif (pool->flags & POOL_DISASSOCIATED) {\\n\\t\\tworker->flags |= WORKER_UNBOUND;\\n\\t} else {\\n\\t\\tWARN_ON_ONCE(pool->flags & POOL_BH);\\n\\t\\tkthread_set_per_cpu(worker->task, pool->cpu);\\n\\t}\\n\\n\\tif (worker->rescue_wq)\\n\\t\\tset_cpus_allowed_ptr(worker->task, pool_allowed_cpus(pool));\\n\\n\\tlist_add_tail(&worker->node, &pool->workers);\\n\\tworker->pool = pool;\\n\\n\\tmutex_unlock(&wq_pool_attach_mutex);\\n}\\n\\nstatic void unbind_worker(struct worker *worker)\\n{\\n\\tlockdep_assert_held(&wq_pool_attach_mutex);\\n\\n\\tkthread_set_per_cpu(worker->task, -1);\\n\\tif (cpumask_intersects(wq_unbound_cpumask, cpu_active_mask))\\n\\t\\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, wq_unbound_cpumask) < 0);\\n\\telse\\n\\t\\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, cpu_possible_mask) < 0);\\n}\\n\\n\\nstatic void detach_worker(struct worker *worker)\\n{\\n\\tlockdep_assert_held(&wq_pool_attach_mutex);\\n\\n\\tunbind_worker(worker);\\n\\tlist_del(&worker->node);\\n}\\n\\n/**\\n * worker_detach_from_pool() - detach a worker from its pool\\n * @worker: worker which is attached to its pool\\n *\\n * Undo the attaching which had been done in worker_attach_to_pool().  The\\n * caller worker shouldn\\'t access to the pool after detached except it has\\n * other reference to the pool.\\n */\\nstatic void worker_detach_from_pool(struct worker *worker)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\t/* there is one permanent BH worker per CPU which should never detach */\\n\\tWARN_ON_ONCE(pool->flags & POOL_BH);\\n\\n\\tmutex_lock(&wq_pool_attach_mutex);\\n\\tdetach_worker(worker);\\n\\tworker->pool = NULL;\\n\\tmutex_unlock(&wq_pool_attach_mutex);\\n\\n\\t/* clear leftover flags without pool->lock after it is detached */\\n\\tworker->flags &= ~(WORKER_UNBOUND | WORKER_REBOUND);\\n}\\n\\nstatic int format_worker_id(char *buf, size_t size, struct worker *worker,\\n\\t\\t\\t    struct worker_pool *pool)\\n{\\n\\tif (worker->rescue_wq)\\n\\t\\treturn scnprintf(buf, size, \"kworker/R-%s\",\\n\\t\\t\\t\\t worker->rescue_wq->name);\\n\\n\\tif (pool) {\\n\\t\\tif (pool->cpu >= 0)\\n\\t\\t\\treturn scnprintf(buf, size, \"kworker/%d:%d%s\",\\n\\t\\t\\t\\t\\t pool->cpu, worker->id,\\n\\t\\t\\t\\t\\t pool->attrs->nice < 0  ? \"H\" : \"\");\\n\\t\\telse\\n\\t\\t\\treturn scnprintf(buf, size, \"kworker/u%d:%d\",\\n\\t\\t\\t\\t\\t pool->id, worker->id);\\n\\t} else {\\n\\t\\treturn scnprintf(buf, size, \"kworker/dying\");\\n\\t}\\n}\\n\\n/**\\n * create_worker - create a new workqueue worker\\n * @pool: pool the new worker will belong to\\n *\\n * Create and start a new worker which is attached to @pool.\\n *\\n * CONTEXT:\\n * Might sleep.  Does GFP_KERNEL allocations.\\n *\\n * Return:\\n * Pointer to the newly created worker.\\n */\\nstatic struct worker *create_worker(struct worker_pool *pool)\\n{\\n\\tstruct worker *worker;\\n\\tint id;\\n\\n\\t/* ID is needed to determine kthread name */\\n\\tid = ida_alloc(&pool->worker_ida, GFP_KERNEL);\\n\\tif (id < 0) {\\n\\t\\tpr_err_once(\"workqueue: Failed to allocate a worker ID: %pe\\\\n\",\\n\\t\\t\\t    ERR_PTR(id));\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tworker = alloc_worker(pool->node);\\n\\tif (!worker) {\\n\\t\\tpr_err_once(\"workqueue: Failed to allocate a worker\\\\n\");\\n\\t\\tgoto fail;\\n\\t}\\n\\n\\tworker->id = id;\\n\\n\\tif (!(pool->flags & POOL_BH)) {\\n\\t\\tchar id_buf[WORKER_ID_LEN];\\n\\n\\t\\tformat_worker_id(id_buf, sizeof(id_buf), worker, pool);\\n\\t\\tworker->task = kthread_create_on_node(worker_thread, worker,\\n\\t\\t\\t\\t\\t\\t      pool->node, \"%s\", id_buf);\\n\\t\\tif (IS_ERR(worker->task)) {\\n\\t\\t\\tif (PTR_ERR(worker->task) == -EINTR) {\\n\\t\\t\\t\\tpr_err(\"workqueue: Interrupted when creating a worker thread \\\\\"%s\\\\\"\\\\n\",\\n\\t\\t\\t\\t       id_buf);\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tpr_err_once(\"workqueue: Failed to create a worker thread: %pe\",\\n\\t\\t\\t\\t\\t    worker->task);\\n\\t\\t\\t}\\n\\t\\t\\tgoto fail;\\n\\t\\t}\\n\\n\\t\\tset_user_nice(worker->task, pool->attrs->nice);\\n\\t\\tkthread_bind_mask(worker->task, pool_allowed_cpus(pool));\\n\\t}\\n\\n\\t/* successful, attach the worker to the pool */\\n\\tworker_attach_to_pool(worker, pool);\\n\\n\\t/* start the newly created worker */\\n\\traw_spin_lock_irq(&pool->lock);\\n\\n\\tworker->pool->nr_workers++;\\n\\tworker_enter_idle(worker);\\n\\n\\t/*\\n\\t * @worker is waiting on a completion in kthread() and will trigger hung\\n\\t * check if not woken up soon. As kick_pool() is noop if @pool is empty,\\n\\t * wake it up explicitly.\\n\\t */\\n\\tif (worker->task)\\n\\t\\twake_up_process(worker->task);\\n\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\treturn worker;\\n\\nfail:\\n\\tida_free(&pool->worker_ida, id);\\n\\tkfree(worker);\\n\\treturn NULL;\\n}\\n\\nstatic void detach_dying_workers(struct list_head *cull_list)\\n{\\n\\tstruct worker *worker;\\n\\n\\tlist_for_each_entry(worker, cull_list, entry)\\n\\t\\tdetach_worker(worker);\\n}\\n\\nstatic void reap_dying_workers(struct list_head *cull_list)\\n{\\n\\tstruct worker *worker, *tmp;\\n\\n\\tlist_for_each_entry_safe(worker, tmp, cull_list, entry) {\\n\\t\\tlist_del_init(&worker->entry);\\n\\t\\tkthread_stop_put(worker->task);\\n\\t\\tkfree(worker);\\n\\t}\\n}\\n\\n/**\\n * set_worker_dying - Tag a worker for destruction\\n * @worker: worker to be destroyed\\n * @list: transfer worker away from its pool->idle_list and into list\\n *\\n * Tag @worker for destruction and adjust @pool stats accordingly.  The worker\\n * should be idle.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock).\\n */\\nstatic void set_worker_dying(struct worker *worker, struct list_head *list)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\tlockdep_assert_held(&pool->lock);\\n\\tlockdep_assert_held(&wq_pool_attach_mutex);\\n\\n\\t/* sanity check frenzy */\\n\\tif (WARN_ON(worker->current_work) ||\\n\\t    WARN_ON(!list_empty(&worker->scheduled)) ||\\n\\t    WARN_ON(!(worker->flags & WORKER_IDLE)))\\n\\t\\treturn;\\n\\n\\tpool->nr_workers--;\\n\\tpool->nr_idle--;\\n\\n\\tworker->flags |= WORKER_DIE;\\n\\n\\tlist_move(&worker->entry, list);\\n\\n\\t/* get an extra task struct reference for later kthread_stop_put() */\\n\\tget_task_struct(worker->task);\\n}\\n\\n/**\\n * idle_worker_timeout - check if some idle workers can now be deleted.\\n * @t: The pool\\'s idle_timer that just expired\\n *\\n * The timer is armed in worker_enter_idle(). Note that it isn\\'t disarmed in\\n * worker_leave_idle(), as a worker flicking between idle and active while its\\n * pool is at the too_many_workers() tipping point would cause too much timer\\n * housekeeping overhead. Since IDLE_WORKER_TIMEOUT is long enough, we just let\\n * it expire and re-evaluate things from there.\\n */\\nstatic void idle_worker_timeout(struct timer_list *t)\\n{\\n\\tstruct worker_pool *pool = from_timer(pool, t, idle_timer);\\n\\tbool do_cull = false;\\n\\n\\tif (work_pending(&pool->idle_cull_work))\\n\\t\\treturn;\\n\\n\\traw_spin_lock_irq(&pool->lock);\\n\\n\\tif (too_many_workers(pool)) {\\n\\t\\tstruct worker *worker;\\n\\t\\tunsigned long expires;\\n\\n\\t\\t/* idle_list is kept in LIFO order, check the last one */\\n\\t\\tworker = list_last_entry(&pool->idle_list, struct worker, entry);\\n\\t\\texpires = worker->last_active + IDLE_WORKER_TIMEOUT;\\n\\t\\tdo_cull = !time_before(jiffies, expires);\\n\\n\\t\\tif (!do_cull)\\n\\t\\t\\tmod_timer(&pool->idle_timer, expires);\\n\\t}\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\tif (do_cull)\\n\\t\\tqueue_work(system_unbound_wq, &pool->idle_cull_work);\\n}\\n\\n/**\\n * idle_cull_fn - cull workers that have been idle for too long.\\n * @work: the pool\\'s work for handling these idle workers\\n *\\n * This goes through a pool\\'s idle workers and gets rid of those that have been\\n * idle for at least IDLE_WORKER_TIMEOUT seconds.\\n *\\n * We don\\'t want to disturb isolated CPUs because of a pcpu kworker being\\n * culled, so this also resets worker affinity. This requires a sleepable\\n * context, hence the split between timer callback and work item.\\n */\\nstatic void idle_cull_fn(struct work_struct *work)\\n{\\n\\tstruct worker_pool *pool = container_of(work, struct worker_pool, idle_cull_work);\\n\\tLIST_HEAD(cull_list);\\n\\n\\t/*\\n\\t * Grabbing wq_pool_attach_mutex here ensures an already-running worker\\n\\t * cannot proceed beyong set_pf_worker() in its self-destruct path.\\n\\t * This is required as a previously-preempted worker could run after\\n\\t * set_worker_dying() has happened but before detach_dying_workers() did.\\n\\t */\\n\\tmutex_lock(&wq_pool_attach_mutex);\\n\\traw_spin_lock_irq(&pool->lock);\\n\\n\\twhile (too_many_workers(pool)) {\\n\\t\\tstruct worker *worker;\\n\\t\\tunsigned long expires;\\n\\n\\t\\tworker = list_last_entry(&pool->idle_list, struct worker, entry);\\n\\t\\texpires = worker->last_active + IDLE_WORKER_TIMEOUT;\\n\\n\\t\\tif (time_before(jiffies, expires)) {\\n\\t\\t\\tmod_timer(&pool->idle_timer, expires);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tset_worker_dying(worker, &cull_list);\\n\\t}\\n\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\tdetach_dying_workers(&cull_list);\\n\\tmutex_unlock(&wq_pool_attach_mutex);\\n\\n\\treap_dying_workers(&cull_list);\\n}\\n\\nstatic void send_mayday(struct work_struct *work)\\n{\\n\\tstruct pool_workqueue *pwq = get_work_pwq(work);\\n\\tstruct workqueue_struct *wq = pwq->wq;\\n\\n\\tlockdep_assert_held(&wq_mayday_lock);\\n\\n\\tif (!wq->rescuer)\\n\\t\\treturn;\\n\\n\\t/* mayday mayday mayday */\\n\\tif (list_empty(&pwq->mayday_node)) {\\n\\t\\t/*\\n\\t\\t * If @pwq is for an unbound wq, its base ref may be put at\\n\\t\\t * any time due to an attribute change.  Pin @pwq until the\\n\\t\\t * rescuer is done with it.\\n\\t\\t */\\n\\t\\tget_pwq(pwq);\\n\\t\\tlist_add_tail(&pwq->mayday_node, &wq->maydays);\\n\\t\\twake_up_process(wq->rescuer->task);\\n\\t\\tpwq->stats[PWQ_STAT_MAYDAY]++;\\n\\t}\\n}\\n\\nstatic void pool_mayday_timeout(struct timer_list *t)\\n{\\n\\tstruct worker_pool *pool = from_timer(pool, t, mayday_timer);\\n\\tstruct work_struct *work;\\n\\n\\traw_spin_lock_irq(&pool->lock);\\n\\traw_spin_lock(&wq_mayday_lock);\\t\\t/* for wq->maydays */\\n\\n\\tif (need_to_create_worker(pool)) {\\n\\t\\t/*\\n\\t\\t * We\\'ve been trying to create a new worker but\\n\\t\\t * haven\\'t been successful.  We might be hitting an\\n\\t\\t * allocation deadlock.  Send distress signals to\\n\\t\\t * rescuers.\\n\\t\\t */\\n\\t\\tlist_for_each_entry(work, &pool->worklist, entry)\\n\\t\\t\\tsend_mayday(work);\\n\\t}\\n\\n\\traw_spin_unlock(&wq_mayday_lock);\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\tmod_timer(&pool->mayday_timer, jiffies + MAYDAY_INTERVAL);\\n}\\n\\n/**\\n * maybe_create_worker - create a new worker if necessary\\n * @pool: pool to create a new worker for\\n *\\n * Create a new worker for @pool if necessary.  @pool is guaranteed to\\n * have at least one idle worker on return from this function.  If\\n * creating a new worker takes longer than MAYDAY_INTERVAL, mayday is\\n * sent to all rescuers with works scheduled on @pool to resolve\\n * possible allocation deadlock.\\n *\\n * On return, need_to_create_worker() is guaranteed to be %false and\\n * may_start_working() %true.\\n *\\n * LOCKING:\\n * raw_spin_lock_irq(pool->lock) which may be released and regrabbed\\n * multiple times.  Does GFP_KERNEL allocations.  Called only from\\n * manager.\\n */\\nstatic void maybe_create_worker(struct worker_pool *pool)\\n__releases(&pool->lock)\\n__acquires(&pool->lock)\\n{\\nrestart:\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\t/* if we don\\'t make progress in MAYDAY_INITIAL_TIMEOUT, call for help */\\n\\tmod_timer(&pool->mayday_timer, jiffies + MAYDAY_INITIAL_TIMEOUT);\\n\\n\\twhile (true) {\\n\\t\\tif (create_worker(pool) || !need_to_create_worker(pool))\\n\\t\\t\\tbreak;\\n\\n\\t\\tschedule_timeout_interruptible(CREATE_COOLDOWN);\\n\\n\\t\\tif (!need_to_create_worker(pool))\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tdel_timer_sync(&pool->mayday_timer);\\n\\traw_spin_lock_irq(&pool->lock);\\n\\t/*\\n\\t * This is necessary even after a new worker was just successfully\\n\\t * created as @pool->lock was dropped and the new worker might have\\n\\t * already become busy.\\n\\t */\\n\\tif (need_to_create_worker(pool))\\n\\t\\tgoto restart;\\n}\\n\\n/**\\n * manage_workers - manage worker pool\\n * @worker: self\\n *\\n * Assume the manager role and manage the worker pool @worker belongs\\n * to.  At any given time, there can be only zero or one manager per\\n * pool.  The exclusion is handled automatically by this function.\\n *\\n * The caller can safely start processing works on false return.  On\\n * true return, it\\'s guaranteed that need_to_create_worker() is false\\n * and may_start_working() is true.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock) which may be released and regrabbed\\n * multiple times.  Does GFP_KERNEL allocations.\\n *\\n * Return:\\n * %false if the pool doesn\\'t need management and the caller can safely\\n * start processing works, %true if management function was performed and\\n * the conditions that the caller verified before calling the function may\\n * no longer be true.\\n */\\nstatic bool manage_workers(struct worker *worker)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\tif (pool->flags & POOL_MANAGER_ACTIVE)\\n\\t\\treturn false;\\n\\n\\tpool->flags |= POOL_MANAGER_ACTIVE;\\n\\tpool->manager = worker;\\n\\n\\tmaybe_create_worker(pool);\\n\\n\\tpool->manager = NULL;\\n\\tpool->flags &= ~POOL_MANAGER_ACTIVE;\\n\\trcuwait_wake_up(&manager_wait);\\n\\treturn true;\\n}\\n\\n/**\\n * process_one_work - process single work\\n * @worker: self\\n * @work: work to process\\n *\\n * Process @work.  This function contains all the logics necessary to\\n * process a single work including synchronization against and\\n * interaction with other workers on the same cpu, queueing and\\n * flushing.  As long as context requirement is met, any worker can\\n * call this function to process a work.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock) which is released and regrabbed.\\n */\\nstatic void process_one_work(struct worker *worker, struct work_struct *work)\\n__releases(&pool->lock)\\n__acquires(&pool->lock)\\n{\\n\\tstruct pool_workqueue *pwq = get_work_pwq(work);\\n\\tstruct worker_pool *pool = worker->pool;\\n\\tunsigned long work_data;\\n\\tint lockdep_start_depth, rcu_start_depth;\\n\\tbool bh_draining = pool->flags & POOL_BH_DRAINING;\\n#ifdef CONFIG_LOCKDEP\\n\\t/*\\n\\t * It is permissible to free the struct work_struct from\\n\\t * inside the function that is called from it, this we need to\\n\\t * take into account for lockdep too.  To avoid bogus \"held\\n\\t * lock freed\" warnings as well as problems when looking into\\n\\t * work->lockdep_map, make a copy and use that here.\\n\\t */\\n\\tstruct lockdep_map lockdep_map;\\n\\n\\tlockdep_copy_map(&lockdep_map, &work->lockdep_map);\\n#endif\\n\\t/* ensure we\\'re on the correct CPU */\\n\\tWARN_ON_ONCE(!(pool->flags & POOL_DISASSOCIATED) &&\\n\\t\\t     raw_smp_processor_id() != pool->cpu);\\n\\n\\t/* claim and dequeue */\\n\\tdebug_work_deactivate(work);\\n\\thash_add(pool->busy_hash, &worker->hentry, (unsigned long)work);\\n\\tworker->current_work = work;\\n\\tworker->current_func = work->func;\\n\\tworker->current_pwq = pwq;\\n\\tif (worker->task)\\n\\t\\tworker->current_at = worker->task->se.sum_exec_runtime;\\n\\twork_data = *work_data_bits(work);\\n\\tworker->current_color = get_work_color(work_data);\\n\\n\\t/*\\n\\t * Record wq name for cmdline and debug reporting, may get\\n\\t * overridden through set_worker_desc().\\n\\t */\\n\\tstrscpy(worker->desc, pwq->wq->name, WORKER_DESC_LEN);\\n\\n\\tlist_del_init(&work->entry);\\n\\n\\t/*\\n\\t * CPU intensive works don\\'t participate in concurrency management.\\n\\t * They\\'re the scheduler\\'s responsibility.  This takes @worker out\\n\\t * of concurrency management and the next code block will chain\\n\\t * execution of the pending work items.\\n\\t */\\n\\tif (unlikely(pwq->wq->flags & WQ_CPU_INTENSIVE))\\n\\t\\tworker_set_flags(worker, WORKER_CPU_INTENSIVE);\\n\\n\\t/*\\n\\t * Kick @pool if necessary. It\\'s always noop for per-cpu worker pools\\n\\t * since nr_running would always be >= 1 at this point. This is used to\\n\\t * chain execution of the pending work items for WORKER_NOT_RUNNING\\n\\t * workers such as the UNBOUND and CPU_INTENSIVE ones.\\n\\t */\\n\\tkick_pool(pool);\\n\\n\\t/*\\n\\t * Record the last pool and clear PENDING which should be the last\\n\\t * update to @work.  Also, do this inside @pool->lock so that\\n\\t * PENDING and queued state changes happen together while IRQ is\\n\\t * disabled.\\n\\t */\\n\\tset_work_pool_and_clear_pending(work, pool->id, pool_offq_flags(pool));\\n\\n\\tpwq->stats[PWQ_STAT_STARTED]++;\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\trcu_start_depth = rcu_preempt_depth();\\n\\tlockdep_start_depth = lockdep_depth(current);\\n\\t/* see drain_dead_softirq_workfn() */\\n\\tif (!bh_draining)\\n\\t\\tlock_map_acquire(pwq->wq->lockdep_map);\\n\\tlock_map_acquire(&lockdep_map);\\n\\t/*\\n\\t * Strictly speaking we should mark the invariant state without holding\\n\\t * any locks, that is, before these two lock_map_acquire()\\'s.\\n\\t *\\n\\t * However, that would result in:\\n\\t *\\n\\t *   A(W1)\\n\\t *   WFC(C)\\n\\t *\\t\\tA(W1)\\n\\t *\\t\\tC(C)\\n\\t *\\n\\t * Which would create W1->C->W1 dependencies, even though there is no\\n\\t * actual deadlock possible. There are two solutions, using a\\n\\t * read-recursive acquire on the work(queue) \\'locks\\', but this will then\\n\\t * hit the lockdep limitation on recursive locks, or simply discard\\n\\t * these locks.\\n\\t *\\n\\t * AFAICT there is no possible deadlock scenario between the\\n\\t * flush_work() and complete() primitives (except for single-threaded\\n\\t * workqueues), so hiding them isn\\'t a problem.\\n\\t */\\n\\tlockdep_invariant_state(true);\\n\\ttrace_workqueue_execute_start(work);\\n\\tworker->current_func(work);\\n\\t/*\\n\\t * While we must be careful to not use \"work\" after this, the trace\\n\\t * point will only record its address.\\n\\t */\\n\\ttrace_workqueue_execute_end(work, worker->current_func);\\n\\tpwq->stats[PWQ_STAT_COMPLETED]++;\\n\\tlock_map_release(&lockdep_map);\\n\\tif (!bh_draining)\\n\\t\\tlock_map_release(pwq->wq->lockdep_map);\\n\\n\\tif (unlikely((worker->task && in_atomic()) ||\\n\\t\\t     lockdep_depth(current) != lockdep_start_depth ||\\n\\t\\t     rcu_preempt_depth() != rcu_start_depth)) {\\n\\t\\tpr_err(\"BUG: workqueue leaked atomic, lock or RCU: %s[%d]\\\\n\"\\n\\t\\t       \"     preempt=0x%08x lock=%d->%d RCU=%d->%d workfn=%ps\\\\n\",\\n\\t\\t       current->comm, task_pid_nr(current), preempt_count(),\\n\\t\\t       lockdep_start_depth, lockdep_depth(current),\\n\\t\\t       rcu_start_depth, rcu_preempt_depth(),\\n\\t\\t       worker->current_func);\\n\\t\\tdebug_show_held_locks(current);\\n\\t\\tdump_stack();\\n\\t}\\n\\n\\t/*\\n\\t * The following prevents a kworker from hogging CPU on !PREEMPTION\\n\\t * kernels, where a requeueing work item waiting for something to\\n\\t * happen could deadlock with stop_machine as such work item could\\n\\t * indefinitely requeue itself while all other CPUs are trapped in\\n\\t * stop_machine. At the same time, report a quiescent RCU state so\\n\\t * the same condition doesn\\'t freeze RCU.\\n\\t */\\n\\tif (worker->task)\\n\\t\\tcond_resched();\\n\\n\\traw_spin_lock_irq(&pool->lock);\\n\\n\\t/*\\n\\t * In addition to %WQ_CPU_INTENSIVE, @worker may also have been marked\\n\\t * CPU intensive by wq_worker_tick() if @work hogged CPU longer than\\n\\t * wq_cpu_intensive_thresh_us. Clear it.\\n\\t */\\n\\tworker_clr_flags(worker, WORKER_CPU_INTENSIVE);\\n\\n\\t/* tag the worker for identification in schedule() */\\n\\tworker->last_func = worker->current_func;\\n\\n\\t/* we\\'re done with it, release */\\n\\thash_del(&worker->hentry);\\n\\tworker->current_work = NULL;\\n\\tworker->current_func = NULL;\\n\\tworker->current_pwq = NULL;\\n\\tworker->current_color = INT_MAX;\\n\\n\\t/* must be the last step, see the function comment */\\n\\tpwq_dec_nr_in_flight(pwq, work_data);\\n}\\n\\n/**\\n * process_scheduled_works - process scheduled works\\n * @worker: self\\n *\\n * Process all scheduled works.  Please note that the scheduled list\\n * may change while processing a work, so this function repeatedly\\n * fetches a work from the top and executes it.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock) which may be released and regrabbed\\n * multiple times.\\n */\\nstatic void process_scheduled_works(struct worker *worker)\\n{\\n\\tstruct work_struct *work;\\n\\tbool first = true;\\n\\n\\twhile ((work = list_first_entry_or_null(&worker->scheduled,\\n\\t\\t\\t\\t\\t\\tstruct work_struct, entry))) {\\n\\t\\tif (first) {\\n\\t\\t\\tworker->pool->watchdog_ts = jiffies;\\n\\t\\t\\tfirst = false;\\n\\t\\t}\\n\\t\\tprocess_one_work(worker, work);\\n\\t}\\n}\\n\\nstatic void set_pf_worker(bool val)\\n{\\n\\tmutex_lock(&wq_pool_attach_mutex);\\n\\tif (val)\\n\\t\\tcurrent->flags |= PF_WQ_WORKER;\\n\\telse\\n\\t\\tcurrent->flags &= ~PF_WQ_WORKER;\\n\\tmutex_unlock(&wq_pool_attach_mutex);\\n}\\n\\n/**\\n * worker_thread - the worker thread function\\n * @__worker: self\\n *\\n * The worker thread function.  All workers belong to a worker_pool -\\n * either a per-cpu one or dynamic unbound one.  These workers process all\\n * work items regardless of their specific target workqueue.  The only\\n * exception is work items which belong to workqueues with a rescuer which\\n * will be explained in rescuer_thread().\\n *\\n * Return: 0\\n */\\nstatic int worker_thread(void *__worker)\\n{\\n\\tstruct worker *worker = __worker;\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\t/* tell the scheduler that this is a workqueue worker */\\n\\tset_pf_worker(true);\\nwoke_up:\\n\\traw_spin_lock_irq(&pool->lock);\\n\\n\\t/* am I supposed to die? */\\n\\tif (unlikely(worker->flags & WORKER_DIE)) {\\n\\t\\traw_spin_unlock_irq(&pool->lock);\\n\\t\\tset_pf_worker(false);\\n\\t\\t/*\\n\\t\\t * The worker is dead and PF_WQ_WORKER is cleared, worker->pool\\n\\t\\t * shouldn\\'t be accessed, reset it to NULL in case otherwise.\\n\\t\\t */\\n\\t\\tworker->pool = NULL;\\n\\t\\tida_free(&pool->worker_ida, worker->id);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tworker_leave_idle(worker);\\nrecheck:\\n\\t/* no more worker necessary? */\\n\\tif (!need_more_worker(pool))\\n\\t\\tgoto sleep;\\n\\n\\t/* do we need to manage? */\\n\\tif (unlikely(!may_start_working(pool)) && manage_workers(worker))\\n\\t\\tgoto recheck;\\n\\n\\t/*\\n\\t * ->scheduled list can only be filled while a worker is\\n\\t * preparing to process a work or actually processing it.\\n\\t * Make sure nobody diddled with it while I was sleeping.\\n\\t */\\n\\tWARN_ON_ONCE(!list_empty(&worker->scheduled));\\n\\n\\t/*\\n\\t * Finish PREP stage.  We\\'re guaranteed to have at least one idle\\n\\t * worker or that someone else has already assumed the manager\\n\\t * role.  This is where @worker starts participating in concurrency\\n\\t * management if applicable and concurrency management is restored\\n\\t * after being rebound.  See rebind_workers() for details.\\n\\t */\\n\\tworker_clr_flags(worker, WORKER_PREP | WORKER_REBOUND);\\n\\n\\tdo {\\n\\t\\tstruct work_struct *work =\\n\\t\\t\\tlist_first_entry(&pool->worklist,\\n\\t\\t\\t\\t\\t struct work_struct, entry);\\n\\n\\t\\tif (assign_work(work, worker, NULL))\\n\\t\\t\\tprocess_scheduled_works(worker);\\n\\t} while (keep_working(pool));\\n\\n\\tworker_set_flags(worker, WORKER_PREP);\\nsleep:\\n\\t/*\\n\\t * pool->lock is held and there\\'s no work to process and no need to\\n\\t * manage, sleep.  Workers are woken up only while holding\\n\\t * pool->lock or from local cpu, so setting the current state\\n\\t * before releasing pool->lock is enough to prevent losing any\\n\\t * event.\\n\\t */\\n\\tworker_enter_idle(worker);\\n\\t__set_current_state(TASK_IDLE);\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\tschedule();\\n\\tgoto woke_up;\\n}\\n\\n/**\\n * rescuer_thread - the rescuer thread function\\n * @__rescuer: self\\n *\\n * Workqueue rescuer thread function.  There\\'s one rescuer for each\\n * workqueue which has WQ_MEM_RECLAIM set.\\n *\\n * Regular work processing on a pool may block trying to create a new\\n * worker which uses GFP_KERNEL allocation which has slight chance of\\n * developing into deadlock if some works currently on the same queue\\n * need to be processed to satisfy the GFP_KERNEL allocation.  This is\\n * the problem rescuer solves.\\n *\\n * When such condition is possible, the pool summons rescuers of all\\n * workqueues which have works queued on the pool and let them process\\n * those works so that forward progress can be guaranteed.\\n *\\n * This should happen rarely.\\n *\\n * Return: 0\\n */\\nstatic int rescuer_thread(void *__rescuer)\\n{\\n\\tstruct worker *rescuer = __rescuer;\\n\\tstruct workqueue_struct *wq = rescuer->rescue_wq;\\n\\tbool should_stop;\\n\\n\\tset_user_nice(current, RESCUER_NICE_LEVEL);\\n\\n\\t/*\\n\\t * Mark rescuer as worker too.  As WORKER_PREP is never cleared, it\\n\\t * doesn\\'t participate in concurrency management.\\n\\t */\\n\\tset_pf_worker(true);\\nrepeat:\\n\\tset_current_state(TASK_IDLE);\\n\\n\\t/*\\n\\t * By the time the rescuer is requested to stop, the workqueue\\n\\t * shouldn\\'t have any work pending, but @wq->maydays may still have\\n\\t * pwq(s) queued.  This can happen by non-rescuer workers consuming\\n\\t * all the work items before the rescuer got to them.  Go through\\n\\t * @wq->maydays processing before acting on should_stop so that the\\n\\t * list is always empty on exit.\\n\\t */\\n\\tshould_stop = kthread_should_stop();\\n\\n\\t/* see whether any pwq is asking for help */\\n\\traw_spin_lock_irq(&wq_mayday_lock);\\n\\n\\twhile (!list_empty(&wq->maydays)) {\\n\\t\\tstruct pool_workqueue *pwq = list_first_entry(&wq->maydays,\\n\\t\\t\\t\\t\\tstruct pool_workqueue, mayday_node);\\n\\t\\tstruct worker_pool *pool = pwq->pool;\\n\\t\\tstruct work_struct *work, *n;\\n\\n\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\tlist_del_init(&pwq->mayday_node);\\n\\n\\t\\traw_spin_unlock_irq(&wq_mayday_lock);\\n\\n\\t\\tworker_attach_to_pool(rescuer, pool);\\n\\n\\t\\traw_spin_lock_irq(&pool->lock);\\n\\n\\t\\t/*\\n\\t\\t * Slurp in all works issued via this workqueue and\\n\\t\\t * process\\'em.\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(!list_empty(&rescuer->scheduled));\\n\\t\\tlist_for_each_entry_safe(work, n, &pool->worklist, entry) {\\n\\t\\t\\tif (get_work_pwq(work) == pwq &&\\n\\t\\t\\t    assign_work(work, rescuer, &n))\\n\\t\\t\\t\\tpwq->stats[PWQ_STAT_RESCUED]++;\\n\\t\\t}\\n\\n\\t\\tif (!list_empty(&rescuer->scheduled)) {\\n\\t\\t\\tprocess_scheduled_works(rescuer);\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * The above execution of rescued work items could\\n\\t\\t\\t * have created more to rescue through\\n\\t\\t\\t * pwq_activate_first_inactive() or chained\\n\\t\\t\\t * queueing.  Let\\'s put @pwq back on mayday list so\\n\\t\\t\\t * that such back-to-back work items, which may be\\n\\t\\t\\t * being used to relieve memory pressure, don\\'t\\n\\t\\t\\t * incur MAYDAY_INTERVAL delay inbetween.\\n\\t\\t\\t */\\n\\t\\t\\tif (pwq->nr_active && need_to_create_worker(pool)) {\\n\\t\\t\\t\\traw_spin_lock(&wq_mayday_lock);\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * Queue iff we aren\\'t racing destruction\\n\\t\\t\\t\\t * and somebody else hasn\\'t queued it already.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\tif (wq->rescuer && list_empty(&pwq->mayday_node)) {\\n\\t\\t\\t\\t\\tget_pwq(pwq);\\n\\t\\t\\t\\t\\tlist_add_tail(&pwq->mayday_node, &wq->maydays);\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\traw_spin_unlock(&wq_mayday_lock);\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Put the reference grabbed by send_mayday().  @pool won\\'t\\n\\t\\t * go away while we\\'re still attached to it.\\n\\t\\t */\\n\\t\\tput_pwq(pwq);\\n\\n\\t\\t/*\\n\\t\\t * Leave this pool. Notify regular workers; otherwise, we end up\\n\\t\\t * with 0 concurrency and stalling the execution.\\n\\t\\t */\\n\\t\\tkick_pool(pool);\\n\\n\\t\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\t\\tworker_detach_from_pool(rescuer);\\n\\n\\t\\traw_spin_lock_irq(&wq_mayday_lock);\\n\\t}\\n\\n\\traw_spin_unlock_irq(&wq_mayday_lock);\\n\\n\\tif (should_stop) {\\n\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\tset_pf_worker(false);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\t/* rescuers should never participate in concurrency management */\\n\\tWARN_ON_ONCE(!(rescuer->flags & WORKER_NOT_RUNNING));\\n\\tschedule();\\n\\tgoto repeat;\\n}\\n\\nstatic void bh_worker(struct worker *worker)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\tint nr_restarts = BH_WORKER_RESTARTS;\\n\\tunsigned long end = jiffies + BH_WORKER_JIFFIES;\\n\\n\\traw_spin_lock_irq(&pool->lock);\\n\\tworker_leave_idle(worker);\\n\\n\\t/*\\n\\t * This function follows the structure of worker_thread(). See there for\\n\\t * explanations on each step.\\n\\t */\\n\\tif (!need_more_worker(pool))\\n\\t\\tgoto done;\\n\\n\\tWARN_ON_ONCE(!list_empty(&worker->scheduled));\\n\\tworker_clr_flags(worker, WORKER_PREP | WORKER_REBOUND);\\n\\n\\tdo {\\n\\t\\tstruct work_struct *work =\\n\\t\\t\\tlist_first_entry(&pool->worklist,\\n\\t\\t\\t\\t\\t struct work_struct, entry);\\n\\n\\t\\tif (assign_work(work, worker, NULL))\\n\\t\\t\\tprocess_scheduled_works(worker);\\n\\t} while (keep_working(pool) &&\\n\\t\\t --nr_restarts && time_before(jiffies, end));\\n\\n\\tworker_set_flags(worker, WORKER_PREP);\\ndone:\\n\\tworker_enter_idle(worker);\\n\\tkick_pool(pool);\\n\\traw_spin_unlock_irq(&pool->lock);\\n}\\n\\n/*\\n * TODO: Convert all tasklet users to workqueue and use softirq directly.\\n *\\n * This is currently called from tasklet[_hi]action() and thus is also called\\n * whenever there are tasklets to run. Let\\'s do an early exit if there\\'s nothing\\n * queued. Once conversion from tasklet is complete, the need_more_worker() test\\n * can be dropped.\\n *\\n * After full conversion, we\\'ll add worker->softirq_action, directly use the\\n * softirq action and obtain the worker pointer from the softirq_action pointer.\\n */\\nvoid workqueue_softirq_action(bool highpri)\\n{\\n\\tstruct worker_pool *pool =\\n\\t\\t&per_cpu(bh_worker_pools, smp_processor_id())[highpri];\\n\\tif (need_more_worker(pool))\\n\\t\\tbh_worker(list_first_entry(&pool->workers, struct worker, node));\\n}\\n\\nstruct wq_drain_dead_softirq_work {\\n\\tstruct work_struct\\twork;\\n\\tstruct worker_pool\\t*pool;\\n\\tstruct completion\\tdone;\\n};\\n\\nstatic void drain_dead_softirq_workfn(struct work_struct *work)\\n{\\n\\tstruct wq_drain_dead_softirq_work *dead_work =\\n\\t\\tcontainer_of(work, struct wq_drain_dead_softirq_work, work);\\n\\tstruct worker_pool *pool = dead_work->pool;\\n\\tbool repeat;\\n\\n\\t/*\\n\\t * @pool\\'s CPU is dead and we want to execute its still pending work\\n\\t * items from this BH work item which is running on a different CPU. As\\n\\t * its CPU is dead, @pool can\\'t be kicked and, as work execution path\\n\\t * will be nested, a lockdep annotation needs to be suppressed. Mark\\n\\t * @pool with %POOL_BH_DRAINING for the special treatments.\\n\\t */\\n\\traw_spin_lock_irq(&pool->lock);\\n\\tpool->flags |= POOL_BH_DRAINING;\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\tbh_worker(list_first_entry(&pool->workers, struct worker, node));\\n\\n\\traw_spin_lock_irq(&pool->lock);\\n\\tpool->flags &= ~POOL_BH_DRAINING;\\n\\trepeat = need_more_worker(pool);\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\t/*\\n\\t * bh_worker() might hit consecutive execution limit and bail. If there\\n\\t * still are pending work items, reschedule self and return so that we\\n\\t * don\\'t hog this CPU\\'s BH.\\n\\t */\\n\\tif (repeat) {\\n\\t\\tif (pool->attrs->nice == HIGHPRI_NICE_LEVEL)\\n\\t\\t\\tqueue_work(system_bh_highpri_wq, work);\\n\\t\\telse\\n\\t\\t\\tqueue_work(system_bh_wq, work);\\n\\t} else {\\n\\t\\tcomplete(&dead_work->done);\\n\\t}\\n}\\n\\n/*\\n * @cpu is dead. Drain the remaining BH work items on the current CPU. It\\'s\\n * possible to allocate dead_work per CPU and avoid flushing. However, then we\\n * have to worry about draining overlapping with CPU coming back online or\\n * nesting (one CPU\\'s dead_work queued on another CPU which is also dead and so\\n * on). Let\\'s keep it simple and drain them synchronously. These are BH work\\n * items which shouldn\\'t be requeued on the same pool. Shouldn\\'t take long.\\n */\\nvoid workqueue_softirq_dead(unsigned int cpu)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < NR_STD_WORKER_POOLS; i++) {\\n\\t\\tstruct worker_pool *pool = &per_cpu(bh_worker_pools, cpu)[i];\\n\\t\\tstruct wq_drain_dead_softirq_work dead_work;\\n\\n\\t\\tif (!need_more_worker(pool))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tINIT_WORK_ONSTACK(&dead_work.work, drain_dead_softirq_workfn);\\n\\t\\tdead_work.pool = pool;\\n\\t\\tinit_completion(&dead_work.done);\\n\\n\\t\\tif (pool->attrs->nice == HIGHPRI_NICE_LEVEL)\\n\\t\\t\\tqueue_work(system_bh_highpri_wq, &dead_work.work);\\n\\t\\telse\\n\\t\\t\\tqueue_work(system_bh_wq, &dead_work.work);\\n\\n\\t\\twait_for_completion(&dead_work.done);\\n\\t\\tdestroy_work_on_stack(&dead_work.work);\\n\\t}\\n}\\n\\n/**\\n * check_flush_dependency - check for flush dependency sanity\\n * @target_wq: workqueue being flushed\\n * @target_work: work item being flushed (NULL for workqueue flushes)\\n * @from_cancel: are we called from the work cancel path\\n *\\n * %current is trying to flush the whole @target_wq or @target_work on it.\\n * If this is not the cancel path (which implies work being flushed is either\\n * already running, or will not be at all), check if @target_wq doesn\\'t have\\n * %WQ_MEM_RECLAIM and verify that %current is not reclaiming memory or running\\n * on a workqueue which doesn\\'t have %WQ_MEM_RECLAIM as that can break forward-\\n * progress guarantee leading to a deadlock.\\n */\\nstatic void check_flush_dependency(struct workqueue_struct *target_wq,\\n\\t\\t\\t\\t   struct work_struct *target_work,\\n\\t\\t\\t\\t   bool from_cancel)\\n{\\n\\twork_func_t target_func;\\n\\tstruct worker *worker;\\n\\n\\tif (from_cancel || target_wq->flags & WQ_MEM_RECLAIM)\\n\\t\\treturn;\\n\\n\\tworker = current_wq_worker();\\n\\ttarget_func = target_work ? target_work->func : NULL;\\n\\n\\tWARN_ONCE(current->flags & PF_MEMALLOC,\\n\\t\\t  \"workqueue: PF_MEMALLOC task %d(%s) is flushing !WQ_MEM_RECLAIM %s:%ps\",\\n\\t\\t  current->pid, current->comm, target_wq->name, target_func);\\n\\tWARN_ONCE(worker && ((worker->current_pwq->wq->flags &\\n\\t\\t\\t      (WQ_MEM_RECLAIM | __WQ_LEGACY)) == WQ_MEM_RECLAIM),\\n\\t\\t  \"workqueue: WQ_MEM_RECLAIM %s:%ps is flushing !WQ_MEM_RECLAIM %s:%ps\",\\n\\t\\t  worker->current_pwq->wq->name, worker->current_func,\\n\\t\\t  target_wq->name, target_func);\\n}\\n\\nstruct wq_barrier {\\n\\tstruct work_struct\\twork;\\n\\tstruct completion\\tdone;\\n\\tstruct task_struct\\t*task;\\t/* purely informational */\\n};\\n\\nstatic void wq_barrier_func(struct work_struct *work)\\n{\\n\\tstruct wq_barrier *barr = container_of(work, struct wq_barrier, work);\\n\\tcomplete(&barr->done);\\n}\\n\\n/**\\n * insert_wq_barrier - insert a barrier work\\n * @pwq: pwq to insert barrier into\\n * @barr: wq_barrier to insert\\n * @target: target work to attach @barr to\\n * @worker: worker currently executing @target, NULL if @target is not executing\\n *\\n * @barr is linked to @target such that @barr is completed only after\\n * @target finishes execution.  Please note that the ordering\\n * guarantee is observed only with respect to @target and on the local\\n * cpu.\\n *\\n * Currently, a queued barrier can\\'t be canceled.  This is because\\n * try_to_grab_pending() can\\'t determine whether the work to be\\n * grabbed is at the head of the queue and thus can\\'t clear LINKED\\n * flag of the previous work while there must be a valid next work\\n * after a work with LINKED flag set.\\n *\\n * Note that when @worker is non-NULL, @target may be modified\\n * underneath us, so we can\\'t reliably determine pwq from @target.\\n *\\n * CONTEXT:\\n * raw_spin_lock_irq(pool->lock).\\n */\\nstatic void insert_wq_barrier(struct pool_workqueue *pwq,\\n\\t\\t\\t      struct wq_barrier *barr,\\n\\t\\t\\t      struct work_struct *target, struct worker *worker)\\n{\\n\\tstatic __maybe_unused struct lock_class_key bh_key, thr_key;\\n\\tunsigned int work_flags = 0;\\n\\tunsigned int work_color;\\n\\tstruct list_head *head;\\n\\n\\t/*\\n\\t * debugobject calls are safe here even with pool->lock locked\\n\\t * as we know for sure that this will not trigger any of the\\n\\t * checks and call back into the fixup functions where we\\n\\t * might deadlock.\\n\\t *\\n\\t * BH and threaded workqueues need separate lockdep keys to avoid\\n\\t * spuriously triggering \"inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W}\\n\\t * usage\".\\n\\t */\\n\\tINIT_WORK_ONSTACK_KEY(&barr->work, wq_barrier_func,\\n\\t\\t\\t      (pwq->wq->flags & WQ_BH) ? &bh_key : &thr_key);\\n\\t__set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&barr->work));\\n\\n\\tinit_completion_map(&barr->done, &target->lockdep_map);\\n\\n\\tbarr->task = current;\\n\\n\\t/* The barrier work item does not participate in nr_active. */\\n\\twork_flags |= WORK_STRUCT_INACTIVE;\\n\\n\\t/*\\n\\t * If @target is currently being executed, schedule the\\n\\t * barrier to the worker; otherwise, put it after @target.\\n\\t */\\n\\tif (worker) {\\n\\t\\thead = worker->scheduled.next;\\n\\t\\twork_color = worker->current_color;\\n\\t} else {\\n\\t\\tunsigned long *bits = work_data_bits(target);\\n\\n\\t\\thead = target->entry.next;\\n\\t\\t/* there can already be other linked works, inherit and set */\\n\\t\\twork_flags |= *bits & WORK_STRUCT_LINKED;\\n\\t\\twork_color = get_work_color(*bits);\\n\\t\\t__set_bit(WORK_STRUCT_LINKED_BIT, bits);\\n\\t}\\n\\n\\tpwq->nr_in_flight[work_color]++;\\n\\twork_flags |= work_color_to_flags(work_color);\\n\\n\\tinsert_work(pwq, &barr->work, head, work_flags);\\n}\\n\\n/**\\n * flush_workqueue_prep_pwqs - prepare pwqs for workqueue flushing\\n * @wq: workqueue being flushed\\n * @flush_color: new flush color, < 0 for no-op\\n * @work_color: new work color, < 0 for no-op\\n *\\n * Prepare pwqs for workqueue flushing.\\n *\\n * If @flush_color is non-negative, flush_color on all pwqs should be\\n * -1.  If no pwq has in-flight commands at the specified color, all\\n * pwq->flush_color\\'s stay at -1 and %false is returned.  If any pwq\\n * has in flight commands, its pwq->flush_color is set to\\n * @flush_color, @wq->nr_pwqs_to_flush is updated accordingly, pwq\\n * wakeup logic is armed and %true is returned.\\n *\\n * The caller should have initialized @wq->first_flusher prior to\\n * calling this function with non-negative @flush_color.  If\\n * @flush_color is negative, no flush color update is done and %false\\n * is returned.\\n *\\n * If @work_color is non-negative, all pwqs should have the same\\n * work_color which is previous to @work_color and all will be\\n * advanced to @work_color.\\n *\\n * CONTEXT:\\n * mutex_lock(wq->mutex).\\n *\\n * Return:\\n * %true if @flush_color >= 0 and there\\'s something to flush.  %false\\n * otherwise.\\n */\\nstatic bool flush_workqueue_prep_pwqs(struct workqueue_struct *wq,\\n\\t\\t\\t\\t      int flush_color, int work_color)\\n{\\n\\tbool wait = false;\\n\\tstruct pool_workqueue *pwq;\\n\\tstruct worker_pool *current_pool = NULL;\\n\\n\\tif (flush_color >= 0) {\\n\\t\\tWARN_ON_ONCE(atomic_read(&wq->nr_pwqs_to_flush));\\n\\t\\tatomic_set(&wq->nr_pwqs_to_flush, 1);\\n\\t}\\n\\n\\t/*\\n\\t * For unbound workqueue, pwqs will map to only a few pools.\\n\\t * Most of the time, pwqs within the same pool will be linked\\n\\t * sequentially to wq->pwqs by cpu index. So in the majority\\n\\t * of pwq iters, the pool is the same, only doing lock/unlock\\n\\t * if the pool has changed. This can largely reduce expensive\\n\\t * lock operations.\\n\\t */\\n\\tfor_each_pwq(pwq, wq) {\\n\\t\\tif (current_pool != pwq->pool) {\\n\\t\\t\\tif (likely(current_pool))\\n\\t\\t\\t\\traw_spin_unlock_irq(&current_pool->lock);\\n\\t\\t\\tcurrent_pool = pwq->pool;\\n\\t\\t\\traw_spin_lock_irq(&current_pool->lock);\\n\\t\\t}\\n\\n\\t\\tif (flush_color >= 0) {\\n\\t\\t\\tWARN_ON_ONCE(pwq->flush_color != -1);\\n\\n\\t\\t\\tif (pwq->nr_in_flight[flush_color]) {\\n\\t\\t\\t\\tpwq->flush_color = flush_color;\\n\\t\\t\\t\\tatomic_inc(&wq->nr_pwqs_to_flush);\\n\\t\\t\\t\\twait = true;\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\tif (work_color >= 0) {\\n\\t\\t\\tWARN_ON_ONCE(work_color != work_next_color(pwq->work_color));\\n\\t\\t\\tpwq->work_color = work_color;\\n\\t\\t}\\n\\n\\t}\\n\\n\\tif (current_pool)\\n\\t\\traw_spin_unlock_irq(&current_pool->lock);\\n\\n\\tif (flush_color >= 0 && atomic_dec_and_test(&wq->nr_pwqs_to_flush))\\n\\t\\tcomplete(&wq->first_flusher->done);\\n\\n\\treturn wait;\\n}\\n\\nstatic void touch_wq_lockdep_map(struct workqueue_struct *wq)\\n{\\n#ifdef CONFIG_LOCKDEP\\n\\tif (unlikely(!wq->lockdep_map))\\n\\t\\treturn;\\n\\n\\tif (wq->flags & WQ_BH)\\n\\t\\tlocal_bh_disable();\\n\\n\\tlock_map_acquire(wq->lockdep_map);\\n\\tlock_map_release(wq->lockdep_map);\\n\\n\\tif (wq->flags & WQ_BH)\\n\\t\\tlocal_bh_enable();\\n#endif\\n}\\n\\nstatic void touch_work_lockdep_map(struct work_struct *work,\\n\\t\\t\\t\\t   struct workqueue_struct *wq)\\n{\\n#ifdef CONFIG_LOCKDEP\\n\\tif (wq->flags & WQ_BH)\\n\\t\\tlocal_bh_disable();\\n\\n\\tlock_map_acquire(&work->lockdep_map);\\n\\tlock_map_release(&work->lockdep_map);\\n\\n\\tif (wq->flags & WQ_BH)\\n\\t\\tlocal_bh_enable();\\n#endif\\n}\\n\\n/**\\n * __flush_workqueue - ensure that any scheduled work has run to completion.\\n * @wq: workqueue to flush\\n *\\n * This function sleeps until all work items which were queued on entry\\n * have finished execution, but it is not livelocked by new incoming ones.\\n */\\nvoid __flush_workqueue(struct workqueue_struct *wq)\\n{\\n\\tstruct wq_flusher this_flusher = {\\n\\t\\t.list = LIST_HEAD_INIT(this_flusher.list),\\n\\t\\t.flush_color = -1,\\n\\t\\t.done = COMPLETION_INITIALIZER_ONSTACK_MAP(this_flusher.done, (*wq->lockdep_map)),\\n\\t};\\n\\tint next_color;\\n\\n\\tif (WARN_ON(!wq_online))\\n\\t\\treturn;\\n\\n\\ttouch_wq_lockdep_map(wq);\\n\\n\\tmutex_lock(&wq->mutex);\\n\\n\\t/*\\n\\t * Start-to-wait phase\\n\\t */\\n\\tnext_color = work_next_color(wq->work_color);\\n\\n\\tif (next_color != wq->flush_color) {\\n\\t\\t/*\\n\\t\\t * Color space is not full.  The current work_color\\n\\t\\t * becomes our flush_color and work_color is advanced\\n\\t\\t * by one.\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow));\\n\\t\\tthis_flusher.flush_color = wq->work_color;\\n\\t\\twq->work_color = next_color;\\n\\n\\t\\tif (!wq->first_flusher) {\\n\\t\\t\\t/* no flush in progress, become the first flusher */\\n\\t\\t\\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\\n\\n\\t\\t\\twq->first_flusher = &this_flusher;\\n\\n\\t\\t\\tif (!flush_workqueue_prep_pwqs(wq, wq->flush_color,\\n\\t\\t\\t\\t\\t\\t       wq->work_color)) {\\n\\t\\t\\t\\t/* nothing to flush, done */\\n\\t\\t\\t\\twq->flush_color = next_color;\\n\\t\\t\\t\\twq->first_flusher = NULL;\\n\\t\\t\\t\\tgoto out_unlock;\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\t/* wait in queue */\\n\\t\\t\\tWARN_ON_ONCE(wq->flush_color == this_flusher.flush_color);\\n\\t\\t\\tlist_add_tail(&this_flusher.list, &wq->flusher_queue);\\n\\t\\t\\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\\n\\t\\t}\\n\\t} else {\\n\\t\\t/*\\n\\t\\t * Oops, color space is full, wait on overflow queue.\\n\\t\\t * The next flush completion will assign us\\n\\t\\t * flush_color and transfer to flusher_queue.\\n\\t\\t */\\n\\t\\tlist_add_tail(&this_flusher.list, &wq->flusher_overflow);\\n\\t}\\n\\n\\tcheck_flush_dependency(wq, NULL, false);\\n\\n\\tmutex_unlock(&wq->mutex);\\n\\n\\twait_for_completion(&this_flusher.done);\\n\\n\\t/*\\n\\t * Wake-up-and-cascade phase\\n\\t *\\n\\t * First flushers are responsible for cascading flushes and\\n\\t * handling overflow.  Non-first flushers can simply return.\\n\\t */\\n\\tif (READ_ONCE(wq->first_flusher) != &this_flusher)\\n\\t\\treturn;\\n\\n\\tmutex_lock(&wq->mutex);\\n\\n\\t/* we might have raced, check again with mutex held */\\n\\tif (wq->first_flusher != &this_flusher)\\n\\t\\tgoto out_unlock;\\n\\n\\tWRITE_ONCE(wq->first_flusher, NULL);\\n\\n\\tWARN_ON_ONCE(!list_empty(&this_flusher.list));\\n\\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\\n\\n\\twhile (true) {\\n\\t\\tstruct wq_flusher *next, *tmp;\\n\\n\\t\\t/* complete all the flushers sharing the current flush color */\\n\\t\\tlist_for_each_entry_safe(next, tmp, &wq->flusher_queue, list) {\\n\\t\\t\\tif (next->flush_color != wq->flush_color)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tlist_del_init(&next->list);\\n\\t\\t\\tcomplete(&next->done);\\n\\t\\t}\\n\\n\\t\\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow) &&\\n\\t\\t\\t     wq->flush_color != work_next_color(wq->work_color));\\n\\n\\t\\t/* this flush_color is finished, advance by one */\\n\\t\\twq->flush_color = work_next_color(wq->flush_color);\\n\\n\\t\\t/* one color has been freed, handle overflow queue */\\n\\t\\tif (!list_empty(&wq->flusher_overflow)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Assign the same color to all overflowed\\n\\t\\t\\t * flushers, advance work_color and append to\\n\\t\\t\\t * flusher_queue.  This is the start-to-wait\\n\\t\\t\\t * phase for these overflowed flushers.\\n\\t\\t\\t */\\n\\t\\t\\tlist_for_each_entry(tmp, &wq->flusher_overflow, list)\\n\\t\\t\\t\\ttmp->flush_color = wq->work_color;\\n\\n\\t\\t\\twq->work_color = work_next_color(wq->work_color);\\n\\n\\t\\t\\tlist_splice_tail_init(&wq->flusher_overflow,\\n\\t\\t\\t\\t\\t      &wq->flusher_queue);\\n\\t\\t\\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\\n\\t\\t}\\n\\n\\t\\tif (list_empty(&wq->flusher_queue)) {\\n\\t\\t\\tWARN_ON_ONCE(wq->flush_color != wq->work_color);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Need to flush more colors.  Make the next flusher\\n\\t\\t * the new first flusher and arm pwqs.\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(wq->flush_color == wq->work_color);\\n\\t\\tWARN_ON_ONCE(wq->flush_color != next->flush_color);\\n\\n\\t\\tlist_del_init(&next->list);\\n\\t\\twq->first_flusher = next;\\n\\n\\t\\tif (flush_workqueue_prep_pwqs(wq, wq->flush_color, -1))\\n\\t\\t\\tbreak;\\n\\n\\t\\t/*\\n\\t\\t * Meh... this color is already done, clear first\\n\\t\\t * flusher and repeat cascading.\\n\\t\\t */\\n\\t\\twq->first_flusher = NULL;\\n\\t}\\n\\nout_unlock:\\n\\tmutex_unlock(&wq->mutex);\\n}\\nEXPORT_SYMBOL(__flush_workqueue);\\n\\n/**\\n * drain_workqueue - drain a workqueue\\n * @wq: workqueue to drain\\n *\\n * Wait until the workqueue becomes empty.  While draining is in progress,\\n * only chain queueing is allowed.  IOW, only currently pending or running\\n * work items on @wq can queue further work items on it.  @wq is flushed\\n * repeatedly until it becomes empty.  The number of flushing is determined\\n * by the depth of chaining and should be relatively short.  Whine if it\\n * takes too long.\\n */\\nvoid drain_workqueue(struct workqueue_struct *wq)\\n{\\n\\tunsigned int flush_cnt = 0;\\n\\tstruct pool_workqueue *pwq;\\n\\n\\t/*\\n\\t * __queue_work() needs to test whether there are drainers, is much\\n\\t * hotter than drain_workqueue() and already looks at @wq->flags.\\n\\t * Use __WQ_DRAINING so that queue doesn\\'t have to check nr_drainers.\\n\\t */\\n\\tmutex_lock(&wq->mutex);\\n\\tif (!wq->nr_drainers++)\\n\\t\\twq->flags |= __WQ_DRAINING;\\n\\tmutex_unlock(&wq->mutex);\\nreflush:\\n\\t__flush_workqueue(wq);\\n\\n\\tmutex_lock(&wq->mutex);\\n\\n\\tfor_each_pwq(pwq, wq) {\\n\\t\\tbool drained;\\n\\n\\t\\traw_spin_lock_irq(&pwq->pool->lock);\\n\\t\\tdrained = pwq_is_empty(pwq);\\n\\t\\traw_spin_unlock_irq(&pwq->pool->lock);\\n\\n\\t\\tif (drained)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (++flush_cnt == 10 ||\\n\\t\\t    (flush_cnt % 100 == 0 && flush_cnt <= 1000))\\n\\t\\t\\tpr_warn(\"workqueue %s: %s() isn\\'t complete after %u tries\\\\n\",\\n\\t\\t\\t\\twq->name, __func__, flush_cnt);\\n\\n\\t\\tmutex_unlock(&wq->mutex);\\n\\t\\tgoto reflush;\\n\\t}\\n\\n\\tif (!--wq->nr_drainers)\\n\\t\\twq->flags &= ~__WQ_DRAINING;\\n\\tmutex_unlock(&wq->mutex);\\n}\\nEXPORT_SYMBOL_GPL(drain_workqueue);\\n\\nstatic bool start_flush_work(struct work_struct *work, struct wq_barrier *barr,\\n\\t\\t\\t     bool from_cancel)\\n{\\n\\tstruct worker *worker = NULL;\\n\\tstruct worker_pool *pool;\\n\\tstruct pool_workqueue *pwq;\\n\\tstruct workqueue_struct *wq;\\n\\n\\trcu_read_lock();\\n\\tpool = get_work_pool(work);\\n\\tif (!pool) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn false;\\n\\t}\\n\\n\\traw_spin_lock_irq(&pool->lock);\\n\\t/* see the comment in try_to_grab_pending() with the same code */\\n\\tpwq = get_work_pwq(work);\\n\\tif (pwq) {\\n\\t\\tif (unlikely(pwq->pool != pool))\\n\\t\\t\\tgoto already_gone;\\n\\t} else {\\n\\t\\tworker = find_worker_executing_work(pool, work);\\n\\t\\tif (!worker)\\n\\t\\t\\tgoto already_gone;\\n\\t\\tpwq = worker->current_pwq;\\n\\t}\\n\\n\\twq = pwq->wq;\\n\\tcheck_flush_dependency(wq, work, from_cancel);\\n\\n\\tinsert_wq_barrier(pwq, barr, work, worker);\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\ttouch_work_lockdep_map(work, wq);\\n\\n\\t/*\\n\\t * Force a lock recursion deadlock when using flush_work() inside a\\n\\t * single-threaded or rescuer equipped workqueue.\\n\\t *\\n\\t * For single threaded workqueues the deadlock happens when the work\\n\\t * is after the work issuing the flush_work(). For rescuer equipped\\n\\t * workqueues the deadlock happens when the rescuer stalls, blocking\\n\\t * forward progress.\\n\\t */\\n\\tif (!from_cancel && (wq->saved_max_active == 1 || wq->rescuer))\\n\\t\\ttouch_wq_lockdep_map(wq);\\n\\n\\trcu_read_unlock();\\n\\treturn true;\\nalready_gone:\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\trcu_read_unlock();\\n\\treturn false;\\n}\\n\\nstatic bool __flush_work(struct work_struct *work, bool from_cancel)\\n{\\n\\tstruct wq_barrier barr;\\n\\n\\tif (WARN_ON(!wq_online))\\n\\t\\treturn false;\\n\\n\\tif (WARN_ON(!work->func))\\n\\t\\treturn false;\\n\\n\\tif (!start_flush_work(work, &barr, from_cancel))\\n\\t\\treturn false;\\n\\n\\t/*\\n\\t * start_flush_work() returned %true. If @from_cancel is set, we know\\n\\t * that @work must have been executing during start_flush_work() and\\n\\t * can\\'t currently be queued. Its data must contain OFFQ bits. If @work\\n\\t * was queued on a BH workqueue, we also know that it was running in the\\n\\t * BH context and thus can be busy-waited.\\n\\t */\\n\\tif (from_cancel) {\\n\\t\\tunsigned long data = *work_data_bits(work);\\n\\n\\t\\tif (!WARN_ON_ONCE(data & WORK_STRUCT_PWQ) &&\\n\\t\\t    (data & WORK_OFFQ_BH)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * On RT, prevent a live lock when %current preempted\\n\\t\\t\\t * soft interrupt processing or prevents ksoftirqd from\\n\\t\\t\\t * running by keeping flipping BH. If the BH work item\\n\\t\\t\\t * runs on a different CPU then this has no effect other\\n\\t\\t\\t * than doing the BH disable/enable dance for nothing.\\n\\t\\t\\t * This is copied from\\n\\t\\t\\t * kernel/softirq.c::tasklet_unlock_spin_wait().\\n\\t\\t\\t */\\n\\t\\t\\twhile (!try_wait_for_completion(&barr.done)) {\\n\\t\\t\\t\\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {\\n\\t\\t\\t\\t\\tlocal_bh_disable();\\n\\t\\t\\t\\t\\tlocal_bh_enable();\\n\\t\\t\\t\\t} else {\\n\\t\\t\\t\\t\\tcpu_relax();\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\tgoto out_destroy;\\n\\t\\t}\\n\\t}\\n\\n\\twait_for_completion(&barr.done);\\n\\nout_destroy:\\n\\tdestroy_work_on_stack(&barr.work);\\n\\treturn true;\\n}\\n\\n/**\\n * flush_work - wait for a work to finish executing the last queueing instance\\n * @work: the work to flush\\n *\\n * Wait until @work has finished execution.  @work is guaranteed to be idle\\n * on return if it hasn\\'t been requeued since flush started.\\n *\\n * Return:\\n * %true if flush_work() waited for the work to finish execution,\\n * %false if it was already idle.\\n */\\nbool flush_work(struct work_struct *work)\\n{\\n\\tmight_sleep();\\n\\treturn __flush_work(work, false);\\n}\\nEXPORT_SYMBOL_GPL(flush_work);\\n\\n/**\\n * flush_delayed_work - wait for a dwork to finish executing the last queueing\\n * @dwork: the delayed work to flush\\n *\\n * Delayed timer is cancelled and the pending work is queued for\\n * immediate execution.  Like flush_work(), this function only\\n * considers the last queueing instance of @dwork.\\n *\\n * Return:\\n * %true if flush_work() waited for the work to finish execution,\\n * %false if it was already idle.\\n */\\nbool flush_delayed_work(struct delayed_work *dwork)\\n{\\n\\tlocal_irq_disable();\\n\\tif (del_timer_sync(&dwork->timer))\\n\\t\\t__queue_work(dwork->cpu, dwork->wq, &dwork->work);\\n\\tlocal_irq_enable();\\n\\treturn flush_work(&dwork->work);\\n}\\nEXPORT_SYMBOL(flush_delayed_work);\\n\\n/**\\n * flush_rcu_work - wait for a rwork to finish executing the last queueing\\n * @rwork: the rcu work to flush\\n *\\n * Return:\\n * %true if flush_rcu_work() waited for the work to finish execution,\\n * %false if it was already idle.\\n */\\nbool flush_rcu_work(struct rcu_work *rwork)\\n{\\n\\tif (test_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&rwork->work))) {\\n\\t\\trcu_barrier();\\n\\t\\tflush_work(&rwork->work);\\n\\t\\treturn true;\\n\\t} else {\\n\\t\\treturn flush_work(&rwork->work);\\n\\t}\\n}\\nEXPORT_SYMBOL(flush_rcu_work);\\n\\nstatic void work_offqd_disable(struct work_offq_data *offqd)\\n{\\n\\tconst unsigned long max = (1lu << WORK_OFFQ_DISABLE_BITS) - 1;\\n\\n\\tif (likely(offqd->disable < max))\\n\\t\\toffqd->disable++;\\n\\telse\\n\\t\\tWARN_ONCE(true, \"workqueue: work disable count overflowed\\\\n\");\\n}\\n\\nstatic void work_offqd_enable(struct work_offq_data *offqd)\\n{\\n\\tif (likely(offqd->disable > 0))\\n\\t\\toffqd->disable--;\\n\\telse\\n\\t\\tWARN_ONCE(true, \"workqueue: work disable count underflowed\\\\n\");\\n}\\n\\nstatic bool __cancel_work(struct work_struct *work, u32 cflags)\\n{\\n\\tstruct work_offq_data offqd;\\n\\tunsigned long irq_flags;\\n\\tint ret;\\n\\n\\tret = work_grab_pending(work, cflags, &irq_flags);\\n\\n\\twork_offqd_unpack(&offqd, *work_data_bits(work));\\n\\n\\tif (cflags & WORK_CANCEL_DISABLE)\\n\\t\\twork_offqd_disable(&offqd);\\n\\n\\tset_work_pool_and_clear_pending(work, offqd.pool_id,\\n\\t\\t\\t\\t\\twork_offqd_pack_flags(&offqd));\\n\\tlocal_irq_restore(irq_flags);\\n\\treturn ret;\\n}\\n\\nstatic bool __cancel_work_sync(struct work_struct *work, u32 cflags)\\n{\\n\\tbool ret;\\n\\n\\tret = __cancel_work(work, cflags | WORK_CANCEL_DISABLE);\\n\\n\\tif (*work_data_bits(work) & WORK_OFFQ_BH)\\n\\t\\tWARN_ON_ONCE(in_hardirq());\\n\\telse\\n\\t\\tmight_sleep();\\n\\n\\t/*\\n\\t * Skip __flush_work() during early boot when we know that @work isn\\'t\\n\\t * executing. This allows canceling during early boot.\\n\\t */\\n\\tif (wq_online)\\n\\t\\t__flush_work(work, true);\\n\\n\\tif (!(cflags & WORK_CANCEL_DISABLE))\\n\\t\\tenable_work(work);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * See cancel_delayed_work()\\n */\\nbool cancel_work(struct work_struct *work)\\n{\\n\\treturn __cancel_work(work, 0);\\n}\\nEXPORT_SYMBOL(cancel_work);\\n\\n/**\\n * cancel_work_sync - cancel a work and wait for it to finish\\n * @work: the work to cancel\\n *\\n * Cancel @work and wait for its execution to finish. This function can be used\\n * even if the work re-queues itself or migrates to another workqueue. On return\\n * from this function, @work is guaranteed to be not pending or executing on any\\n * CPU as long as there aren\\'t racing enqueues.\\n *\\n * cancel_work_sync(&delayed_work->work) must not be used for delayed_work\\'s.\\n * Use cancel_delayed_work_sync() instead.\\n *\\n * Must be called from a sleepable context if @work was last queued on a non-BH\\n * workqueue. Can also be called from non-hardirq atomic contexts including BH\\n * if @work was last queued on a BH workqueue.\\n *\\n * Returns %true if @work was pending, %false otherwise.\\n */\\nbool cancel_work_sync(struct work_struct *work)\\n{\\n\\treturn __cancel_work_sync(work, 0);\\n}\\nEXPORT_SYMBOL_GPL(cancel_work_sync);\\n\\n/**\\n * cancel_delayed_work - cancel a delayed work\\n * @dwork: delayed_work to cancel\\n *\\n * Kill off a pending delayed_work.\\n *\\n * Return: %true if @dwork was pending and canceled; %false if it wasn\\'t\\n * pending.\\n *\\n * Note:\\n * The work callback function may still be running on return, unless\\n * it returns %true and the work doesn\\'t re-arm itself.  Explicitly flush or\\n * use cancel_delayed_work_sync() to wait on it.\\n *\\n * This function is safe to call from any context including IRQ handler.\\n */\\nbool cancel_delayed_work(struct delayed_work *dwork)\\n{\\n\\treturn __cancel_work(&dwork->work, WORK_CANCEL_DELAYED);\\n}\\nEXPORT_SYMBOL(cancel_delayed_work);\\n\\n/**\\n * cancel_delayed_work_sync - cancel a delayed work and wait for it to finish\\n * @dwork: the delayed work cancel\\n *\\n * This is cancel_work_sync() for delayed works.\\n *\\n * Return:\\n * %true if @dwork was pending, %false otherwise.\\n */\\nbool cancel_delayed_work_sync(struct delayed_work *dwork)\\n{\\n\\treturn __cancel_work_sync(&dwork->work, WORK_CANCEL_DELAYED);\\n}\\nEXPORT_SYMBOL(cancel_delayed_work_sync);\\n\\n/**\\n * disable_work - Disable and cancel a work item\\n * @work: work item to disable\\n *\\n * Disable @work by incrementing its disable count and cancel it if currently\\n * pending. As long as the disable count is non-zero, any attempt to queue @work\\n * will fail and return %false. The maximum supported disable depth is 2 to the\\n * power of %WORK_OFFQ_DISABLE_BITS, currently 65536.\\n *\\n * Can be called from any context. Returns %true if @work was pending, %false\\n * otherwise.\\n */\\nbool disable_work(struct work_struct *work)\\n{\\n\\treturn __cancel_work(work, WORK_CANCEL_DISABLE);\\n}\\nEXPORT_SYMBOL_GPL(disable_work);\\n\\n/**\\n * disable_work_sync - Disable, cancel and drain a work item\\n * @work: work item to disable\\n *\\n * Similar to disable_work() but also wait for @work to finish if currently\\n * executing.\\n *\\n * Must be called from a sleepable context if @work was last queued on a non-BH\\n * workqueue. Can also be called from non-hardirq atomic contexts including BH\\n * if @work was last queued on a BH workqueue.\\n *\\n * Returns %true if @work was pending, %false otherwise.\\n */\\nbool disable_work_sync(struct work_struct *work)\\n{\\n\\treturn __cancel_work_sync(work, WORK_CANCEL_DISABLE);\\n}\\nEXPORT_SYMBOL_GPL(disable_work_sync);\\n\\n/**\\n * enable_work - Enable a work item\\n * @work: work item to enable\\n *\\n * Undo disable_work[_sync]() by decrementing @work\\'s disable count. @work can\\n * only be queued if its disable count is 0.\\n *\\n * Can be called from any context. Returns %true if the disable count reached 0.\\n * Otherwise, %false.\\n */\\nbool enable_work(struct work_struct *work)\\n{\\n\\tstruct work_offq_data offqd;\\n\\tunsigned long irq_flags;\\n\\n\\twork_grab_pending(work, 0, &irq_flags);\\n\\n\\twork_offqd_unpack(&offqd, *work_data_bits(work));\\n\\twork_offqd_enable(&offqd);\\n\\tset_work_pool_and_clear_pending(work, offqd.pool_id,\\n\\t\\t\\t\\t\\twork_offqd_pack_flags(&offqd));\\n\\tlocal_irq_restore(irq_flags);\\n\\n\\treturn !offqd.disable;\\n}\\nEXPORT_SYMBOL_GPL(enable_work);\\n\\n/**\\n * disable_delayed_work - Disable and cancel a delayed work item\\n * @dwork: delayed work item to disable\\n *\\n * disable_work() for delayed work items.\\n */\\nbool disable_delayed_work(struct delayed_work *dwork)\\n{\\n\\treturn __cancel_work(&dwork->work,\\n\\t\\t\\t     WORK_CANCEL_DELAYED | WORK_CANCEL_DISABLE);\\n}\\nEXPORT_SYMBOL_GPL(disable_delayed_work);\\n\\n/**\\n * disable_delayed_work_sync - Disable, cancel and drain a delayed work item\\n * @dwork: delayed work item to disable\\n *\\n * disable_work_sync() for delayed work items.\\n */\\nbool disable_delayed_work_sync(struct delayed_work *dwork)\\n{\\n\\treturn __cancel_work_sync(&dwork->work,\\n\\t\\t\\t\\t  WORK_CANCEL_DELAYED | WORK_CANCEL_DISABLE);\\n}\\nEXPORT_SYMBOL_GPL(disable_delayed_work_sync);\\n\\n/**\\n * enable_delayed_work - Enable a delayed work item\\n * @dwork: delayed work item to enable\\n *\\n * enable_work() for delayed work items.\\n */\\nbool enable_delayed_work(struct delayed_work *dwork)\\n{\\n\\treturn enable_work(&dwork->work);\\n}\\nEXPORT_SYMBOL_GPL(enable_delayed_work);\\n\\n/**\\n * schedule_on_each_cpu - execute a function synchronously on each online CPU\\n * @func: the function to call\\n *\\n * schedule_on_each_cpu() executes @func on each online CPU using the\\n * system workqueue and blocks until all CPUs have completed.\\n * schedule_on_each_cpu() is very slow.\\n *\\n * Return:\\n * 0 on success, -errno on failure.\\n */\\nint schedule_on_each_cpu(work_func_t func)\\n{\\n\\tint cpu;\\n\\tstruct work_struct __percpu *works;\\n\\n\\tworks = alloc_percpu(struct work_struct);\\n\\tif (!works)\\n\\t\\treturn -ENOMEM;\\n\\n\\tcpus_read_lock();\\n\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tstruct work_struct *work = per_cpu_ptr(works, cpu);\\n\\n\\t\\tINIT_WORK(work, func);\\n\\t\\tschedule_work_on(cpu, work);\\n\\t}\\n\\n\\tfor_each_online_cpu(cpu)\\n\\t\\tflush_work(per_cpu_ptr(works, cpu));\\n\\n\\tcpus_read_unlock();\\n\\tfree_percpu(works);\\n\\treturn 0;\\n}\\n\\n/**\\n * execute_in_process_context - reliably execute the routine with user context\\n * @fn:\\t\\tthe function to execute\\n * @ew:\\t\\tguaranteed storage for the execute work structure (must\\n *\\t\\tbe available when the work executes)\\n *\\n * Executes the function immediately if process context is available,\\n * otherwise schedules the function for delayed execution.\\n *\\n * Return:\\t0 - function was executed\\n *\\t\\t1 - function was scheduled for execution\\n */\\nint execute_in_process_context(work_func_t fn, struct execute_work *ew)\\n{\\n\\tif (!in_interrupt()) {\\n\\t\\tfn(&ew->work);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tINIT_WORK(&ew->work, fn);\\n\\tschedule_work(&ew->work);\\n\\n\\treturn 1;\\n}\\nEXPORT_SYMBOL_GPL(execute_in_process_context);\\n\\n/**\\n * free_workqueue_attrs - free a workqueue_attrs\\n * @attrs: workqueue_attrs to free\\n *\\n * Undo alloc_workqueue_attrs().\\n */\\nvoid free_workqueue_attrs(struct workqueue_attrs *attrs)\\n{\\n\\tif (attrs) {\\n\\t\\tfree_cpumask_var(attrs->cpumask);\\n\\t\\tfree_cpumask_var(attrs->__pod_cpumask);\\n\\t\\tkfree(attrs);\\n\\t}\\n}\\n\\n/**\\n * alloc_workqueue_attrs - allocate a workqueue_attrs\\n *\\n * Allocate a new workqueue_attrs, initialize with default settings and\\n * return it.\\n *\\n * Return: The allocated new workqueue_attr on success. %NULL on failure.\\n */\\nstruct workqueue_attrs *alloc_workqueue_attrs(void)\\n{\\n\\tstruct workqueue_attrs *attrs;\\n\\n\\tattrs = kzalloc(sizeof(*attrs), GFP_KERNEL);\\n\\tif (!attrs)\\n\\t\\tgoto fail;\\n\\tif (!alloc_cpumask_var(&attrs->cpumask, GFP_KERNEL))\\n\\t\\tgoto fail;\\n\\tif (!alloc_cpumask_var(&attrs->__pod_cpumask, GFP_KERNEL))\\n\\t\\tgoto fail;\\n\\n\\tcpumask_copy(attrs->cpumask, cpu_possible_mask);\\n\\tattrs->affn_scope = WQ_AFFN_DFL;\\n\\treturn attrs;\\nfail:\\n\\tfree_workqueue_attrs(attrs);\\n\\treturn NULL;\\n}\\n\\nstatic void copy_workqueue_attrs(struct workqueue_attrs *to,\\n\\t\\t\\t\\t const struct workqueue_attrs *from)\\n{\\n\\tto->nice = from->nice;\\n\\tcpumask_copy(to->cpumask, from->cpumask);\\n\\tcpumask_copy(to->__pod_cpumask, from->__pod_cpumask);\\n\\tto->affn_strict = from->affn_strict;\\n\\n\\t/*\\n\\t * Unlike hash and equality test, copying shouldn\\'t ignore wq-only\\n\\t * fields as copying is used for both pool and wq attrs. Instead,\\n\\t * get_unbound_pool() explicitly clears the fields.\\n\\t */\\n\\tto->affn_scope = from->affn_scope;\\n\\tto->ordered = from->ordered;\\n}\\n\\n/*\\n * Some attrs fields are workqueue-only. Clear them for worker_pool\\'s. See the\\n * comments in \\'struct workqueue_attrs\\' definition.\\n */\\nstatic void wqattrs_clear_for_pool(struct workqueue_attrs *attrs)\\n{\\n\\tattrs->affn_scope = WQ_AFFN_NR_TYPES;\\n\\tattrs->ordered = false;\\n\\tif (attrs->affn_strict)\\n\\t\\tcpumask_copy(attrs->cpumask, cpu_possible_mask);\\n}\\n\\n/* hash value of the content of @attr */\\nstatic u32 wqattrs_hash(const struct workqueue_attrs *attrs)\\n{\\n\\tu32 hash = 0;\\n\\n\\thash = jhash_1word(attrs->nice, hash);\\n\\thash = jhash_1word(attrs->affn_strict, hash);\\n\\thash = jhash(cpumask_bits(attrs->__pod_cpumask),\\n\\t\\t     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);\\n\\tif (!attrs->affn_strict)\\n\\t\\thash = jhash(cpumask_bits(attrs->cpumask),\\n\\t\\t\\t     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);\\n\\treturn hash;\\n}\\n\\n/* content equality test */\\nstatic bool wqattrs_equal(const struct workqueue_attrs *a,\\n\\t\\t\\t  const struct workqueue_attrs *b)\\n{\\n\\tif (a->nice != b->nice)\\n\\t\\treturn false;\\n\\tif (a->affn_strict != b->affn_strict)\\n\\t\\treturn false;\\n\\tif (!cpumask_equal(a->__pod_cpumask, b->__pod_cpumask))\\n\\t\\treturn false;\\n\\tif (!a->affn_strict && !cpumask_equal(a->cpumask, b->cpumask))\\n\\t\\treturn false;\\n\\treturn true;\\n}\\n\\n/* Update @attrs with actually available CPUs */\\nstatic void wqattrs_actualize_cpumask(struct workqueue_attrs *attrs,\\n\\t\\t\\t\\t      const cpumask_t *unbound_cpumask)\\n{\\n\\t/*\\n\\t * Calculate the effective CPU mask of @attrs given @unbound_cpumask. If\\n\\t * @attrs->cpumask doesn\\'t overlap with @unbound_cpumask, we fallback to\\n\\t * @unbound_cpumask.\\n\\t */\\n\\tcpumask_and(attrs->cpumask, attrs->cpumask, unbound_cpumask);\\n\\tif (unlikely(cpumask_empty(attrs->cpumask)))\\n\\t\\tcpumask_copy(attrs->cpumask, unbound_cpumask);\\n}\\n\\n/* find wq_pod_type to use for @attrs */\\nstatic const struct wq_pod_type *\\nwqattrs_pod_type(const struct workqueue_attrs *attrs)\\n{\\n\\tenum wq_affn_scope scope;\\n\\tstruct wq_pod_type *pt;\\n\\n\\t/* to synchronize access to wq_affn_dfl */\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tif (attrs->affn_scope == WQ_AFFN_DFL)\\n\\t\\tscope = wq_affn_dfl;\\n\\telse\\n\\t\\tscope = attrs->affn_scope;\\n\\n\\tpt = &wq_pod_types[scope];\\n\\n\\tif (!WARN_ON_ONCE(attrs->affn_scope == WQ_AFFN_NR_TYPES) &&\\n\\t    likely(pt->nr_pods))\\n\\t\\treturn pt;\\n\\n\\t/*\\n\\t * Before workqueue_init_topology(), only SYSTEM is available which is\\n\\t * initialized in workqueue_init_early().\\n\\t */\\n\\tpt = &wq_pod_types[WQ_AFFN_SYSTEM];\\n\\tBUG_ON(!pt->nr_pods);\\n\\treturn pt;\\n}\\n\\n/**\\n * init_worker_pool - initialize a newly zalloc\\'d worker_pool\\n * @pool: worker_pool to initialize\\n *\\n * Initialize a newly zalloc\\'d @pool.  It also allocates @pool->attrs.\\n *\\n * Return: 0 on success, -errno on failure.  Even on failure, all fields\\n * inside @pool proper are initialized and put_unbound_pool() can be called\\n * on @pool safely to release it.\\n */\\nstatic int init_worker_pool(struct worker_pool *pool)\\n{\\n\\traw_spin_lock_init(&pool->lock);\\n\\tpool->id = -1;\\n\\tpool->cpu = -1;\\n\\tpool->node = NUMA_NO_NODE;\\n\\tpool->flags |= POOL_DISASSOCIATED;\\n\\tpool->watchdog_ts = jiffies;\\n\\tINIT_LIST_HEAD(&pool->worklist);\\n\\tINIT_LIST_HEAD(&pool->idle_list);\\n\\thash_init(pool->busy_hash);\\n\\n\\ttimer_setup(&pool->idle_timer, idle_worker_timeout, TIMER_DEFERRABLE);\\n\\tINIT_WORK(&pool->idle_cull_work, idle_cull_fn);\\n\\n\\ttimer_setup(&pool->mayday_timer, pool_mayday_timeout, 0);\\n\\n\\tINIT_LIST_HEAD(&pool->workers);\\n\\n\\tida_init(&pool->worker_ida);\\n\\tINIT_HLIST_NODE(&pool->hash_node);\\n\\tpool->refcnt = 1;\\n\\n\\t/* shouldn\\'t fail above this point */\\n\\tpool->attrs = alloc_workqueue_attrs();\\n\\tif (!pool->attrs)\\n\\t\\treturn -ENOMEM;\\n\\n\\twqattrs_clear_for_pool(pool->attrs);\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_LOCKDEP\\nstatic void wq_init_lockdep(struct workqueue_struct *wq)\\n{\\n\\tchar *lock_name;\\n\\n\\tlockdep_register_key(&wq->key);\\n\\tlock_name = kasprintf(GFP_KERNEL, \"%s%s\", \"(wq_completion)\", wq->name);\\n\\tif (!lock_name)\\n\\t\\tlock_name = wq->name;\\n\\n\\twq->lock_name = lock_name;\\n\\twq->lockdep_map = &wq->__lockdep_map;\\n\\tlockdep_init_map(wq->lockdep_map, lock_name, &wq->key, 0);\\n}\\n\\nstatic void wq_unregister_lockdep(struct workqueue_struct *wq)\\n{\\n\\tif (wq->lockdep_map != &wq->__lockdep_map)\\n\\t\\treturn;\\n\\n\\tlockdep_unregister_key(&wq->key);\\n}\\n\\nstatic void wq_free_lockdep(struct workqueue_struct *wq)\\n{\\n\\tif (wq->lockdep_map != &wq->__lockdep_map)\\n\\t\\treturn;\\n\\n\\tif (wq->lock_name != wq->name)\\n\\t\\tkfree(wq->lock_name);\\n}\\n#else\\nstatic void wq_init_lockdep(struct workqueue_struct *wq)\\n{\\n}\\n\\nstatic void wq_unregister_lockdep(struct workqueue_struct *wq)\\n{\\n}\\n\\nstatic void wq_free_lockdep(struct workqueue_struct *wq)\\n{\\n}\\n#endif\\n\\nstatic void free_node_nr_active(struct wq_node_nr_active **nna_ar)\\n{\\n\\tint node;\\n\\n\\tfor_each_node(node) {\\n\\t\\tkfree(nna_ar[node]);\\n\\t\\tnna_ar[node] = NULL;\\n\\t}\\n\\n\\tkfree(nna_ar[nr_node_ids]);\\n\\tnna_ar[nr_node_ids] = NULL;\\n}\\n\\nstatic void init_node_nr_active(struct wq_node_nr_active *nna)\\n{\\n\\tnna->max = WQ_DFL_MIN_ACTIVE;\\n\\tatomic_set(&nna->nr, 0);\\n\\traw_spin_lock_init(&nna->lock);\\n\\tINIT_LIST_HEAD(&nna->pending_pwqs);\\n}\\n\\n/*\\n * Each node\\'s nr_active counter will be accessed mostly from its own node and\\n * should be allocated in the node.\\n */\\nstatic int alloc_node_nr_active(struct wq_node_nr_active **nna_ar)\\n{\\n\\tstruct wq_node_nr_active *nna;\\n\\tint node;\\n\\n\\tfor_each_node(node) {\\n\\t\\tnna = kzalloc_node(sizeof(*nna), GFP_KERNEL, node);\\n\\t\\tif (!nna)\\n\\t\\t\\tgoto err_free;\\n\\t\\tinit_node_nr_active(nna);\\n\\t\\tnna_ar[node] = nna;\\n\\t}\\n\\n\\t/* [nr_node_ids] is used as the fallback */\\n\\tnna = kzalloc_node(sizeof(*nna), GFP_KERNEL, NUMA_NO_NODE);\\n\\tif (!nna)\\n\\t\\tgoto err_free;\\n\\tinit_node_nr_active(nna);\\n\\tnna_ar[nr_node_ids] = nna;\\n\\n\\treturn 0;\\n\\nerr_free:\\n\\tfree_node_nr_active(nna_ar);\\n\\treturn -ENOMEM;\\n}\\n\\nstatic void rcu_free_wq(struct rcu_head *rcu)\\n{\\n\\tstruct workqueue_struct *wq =\\n\\t\\tcontainer_of(rcu, struct workqueue_struct, rcu);\\n\\n\\tif (wq->flags & WQ_UNBOUND)\\n\\t\\tfree_node_nr_active(wq->node_nr_active);\\n\\n\\twq_free_lockdep(wq);\\n\\tfree_percpu(wq->cpu_pwq);\\n\\tfree_workqueue_attrs(wq->unbound_attrs);\\n\\tkfree(wq);\\n}\\n\\nstatic void rcu_free_pool(struct rcu_head *rcu)\\n{\\n\\tstruct worker_pool *pool = container_of(rcu, struct worker_pool, rcu);\\n\\n\\tida_destroy(&pool->worker_ida);\\n\\tfree_workqueue_attrs(pool->attrs);\\n\\tkfree(pool);\\n}\\n\\n/**\\n * put_unbound_pool - put a worker_pool\\n * @pool: worker_pool to put\\n *\\n * Put @pool.  If its refcnt reaches zero, it gets destroyed in RCU\\n * safe manner.  get_unbound_pool() calls this function on its failure path\\n * and this function should be able to release pools which went through,\\n * successfully or not, init_worker_pool().\\n *\\n * Should be called with wq_pool_mutex held.\\n */\\nstatic void put_unbound_pool(struct worker_pool *pool)\\n{\\n\\tstruct worker *worker;\\n\\tLIST_HEAD(cull_list);\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tif (--pool->refcnt)\\n\\t\\treturn;\\n\\n\\t/* sanity checks */\\n\\tif (WARN_ON(!(pool->cpu < 0)) ||\\n\\t    WARN_ON(!list_empty(&pool->worklist)))\\n\\t\\treturn;\\n\\n\\t/* release id and unhash */\\n\\tif (pool->id >= 0)\\n\\t\\tidr_remove(&worker_pool_idr, pool->id);\\n\\thash_del(&pool->hash_node);\\n\\n\\t/*\\n\\t * Become the manager and destroy all workers.  This prevents\\n\\t * @pool\\'s workers from blocking on attach_mutex.  We\\'re the last\\n\\t * manager and @pool gets freed with the flag set.\\n\\t *\\n\\t * Having a concurrent manager is quite unlikely to happen as we can\\n\\t * only get here with\\n\\t *   pwq->refcnt == pool->refcnt == 0\\n\\t * which implies no work queued to the pool, which implies no worker can\\n\\t * become the manager. However a worker could have taken the role of\\n\\t * manager before the refcnts dropped to 0, since maybe_create_worker()\\n\\t * drops pool->lock\\n\\t */\\n\\twhile (true) {\\n\\t\\trcuwait_wait_event(&manager_wait,\\n\\t\\t\\t\\t   !(pool->flags & POOL_MANAGER_ACTIVE),\\n\\t\\t\\t\\t   TASK_UNINTERRUPTIBLE);\\n\\n\\t\\tmutex_lock(&wq_pool_attach_mutex);\\n\\t\\traw_spin_lock_irq(&pool->lock);\\n\\t\\tif (!(pool->flags & POOL_MANAGER_ACTIVE)) {\\n\\t\\t\\tpool->flags |= POOL_MANAGER_ACTIVE;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\traw_spin_unlock_irq(&pool->lock);\\n\\t\\tmutex_unlock(&wq_pool_attach_mutex);\\n\\t}\\n\\n\\twhile ((worker = first_idle_worker(pool)))\\n\\t\\tset_worker_dying(worker, &cull_list);\\n\\tWARN_ON(pool->nr_workers || pool->nr_idle);\\n\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\tdetach_dying_workers(&cull_list);\\n\\n\\tmutex_unlock(&wq_pool_attach_mutex);\\n\\n\\treap_dying_workers(&cull_list);\\n\\n\\t/* shut down the timers */\\n\\tdel_timer_sync(&pool->idle_timer);\\n\\tcancel_work_sync(&pool->idle_cull_work);\\n\\tdel_timer_sync(&pool->mayday_timer);\\n\\n\\t/* RCU protected to allow dereferences from get_work_pool() */\\n\\tcall_rcu(&pool->rcu, rcu_free_pool);\\n}\\n\\n/**\\n * get_unbound_pool - get a worker_pool with the specified attributes\\n * @attrs: the attributes of the worker_pool to get\\n *\\n * Obtain a worker_pool which has the same attributes as @attrs, bump the\\n * reference count and return it.  If there already is a matching\\n * worker_pool, it will be used; otherwise, this function attempts to\\n * create a new one.\\n *\\n * Should be called with wq_pool_mutex held.\\n *\\n * Return: On success, a worker_pool with the same attributes as @attrs.\\n * On failure, %NULL.\\n */\\nstatic struct worker_pool *get_unbound_pool(const struct workqueue_attrs *attrs)\\n{\\n\\tstruct wq_pod_type *pt = &wq_pod_types[WQ_AFFN_NUMA];\\n\\tu32 hash = wqattrs_hash(attrs);\\n\\tstruct worker_pool *pool;\\n\\tint pod, node = NUMA_NO_NODE;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\t/* do we already have a matching pool? */\\n\\thash_for_each_possible(unbound_pool_hash, pool, hash_node, hash) {\\n\\t\\tif (wqattrs_equal(pool->attrs, attrs)) {\\n\\t\\t\\tpool->refcnt++;\\n\\t\\t\\treturn pool;\\n\\t\\t}\\n\\t}\\n\\n\\t/* If __pod_cpumask is contained inside a NUMA pod, that\\'s our node */\\n\\tfor (pod = 0; pod < pt->nr_pods; pod++) {\\n\\t\\tif (cpumask_subset(attrs->__pod_cpumask, pt->pod_cpus[pod])) {\\n\\t\\t\\tnode = pt->pod_node[pod];\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\t/* nope, create a new one */\\n\\tpool = kzalloc_node(sizeof(*pool), GFP_KERNEL, node);\\n\\tif (!pool || init_worker_pool(pool) < 0)\\n\\t\\tgoto fail;\\n\\n\\tpool->node = node;\\n\\tcopy_workqueue_attrs(pool->attrs, attrs);\\n\\twqattrs_clear_for_pool(pool->attrs);\\n\\n\\tif (worker_pool_assign_id(pool) < 0)\\n\\t\\tgoto fail;\\n\\n\\t/* create and start the initial worker */\\n\\tif (wq_online && !create_worker(pool))\\n\\t\\tgoto fail;\\n\\n\\t/* install */\\n\\thash_add(unbound_pool_hash, &pool->hash_node, hash);\\n\\n\\treturn pool;\\nfail:\\n\\tif (pool)\\n\\t\\tput_unbound_pool(pool);\\n\\treturn NULL;\\n}\\n\\n/*\\n * Scheduled on pwq_release_worker by put_pwq() when an unbound pwq hits zero\\n * refcnt and needs to be destroyed.\\n */\\nstatic void pwq_release_workfn(struct kthread_work *work)\\n{\\n\\tstruct pool_workqueue *pwq = container_of(work, struct pool_workqueue,\\n\\t\\t\\t\\t\\t\\t  release_work);\\n\\tstruct workqueue_struct *wq = pwq->wq;\\n\\tstruct worker_pool *pool = pwq->pool;\\n\\tbool is_last = false;\\n\\n\\t/*\\n\\t * When @pwq is not linked, it doesn\\'t hold any reference to the\\n\\t * @wq, and @wq is invalid to access.\\n\\t */\\n\\tif (!list_empty(&pwq->pwqs_node)) {\\n\\t\\tmutex_lock(&wq->mutex);\\n\\t\\tlist_del_rcu(&pwq->pwqs_node);\\n\\t\\tis_last = list_empty(&wq->pwqs);\\n\\n\\t\\t/*\\n\\t\\t * For ordered workqueue with a plugged dfl_pwq, restart it now.\\n\\t\\t */\\n\\t\\tif (!is_last && (wq->flags & __WQ_ORDERED))\\n\\t\\t\\tunplug_oldest_pwq(wq);\\n\\n\\t\\tmutex_unlock(&wq->mutex);\\n\\t}\\n\\n\\tif (wq->flags & WQ_UNBOUND) {\\n\\t\\tmutex_lock(&wq_pool_mutex);\\n\\t\\tput_unbound_pool(pool);\\n\\t\\tmutex_unlock(&wq_pool_mutex);\\n\\t}\\n\\n\\tif (!list_empty(&pwq->pending_node)) {\\n\\t\\tstruct wq_node_nr_active *nna =\\n\\t\\t\\twq_node_nr_active(pwq->wq, pwq->pool->node);\\n\\n\\t\\traw_spin_lock_irq(&nna->lock);\\n\\t\\tlist_del_init(&pwq->pending_node);\\n\\t\\traw_spin_unlock_irq(&nna->lock);\\n\\t}\\n\\n\\tkfree_rcu(pwq, rcu);\\n\\n\\t/*\\n\\t * If we\\'re the last pwq going away, @wq is already dead and no one\\n\\t * is gonna access it anymore.  Schedule RCU free.\\n\\t */\\n\\tif (is_last) {\\n\\t\\twq_unregister_lockdep(wq);\\n\\t\\tcall_rcu(&wq->rcu, rcu_free_wq);\\n\\t}\\n}\\n\\n/* initialize newly allocated @pwq which is associated with @wq and @pool */\\nstatic void init_pwq(struct pool_workqueue *pwq, struct workqueue_struct *wq,\\n\\t\\t     struct worker_pool *pool)\\n{\\n\\tBUG_ON((unsigned long)pwq & ~WORK_STRUCT_PWQ_MASK);\\n\\n\\tmemset(pwq, 0, sizeof(*pwq));\\n\\n\\tpwq->pool = pool;\\n\\tpwq->wq = wq;\\n\\tpwq->flush_color = -1;\\n\\tpwq->refcnt = 1;\\n\\tINIT_LIST_HEAD(&pwq->inactive_works);\\n\\tINIT_LIST_HEAD(&pwq->pending_node);\\n\\tINIT_LIST_HEAD(&pwq->pwqs_node);\\n\\tINIT_LIST_HEAD(&pwq->mayday_node);\\n\\tkthread_init_work(&pwq->release_work, pwq_release_workfn);\\n}\\n\\n/* sync @pwq with the current state of its associated wq and link it */\\nstatic void link_pwq(struct pool_workqueue *pwq)\\n{\\n\\tstruct workqueue_struct *wq = pwq->wq;\\n\\n\\tlockdep_assert_held(&wq->mutex);\\n\\n\\t/* may be called multiple times, ignore if already linked */\\n\\tif (!list_empty(&pwq->pwqs_node))\\n\\t\\treturn;\\n\\n\\t/* set the matching work_color */\\n\\tpwq->work_color = wq->work_color;\\n\\n\\t/* link in @pwq */\\n\\tlist_add_tail_rcu(&pwq->pwqs_node, &wq->pwqs);\\n}\\n\\n/* obtain a pool matching @attr and create a pwq associating the pool and @wq */\\nstatic struct pool_workqueue *alloc_unbound_pwq(struct workqueue_struct *wq,\\n\\t\\t\\t\\t\\tconst struct workqueue_attrs *attrs)\\n{\\n\\tstruct worker_pool *pool;\\n\\tstruct pool_workqueue *pwq;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tpool = get_unbound_pool(attrs);\\n\\tif (!pool)\\n\\t\\treturn NULL;\\n\\n\\tpwq = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL, pool->node);\\n\\tif (!pwq) {\\n\\t\\tput_unbound_pool(pool);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tinit_pwq(pwq, wq, pool);\\n\\treturn pwq;\\n}\\n\\nstatic void apply_wqattrs_lock(void)\\n{\\n\\tmutex_lock(&wq_pool_mutex);\\n}\\n\\nstatic void apply_wqattrs_unlock(void)\\n{\\n\\tmutex_unlock(&wq_pool_mutex);\\n}\\n\\n/**\\n * wq_calc_pod_cpumask - calculate a wq_attrs\\' cpumask for a pod\\n * @attrs: the wq_attrs of the default pwq of the target workqueue\\n * @cpu: the target CPU\\n *\\n * Calculate the cpumask a workqueue with @attrs should use on @pod.\\n * The result is stored in @attrs->__pod_cpumask.\\n *\\n * If pod affinity is not enabled, @attrs->cpumask is always used. If enabled\\n * and @pod has online CPUs requested by @attrs, the returned cpumask is the\\n * intersection of the possible CPUs of @pod and @attrs->cpumask.\\n *\\n * The caller is responsible for ensuring that the cpumask of @pod stays stable.\\n */\\nstatic void wq_calc_pod_cpumask(struct workqueue_attrs *attrs, int cpu)\\n{\\n\\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);\\n\\tint pod = pt->cpu_pod[cpu];\\n\\n\\t/* calculate possible CPUs in @pod that @attrs wants */\\n\\tcpumask_and(attrs->__pod_cpumask, pt->pod_cpus[pod], attrs->cpumask);\\n\\t/* does @pod have any online CPUs @attrs wants? */\\n\\tif (!cpumask_intersects(attrs->__pod_cpumask, wq_online_cpumask)) {\\n\\t\\tcpumask_copy(attrs->__pod_cpumask, attrs->cpumask);\\n\\t\\treturn;\\n\\t}\\n}\\n\\n/* install @pwq into @wq and return the old pwq, @cpu < 0 for dfl_pwq */\\nstatic struct pool_workqueue *install_unbound_pwq(struct workqueue_struct *wq,\\n\\t\\t\\t\\t\\tint cpu, struct pool_workqueue *pwq)\\n{\\n\\tstruct pool_workqueue __rcu **slot = unbound_pwq_slot(wq, cpu);\\n\\tstruct pool_workqueue *old_pwq;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\tlockdep_assert_held(&wq->mutex);\\n\\n\\t/* link_pwq() can handle duplicate calls */\\n\\tlink_pwq(pwq);\\n\\n\\told_pwq = rcu_access_pointer(*slot);\\n\\trcu_assign_pointer(*slot, pwq);\\n\\treturn old_pwq;\\n}\\n\\n/* context to store the prepared attrs & pwqs before applying */\\nstruct apply_wqattrs_ctx {\\n\\tstruct workqueue_struct\\t*wq;\\t\\t/* target workqueue */\\n\\tstruct workqueue_attrs\\t*attrs;\\t\\t/* attrs to apply */\\n\\tstruct list_head\\tlist;\\t\\t/* queued for batching commit */\\n\\tstruct pool_workqueue\\t*dfl_pwq;\\n\\tstruct pool_workqueue\\t*pwq_tbl[];\\n};\\n\\n/* free the resources after success or abort */\\nstatic void apply_wqattrs_cleanup(struct apply_wqattrs_ctx *ctx)\\n{\\n\\tif (ctx) {\\n\\t\\tint cpu;\\n\\n\\t\\tfor_each_possible_cpu(cpu)\\n\\t\\t\\tput_pwq_unlocked(ctx->pwq_tbl[cpu]);\\n\\t\\tput_pwq_unlocked(ctx->dfl_pwq);\\n\\n\\t\\tfree_workqueue_attrs(ctx->attrs);\\n\\n\\t\\tkfree(ctx);\\n\\t}\\n}\\n\\n/* allocate the attrs and pwqs for later installation */\\nstatic struct apply_wqattrs_ctx *\\napply_wqattrs_prepare(struct workqueue_struct *wq,\\n\\t\\t      const struct workqueue_attrs *attrs,\\n\\t\\t      const cpumask_var_t unbound_cpumask)\\n{\\n\\tstruct apply_wqattrs_ctx *ctx;\\n\\tstruct workqueue_attrs *new_attrs;\\n\\tint cpu;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tif (WARN_ON(attrs->affn_scope < 0 ||\\n\\t\\t    attrs->affn_scope >= WQ_AFFN_NR_TYPES))\\n\\t\\treturn ERR_PTR(-EINVAL);\\n\\n\\tctx = kzalloc(struct_size(ctx, pwq_tbl, nr_cpu_ids), GFP_KERNEL);\\n\\n\\tnew_attrs = alloc_workqueue_attrs();\\n\\tif (!ctx || !new_attrs)\\n\\t\\tgoto out_free;\\n\\n\\t/*\\n\\t * If something goes wrong during CPU up/down, we\\'ll fall back to\\n\\t * the default pwq covering whole @attrs->cpumask.  Always create\\n\\t * it even if we don\\'t use it immediately.\\n\\t */\\n\\tcopy_workqueue_attrs(new_attrs, attrs);\\n\\twqattrs_actualize_cpumask(new_attrs, unbound_cpumask);\\n\\tcpumask_copy(new_attrs->__pod_cpumask, new_attrs->cpumask);\\n\\tctx->dfl_pwq = alloc_unbound_pwq(wq, new_attrs);\\n\\tif (!ctx->dfl_pwq)\\n\\t\\tgoto out_free;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tif (new_attrs->ordered) {\\n\\t\\t\\tctx->dfl_pwq->refcnt++;\\n\\t\\t\\tctx->pwq_tbl[cpu] = ctx->dfl_pwq;\\n\\t\\t} else {\\n\\t\\t\\twq_calc_pod_cpumask(new_attrs, cpu);\\n\\t\\t\\tctx->pwq_tbl[cpu] = alloc_unbound_pwq(wq, new_attrs);\\n\\t\\t\\tif (!ctx->pwq_tbl[cpu])\\n\\t\\t\\t\\tgoto out_free;\\n\\t\\t}\\n\\t}\\n\\n\\t/* save the user configured attrs and sanitize it. */\\n\\tcopy_workqueue_attrs(new_attrs, attrs);\\n\\tcpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);\\n\\tcpumask_copy(new_attrs->__pod_cpumask, new_attrs->cpumask);\\n\\tctx->attrs = new_attrs;\\n\\n\\t/*\\n\\t * For initialized ordered workqueues, there should only be one pwq\\n\\t * (dfl_pwq). Set the plugged flag of ctx->dfl_pwq to suspend execution\\n\\t * of newly queued work items until execution of older work items in\\n\\t * the old pwq\\'s have completed.\\n\\t */\\n\\tif ((wq->flags & __WQ_ORDERED) && !list_empty(&wq->pwqs))\\n\\t\\tctx->dfl_pwq->plugged = true;\\n\\n\\tctx->wq = wq;\\n\\treturn ctx;\\n\\nout_free:\\n\\tfree_workqueue_attrs(new_attrs);\\n\\tapply_wqattrs_cleanup(ctx);\\n\\treturn ERR_PTR(-ENOMEM);\\n}\\n\\n/* set attrs and install prepared pwqs, @ctx points to old pwqs on return */\\nstatic void apply_wqattrs_commit(struct apply_wqattrs_ctx *ctx)\\n{\\n\\tint cpu;\\n\\n\\t/* all pwqs have been created successfully, let\\'s install\\'em */\\n\\tmutex_lock(&ctx->wq->mutex);\\n\\n\\tcopy_workqueue_attrs(ctx->wq->unbound_attrs, ctx->attrs);\\n\\n\\t/* save the previous pwqs and install the new ones */\\n\\tfor_each_possible_cpu(cpu)\\n\\t\\tctx->pwq_tbl[cpu] = install_unbound_pwq(ctx->wq, cpu,\\n\\t\\t\\t\\t\\t\\t\\tctx->pwq_tbl[cpu]);\\n\\tctx->dfl_pwq = install_unbound_pwq(ctx->wq, -1, ctx->dfl_pwq);\\n\\n\\t/* update node_nr_active->max */\\n\\twq_update_node_max_active(ctx->wq, -1);\\n\\n\\t/* rescuer needs to respect wq cpumask changes */\\n\\tif (ctx->wq->rescuer)\\n\\t\\tset_cpus_allowed_ptr(ctx->wq->rescuer->task,\\n\\t\\t\\t\\t     unbound_effective_cpumask(ctx->wq));\\n\\n\\tmutex_unlock(&ctx->wq->mutex);\\n}\\n\\nstatic int apply_workqueue_attrs_locked(struct workqueue_struct *wq,\\n\\t\\t\\t\\t\\tconst struct workqueue_attrs *attrs)\\n{\\n\\tstruct apply_wqattrs_ctx *ctx;\\n\\n\\t/* only unbound workqueues can change attributes */\\n\\tif (WARN_ON(!(wq->flags & WQ_UNBOUND)))\\n\\t\\treturn -EINVAL;\\n\\n\\tctx = apply_wqattrs_prepare(wq, attrs, wq_unbound_cpumask);\\n\\tif (IS_ERR(ctx))\\n\\t\\treturn PTR_ERR(ctx);\\n\\n\\t/* the ctx has been prepared successfully, let\\'s commit it */\\n\\tapply_wqattrs_commit(ctx);\\n\\tapply_wqattrs_cleanup(ctx);\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * apply_workqueue_attrs - apply new workqueue_attrs to an unbound workqueue\\n * @wq: the target workqueue\\n * @attrs: the workqueue_attrs to apply, allocated with alloc_workqueue_attrs()\\n *\\n * Apply @attrs to an unbound workqueue @wq. Unless disabled, this function maps\\n * a separate pwq to each CPU pod with possibles CPUs in @attrs->cpumask so that\\n * work items are affine to the pod it was issued on. Older pwqs are released as\\n * in-flight work items finish. Note that a work item which repeatedly requeues\\n * itself back-to-back will stay on its current pwq.\\n *\\n * Performs GFP_KERNEL allocations.\\n *\\n * Return: 0 on success and -errno on failure.\\n */\\nint apply_workqueue_attrs(struct workqueue_struct *wq,\\n\\t\\t\\t  const struct workqueue_attrs *attrs)\\n{\\n\\tint ret;\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\tret = apply_workqueue_attrs_locked(wq, attrs);\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\n\\treturn ret;\\n}\\n\\n/**\\n * unbound_wq_update_pwq - update a pwq slot for CPU hot[un]plug\\n * @wq: the target workqueue\\n * @cpu: the CPU to update the pwq slot for\\n *\\n * This function is to be called from %CPU_DOWN_PREPARE, %CPU_ONLINE and\\n * %CPU_DOWN_FAILED.  @cpu is in the same pod of the CPU being hot[un]plugged.\\n *\\n *\\n * If pod affinity can\\'t be adjusted due to memory allocation failure, it falls\\n * back to @wq->dfl_pwq which may not be optimal but is always correct.\\n *\\n * Note that when the last allowed CPU of a pod goes offline for a workqueue\\n * with a cpumask spanning multiple pods, the workers which were already\\n * executing the work items for the workqueue will lose their CPU affinity and\\n * may execute on any CPU. This is similar to how per-cpu workqueues behave on\\n * CPU_DOWN. If a workqueue user wants strict affinity, it\\'s the user\\'s\\n * responsibility to flush the work item from CPU_DOWN_PREPARE.\\n */\\nstatic void unbound_wq_update_pwq(struct workqueue_struct *wq, int cpu)\\n{\\n\\tstruct pool_workqueue *old_pwq = NULL, *pwq;\\n\\tstruct workqueue_attrs *target_attrs;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tif (!(wq->flags & WQ_UNBOUND) || wq->unbound_attrs->ordered)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * We don\\'t wanna alloc/free wq_attrs for each wq for each CPU.\\n\\t * Let\\'s use a preallocated one.  The following buf is protected by\\n\\t * CPU hotplug exclusion.\\n\\t */\\n\\ttarget_attrs = unbound_wq_update_pwq_attrs_buf;\\n\\n\\tcopy_workqueue_attrs(target_attrs, wq->unbound_attrs);\\n\\twqattrs_actualize_cpumask(target_attrs, wq_unbound_cpumask);\\n\\n\\t/* nothing to do if the target cpumask matches the current pwq */\\n\\twq_calc_pod_cpumask(target_attrs, cpu);\\n\\tif (wqattrs_equal(target_attrs, unbound_pwq(wq, cpu)->pool->attrs))\\n\\t\\treturn;\\n\\n\\t/* create a new pwq */\\n\\tpwq = alloc_unbound_pwq(wq, target_attrs);\\n\\tif (!pwq) {\\n\\t\\tpr_warn(\"workqueue: allocation failed while updating CPU pod affinity of \\\\\"%s\\\\\"\\\\n\",\\n\\t\\t\\twq->name);\\n\\t\\tgoto use_dfl_pwq;\\n\\t}\\n\\n\\t/* Install the new pwq. */\\n\\tmutex_lock(&wq->mutex);\\n\\told_pwq = install_unbound_pwq(wq, cpu, pwq);\\n\\tgoto out_unlock;\\n\\nuse_dfl_pwq:\\n\\tmutex_lock(&wq->mutex);\\n\\tpwq = unbound_pwq(wq, -1);\\n\\traw_spin_lock_irq(&pwq->pool->lock);\\n\\tget_pwq(pwq);\\n\\traw_spin_unlock_irq(&pwq->pool->lock);\\n\\told_pwq = install_unbound_pwq(wq, cpu, pwq);\\nout_unlock:\\n\\tmutex_unlock(&wq->mutex);\\n\\tput_pwq_unlocked(old_pwq);\\n}\\n\\nstatic int alloc_and_link_pwqs(struct workqueue_struct *wq)\\n{\\n\\tbool highpri = wq->flags & WQ_HIGHPRI;\\n\\tint cpu, ret;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\twq->cpu_pwq = alloc_percpu(struct pool_workqueue *);\\n\\tif (!wq->cpu_pwq)\\n\\t\\tgoto enomem;\\n\\n\\tif (!(wq->flags & WQ_UNBOUND)) {\\n\\t\\tstruct worker_pool __percpu *pools;\\n\\n\\t\\tif (wq->flags & WQ_BH)\\n\\t\\t\\tpools = bh_worker_pools;\\n\\t\\telse\\n\\t\\t\\tpools = cpu_worker_pools;\\n\\n\\t\\tfor_each_possible_cpu(cpu) {\\n\\t\\t\\tstruct pool_workqueue **pwq_p;\\n\\t\\t\\tstruct worker_pool *pool;\\n\\n\\t\\t\\tpool = &(per_cpu_ptr(pools, cpu)[highpri]);\\n\\t\\t\\tpwq_p = per_cpu_ptr(wq->cpu_pwq, cpu);\\n\\n\\t\\t\\t*pwq_p = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL,\\n\\t\\t\\t\\t\\t\\t       pool->node);\\n\\t\\t\\tif (!*pwq_p)\\n\\t\\t\\t\\tgoto enomem;\\n\\n\\t\\t\\tinit_pwq(*pwq_p, wq, pool);\\n\\n\\t\\t\\tmutex_lock(&wq->mutex);\\n\\t\\t\\tlink_pwq(*pwq_p);\\n\\t\\t\\tmutex_unlock(&wq->mutex);\\n\\t\\t}\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (wq->flags & __WQ_ORDERED) {\\n\\t\\tstruct pool_workqueue *dfl_pwq;\\n\\n\\t\\tret = apply_workqueue_attrs_locked(wq, ordered_wq_attrs[highpri]);\\n\\t\\t/* there should only be single pwq for ordering guarantee */\\n\\t\\tdfl_pwq = rcu_access_pointer(wq->dfl_pwq);\\n\\t\\tWARN(!ret && (wq->pwqs.next != &dfl_pwq->pwqs_node ||\\n\\t\\t\\t      wq->pwqs.prev != &dfl_pwq->pwqs_node),\\n\\t\\t     \"ordering guarantee broken for workqueue %s\\\\n\", wq->name);\\n\\t} else {\\n\\t\\tret = apply_workqueue_attrs_locked(wq, unbound_std_wq_attrs[highpri]);\\n\\t}\\n\\n\\treturn ret;\\n\\nenomem:\\n\\tif (wq->cpu_pwq) {\\n\\t\\tfor_each_possible_cpu(cpu) {\\n\\t\\t\\tstruct pool_workqueue *pwq = *per_cpu_ptr(wq->cpu_pwq, cpu);\\n\\n\\t\\t\\tif (pwq)\\n\\t\\t\\t\\tkmem_cache_free(pwq_cache, pwq);\\n\\t\\t}\\n\\t\\tfree_percpu(wq->cpu_pwq);\\n\\t\\twq->cpu_pwq = NULL;\\n\\t}\\n\\treturn -ENOMEM;\\n}\\n\\nstatic int wq_clamp_max_active(int max_active, unsigned int flags,\\n\\t\\t\\t       const char *name)\\n{\\n\\tif (max_active < 1 || max_active > WQ_MAX_ACTIVE)\\n\\t\\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\\\n\",\\n\\t\\t\\tmax_active, name, 1, WQ_MAX_ACTIVE);\\n\\n\\treturn clamp_val(max_active, 1, WQ_MAX_ACTIVE);\\n}\\n\\n/*\\n * Workqueues which may be used during memory reclaim should have a rescuer\\n * to guarantee forward progress.\\n */\\nstatic int init_rescuer(struct workqueue_struct *wq)\\n{\\n\\tstruct worker *rescuer;\\n\\tchar id_buf[WORKER_ID_LEN];\\n\\tint ret;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tif (!(wq->flags & WQ_MEM_RECLAIM))\\n\\t\\treturn 0;\\n\\n\\trescuer = alloc_worker(NUMA_NO_NODE);\\n\\tif (!rescuer) {\\n\\t\\tpr_err(\"workqueue: Failed to allocate a rescuer for wq \\\\\"%s\\\\\"\\\\n\",\\n\\t\\t       wq->name);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\trescuer->rescue_wq = wq;\\n\\tformat_worker_id(id_buf, sizeof(id_buf), rescuer, NULL);\\n\\n\\trescuer->task = kthread_create(rescuer_thread, rescuer, \"%s\", id_buf);\\n\\tif (IS_ERR(rescuer->task)) {\\n\\t\\tret = PTR_ERR(rescuer->task);\\n\\t\\tpr_err(\"workqueue: Failed to create a rescuer kthread for wq \\\\\"%s\\\\\": %pe\",\\n\\t\\t       wq->name, ERR_PTR(ret));\\n\\t\\tkfree(rescuer);\\n\\t\\treturn ret;\\n\\t}\\n\\n\\twq->rescuer = rescuer;\\n\\tif (wq->flags & WQ_UNBOUND)\\n\\t\\tkthread_bind_mask(rescuer->task, unbound_effective_cpumask(wq));\\n\\telse\\n\\t\\tkthread_bind_mask(rescuer->task, cpu_possible_mask);\\n\\twake_up_process(rescuer->task);\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * wq_adjust_max_active - update a wq\\'s max_active to the current setting\\n * @wq: target workqueue\\n *\\n * If @wq isn\\'t freezing, set @wq->max_active to the saved_max_active and\\n * activate inactive work items accordingly. If @wq is freezing, clear\\n * @wq->max_active to zero.\\n */\\nstatic void wq_adjust_max_active(struct workqueue_struct *wq)\\n{\\n\\tbool activated;\\n\\tint new_max, new_min;\\n\\n\\tlockdep_assert_held(&wq->mutex);\\n\\n\\tif ((wq->flags & WQ_FREEZABLE) && workqueue_freezing) {\\n\\t\\tnew_max = 0;\\n\\t\\tnew_min = 0;\\n\\t} else {\\n\\t\\tnew_max = wq->saved_max_active;\\n\\t\\tnew_min = wq->saved_min_active;\\n\\t}\\n\\n\\tif (wq->max_active == new_max && wq->min_active == new_min)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Update @wq->max/min_active and then kick inactive work items if more\\n\\t * active work items are allowed. This doesn\\'t break work item ordering\\n\\t * because new work items are always queued behind existing inactive\\n\\t * work items if there are any.\\n\\t */\\n\\tWRITE_ONCE(wq->max_active, new_max);\\n\\tWRITE_ONCE(wq->min_active, new_min);\\n\\n\\tif (wq->flags & WQ_UNBOUND)\\n\\t\\twq_update_node_max_active(wq, -1);\\n\\n\\tif (new_max == 0)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * Round-robin through pwq\\'s activating the first inactive work item\\n\\t * until max_active is filled.\\n\\t */\\n\\tdo {\\n\\t\\tstruct pool_workqueue *pwq;\\n\\n\\t\\tactivated = false;\\n\\t\\tfor_each_pwq(pwq, wq) {\\n\\t\\t\\tunsigned long irq_flags;\\n\\n\\t\\t\\t/* can be called during early boot w/ irq disabled */\\n\\t\\t\\traw_spin_lock_irqsave(&pwq->pool->lock, irq_flags);\\n\\t\\t\\tif (pwq_activate_first_inactive(pwq, true)) {\\n\\t\\t\\t\\tactivated = true;\\n\\t\\t\\t\\tkick_pool(pwq->pool);\\n\\t\\t\\t}\\n\\t\\t\\traw_spin_unlock_irqrestore(&pwq->pool->lock, irq_flags);\\n\\t\\t}\\n\\t} while (activated);\\n}\\n\\n__printf(1, 0)\\nstatic struct workqueue_struct *__alloc_workqueue(const char *fmt,\\n\\t\\t\\t\\t\\t\\t  unsigned int flags,\\n\\t\\t\\t\\t\\t\\t  int max_active, va_list args)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\tsize_t wq_size;\\n\\tint name_len;\\n\\n\\tif (flags & WQ_BH) {\\n\\t\\tif (WARN_ON_ONCE(flags & ~__WQ_BH_ALLOWS))\\n\\t\\t\\treturn NULL;\\n\\t\\tif (WARN_ON_ONCE(max_active))\\n\\t\\t\\treturn NULL;\\n\\t}\\n\\n\\t/* see the comment above the definition of WQ_POWER_EFFICIENT */\\n\\tif ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)\\n\\t\\tflags |= WQ_UNBOUND;\\n\\n\\t/* allocate wq and format name */\\n\\tif (flags & WQ_UNBOUND)\\n\\t\\twq_size = struct_size(wq, node_nr_active, nr_node_ids + 1);\\n\\telse\\n\\t\\twq_size = sizeof(*wq);\\n\\n\\twq = kzalloc(wq_size, GFP_KERNEL);\\n\\tif (!wq)\\n\\t\\treturn NULL;\\n\\n\\tif (flags & WQ_UNBOUND) {\\n\\t\\twq->unbound_attrs = alloc_workqueue_attrs();\\n\\t\\tif (!wq->unbound_attrs)\\n\\t\\t\\tgoto err_free_wq;\\n\\t}\\n\\n\\tname_len = vsnprintf(wq->name, sizeof(wq->name), fmt, args);\\n\\n\\tif (name_len >= WQ_NAME_LEN)\\n\\t\\tpr_warn_once(\"workqueue: name exceeds WQ_NAME_LEN. Truncating to: %s\\\\n\",\\n\\t\\t\\t     wq->name);\\n\\n\\tif (flags & WQ_BH) {\\n\\t\\t/*\\n\\t\\t * BH workqueues always share a single execution context per CPU\\n\\t\\t * and don\\'t impose any max_active limit.\\n\\t\\t */\\n\\t\\tmax_active = INT_MAX;\\n\\t} else {\\n\\t\\tmax_active = max_active ?: WQ_DFL_ACTIVE;\\n\\t\\tmax_active = wq_clamp_max_active(max_active, flags, wq->name);\\n\\t}\\n\\n\\t/* init wq */\\n\\twq->flags = flags;\\n\\twq->max_active = max_active;\\n\\twq->min_active = min(max_active, WQ_DFL_MIN_ACTIVE);\\n\\twq->saved_max_active = wq->max_active;\\n\\twq->saved_min_active = wq->min_active;\\n\\tmutex_init(&wq->mutex);\\n\\tatomic_set(&wq->nr_pwqs_to_flush, 0);\\n\\tINIT_LIST_HEAD(&wq->pwqs);\\n\\tINIT_LIST_HEAD(&wq->flusher_queue);\\n\\tINIT_LIST_HEAD(&wq->flusher_overflow);\\n\\tINIT_LIST_HEAD(&wq->maydays);\\n\\n\\tINIT_LIST_HEAD(&wq->list);\\n\\n\\tif (flags & WQ_UNBOUND) {\\n\\t\\tif (alloc_node_nr_active(wq->node_nr_active) < 0)\\n\\t\\t\\tgoto err_free_wq;\\n\\t}\\n\\n\\t/*\\n\\t * wq_pool_mutex protects the workqueues list, allocations of PWQs,\\n\\t * and the global freeze state.\\n\\t */\\n\\tapply_wqattrs_lock();\\n\\n\\tif (alloc_and_link_pwqs(wq) < 0)\\n\\t\\tgoto err_unlock_free_node_nr_active;\\n\\n\\tmutex_lock(&wq->mutex);\\n\\twq_adjust_max_active(wq);\\n\\tmutex_unlock(&wq->mutex);\\n\\n\\tlist_add_tail_rcu(&wq->list, &workqueues);\\n\\n\\tif (wq_online && init_rescuer(wq) < 0)\\n\\t\\tgoto err_unlock_destroy;\\n\\n\\tapply_wqattrs_unlock();\\n\\n\\tif ((wq->flags & WQ_SYSFS) && workqueue_sysfs_register(wq))\\n\\t\\tgoto err_destroy;\\n\\n\\treturn wq;\\n\\nerr_unlock_free_node_nr_active:\\n\\tapply_wqattrs_unlock();\\n\\t/*\\n\\t * Failed alloc_and_link_pwqs() may leave pending pwq->release_work,\\n\\t * flushing the pwq_release_worker ensures that the pwq_release_workfn()\\n\\t * completes before calling kfree(wq).\\n\\t */\\n\\tif (wq->flags & WQ_UNBOUND) {\\n\\t\\tkthread_flush_worker(pwq_release_worker);\\n\\t\\tfree_node_nr_active(wq->node_nr_active);\\n\\t}\\nerr_free_wq:\\n\\tfree_workqueue_attrs(wq->unbound_attrs);\\n\\tkfree(wq);\\n\\treturn NULL;\\nerr_unlock_destroy:\\n\\tapply_wqattrs_unlock();\\nerr_destroy:\\n\\tdestroy_workqueue(wq);\\n\\treturn NULL;\\n}\\n\\n__printf(1, 4)\\nstruct workqueue_struct *alloc_workqueue(const char *fmt,\\n\\t\\t\\t\\t\\t unsigned int flags,\\n\\t\\t\\t\\t\\t int max_active, ...)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\tva_list args;\\n\\n\\tva_start(args, max_active);\\n\\twq = __alloc_workqueue(fmt, flags, max_active, args);\\n\\tva_end(args);\\n\\tif (!wq)\\n\\t\\treturn NULL;\\n\\n\\twq_init_lockdep(wq);\\n\\n\\treturn wq;\\n}\\nEXPORT_SYMBOL_GPL(alloc_workqueue);\\n\\n#ifdef CONFIG_LOCKDEP\\n__printf(1, 5)\\nstruct workqueue_struct *\\nalloc_workqueue_lockdep_map(const char *fmt, unsigned int flags,\\n\\t\\t\\t    int max_active, struct lockdep_map *lockdep_map, ...)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\tva_list args;\\n\\n\\tva_start(args, lockdep_map);\\n\\twq = __alloc_workqueue(fmt, flags, max_active, args);\\n\\tva_end(args);\\n\\tif (!wq)\\n\\t\\treturn NULL;\\n\\n\\twq->lockdep_map = lockdep_map;\\n\\n\\treturn wq;\\n}\\nEXPORT_SYMBOL_GPL(alloc_workqueue_lockdep_map);\\n#endif\\n\\nstatic bool pwq_busy(struct pool_workqueue *pwq)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < WORK_NR_COLORS; i++)\\n\\t\\tif (pwq->nr_in_flight[i])\\n\\t\\t\\treturn true;\\n\\n\\tif ((pwq != rcu_access_pointer(pwq->wq->dfl_pwq)) && (pwq->refcnt > 1))\\n\\t\\treturn true;\\n\\tif (!pwq_is_empty(pwq))\\n\\t\\treturn true;\\n\\n\\treturn false;\\n}\\n\\n/**\\n * destroy_workqueue - safely terminate a workqueue\\n * @wq: target workqueue\\n *\\n * Safely destroy a workqueue. All work currently pending will be done first.\\n */\\nvoid destroy_workqueue(struct workqueue_struct *wq)\\n{\\n\\tstruct pool_workqueue *pwq;\\n\\tint cpu;\\n\\n\\t/*\\n\\t * Remove it from sysfs first so that sanity check failure doesn\\'t\\n\\t * lead to sysfs name conflicts.\\n\\t */\\n\\tworkqueue_sysfs_unregister(wq);\\n\\n\\t/* mark the workqueue destruction is in progress */\\n\\tmutex_lock(&wq->mutex);\\n\\twq->flags |= __WQ_DESTROYING;\\n\\tmutex_unlock(&wq->mutex);\\n\\n\\t/* drain it before proceeding with destruction */\\n\\tdrain_workqueue(wq);\\n\\n\\t/* kill rescuer, if sanity checks fail, leave it w/o rescuer */\\n\\tif (wq->rescuer) {\\n\\t\\tstruct worker *rescuer = wq->rescuer;\\n\\n\\t\\t/* this prevents new queueing */\\n\\t\\traw_spin_lock_irq(&wq_mayday_lock);\\n\\t\\twq->rescuer = NULL;\\n\\t\\traw_spin_unlock_irq(&wq_mayday_lock);\\n\\n\\t\\t/* rescuer will empty maydays list before exiting */\\n\\t\\tkthread_stop(rescuer->task);\\n\\t\\tkfree(rescuer);\\n\\t}\\n\\n\\t/*\\n\\t * Sanity checks - grab all the locks so that we wait for all\\n\\t * in-flight operations which may do put_pwq().\\n\\t */\\n\\tmutex_lock(&wq_pool_mutex);\\n\\tmutex_lock(&wq->mutex);\\n\\tfor_each_pwq(pwq, wq) {\\n\\t\\traw_spin_lock_irq(&pwq->pool->lock);\\n\\t\\tif (WARN_ON(pwq_busy(pwq))) {\\n\\t\\t\\tpr_warn(\"%s: %s has the following busy pwq\\\\n\",\\n\\t\\t\\t\\t__func__, wq->name);\\n\\t\\t\\tshow_pwq(pwq);\\n\\t\\t\\traw_spin_unlock_irq(&pwq->pool->lock);\\n\\t\\t\\tmutex_unlock(&wq->mutex);\\n\\t\\t\\tmutex_unlock(&wq_pool_mutex);\\n\\t\\t\\tshow_one_workqueue(wq);\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\traw_spin_unlock_irq(&pwq->pool->lock);\\n\\t}\\n\\tmutex_unlock(&wq->mutex);\\n\\n\\t/*\\n\\t * wq list is used to freeze wq, remove from list after\\n\\t * flushing is complete in case freeze races us.\\n\\t */\\n\\tlist_del_rcu(&wq->list);\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\n\\t/*\\n\\t * We\\'re the sole accessor of @wq. Directly access cpu_pwq and dfl_pwq\\n\\t * to put the base refs. @wq will be auto-destroyed from the last\\n\\t * pwq_put. RCU read lock prevents @wq from going away from under us.\\n\\t */\\n\\trcu_read_lock();\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tput_pwq_unlocked(unbound_pwq(wq, cpu));\\n\\t\\tRCU_INIT_POINTER(*unbound_pwq_slot(wq, cpu), NULL);\\n\\t}\\n\\n\\tput_pwq_unlocked(unbound_pwq(wq, -1));\\n\\tRCU_INIT_POINTER(*unbound_pwq_slot(wq, -1), NULL);\\n\\n\\trcu_read_unlock();\\n}\\nEXPORT_SYMBOL_GPL(destroy_workqueue);\\n\\n/**\\n * workqueue_set_max_active - adjust max_active of a workqueue\\n * @wq: target workqueue\\n * @max_active: new max_active value.\\n *\\n * Set max_active of @wq to @max_active. See the alloc_workqueue() function\\n * comment.\\n *\\n * CONTEXT:\\n * Don\\'t call from IRQ context.\\n */\\nvoid workqueue_set_max_active(struct workqueue_struct *wq, int max_active)\\n{\\n\\t/* max_active doesn\\'t mean anything for BH workqueues */\\n\\tif (WARN_ON(wq->flags & WQ_BH))\\n\\t\\treturn;\\n\\t/* disallow meddling with max_active for ordered workqueues */\\n\\tif (WARN_ON(wq->flags & __WQ_ORDERED))\\n\\t\\treturn;\\n\\n\\tmax_active = wq_clamp_max_active(max_active, wq->flags, wq->name);\\n\\n\\tmutex_lock(&wq->mutex);\\n\\n\\twq->saved_max_active = max_active;\\n\\tif (wq->flags & WQ_UNBOUND)\\n\\t\\twq->saved_min_active = min(wq->saved_min_active, max_active);\\n\\n\\twq_adjust_max_active(wq);\\n\\n\\tmutex_unlock(&wq->mutex);\\n}\\nEXPORT_SYMBOL_GPL(workqueue_set_max_active);\\n\\n/**\\n * workqueue_set_min_active - adjust min_active of an unbound workqueue\\n * @wq: target unbound workqueue\\n * @min_active: new min_active value\\n *\\n * Set min_active of an unbound workqueue. Unlike other types of workqueues, an\\n * unbound workqueue is not guaranteed to be able to process max_active\\n * interdependent work items. Instead, an unbound workqueue is guaranteed to be\\n * able to process min_active number of interdependent work items which is\\n * %WQ_DFL_MIN_ACTIVE by default.\\n *\\n * Use this function to adjust the min_active value between 0 and the current\\n * max_active.\\n */\\nvoid workqueue_set_min_active(struct workqueue_struct *wq, int min_active)\\n{\\n\\t/* min_active is only meaningful for non-ordered unbound workqueues */\\n\\tif (WARN_ON((wq->flags & (WQ_BH | WQ_UNBOUND | __WQ_ORDERED)) !=\\n\\t\\t    WQ_UNBOUND))\\n\\t\\treturn;\\n\\n\\tmutex_lock(&wq->mutex);\\n\\twq->saved_min_active = clamp(min_active, 0, wq->saved_max_active);\\n\\twq_adjust_max_active(wq);\\n\\tmutex_unlock(&wq->mutex);\\n}\\n\\n/**\\n * current_work - retrieve %current task\\'s work struct\\n *\\n * Determine if %current task is a workqueue worker and what it\\'s working on.\\n * Useful to find out the context that the %current task is running in.\\n *\\n * Return: work struct if %current task is a workqueue worker, %NULL otherwise.\\n */\\nstruct work_struct *current_work(void)\\n{\\n\\tstruct worker *worker = current_wq_worker();\\n\\n\\treturn worker ? worker->current_work : NULL;\\n}\\nEXPORT_SYMBOL(current_work);\\n\\n/**\\n * current_is_workqueue_rescuer - is %current workqueue rescuer?\\n *\\n * Determine whether %current is a workqueue rescuer.  Can be used from\\n * work functions to determine whether it\\'s being run off the rescuer task.\\n *\\n * Return: %true if %current is a workqueue rescuer. %false otherwise.\\n */\\nbool current_is_workqueue_rescuer(void)\\n{\\n\\tstruct worker *worker = current_wq_worker();\\n\\n\\treturn worker && worker->rescue_wq;\\n}\\n\\n/**\\n * workqueue_congested - test whether a workqueue is congested\\n * @cpu: CPU in question\\n * @wq: target workqueue\\n *\\n * Test whether @wq\\'s cpu workqueue for @cpu is congested.  There is\\n * no synchronization around this function and the test result is\\n * unreliable and only useful as advisory hints or for debugging.\\n *\\n * If @cpu is WORK_CPU_UNBOUND, the test is performed on the local CPU.\\n *\\n * With the exception of ordered workqueues, all workqueues have per-cpu\\n * pool_workqueues, each with its own congested state. A workqueue being\\n * congested on one CPU doesn\\'t mean that the workqueue is contested on any\\n * other CPUs.\\n *\\n * Return:\\n * %true if congested, %false otherwise.\\n */\\nbool workqueue_congested(int cpu, struct workqueue_struct *wq)\\n{\\n\\tstruct pool_workqueue *pwq;\\n\\tbool ret;\\n\\n\\trcu_read_lock();\\n\\tpreempt_disable();\\n\\n\\tif (cpu == WORK_CPU_UNBOUND)\\n\\t\\tcpu = smp_processor_id();\\n\\n\\tpwq = *per_cpu_ptr(wq->cpu_pwq, cpu);\\n\\tret = !list_empty(&pwq->inactive_works);\\n\\n\\tpreempt_enable();\\n\\trcu_read_unlock();\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(workqueue_congested);\\n\\n/**\\n * work_busy - test whether a work is currently pending or running\\n * @work: the work to be tested\\n *\\n * Test whether @work is currently pending or running.  There is no\\n * synchronization around this function and the test result is\\n * unreliable and only useful as advisory hints or for debugging.\\n *\\n * Return:\\n * OR\\'d bitmask of WORK_BUSY_* bits.\\n */\\nunsigned int work_busy(struct work_struct *work)\\n{\\n\\tstruct worker_pool *pool;\\n\\tunsigned long irq_flags;\\n\\tunsigned int ret = 0;\\n\\n\\tif (work_pending(work))\\n\\t\\tret |= WORK_BUSY_PENDING;\\n\\n\\trcu_read_lock();\\n\\tpool = get_work_pool(work);\\n\\tif (pool) {\\n\\t\\traw_spin_lock_irqsave(&pool->lock, irq_flags);\\n\\t\\tif (find_worker_executing_work(pool, work))\\n\\t\\t\\tret |= WORK_BUSY_RUNNING;\\n\\t\\traw_spin_unlock_irqrestore(&pool->lock, irq_flags);\\n\\t}\\n\\trcu_read_unlock();\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(work_busy);\\n\\n/**\\n * set_worker_desc - set description for the current work item\\n * @fmt: printf-style format string\\n * @...: arguments for the format string\\n *\\n * This function can be called by a running work function to describe what\\n * the work item is about.  If the worker task gets dumped, this\\n * information will be printed out together to help debugging.  The\\n * description can be at most WORKER_DESC_LEN including the trailing \\'\\\\0\\'.\\n */\\nvoid set_worker_desc(const char *fmt, ...)\\n{\\n\\tstruct worker *worker = current_wq_worker();\\n\\tva_list args;\\n\\n\\tif (worker) {\\n\\t\\tva_start(args, fmt);\\n\\t\\tvsnprintf(worker->desc, sizeof(worker->desc), fmt, args);\\n\\t\\tva_end(args);\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(set_worker_desc);\\n\\n/**\\n * print_worker_info - print out worker information and description\\n * @log_lvl: the log level to use when printing\\n * @task: target task\\n *\\n * If @task is a worker and currently executing a work item, print out the\\n * name of the workqueue being serviced and worker description set with\\n * set_worker_desc() by the currently executing work item.\\n *\\n * This function can be safely called on any task as long as the\\n * task_struct itself is accessible.  While safe, this function isn\\'t\\n * synchronized and may print out mixups or garbages of limited length.\\n */\\nvoid print_worker_info(const char *log_lvl, struct task_struct *task)\\n{\\n\\twork_func_t *fn = NULL;\\n\\tchar name[WQ_NAME_LEN] = { };\\n\\tchar desc[WORKER_DESC_LEN] = { };\\n\\tstruct pool_workqueue *pwq = NULL;\\n\\tstruct workqueue_struct *wq = NULL;\\n\\tstruct worker *worker;\\n\\n\\tif (!(task->flags & PF_WQ_WORKER))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * This function is called without any synchronization and @task\\n\\t * could be in any state.  Be careful with dereferences.\\n\\t */\\n\\tworker = kthread_probe_data(task);\\n\\n\\t/*\\n\\t * Carefully copy the associated workqueue\\'s workfn, name and desc.\\n\\t * Keep the original last \\'\\\\0\\' in case the original is garbage.\\n\\t */\\n\\tcopy_from_kernel_nofault(&fn, &worker->current_func, sizeof(fn));\\n\\tcopy_from_kernel_nofault(&pwq, &worker->current_pwq, sizeof(pwq));\\n\\tcopy_from_kernel_nofault(&wq, &pwq->wq, sizeof(wq));\\n\\tcopy_from_kernel_nofault(name, wq->name, sizeof(name) - 1);\\n\\tcopy_from_kernel_nofault(desc, worker->desc, sizeof(desc) - 1);\\n\\n\\tif (fn || name[0] || desc[0]) {\\n\\t\\tprintk(\"%sWorkqueue: %s %ps\", log_lvl, name, fn);\\n\\t\\tif (strcmp(name, desc))\\n\\t\\t\\tpr_cont(\" (%s)\", desc);\\n\\t\\tpr_cont(\"\\\\n\");\\n\\t}\\n}\\n\\nstatic void pr_cont_pool_info(struct worker_pool *pool)\\n{\\n\\tpr_cont(\" cpus=%*pbl\", nr_cpumask_bits, pool->attrs->cpumask);\\n\\tif (pool->node != NUMA_NO_NODE)\\n\\t\\tpr_cont(\" node=%d\", pool->node);\\n\\tpr_cont(\" flags=0x%x\", pool->flags);\\n\\tif (pool->flags & POOL_BH)\\n\\t\\tpr_cont(\" bh%s\",\\n\\t\\t\\tpool->attrs->nice == HIGHPRI_NICE_LEVEL ? \"-hi\" : \"\");\\n\\telse\\n\\t\\tpr_cont(\" nice=%d\", pool->attrs->nice);\\n}\\n\\nstatic void pr_cont_worker_id(struct worker *worker)\\n{\\n\\tstruct worker_pool *pool = worker->pool;\\n\\n\\tif (pool->flags & WQ_BH)\\n\\t\\tpr_cont(\"bh%s\",\\n\\t\\t\\tpool->attrs->nice == HIGHPRI_NICE_LEVEL ? \"-hi\" : \"\");\\n\\telse\\n\\t\\tpr_cont(\"%d%s\", task_pid_nr(worker->task),\\n\\t\\t\\tworker->rescue_wq ? \"(RESCUER)\" : \"\");\\n}\\n\\nstruct pr_cont_work_struct {\\n\\tbool comma;\\n\\twork_func_t func;\\n\\tlong ctr;\\n};\\n\\nstatic void pr_cont_work_flush(bool comma, work_func_t func, struct pr_cont_work_struct *pcwsp)\\n{\\n\\tif (!pcwsp->ctr)\\n\\t\\tgoto out_record;\\n\\tif (func == pcwsp->func) {\\n\\t\\tpcwsp->ctr++;\\n\\t\\treturn;\\n\\t}\\n\\tif (pcwsp->ctr == 1)\\n\\t\\tpr_cont(\"%s %ps\", pcwsp->comma ? \",\" : \"\", pcwsp->func);\\n\\telse\\n\\t\\tpr_cont(\"%s %ld*%ps\", pcwsp->comma ? \",\" : \"\", pcwsp->ctr, pcwsp->func);\\n\\tpcwsp->ctr = 0;\\nout_record:\\n\\tif ((long)func == -1L)\\n\\t\\treturn;\\n\\tpcwsp->comma = comma;\\n\\tpcwsp->func = func;\\n\\tpcwsp->ctr = 1;\\n}\\n\\nstatic void pr_cont_work(bool comma, struct work_struct *work, struct pr_cont_work_struct *pcwsp)\\n{\\n\\tif (work->func == wq_barrier_func) {\\n\\t\\tstruct wq_barrier *barr;\\n\\n\\t\\tbarr = container_of(work, struct wq_barrier, work);\\n\\n\\t\\tpr_cont_work_flush(comma, (work_func_t)-1, pcwsp);\\n\\t\\tpr_cont(\"%s BAR(%d)\", comma ? \",\" : \"\",\\n\\t\\t\\ttask_pid_nr(barr->task));\\n\\t} else {\\n\\t\\tif (!comma)\\n\\t\\t\\tpr_cont_work_flush(comma, (work_func_t)-1, pcwsp);\\n\\t\\tpr_cont_work_flush(comma, work->func, pcwsp);\\n\\t}\\n}\\n\\nstatic void show_pwq(struct pool_workqueue *pwq)\\n{\\n\\tstruct pr_cont_work_struct pcws = { .ctr = 0, };\\n\\tstruct worker_pool *pool = pwq->pool;\\n\\tstruct work_struct *work;\\n\\tstruct worker *worker;\\n\\tbool has_in_flight = false, has_pending = false;\\n\\tint bkt;\\n\\n\\tpr_info(\"  pwq %d:\", pool->id);\\n\\tpr_cont_pool_info(pool);\\n\\n\\tpr_cont(\" active=%d refcnt=%d%s\\\\n\",\\n\\t\\tpwq->nr_active, pwq->refcnt,\\n\\t\\t!list_empty(&pwq->mayday_node) ? \" MAYDAY\" : \"\");\\n\\n\\thash_for_each(pool->busy_hash, bkt, worker, hentry) {\\n\\t\\tif (worker->current_pwq == pwq) {\\n\\t\\t\\thas_in_flight = true;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\tif (has_in_flight) {\\n\\t\\tbool comma = false;\\n\\n\\t\\tpr_info(\"    in-flight:\");\\n\\t\\thash_for_each(pool->busy_hash, bkt, worker, hentry) {\\n\\t\\t\\tif (worker->current_pwq != pwq)\\n\\t\\t\\t\\tcontinue;\\n\\n\\t\\t\\tpr_cont(\" %s\", comma ? \",\" : \"\");\\n\\t\\t\\tpr_cont_worker_id(worker);\\n\\t\\t\\tpr_cont(\":%ps\", worker->current_func);\\n\\t\\t\\tlist_for_each_entry(work, &worker->scheduled, entry)\\n\\t\\t\\t\\tpr_cont_work(false, work, &pcws);\\n\\t\\t\\tpr_cont_work_flush(comma, (work_func_t)-1L, &pcws);\\n\\t\\t\\tcomma = true;\\n\\t\\t}\\n\\t\\tpr_cont(\"\\\\n\");\\n\\t}\\n\\n\\tlist_for_each_entry(work, &pool->worklist, entry) {\\n\\t\\tif (get_work_pwq(work) == pwq) {\\n\\t\\t\\thas_pending = true;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\tif (has_pending) {\\n\\t\\tbool comma = false;\\n\\n\\t\\tpr_info(\"    pending:\");\\n\\t\\tlist_for_each_entry(work, &pool->worklist, entry) {\\n\\t\\t\\tif (get_work_pwq(work) != pwq)\\n\\t\\t\\t\\tcontinue;\\n\\n\\t\\t\\tpr_cont_work(comma, work, &pcws);\\n\\t\\t\\tcomma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);\\n\\t\\t}\\n\\t\\tpr_cont_work_flush(comma, (work_func_t)-1L, &pcws);\\n\\t\\tpr_cont(\"\\\\n\");\\n\\t}\\n\\n\\tif (!list_empty(&pwq->inactive_works)) {\\n\\t\\tbool comma = false;\\n\\n\\t\\tpr_info(\"    inactive:\");\\n\\t\\tlist_for_each_entry(work, &pwq->inactive_works, entry) {\\n\\t\\t\\tpr_cont_work(comma, work, &pcws);\\n\\t\\t\\tcomma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);\\n\\t\\t}\\n\\t\\tpr_cont_work_flush(comma, (work_func_t)-1L, &pcws);\\n\\t\\tpr_cont(\"\\\\n\");\\n\\t}\\n}\\n\\n/**\\n * show_one_workqueue - dump state of specified workqueue\\n * @wq: workqueue whose state will be printed\\n */\\nvoid show_one_workqueue(struct workqueue_struct *wq)\\n{\\n\\tstruct pool_workqueue *pwq;\\n\\tbool idle = true;\\n\\tunsigned long irq_flags;\\n\\n\\tfor_each_pwq(pwq, wq) {\\n\\t\\tif (!pwq_is_empty(pwq)) {\\n\\t\\t\\tidle = false;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\tif (idle) /* Nothing to print for idle workqueue */\\n\\t\\treturn;\\n\\n\\tpr_info(\"workqueue %s: flags=0x%x\\\\n\", wq->name, wq->flags);\\n\\n\\tfor_each_pwq(pwq, wq) {\\n\\t\\traw_spin_lock_irqsave(&pwq->pool->lock, irq_flags);\\n\\t\\tif (!pwq_is_empty(pwq)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Defer printing to avoid deadlocks in console\\n\\t\\t\\t * drivers that queue work while holding locks\\n\\t\\t\\t * also taken in their write paths.\\n\\t\\t\\t */\\n\\t\\t\\tprintk_deferred_enter();\\n\\t\\t\\tshow_pwq(pwq);\\n\\t\\t\\tprintk_deferred_exit();\\n\\t\\t}\\n\\t\\traw_spin_unlock_irqrestore(&pwq->pool->lock, irq_flags);\\n\\t\\t/*\\n\\t\\t * We could be printing a lot from atomic context, e.g.\\n\\t\\t * sysrq-t -> show_all_workqueues(). Avoid triggering\\n\\t\\t * hard lockup.\\n\\t\\t */\\n\\t\\ttouch_nmi_watchdog();\\n\\t}\\n\\n}\\n\\n/**\\n * show_one_worker_pool - dump state of specified worker pool\\n * @pool: worker pool whose state will be printed\\n */\\nstatic void show_one_worker_pool(struct worker_pool *pool)\\n{\\n\\tstruct worker *worker;\\n\\tbool first = true;\\n\\tunsigned long irq_flags;\\n\\tunsigned long hung = 0;\\n\\n\\traw_spin_lock_irqsave(&pool->lock, irq_flags);\\n\\tif (pool->nr_workers == pool->nr_idle)\\n\\t\\tgoto next_pool;\\n\\n\\t/* How long the first pending work is waiting for a worker. */\\n\\tif (!list_empty(&pool->worklist))\\n\\t\\thung = jiffies_to_msecs(jiffies - pool->watchdog_ts) / 1000;\\n\\n\\t/*\\n\\t * Defer printing to avoid deadlocks in console drivers that\\n\\t * queue work while holding locks also taken in their write\\n\\t * paths.\\n\\t */\\n\\tprintk_deferred_enter();\\n\\tpr_info(\"pool %d:\", pool->id);\\n\\tpr_cont_pool_info(pool);\\n\\tpr_cont(\" hung=%lus workers=%d\", hung, pool->nr_workers);\\n\\tif (pool->manager)\\n\\t\\tpr_cont(\" manager: %d\",\\n\\t\\t\\ttask_pid_nr(pool->manager->task));\\n\\tlist_for_each_entry(worker, &pool->idle_list, entry) {\\n\\t\\tpr_cont(\" %s\", first ? \"idle: \" : \"\");\\n\\t\\tpr_cont_worker_id(worker);\\n\\t\\tfirst = false;\\n\\t}\\n\\tpr_cont(\"\\\\n\");\\n\\tprintk_deferred_exit();\\nnext_pool:\\n\\traw_spin_unlock_irqrestore(&pool->lock, irq_flags);\\n\\t/*\\n\\t * We could be printing a lot from atomic context, e.g.\\n\\t * sysrq-t -> show_all_workqueues(). Avoid triggering\\n\\t * hard lockup.\\n\\t */\\n\\ttouch_nmi_watchdog();\\n\\n}\\n\\n/**\\n * show_all_workqueues - dump workqueue state\\n *\\n * Called from a sysrq handler and prints out all busy workqueues and pools.\\n */\\nvoid show_all_workqueues(void)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\tstruct worker_pool *pool;\\n\\tint pi;\\n\\n\\trcu_read_lock();\\n\\n\\tpr_info(\"Showing busy workqueues and worker pools:\\\\n\");\\n\\n\\tlist_for_each_entry_rcu(wq, &workqueues, list)\\n\\t\\tshow_one_workqueue(wq);\\n\\n\\tfor_each_pool(pool, pi)\\n\\t\\tshow_one_worker_pool(pool);\\n\\n\\trcu_read_unlock();\\n}\\n\\n/**\\n * show_freezable_workqueues - dump freezable workqueue state\\n *\\n * Called from try_to_freeze_tasks() and prints out all freezable workqueues\\n * still busy.\\n */\\nvoid show_freezable_workqueues(void)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\n\\trcu_read_lock();\\n\\n\\tpr_info(\"Showing freezable workqueues that are still busy:\\\\n\");\\n\\n\\tlist_for_each_entry_rcu(wq, &workqueues, list) {\\n\\t\\tif (!(wq->flags & WQ_FREEZABLE))\\n\\t\\t\\tcontinue;\\n\\t\\tshow_one_workqueue(wq);\\n\\t}\\n\\n\\trcu_read_unlock();\\n}\\n\\n/* used to show worker information through /proc/PID/{comm,stat,status} */\\nvoid wq_worker_comm(char *buf, size_t size, struct task_struct *task)\\n{\\n\\t/* stabilize PF_WQ_WORKER and worker pool association */\\n\\tmutex_lock(&wq_pool_attach_mutex);\\n\\n\\tif (task->flags & PF_WQ_WORKER) {\\n\\t\\tstruct worker *worker = kthread_data(task);\\n\\t\\tstruct worker_pool *pool = worker->pool;\\n\\t\\tint off;\\n\\n\\t\\toff = format_worker_id(buf, size, worker, pool);\\n\\n\\t\\tif (pool) {\\n\\t\\t\\traw_spin_lock_irq(&pool->lock);\\n\\t\\t\\t/*\\n\\t\\t\\t * ->desc tracks information (wq name or\\n\\t\\t\\t * set_worker_desc()) for the latest execution.  If\\n\\t\\t\\t * current, prepend \\'+\\', otherwise \\'-\\'.\\n\\t\\t\\t */\\n\\t\\t\\tif (worker->desc[0] != \\'\\\\0\\') {\\n\\t\\t\\t\\tif (worker->current_work)\\n\\t\\t\\t\\t\\tscnprintf(buf + off, size - off, \"+%s\",\\n\\t\\t\\t\\t\\t\\t  worker->desc);\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tscnprintf(buf + off, size - off, \"-%s\",\\n\\t\\t\\t\\t\\t\\t  worker->desc);\\n\\t\\t\\t}\\n\\t\\t\\traw_spin_unlock_irq(&pool->lock);\\n\\t\\t}\\n\\t} else {\\n\\t\\tstrscpy(buf, task->comm, size);\\n\\t}\\n\\n\\tmutex_unlock(&wq_pool_attach_mutex);\\n}\\n\\n#ifdef CONFIG_SMP\\n\\n/*\\n * CPU hotplug.\\n *\\n * There are two challenges in supporting CPU hotplug.  Firstly, there\\n * are a lot of assumptions on strong associations among work, pwq and\\n * pool which make migrating pending and scheduled works very\\n * difficult to implement without impacting hot paths.  Secondly,\\n * worker pools serve mix of short, long and very long running works making\\n * blocked draining impractical.\\n *\\n * This is solved by allowing the pools to be disassociated from the CPU\\n * running as an unbound one and allowing it to be reattached later if the\\n * cpu comes back online.\\n */\\n\\nstatic void unbind_workers(int cpu)\\n{\\n\\tstruct worker_pool *pool;\\n\\tstruct worker *worker;\\n\\n\\tfor_each_cpu_worker_pool(pool, cpu) {\\n\\t\\tmutex_lock(&wq_pool_attach_mutex);\\n\\t\\traw_spin_lock_irq(&pool->lock);\\n\\n\\t\\t/*\\n\\t\\t * We\\'ve blocked all attach/detach operations. Make all workers\\n\\t\\t * unbound and set DISASSOCIATED.  Before this, all workers\\n\\t\\t * must be on the cpu.  After this, they may become diasporas.\\n\\t\\t * And the preemption disabled section in their sched callbacks\\n\\t\\t * are guaranteed to see WORKER_UNBOUND since the code here\\n\\t\\t * is on the same cpu.\\n\\t\\t */\\n\\t\\tfor_each_pool_worker(worker, pool)\\n\\t\\t\\tworker->flags |= WORKER_UNBOUND;\\n\\n\\t\\tpool->flags |= POOL_DISASSOCIATED;\\n\\n\\t\\t/*\\n\\t\\t * The handling of nr_running in sched callbacks are disabled\\n\\t\\t * now.  Zap nr_running.  After this, nr_running stays zero and\\n\\t\\t * need_more_worker() and keep_working() are always true as\\n\\t\\t * long as the worklist is not empty.  This pool now behaves as\\n\\t\\t * an unbound (in terms of concurrency management) pool which\\n\\t\\t * are served by workers tied to the pool.\\n\\t\\t */\\n\\t\\tpool->nr_running = 0;\\n\\n\\t\\t/*\\n\\t\\t * With concurrency management just turned off, a busy\\n\\t\\t * worker blocking could lead to lengthy stalls.  Kick off\\n\\t\\t * unbound chain execution of currently pending work items.\\n\\t\\t */\\n\\t\\tkick_pool(pool);\\n\\n\\t\\traw_spin_unlock_irq(&pool->lock);\\n\\n\\t\\tfor_each_pool_worker(worker, pool)\\n\\t\\t\\tunbind_worker(worker);\\n\\n\\t\\tmutex_unlock(&wq_pool_attach_mutex);\\n\\t}\\n}\\n\\n/**\\n * rebind_workers - rebind all workers of a pool to the associated CPU\\n * @pool: pool of interest\\n *\\n * @pool->cpu is coming online.  Rebind all workers to the CPU.\\n */\\nstatic void rebind_workers(struct worker_pool *pool)\\n{\\n\\tstruct worker *worker;\\n\\n\\tlockdep_assert_held(&wq_pool_attach_mutex);\\n\\n\\t/*\\n\\t * Restore CPU affinity of all workers.  As all idle workers should\\n\\t * be on the run-queue of the associated CPU before any local\\n\\t * wake-ups for concurrency management happen, restore CPU affinity\\n\\t * of all workers first and then clear UNBOUND.  As we\\'re called\\n\\t * from CPU_ONLINE, the following shouldn\\'t fail.\\n\\t */\\n\\tfor_each_pool_worker(worker, pool) {\\n\\t\\tkthread_set_per_cpu(worker->task, pool->cpu);\\n\\t\\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,\\n\\t\\t\\t\\t\\t\\t  pool_allowed_cpus(pool)) < 0);\\n\\t}\\n\\n\\traw_spin_lock_irq(&pool->lock);\\n\\n\\tpool->flags &= ~POOL_DISASSOCIATED;\\n\\n\\tfor_each_pool_worker(worker, pool) {\\n\\t\\tunsigned int worker_flags = worker->flags;\\n\\n\\t\\t/*\\n\\t\\t * We want to clear UNBOUND but can\\'t directly call\\n\\t\\t * worker_clr_flags() or adjust nr_running.  Atomically\\n\\t\\t * replace UNBOUND with another NOT_RUNNING flag REBOUND.\\n\\t\\t * @worker will clear REBOUND using worker_clr_flags() when\\n\\t\\t * it initiates the next execution cycle thus restoring\\n\\t\\t * concurrency management.  Note that when or whether\\n\\t\\t * @worker clears REBOUND doesn\\'t affect correctness.\\n\\t\\t *\\n\\t\\t * WRITE_ONCE() is necessary because @worker->flags may be\\n\\t\\t * tested without holding any lock in\\n\\t\\t * wq_worker_running().  Without it, NOT_RUNNING test may\\n\\t\\t * fail incorrectly leading to premature concurrency\\n\\t\\t * management operations.\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(!(worker_flags & WORKER_UNBOUND));\\n\\t\\tworker_flags |= WORKER_REBOUND;\\n\\t\\tworker_flags &= ~WORKER_UNBOUND;\\n\\t\\tWRITE_ONCE(worker->flags, worker_flags);\\n\\t}\\n\\n\\traw_spin_unlock_irq(&pool->lock);\\n}\\n\\n/**\\n * restore_unbound_workers_cpumask - restore cpumask of unbound workers\\n * @pool: unbound pool of interest\\n * @cpu: the CPU which is coming up\\n *\\n * An unbound pool may end up with a cpumask which doesn\\'t have any online\\n * CPUs.  When a worker of such pool get scheduled, the scheduler resets\\n * its cpus_allowed.  If @cpu is in @pool\\'s cpumask which didn\\'t have any\\n * online CPU before, cpus_allowed of all its workers should be restored.\\n */\\nstatic void restore_unbound_workers_cpumask(struct worker_pool *pool, int cpu)\\n{\\n\\tstatic cpumask_t cpumask;\\n\\tstruct worker *worker;\\n\\n\\tlockdep_assert_held(&wq_pool_attach_mutex);\\n\\n\\t/* is @cpu allowed for @pool? */\\n\\tif (!cpumask_test_cpu(cpu, pool->attrs->cpumask))\\n\\t\\treturn;\\n\\n\\tcpumask_and(&cpumask, pool->attrs->cpumask, cpu_online_mask);\\n\\n\\t/* as we\\'re called from CPU_ONLINE, the following shouldn\\'t fail */\\n\\tfor_each_pool_worker(worker, pool)\\n\\t\\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, &cpumask) < 0);\\n}\\n\\nint workqueue_prepare_cpu(unsigned int cpu)\\n{\\n\\tstruct worker_pool *pool;\\n\\n\\tfor_each_cpu_worker_pool(pool, cpu) {\\n\\t\\tif (pool->nr_workers)\\n\\t\\t\\tcontinue;\\n\\t\\tif (!create_worker(pool))\\n\\t\\t\\treturn -ENOMEM;\\n\\t}\\n\\treturn 0;\\n}\\n\\nint workqueue_online_cpu(unsigned int cpu)\\n{\\n\\tstruct worker_pool *pool;\\n\\tstruct workqueue_struct *wq;\\n\\tint pi;\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\tcpumask_set_cpu(cpu, wq_online_cpumask);\\n\\n\\tfor_each_pool(pool, pi) {\\n\\t\\t/* BH pools aren\\'t affected by hotplug */\\n\\t\\tif (pool->flags & POOL_BH)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tmutex_lock(&wq_pool_attach_mutex);\\n\\t\\tif (pool->cpu == cpu)\\n\\t\\t\\trebind_workers(pool);\\n\\t\\telse if (pool->cpu < 0)\\n\\t\\t\\trestore_unbound_workers_cpumask(pool, cpu);\\n\\t\\tmutex_unlock(&wq_pool_attach_mutex);\\n\\t}\\n\\n\\t/* update pod affinity of unbound workqueues */\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tstruct workqueue_attrs *attrs = wq->unbound_attrs;\\n\\n\\t\\tif (attrs) {\\n\\t\\t\\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);\\n\\t\\t\\tint tcpu;\\n\\n\\t\\t\\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])\\n\\t\\t\\t\\tunbound_wq_update_pwq(wq, tcpu);\\n\\n\\t\\t\\tmutex_lock(&wq->mutex);\\n\\t\\t\\twq_update_node_max_active(wq, -1);\\n\\t\\t\\tmutex_unlock(&wq->mutex);\\n\\t\\t}\\n\\t}\\n\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\treturn 0;\\n}\\n\\nint workqueue_offline_cpu(unsigned int cpu)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\n\\t/* unbinding per-cpu workers should happen on the local CPU */\\n\\tif (WARN_ON(cpu != smp_processor_id()))\\n\\t\\treturn -1;\\n\\n\\tunbind_workers(cpu);\\n\\n\\t/* update pod affinity of unbound workqueues */\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\tcpumask_clear_cpu(cpu, wq_online_cpumask);\\n\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tstruct workqueue_attrs *attrs = wq->unbound_attrs;\\n\\n\\t\\tif (attrs) {\\n\\t\\t\\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);\\n\\t\\t\\tint tcpu;\\n\\n\\t\\t\\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])\\n\\t\\t\\t\\tunbound_wq_update_pwq(wq, tcpu);\\n\\n\\t\\t\\tmutex_lock(&wq->mutex);\\n\\t\\t\\twq_update_node_max_active(wq, cpu);\\n\\t\\t\\tmutex_unlock(&wq->mutex);\\n\\t\\t}\\n\\t}\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\n\\treturn 0;\\n}\\n\\nstruct work_for_cpu {\\n\\tstruct work_struct work;\\n\\tlong (*fn)(void *);\\n\\tvoid *arg;\\n\\tlong ret;\\n};\\n\\nstatic void work_for_cpu_fn(struct work_struct *work)\\n{\\n\\tstruct work_for_cpu *wfc = container_of(work, struct work_for_cpu, work);\\n\\n\\twfc->ret = wfc->fn(wfc->arg);\\n}\\n\\n/**\\n * work_on_cpu_key - run a function in thread context on a particular cpu\\n * @cpu: the cpu to run on\\n * @fn: the function to run\\n * @arg: the function arg\\n * @key: The lock class key for lock debugging purposes\\n *\\n * It is up to the caller to ensure that the cpu doesn\\'t go offline.\\n * The caller must not hold any locks which would prevent @fn from completing.\\n *\\n * Return: The value @fn returns.\\n */\\nlong work_on_cpu_key(int cpu, long (*fn)(void *),\\n\\t\\t     void *arg, struct lock_class_key *key)\\n{\\n\\tstruct work_for_cpu wfc = { .fn = fn, .arg = arg };\\n\\n\\tINIT_WORK_ONSTACK_KEY(&wfc.work, work_for_cpu_fn, key);\\n\\tschedule_work_on(cpu, &wfc.work);\\n\\tflush_work(&wfc.work);\\n\\tdestroy_work_on_stack(&wfc.work);\\n\\treturn wfc.ret;\\n}\\nEXPORT_SYMBOL_GPL(work_on_cpu_key);\\n\\n/**\\n * work_on_cpu_safe_key - run a function in thread context on a particular cpu\\n * @cpu: the cpu to run on\\n * @fn:  the function to run\\n * @arg: the function argument\\n * @key: The lock class key for lock debugging purposes\\n *\\n * Disables CPU hotplug and calls work_on_cpu(). The caller must not hold\\n * any locks which would prevent @fn from completing.\\n *\\n * Return: The value @fn returns.\\n */\\nlong work_on_cpu_safe_key(int cpu, long (*fn)(void *),\\n\\t\\t\\t  void *arg, struct lock_class_key *key)\\n{\\n\\tlong ret = -ENODEV;\\n\\n\\tcpus_read_lock();\\n\\tif (cpu_online(cpu))\\n\\t\\tret = work_on_cpu_key(cpu, fn, arg, key);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(work_on_cpu_safe_key);\\n#endif /* CONFIG_SMP */\\n\\n#ifdef CONFIG_FREEZER\\n\\n/**\\n * freeze_workqueues_begin - begin freezing workqueues\\n *\\n * Start freezing workqueues.  After this function returns, all freezable\\n * workqueues will queue new works to their inactive_works list instead of\\n * pool->worklist.\\n *\\n * CONTEXT:\\n * Grabs and releases wq_pool_mutex, wq->mutex and pool->lock\\'s.\\n */\\nvoid freeze_workqueues_begin(void)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\tWARN_ON_ONCE(workqueue_freezing);\\n\\tworkqueue_freezing = true;\\n\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tmutex_lock(&wq->mutex);\\n\\t\\twq_adjust_max_active(wq);\\n\\t\\tmutex_unlock(&wq->mutex);\\n\\t}\\n\\n\\tmutex_unlock(&wq_pool_mutex);\\n}\\n\\n/**\\n * freeze_workqueues_busy - are freezable workqueues still busy?\\n *\\n * Check whether freezing is complete.  This function must be called\\n * between freeze_workqueues_begin() and thaw_workqueues().\\n *\\n * CONTEXT:\\n * Grabs and releases wq_pool_mutex.\\n *\\n * Return:\\n * %true if some freezable workqueues are still busy.  %false if freezing\\n * is complete.\\n */\\nbool freeze_workqueues_busy(void)\\n{\\n\\tbool busy = false;\\n\\tstruct workqueue_struct *wq;\\n\\tstruct pool_workqueue *pwq;\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\tWARN_ON_ONCE(!workqueue_freezing);\\n\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tif (!(wq->flags & WQ_FREEZABLE))\\n\\t\\t\\tcontinue;\\n\\t\\t/*\\n\\t\\t * nr_active is monotonically decreasing.  It\\'s safe\\n\\t\\t * to peek without lock.\\n\\t\\t */\\n\\t\\trcu_read_lock();\\n\\t\\tfor_each_pwq(pwq, wq) {\\n\\t\\t\\tWARN_ON_ONCE(pwq->nr_active < 0);\\n\\t\\t\\tif (pwq->nr_active) {\\n\\t\\t\\t\\tbusy = true;\\n\\t\\t\\t\\trcu_read_unlock();\\n\\t\\t\\t\\tgoto out_unlock;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\trcu_read_unlock();\\n\\t}\\nout_unlock:\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\treturn busy;\\n}\\n\\n/**\\n * thaw_workqueues - thaw workqueues\\n *\\n * Thaw workqueues.  Normal queueing is restored and all collected\\n * frozen works are transferred to their respective pool worklists.\\n *\\n * CONTEXT:\\n * Grabs and releases wq_pool_mutex, wq->mutex and pool->lock\\'s.\\n */\\nvoid thaw_workqueues(void)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\tif (!workqueue_freezing)\\n\\t\\tgoto out_unlock;\\n\\n\\tworkqueue_freezing = false;\\n\\n\\t/* restore max_active and repopulate worklist */\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tmutex_lock(&wq->mutex);\\n\\t\\twq_adjust_max_active(wq);\\n\\t\\tmutex_unlock(&wq->mutex);\\n\\t}\\n\\nout_unlock:\\n\\tmutex_unlock(&wq_pool_mutex);\\n}\\n#endif /* CONFIG_FREEZER */\\n\\nstatic int workqueue_apply_unbound_cpumask(const cpumask_var_t unbound_cpumask)\\n{\\n\\tLIST_HEAD(ctxs);\\n\\tint ret = 0;\\n\\tstruct workqueue_struct *wq;\\n\\tstruct apply_wqattrs_ctx *ctx, *n;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tif (!(wq->flags & WQ_UNBOUND) || (wq->flags & __WQ_DESTROYING))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tctx = apply_wqattrs_prepare(wq, wq->unbound_attrs, unbound_cpumask);\\n\\t\\tif (IS_ERR(ctx)) {\\n\\t\\t\\tret = PTR_ERR(ctx);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tlist_add_tail(&ctx->list, &ctxs);\\n\\t}\\n\\n\\tlist_for_each_entry_safe(ctx, n, &ctxs, list) {\\n\\t\\tif (!ret)\\n\\t\\t\\tapply_wqattrs_commit(ctx);\\n\\t\\tapply_wqattrs_cleanup(ctx);\\n\\t}\\n\\n\\tif (!ret) {\\n\\t\\tmutex_lock(&wq_pool_attach_mutex);\\n\\t\\tcpumask_copy(wq_unbound_cpumask, unbound_cpumask);\\n\\t\\tmutex_unlock(&wq_pool_attach_mutex);\\n\\t}\\n\\treturn ret;\\n}\\n\\n/**\\n * workqueue_unbound_exclude_cpumask - Exclude given CPUs from unbound cpumask\\n * @exclude_cpumask: the cpumask to be excluded from wq_unbound_cpumask\\n *\\n * This function can be called from cpuset code to provide a set of isolated\\n * CPUs that should be excluded from wq_unbound_cpumask.\\n */\\nint workqueue_unbound_exclude_cpumask(cpumask_var_t exclude_cpumask)\\n{\\n\\tcpumask_var_t cpumask;\\n\\tint ret = 0;\\n\\n\\tif (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\t/*\\n\\t * If the operation fails, it will fall back to\\n\\t * wq_requested_unbound_cpumask which is initially set to\\n\\t * (HK_TYPE_WQ ∩ HK_TYPE_DOMAIN) house keeping mask and rewritten\\n\\t * by any subsequent write to workqueue/cpumask sysfs file.\\n\\t */\\n\\tif (!cpumask_andnot(cpumask, wq_requested_unbound_cpumask, exclude_cpumask))\\n\\t\\tcpumask_copy(cpumask, wq_requested_unbound_cpumask);\\n\\tif (!cpumask_equal(cpumask, wq_unbound_cpumask))\\n\\t\\tret = workqueue_apply_unbound_cpumask(cpumask);\\n\\n\\t/* Save the current isolated cpumask & export it via sysfs */\\n\\tif (!ret)\\n\\t\\tcpumask_copy(wq_isolated_cpumask, exclude_cpumask);\\n\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\tfree_cpumask_var(cpumask);\\n\\treturn ret;\\n}\\n\\nstatic int parse_affn_scope(const char *val)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < ARRAY_SIZE(wq_affn_names); i++) {\\n\\t\\tif (!strncasecmp(val, wq_affn_names[i], strlen(wq_affn_names[i])))\\n\\t\\t\\treturn i;\\n\\t}\\n\\treturn -EINVAL;\\n}\\n\\nstatic int wq_affn_dfl_set(const char *val, const struct kernel_param *kp)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\tint affn, cpu;\\n\\n\\taffn = parse_affn_scope(val);\\n\\tif (affn < 0)\\n\\t\\treturn affn;\\n\\tif (affn == WQ_AFFN_DFL)\\n\\t\\treturn -EINVAL;\\n\\n\\tcpus_read_lock();\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\twq_affn_dfl = affn;\\n\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tfor_each_online_cpu(cpu)\\n\\t\\t\\tunbound_wq_update_pwq(wq, cpu);\\n\\t}\\n\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\tcpus_read_unlock();\\n\\n\\treturn 0;\\n}\\n\\nstatic int wq_affn_dfl_get(char *buffer, const struct kernel_param *kp)\\n{\\n\\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\\\n\", wq_affn_names[wq_affn_dfl]);\\n}\\n\\nstatic const struct kernel_param_ops wq_affn_dfl_ops = {\\n\\t.set\\t= wq_affn_dfl_set,\\n\\t.get\\t= wq_affn_dfl_get,\\n};\\n\\nmodule_param_cb(default_affinity_scope, &wq_affn_dfl_ops, NULL, 0644);\\n\\n#ifdef CONFIG_SYSFS\\n/*\\n * Workqueues with WQ_SYSFS flag set is visible to userland via\\n * /sys/bus/workqueue/devices/WQ_NAME.  All visible workqueues have the\\n * following attributes.\\n *\\n *  per_cpu\\t\\tRO bool\\t: whether the workqueue is per-cpu or unbound\\n *  max_active\\t\\tRW int\\t: maximum number of in-flight work items\\n *\\n * Unbound workqueues have the following extra attributes.\\n *\\n *  nice\\t\\tRW int\\t: nice value of the workers\\n *  cpumask\\t\\tRW mask\\t: bitmask of allowed CPUs for the workers\\n *  affinity_scope\\tRW str  : worker CPU affinity scope (cache, numa, none)\\n *  affinity_strict\\tRW bool : worker CPU affinity is strict\\n */\\nstruct wq_device {\\n\\tstruct workqueue_struct\\t\\t*wq;\\n\\tstruct device\\t\\t\\tdev;\\n};\\n\\nstatic struct workqueue_struct *dev_to_wq(struct device *dev)\\n{\\n\\tstruct wq_device *wq_dev = container_of(dev, struct wq_device, dev);\\n\\n\\treturn wq_dev->wq;\\n}\\n\\nstatic ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,\\n\\t\\t\\t    char *buf)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\n\\treturn scnprintf(buf, PAGE_SIZE, \"%d\\\\n\", (bool)!(wq->flags & WQ_UNBOUND));\\n}\\nstatic DEVICE_ATTR_RO(per_cpu);\\n\\nstatic ssize_t max_active_show(struct device *dev,\\n\\t\\t\\t       struct device_attribute *attr, char *buf)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\n\\treturn scnprintf(buf, PAGE_SIZE, \"%d\\\\n\", wq->saved_max_active);\\n}\\n\\nstatic ssize_t max_active_store(struct device *dev,\\n\\t\\t\\t\\tstruct device_attribute *attr, const char *buf,\\n\\t\\t\\t\\tsize_t count)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\tint val;\\n\\n\\tif (sscanf(buf, \"%d\", &val) != 1 || val <= 0)\\n\\t\\treturn -EINVAL;\\n\\n\\tworkqueue_set_max_active(wq, val);\\n\\treturn count;\\n}\\nstatic DEVICE_ATTR_RW(max_active);\\n\\nstatic struct attribute *wq_sysfs_attrs[] = {\\n\\t&dev_attr_per_cpu.attr,\\n\\t&dev_attr_max_active.attr,\\n\\tNULL,\\n};\\nATTRIBUTE_GROUPS(wq_sysfs);\\n\\nstatic ssize_t wq_nice_show(struct device *dev, struct device_attribute *attr,\\n\\t\\t\\t    char *buf)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\tint written;\\n\\n\\tmutex_lock(&wq->mutex);\\n\\twritten = scnprintf(buf, PAGE_SIZE, \"%d\\\\n\", wq->unbound_attrs->nice);\\n\\tmutex_unlock(&wq->mutex);\\n\\n\\treturn written;\\n}\\n\\n/* prepare workqueue_attrs for sysfs store operations */\\nstatic struct workqueue_attrs *wq_sysfs_prep_attrs(struct workqueue_struct *wq)\\n{\\n\\tstruct workqueue_attrs *attrs;\\n\\n\\tlockdep_assert_held(&wq_pool_mutex);\\n\\n\\tattrs = alloc_workqueue_attrs();\\n\\tif (!attrs)\\n\\t\\treturn NULL;\\n\\n\\tcopy_workqueue_attrs(attrs, wq->unbound_attrs);\\n\\treturn attrs;\\n}\\n\\nstatic ssize_t wq_nice_store(struct device *dev, struct device_attribute *attr,\\n\\t\\t\\t     const char *buf, size_t count)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\tstruct workqueue_attrs *attrs;\\n\\tint ret = -ENOMEM;\\n\\n\\tapply_wqattrs_lock();\\n\\n\\tattrs = wq_sysfs_prep_attrs(wq);\\n\\tif (!attrs)\\n\\t\\tgoto out_unlock;\\n\\n\\tif (sscanf(buf, \"%d\", &attrs->nice) == 1 &&\\n\\t    attrs->nice >= MIN_NICE && attrs->nice <= MAX_NICE)\\n\\t\\tret = apply_workqueue_attrs_locked(wq, attrs);\\n\\telse\\n\\t\\tret = -EINVAL;\\n\\nout_unlock:\\n\\tapply_wqattrs_unlock();\\n\\tfree_workqueue_attrs(attrs);\\n\\treturn ret ?: count;\\n}\\n\\nstatic ssize_t wq_cpumask_show(struct device *dev,\\n\\t\\t\\t       struct device_attribute *attr, char *buf)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\tint written;\\n\\n\\tmutex_lock(&wq->mutex);\\n\\twritten = scnprintf(buf, PAGE_SIZE, \"%*pb\\\\n\",\\n\\t\\t\\t    cpumask_pr_args(wq->unbound_attrs->cpumask));\\n\\tmutex_unlock(&wq->mutex);\\n\\treturn written;\\n}\\n\\nstatic ssize_t wq_cpumask_store(struct device *dev,\\n\\t\\t\\t\\tstruct device_attribute *attr,\\n\\t\\t\\t\\tconst char *buf, size_t count)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\tstruct workqueue_attrs *attrs;\\n\\tint ret = -ENOMEM;\\n\\n\\tapply_wqattrs_lock();\\n\\n\\tattrs = wq_sysfs_prep_attrs(wq);\\n\\tif (!attrs)\\n\\t\\tgoto out_unlock;\\n\\n\\tret = cpumask_parse(buf, attrs->cpumask);\\n\\tif (!ret)\\n\\t\\tret = apply_workqueue_attrs_locked(wq, attrs);\\n\\nout_unlock:\\n\\tapply_wqattrs_unlock();\\n\\tfree_workqueue_attrs(attrs);\\n\\treturn ret ?: count;\\n}\\n\\nstatic ssize_t wq_affn_scope_show(struct device *dev,\\n\\t\\t\\t\\t  struct device_attribute *attr, char *buf)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\tint written;\\n\\n\\tmutex_lock(&wq->mutex);\\n\\tif (wq->unbound_attrs->affn_scope == WQ_AFFN_DFL)\\n\\t\\twritten = scnprintf(buf, PAGE_SIZE, \"%s (%s)\\\\n\",\\n\\t\\t\\t\\t    wq_affn_names[WQ_AFFN_DFL],\\n\\t\\t\\t\\t    wq_affn_names[wq_affn_dfl]);\\n\\telse\\n\\t\\twritten = scnprintf(buf, PAGE_SIZE, \"%s\\\\n\",\\n\\t\\t\\t\\t    wq_affn_names[wq->unbound_attrs->affn_scope]);\\n\\tmutex_unlock(&wq->mutex);\\n\\n\\treturn written;\\n}\\n\\nstatic ssize_t wq_affn_scope_store(struct device *dev,\\n\\t\\t\\t\\t   struct device_attribute *attr,\\n\\t\\t\\t\\t   const char *buf, size_t count)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\tstruct workqueue_attrs *attrs;\\n\\tint affn, ret = -ENOMEM;\\n\\n\\taffn = parse_affn_scope(buf);\\n\\tif (affn < 0)\\n\\t\\treturn affn;\\n\\n\\tapply_wqattrs_lock();\\n\\tattrs = wq_sysfs_prep_attrs(wq);\\n\\tif (attrs) {\\n\\t\\tattrs->affn_scope = affn;\\n\\t\\tret = apply_workqueue_attrs_locked(wq, attrs);\\n\\t}\\n\\tapply_wqattrs_unlock();\\n\\tfree_workqueue_attrs(attrs);\\n\\treturn ret ?: count;\\n}\\n\\nstatic ssize_t wq_affinity_strict_show(struct device *dev,\\n\\t\\t\\t\\t       struct device_attribute *attr, char *buf)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\n\\treturn scnprintf(buf, PAGE_SIZE, \"%d\\\\n\",\\n\\t\\t\\t wq->unbound_attrs->affn_strict);\\n}\\n\\nstatic ssize_t wq_affinity_strict_store(struct device *dev,\\n\\t\\t\\t\\t\\tstruct device_attribute *attr,\\n\\t\\t\\t\\t\\tconst char *buf, size_t count)\\n{\\n\\tstruct workqueue_struct *wq = dev_to_wq(dev);\\n\\tstruct workqueue_attrs *attrs;\\n\\tint v, ret = -ENOMEM;\\n\\n\\tif (sscanf(buf, \"%d\", &v) != 1)\\n\\t\\treturn -EINVAL;\\n\\n\\tapply_wqattrs_lock();\\n\\tattrs = wq_sysfs_prep_attrs(wq);\\n\\tif (attrs) {\\n\\t\\tattrs->affn_strict = (bool)v;\\n\\t\\tret = apply_workqueue_attrs_locked(wq, attrs);\\n\\t}\\n\\tapply_wqattrs_unlock();\\n\\tfree_workqueue_attrs(attrs);\\n\\treturn ret ?: count;\\n}\\n\\nstatic struct device_attribute wq_sysfs_unbound_attrs[] = {\\n\\t__ATTR(nice, 0644, wq_nice_show, wq_nice_store),\\n\\t__ATTR(cpumask, 0644, wq_cpumask_show, wq_cpumask_store),\\n\\t__ATTR(affinity_scope, 0644, wq_affn_scope_show, wq_affn_scope_store),\\n\\t__ATTR(affinity_strict, 0644, wq_affinity_strict_show, wq_affinity_strict_store),\\n\\t__ATTR_NULL,\\n};\\n\\nstatic const struct bus_type wq_subsys = {\\n\\t.name\\t\\t\\t\\t= \"workqueue\",\\n\\t.dev_groups\\t\\t\\t= wq_sysfs_groups,\\n};\\n\\n/**\\n *  workqueue_set_unbound_cpumask - Set the low-level unbound cpumask\\n *  @cpumask: the cpumask to set\\n *\\n *  The low-level workqueues cpumask is a global cpumask that limits\\n *  the affinity of all unbound workqueues.  This function check the @cpumask\\n *  and apply it to all unbound workqueues and updates all pwqs of them.\\n *\\n *  Return:\\t0\\t- Success\\n *\\t\\t-EINVAL\\t- Invalid @cpumask\\n *\\t\\t-ENOMEM\\t- Failed to allocate memory for attrs or pwqs.\\n */\\nstatic int workqueue_set_unbound_cpumask(cpumask_var_t cpumask)\\n{\\n\\tint ret = -EINVAL;\\n\\n\\t/*\\n\\t * Not excluding isolated cpus on purpose.\\n\\t * If the user wishes to include them, we allow that.\\n\\t */\\n\\tcpumask_and(cpumask, cpumask, cpu_possible_mask);\\n\\tif (!cpumask_empty(cpumask)) {\\n\\t\\tret = 0;\\n\\t\\tapply_wqattrs_lock();\\n\\t\\tif (!cpumask_equal(cpumask, wq_unbound_cpumask))\\n\\t\\t\\tret = workqueue_apply_unbound_cpumask(cpumask);\\n\\t\\tif (!ret)\\n\\t\\t\\tcpumask_copy(wq_requested_unbound_cpumask, cpumask);\\n\\t\\tapply_wqattrs_unlock();\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic ssize_t __wq_cpumask_show(struct device *dev,\\n\\t\\tstruct device_attribute *attr, char *buf, cpumask_var_t mask)\\n{\\n\\tint written;\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\twritten = scnprintf(buf, PAGE_SIZE, \"%*pb\\\\n\", cpumask_pr_args(mask));\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\n\\treturn written;\\n}\\n\\nstatic ssize_t cpumask_requested_show(struct device *dev,\\n\\t\\tstruct device_attribute *attr, char *buf)\\n{\\n\\treturn __wq_cpumask_show(dev, attr, buf, wq_requested_unbound_cpumask);\\n}\\nstatic DEVICE_ATTR_RO(cpumask_requested);\\n\\nstatic ssize_t cpumask_isolated_show(struct device *dev,\\n\\t\\tstruct device_attribute *attr, char *buf)\\n{\\n\\treturn __wq_cpumask_show(dev, attr, buf, wq_isolated_cpumask);\\n}\\nstatic DEVICE_ATTR_RO(cpumask_isolated);\\n\\nstatic ssize_t cpumask_show(struct device *dev,\\n\\t\\tstruct device_attribute *attr, char *buf)\\n{\\n\\treturn __wq_cpumask_show(dev, attr, buf, wq_unbound_cpumask);\\n}\\n\\nstatic ssize_t cpumask_store(struct device *dev,\\n\\t\\tstruct device_attribute *attr, const char *buf, size_t count)\\n{\\n\\tcpumask_var_t cpumask;\\n\\tint ret;\\n\\n\\tif (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\n\\tret = cpumask_parse(buf, cpumask);\\n\\tif (!ret)\\n\\t\\tret = workqueue_set_unbound_cpumask(cpumask);\\n\\n\\tfree_cpumask_var(cpumask);\\n\\treturn ret ? ret : count;\\n}\\nstatic DEVICE_ATTR_RW(cpumask);\\n\\nstatic struct attribute *wq_sysfs_cpumask_attrs[] = {\\n\\t&dev_attr_cpumask.attr,\\n\\t&dev_attr_cpumask_requested.attr,\\n\\t&dev_attr_cpumask_isolated.attr,\\n\\tNULL,\\n};\\nATTRIBUTE_GROUPS(wq_sysfs_cpumask);\\n\\nstatic int __init wq_sysfs_init(void)\\n{\\n\\treturn subsys_virtual_register(&wq_subsys, wq_sysfs_cpumask_groups);\\n}\\ncore_initcall(wq_sysfs_init);\\n\\nstatic void wq_device_release(struct device *dev)\\n{\\n\\tstruct wq_device *wq_dev = container_of(dev, struct wq_device, dev);\\n\\n\\tkfree(wq_dev);\\n}\\n\\n/**\\n * workqueue_sysfs_register - make a workqueue visible in sysfs\\n * @wq: the workqueue to register\\n *\\n * Expose @wq in sysfs under /sys/bus/workqueue/devices.\\n * alloc_workqueue*() automatically calls this function if WQ_SYSFS is set\\n * which is the preferred method.\\n *\\n * Workqueue user should use this function directly iff it wants to apply\\n * workqueue_attrs before making the workqueue visible in sysfs; otherwise,\\n * apply_workqueue_attrs() may race against userland updating the\\n * attributes.\\n *\\n * Return: 0 on success, -errno on failure.\\n */\\nint workqueue_sysfs_register(struct workqueue_struct *wq)\\n{\\n\\tstruct wq_device *wq_dev;\\n\\tint ret;\\n\\n\\t/*\\n\\t * Adjusting max_active breaks ordering guarantee.  Disallow exposing\\n\\t * ordered workqueues.\\n\\t */\\n\\tif (WARN_ON(wq->flags & __WQ_ORDERED))\\n\\t\\treturn -EINVAL;\\n\\n\\twq->wq_dev = wq_dev = kzalloc(sizeof(*wq_dev), GFP_KERNEL);\\n\\tif (!wq_dev)\\n\\t\\treturn -ENOMEM;\\n\\n\\twq_dev->wq = wq;\\n\\twq_dev->dev.bus = &wq_subsys;\\n\\twq_dev->dev.release = wq_device_release;\\n\\tdev_set_name(&wq_dev->dev, \"%s\", wq->name);\\n\\n\\t/*\\n\\t * unbound_attrs are created separately.  Suppress uevent until\\n\\t * everything is ready.\\n\\t */\\n\\tdev_set_uevent_suppress(&wq_dev->dev, true);\\n\\n\\tret = device_register(&wq_dev->dev);\\n\\tif (ret) {\\n\\t\\tput_device(&wq_dev->dev);\\n\\t\\twq->wq_dev = NULL;\\n\\t\\treturn ret;\\n\\t}\\n\\n\\tif (wq->flags & WQ_UNBOUND) {\\n\\t\\tstruct device_attribute *attr;\\n\\n\\t\\tfor (attr = wq_sysfs_unbound_attrs; attr->attr.name; attr++) {\\n\\t\\t\\tret = device_create_file(&wq_dev->dev, attr);\\n\\t\\t\\tif (ret) {\\n\\t\\t\\t\\tdevice_unregister(&wq_dev->dev);\\n\\t\\t\\t\\twq->wq_dev = NULL;\\n\\t\\t\\t\\treturn ret;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\tdev_set_uevent_suppress(&wq_dev->dev, false);\\n\\tkobject_uevent(&wq_dev->dev.kobj, KOBJ_ADD);\\n\\treturn 0;\\n}\\n\\n/**\\n * workqueue_sysfs_unregister - undo workqueue_sysfs_register()\\n * @wq: the workqueue to unregister\\n *\\n * If @wq is registered to sysfs by workqueue_sysfs_register(), unregister.\\n */\\nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq)\\n{\\n\\tstruct wq_device *wq_dev = wq->wq_dev;\\n\\n\\tif (!wq->wq_dev)\\n\\t\\treturn;\\n\\n\\twq->wq_dev = NULL;\\n\\tdevice_unregister(&wq_dev->dev);\\n}\\n#else\\t/* CONFIG_SYSFS */\\nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq)\\t{ }\\n#endif\\t/* CONFIG_SYSFS */\\n\\n/*\\n * Workqueue watchdog.\\n *\\n * Stall may be caused by various bugs - missing WQ_MEM_RECLAIM, illegal\\n * flush dependency, a concurrency managed work item which stays RUNNING\\n * indefinitely.  Workqueue stalls can be very difficult to debug as the\\n * usual warning mechanisms don\\'t trigger and internal workqueue state is\\n * largely opaque.\\n *\\n * Workqueue watchdog monitors all worker pools periodically and dumps\\n * state if some pools failed to make forward progress for a while where\\n * forward progress is defined as the first item on ->worklist changing.\\n *\\n * This mechanism is controlled through the kernel parameter\\n * \"workqueue.watchdog_thresh\" which can be updated at runtime through the\\n * corresponding sysfs parameter file.\\n */\\n#ifdef CONFIG_WQ_WATCHDOG\\n\\nstatic unsigned long wq_watchdog_thresh = 30;\\nstatic struct timer_list wq_watchdog_timer;\\n\\nstatic unsigned long wq_watchdog_touched = INITIAL_JIFFIES;\\nstatic DEFINE_PER_CPU(unsigned long, wq_watchdog_touched_cpu) = INITIAL_JIFFIES;\\n\\nstatic unsigned int wq_panic_on_stall;\\nmodule_param_named(panic_on_stall, wq_panic_on_stall, uint, 0644);\\n\\n/*\\n * Show workers that might prevent the processing of pending work items.\\n * The only candidates are CPU-bound workers in the running state.\\n * Pending work items should be handled by another idle worker\\n * in all other situations.\\n */\\nstatic void show_cpu_pool_hog(struct worker_pool *pool)\\n{\\n\\tstruct worker *worker;\\n\\tunsigned long irq_flags;\\n\\tint bkt;\\n\\n\\traw_spin_lock_irqsave(&pool->lock, irq_flags);\\n\\n\\thash_for_each(pool->busy_hash, bkt, worker, hentry) {\\n\\t\\tif (task_is_running(worker->task)) {\\n\\t\\t\\t/*\\n\\t\\t\\t * Defer printing to avoid deadlocks in console\\n\\t\\t\\t * drivers that queue work while holding locks\\n\\t\\t\\t * also taken in their write paths.\\n\\t\\t\\t */\\n\\t\\t\\tprintk_deferred_enter();\\n\\n\\t\\t\\tpr_info(\"pool %d:\\\\n\", pool->id);\\n\\t\\t\\tsched_show_task(worker->task);\\n\\n\\t\\t\\tprintk_deferred_exit();\\n\\t\\t}\\n\\t}\\n\\n\\traw_spin_unlock_irqrestore(&pool->lock, irq_flags);\\n}\\n\\nstatic void show_cpu_pools_hogs(void)\\n{\\n\\tstruct worker_pool *pool;\\n\\tint pi;\\n\\n\\tpr_info(\"Showing backtraces of running workers in stalled CPU-bound worker pools:\\\\n\");\\n\\n\\trcu_read_lock();\\n\\n\\tfor_each_pool(pool, pi) {\\n\\t\\tif (pool->cpu_stall)\\n\\t\\t\\tshow_cpu_pool_hog(pool);\\n\\n\\t}\\n\\n\\trcu_read_unlock();\\n}\\n\\nstatic void panic_on_wq_watchdog(void)\\n{\\n\\tstatic unsigned int wq_stall;\\n\\n\\tif (wq_panic_on_stall) {\\n\\t\\twq_stall++;\\n\\t\\tBUG_ON(wq_stall >= wq_panic_on_stall);\\n\\t}\\n}\\n\\nstatic void wq_watchdog_reset_touched(void)\\n{\\n\\tint cpu;\\n\\n\\twq_watchdog_touched = jiffies;\\n\\tfor_each_possible_cpu(cpu)\\n\\t\\tper_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;\\n}\\n\\nstatic void wq_watchdog_timer_fn(struct timer_list *unused)\\n{\\n\\tunsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;\\n\\tbool lockup_detected = false;\\n\\tbool cpu_pool_stall = false;\\n\\tunsigned long now = jiffies;\\n\\tstruct worker_pool *pool;\\n\\tint pi;\\n\\n\\tif (!thresh)\\n\\t\\treturn;\\n\\n\\trcu_read_lock();\\n\\n\\tfor_each_pool(pool, pi) {\\n\\t\\tunsigned long pool_ts, touched, ts;\\n\\n\\t\\tpool->cpu_stall = false;\\n\\t\\tif (list_empty(&pool->worklist))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/*\\n\\t\\t * If a virtual machine is stopped by the host it can look to\\n\\t\\t * the watchdog like a stall.\\n\\t\\t */\\n\\t\\tkvm_check_and_clear_guest_paused();\\n\\n\\t\\t/* get the latest of pool and touched timestamps */\\n\\t\\tif (pool->cpu >= 0)\\n\\t\\t\\ttouched = READ_ONCE(per_cpu(wq_watchdog_touched_cpu, pool->cpu));\\n\\t\\telse\\n\\t\\t\\ttouched = READ_ONCE(wq_watchdog_touched);\\n\\t\\tpool_ts = READ_ONCE(pool->watchdog_ts);\\n\\n\\t\\tif (time_after(pool_ts, touched))\\n\\t\\t\\tts = pool_ts;\\n\\t\\telse\\n\\t\\t\\tts = touched;\\n\\n\\t\\t/* did we stall? */\\n\\t\\tif (time_after(now, ts + thresh)) {\\n\\t\\t\\tlockup_detected = true;\\n\\t\\t\\tif (pool->cpu >= 0 && !(pool->flags & POOL_BH)) {\\n\\t\\t\\t\\tpool->cpu_stall = true;\\n\\t\\t\\t\\tcpu_pool_stall = true;\\n\\t\\t\\t}\\n\\t\\t\\tpr_emerg(\"BUG: workqueue lockup - pool\");\\n\\t\\t\\tpr_cont_pool_info(pool);\\n\\t\\t\\tpr_cont(\" stuck for %us!\\\\n\",\\n\\t\\t\\t\\tjiffies_to_msecs(now - pool_ts) / 1000);\\n\\t\\t}\\n\\n\\n\\t}\\n\\n\\trcu_read_unlock();\\n\\n\\tif (lockup_detected)\\n\\t\\tshow_all_workqueues();\\n\\n\\tif (cpu_pool_stall)\\n\\t\\tshow_cpu_pools_hogs();\\n\\n\\tif (lockup_detected)\\n\\t\\tpanic_on_wq_watchdog();\\n\\n\\twq_watchdog_reset_touched();\\n\\tmod_timer(&wq_watchdog_timer, jiffies + thresh);\\n}\\n\\nnotrace void wq_watchdog_touch(int cpu)\\n{\\n\\tunsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;\\n\\tunsigned long touch_ts = READ_ONCE(wq_watchdog_touched);\\n\\tunsigned long now = jiffies;\\n\\n\\tif (cpu >= 0)\\n\\t\\tper_cpu(wq_watchdog_touched_cpu, cpu) = now;\\n\\telse\\n\\t\\tWARN_ONCE(1, \"%s should be called with valid CPU\", __func__);\\n\\n\\t/* Don\\'t unnecessarily store to global cacheline */\\n\\tif (time_after(now, touch_ts + thresh / 4))\\n\\t\\tWRITE_ONCE(wq_watchdog_touched, jiffies);\\n}\\n\\nstatic void wq_watchdog_set_thresh(unsigned long thresh)\\n{\\n\\twq_watchdog_thresh = 0;\\n\\tdel_timer_sync(&wq_watchdog_timer);\\n\\n\\tif (thresh) {\\n\\t\\twq_watchdog_thresh = thresh;\\n\\t\\twq_watchdog_reset_touched();\\n\\t\\tmod_timer(&wq_watchdog_timer, jiffies + thresh * HZ);\\n\\t}\\n}\\n\\nstatic int wq_watchdog_param_set_thresh(const char *val,\\n\\t\\t\\t\\t\\tconst struct kernel_param *kp)\\n{\\n\\tunsigned long thresh;\\n\\tint ret;\\n\\n\\tret = kstrtoul(val, 0, &thresh);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (system_wq)\\n\\t\\twq_watchdog_set_thresh(thresh);\\n\\telse\\n\\t\\twq_watchdog_thresh = thresh;\\n\\n\\treturn 0;\\n}\\n\\nstatic const struct kernel_param_ops wq_watchdog_thresh_ops = {\\n\\t.set\\t= wq_watchdog_param_set_thresh,\\n\\t.get\\t= param_get_ulong,\\n};\\n\\nmodule_param_cb(watchdog_thresh, &wq_watchdog_thresh_ops, &wq_watchdog_thresh,\\n\\t\\t0644);\\n\\nstatic void wq_watchdog_init(void)\\n{\\n\\ttimer_setup(&wq_watchdog_timer, wq_watchdog_timer_fn, TIMER_DEFERRABLE);\\n\\twq_watchdog_set_thresh(wq_watchdog_thresh);\\n}\\n\\n#else\\t/* CONFIG_WQ_WATCHDOG */\\n\\nstatic inline void wq_watchdog_init(void) { }\\n\\n#endif\\t/* CONFIG_WQ_WATCHDOG */\\n\\nstatic void bh_pool_kick_normal(struct irq_work *irq_work)\\n{\\n\\traise_softirq_irqoff(TASKLET_SOFTIRQ);\\n}\\n\\nstatic void bh_pool_kick_highpri(struct irq_work *irq_work)\\n{\\n\\traise_softirq_irqoff(HI_SOFTIRQ);\\n}\\n\\nstatic void __init restrict_unbound_cpumask(const char *name, const struct cpumask *mask)\\n{\\n\\tif (!cpumask_intersects(wq_unbound_cpumask, mask)) {\\n\\t\\tpr_warn(\"workqueue: Restricting unbound_cpumask (%*pb) with %s (%*pb) leaves no CPU, ignoring\\\\n\",\\n\\t\\t\\tcpumask_pr_args(wq_unbound_cpumask), name, cpumask_pr_args(mask));\\n\\t\\treturn;\\n\\t}\\n\\n\\tcpumask_and(wq_unbound_cpumask, wq_unbound_cpumask, mask);\\n}\\n\\nstatic void __init init_cpu_worker_pool(struct worker_pool *pool, int cpu, int nice)\\n{\\n\\tBUG_ON(init_worker_pool(pool));\\n\\tpool->cpu = cpu;\\n\\tcpumask_copy(pool->attrs->cpumask, cpumask_of(cpu));\\n\\tcpumask_copy(pool->attrs->__pod_cpumask, cpumask_of(cpu));\\n\\tpool->attrs->nice = nice;\\n\\tpool->attrs->affn_strict = true;\\n\\tpool->node = cpu_to_node(cpu);\\n\\n\\t/* alloc pool ID */\\n\\tmutex_lock(&wq_pool_mutex);\\n\\tBUG_ON(worker_pool_assign_id(pool));\\n\\tmutex_unlock(&wq_pool_mutex);\\n}\\n\\n/**\\n * workqueue_init_early - early init for workqueue subsystem\\n *\\n * This is the first step of three-staged workqueue subsystem initialization and\\n * invoked as soon as the bare basics - memory allocation, cpumasks and idr are\\n * up. It sets up all the data structures and system workqueues and allows early\\n * boot code to create workqueues and queue/cancel work items. Actual work item\\n * execution starts only after kthreads can be created and scheduled right\\n * before early initcalls.\\n */\\nvoid __init workqueue_init_early(void)\\n{\\n\\tstruct wq_pod_type *pt = &wq_pod_types[WQ_AFFN_SYSTEM];\\n\\tint std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };\\n\\tvoid (*irq_work_fns[2])(struct irq_work *) = { bh_pool_kick_normal,\\n\\t\\t\\t\\t\\t\\t       bh_pool_kick_highpri };\\n\\tint i, cpu;\\n\\n\\tBUILD_BUG_ON(__alignof__(struct pool_workqueue) < __alignof__(long long));\\n\\n\\tBUG_ON(!alloc_cpumask_var(&wq_online_cpumask, GFP_KERNEL));\\n\\tBUG_ON(!alloc_cpumask_var(&wq_unbound_cpumask, GFP_KERNEL));\\n\\tBUG_ON(!alloc_cpumask_var(&wq_requested_unbound_cpumask, GFP_KERNEL));\\n\\tBUG_ON(!zalloc_cpumask_var(&wq_isolated_cpumask, GFP_KERNEL));\\n\\n\\tcpumask_copy(wq_online_cpumask, cpu_online_mask);\\n\\tcpumask_copy(wq_unbound_cpumask, cpu_possible_mask);\\n\\trestrict_unbound_cpumask(\"HK_TYPE_WQ\", housekeeping_cpumask(HK_TYPE_WQ));\\n\\trestrict_unbound_cpumask(\"HK_TYPE_DOMAIN\", housekeeping_cpumask(HK_TYPE_DOMAIN));\\n\\tif (!cpumask_empty(&wq_cmdline_cpumask))\\n\\t\\trestrict_unbound_cpumask(\"workqueue.unbound_cpus\", &wq_cmdline_cpumask);\\n\\n\\tcpumask_copy(wq_requested_unbound_cpumask, wq_unbound_cpumask);\\n\\n\\tpwq_cache = KMEM_CACHE(pool_workqueue, SLAB_PANIC);\\n\\n\\tunbound_wq_update_pwq_attrs_buf = alloc_workqueue_attrs();\\n\\tBUG_ON(!unbound_wq_update_pwq_attrs_buf);\\n\\n\\t/*\\n\\t * If nohz_full is enabled, set power efficient workqueue as unbound.\\n\\t * This allows workqueue items to be moved to HK CPUs.\\n\\t */\\n\\tif (housekeeping_enabled(HK_TYPE_TICK))\\n\\t\\twq_power_efficient = true;\\n\\n\\t/* initialize WQ_AFFN_SYSTEM pods */\\n\\tpt->pod_cpus = kcalloc(1, sizeof(pt->pod_cpus[0]), GFP_KERNEL);\\n\\tpt->pod_node = kcalloc(1, sizeof(pt->pod_node[0]), GFP_KERNEL);\\n\\tpt->cpu_pod = kcalloc(nr_cpu_ids, sizeof(pt->cpu_pod[0]), GFP_KERNEL);\\n\\tBUG_ON(!pt->pod_cpus || !pt->pod_node || !pt->cpu_pod);\\n\\n\\tBUG_ON(!zalloc_cpumask_var_node(&pt->pod_cpus[0], GFP_KERNEL, NUMA_NO_NODE));\\n\\n\\tpt->nr_pods = 1;\\n\\tcpumask_copy(pt->pod_cpus[0], cpu_possible_mask);\\n\\tpt->pod_node[0] = NUMA_NO_NODE;\\n\\tpt->cpu_pod[0] = 0;\\n\\n\\t/* initialize BH and CPU pools */\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tstruct worker_pool *pool;\\n\\n\\t\\ti = 0;\\n\\t\\tfor_each_bh_worker_pool(pool, cpu) {\\n\\t\\t\\tinit_cpu_worker_pool(pool, cpu, std_nice[i]);\\n\\t\\t\\tpool->flags |= POOL_BH;\\n\\t\\t\\tinit_irq_work(bh_pool_irq_work(pool), irq_work_fns[i]);\\n\\t\\t\\ti++;\\n\\t\\t}\\n\\n\\t\\ti = 0;\\n\\t\\tfor_each_cpu_worker_pool(pool, cpu)\\n\\t\\t\\tinit_cpu_worker_pool(pool, cpu, std_nice[i++]);\\n\\t}\\n\\n\\t/* create default unbound and ordered wq attrs */\\n\\tfor (i = 0; i < NR_STD_WORKER_POOLS; i++) {\\n\\t\\tstruct workqueue_attrs *attrs;\\n\\n\\t\\tBUG_ON(!(attrs = alloc_workqueue_attrs()));\\n\\t\\tattrs->nice = std_nice[i];\\n\\t\\tunbound_std_wq_attrs[i] = attrs;\\n\\n\\t\\t/*\\n\\t\\t * An ordered wq should have only one pwq as ordering is\\n\\t\\t * guaranteed by max_active which is enforced by pwqs.\\n\\t\\t */\\n\\t\\tBUG_ON(!(attrs = alloc_workqueue_attrs()));\\n\\t\\tattrs->nice = std_nice[i];\\n\\t\\tattrs->ordered = true;\\n\\t\\tordered_wq_attrs[i] = attrs;\\n\\t}\\n\\n\\tsystem_wq = alloc_workqueue(\"events\", 0, 0);\\n\\tsystem_highpri_wq = alloc_workqueue(\"events_highpri\", WQ_HIGHPRI, 0);\\n\\tsystem_long_wq = alloc_workqueue(\"events_long\", 0, 0);\\n\\tsystem_unbound_wq = alloc_workqueue(\"events_unbound\", WQ_UNBOUND,\\n\\t\\t\\t\\t\\t    WQ_MAX_ACTIVE);\\n\\tsystem_freezable_wq = alloc_workqueue(\"events_freezable\",\\n\\t\\t\\t\\t\\t      WQ_FREEZABLE, 0);\\n\\tsystem_power_efficient_wq = alloc_workqueue(\"events_power_efficient\",\\n\\t\\t\\t\\t\\t      WQ_POWER_EFFICIENT, 0);\\n\\tsystem_freezable_power_efficient_wq = alloc_workqueue(\"events_freezable_pwr_efficient\",\\n\\t\\t\\t\\t\\t      WQ_FREEZABLE | WQ_POWER_EFFICIENT,\\n\\t\\t\\t\\t\\t      0);\\n\\tsystem_bh_wq = alloc_workqueue(\"events_bh\", WQ_BH, 0);\\n\\tsystem_bh_highpri_wq = alloc_workqueue(\"events_bh_highpri\",\\n\\t\\t\\t\\t\\t       WQ_BH | WQ_HIGHPRI, 0);\\n\\tBUG_ON(!system_wq || !system_highpri_wq || !system_long_wq ||\\n\\t       !system_unbound_wq || !system_freezable_wq ||\\n\\t       !system_power_efficient_wq ||\\n\\t       !system_freezable_power_efficient_wq ||\\n\\t       !system_bh_wq || !system_bh_highpri_wq);\\n}\\n\\nstatic void __init wq_cpu_intensive_thresh_init(void)\\n{\\n\\tunsigned long thresh;\\n\\tunsigned long bogo;\\n\\n\\tpwq_release_worker = kthread_create_worker(0, \"pool_workqueue_release\");\\n\\tBUG_ON(IS_ERR(pwq_release_worker));\\n\\n\\t/* if the user set it to a specific value, keep it */\\n\\tif (wq_cpu_intensive_thresh_us != ULONG_MAX)\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * The default of 10ms is derived from the fact that most modern (as of\\n\\t * 2023) processors can do a lot in 10ms and that it\\'s just below what\\n\\t * most consider human-perceivable. However, the kernel also runs on a\\n\\t * lot slower CPUs including microcontrollers where the threshold is way\\n\\t * too low.\\n\\t *\\n\\t * Let\\'s scale up the threshold upto 1 second if BogoMips is below 4000.\\n\\t * This is by no means accurate but it doesn\\'t have to be. The mechanism\\n\\t * is still useful even when the threshold is fully scaled up. Also, as\\n\\t * the reports would usually be applicable to everyone, some machines\\n\\t * operating on longer thresholds won\\'t significantly diminish their\\n\\t * usefulness.\\n\\t */\\n\\tthresh = 10 * USEC_PER_MSEC;\\n\\n\\t/* see init/calibrate.c for lpj -> BogoMIPS calculation */\\n\\tbogo = max_t(unsigned long, loops_per_jiffy / 500000 * HZ, 1);\\n\\tif (bogo < 4000)\\n\\t\\tthresh = min_t(unsigned long, thresh * 4000 / bogo, USEC_PER_SEC);\\n\\n\\tpr_debug(\"wq_cpu_intensive_thresh: lpj=%lu BogoMIPS=%lu thresh_us=%lu\\\\n\",\\n\\t\\t loops_per_jiffy, bogo, thresh);\\n\\n\\twq_cpu_intensive_thresh_us = thresh;\\n}\\n\\n/**\\n * workqueue_init - bring workqueue subsystem fully online\\n *\\n * This is the second step of three-staged workqueue subsystem initialization\\n * and invoked as soon as kthreads can be created and scheduled. Workqueues have\\n * been created and work items queued on them, but there are no kworkers\\n * executing the work items yet. Populate the worker pools with the initial\\n * workers and enable future kworker creations.\\n */\\nvoid __init workqueue_init(void)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\tstruct worker_pool *pool;\\n\\tint cpu, bkt;\\n\\n\\twq_cpu_intensive_thresh_init();\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\t/*\\n\\t * Per-cpu pools created earlier could be missing node hint. Fix them\\n\\t * up. Also, create a rescuer for workqueues that requested it.\\n\\t */\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tfor_each_bh_worker_pool(pool, cpu)\\n\\t\\t\\tpool->node = cpu_to_node(cpu);\\n\\t\\tfor_each_cpu_worker_pool(pool, cpu)\\n\\t\\t\\tpool->node = cpu_to_node(cpu);\\n\\t}\\n\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tWARN(init_rescuer(wq),\\n\\t\\t     \"workqueue: failed to create early rescuer for %s\",\\n\\t\\t     wq->name);\\n\\t}\\n\\n\\tmutex_unlock(&wq_pool_mutex);\\n\\n\\t/*\\n\\t * Create the initial workers. A BH pool has one pseudo worker that\\n\\t * represents the shared BH execution context and thus doesn\\'t get\\n\\t * affected by hotplug events. Create the BH pseudo workers for all\\n\\t * possible CPUs here.\\n\\t */\\n\\tfor_each_possible_cpu(cpu)\\n\\t\\tfor_each_bh_worker_pool(pool, cpu)\\n\\t\\t\\tBUG_ON(!create_worker(pool));\\n\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tfor_each_cpu_worker_pool(pool, cpu) {\\n\\t\\t\\tpool->flags &= ~POOL_DISASSOCIATED;\\n\\t\\t\\tBUG_ON(!create_worker(pool));\\n\\t\\t}\\n\\t}\\n\\n\\thash_for_each(unbound_pool_hash, bkt, pool, hash_node)\\n\\t\\tBUG_ON(!create_worker(pool));\\n\\n\\twq_online = true;\\n\\twq_watchdog_init();\\n}\\n\\n/*\\n * Initialize @pt by first initializing @pt->cpu_pod[] with pod IDs according to\\n * @cpu_shares_pod(). Each subset of CPUs that share a pod is assigned a unique\\n * and consecutive pod ID. The rest of @pt is initialized accordingly.\\n */\\nstatic void __init init_pod_type(struct wq_pod_type *pt,\\n\\t\\t\\t\\t bool (*cpus_share_pod)(int, int))\\n{\\n\\tint cur, pre, cpu, pod;\\n\\n\\tpt->nr_pods = 0;\\n\\n\\t/* init @pt->cpu_pod[] according to @cpus_share_pod() */\\n\\tpt->cpu_pod = kcalloc(nr_cpu_ids, sizeof(pt->cpu_pod[0]), GFP_KERNEL);\\n\\tBUG_ON(!pt->cpu_pod);\\n\\n\\tfor_each_possible_cpu(cur) {\\n\\t\\tfor_each_possible_cpu(pre) {\\n\\t\\t\\tif (pre >= cur) {\\n\\t\\t\\t\\tpt->cpu_pod[cur] = pt->nr_pods++;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tif (cpus_share_pod(cur, pre)) {\\n\\t\\t\\t\\tpt->cpu_pod[cur] = pt->cpu_pod[pre];\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\t/* init the rest to match @pt->cpu_pod[] */\\n\\tpt->pod_cpus = kcalloc(pt->nr_pods, sizeof(pt->pod_cpus[0]), GFP_KERNEL);\\n\\tpt->pod_node = kcalloc(pt->nr_pods, sizeof(pt->pod_node[0]), GFP_KERNEL);\\n\\tBUG_ON(!pt->pod_cpus || !pt->pod_node);\\n\\n\\tfor (pod = 0; pod < pt->nr_pods; pod++)\\n\\t\\tBUG_ON(!zalloc_cpumask_var(&pt->pod_cpus[pod], GFP_KERNEL));\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tcpumask_set_cpu(cpu, pt->pod_cpus[pt->cpu_pod[cpu]]);\\n\\t\\tpt->pod_node[pt->cpu_pod[cpu]] = cpu_to_node(cpu);\\n\\t}\\n}\\n\\nstatic bool __init cpus_dont_share(int cpu0, int cpu1)\\n{\\n\\treturn false;\\n}\\n\\nstatic bool __init cpus_share_smt(int cpu0, int cpu1)\\n{\\n#ifdef CONFIG_SCHED_SMT\\n\\treturn cpumask_test_cpu(cpu0, cpu_smt_mask(cpu1));\\n#else\\n\\treturn false;\\n#endif\\n}\\n\\nstatic bool __init cpus_share_numa(int cpu0, int cpu1)\\n{\\n\\treturn cpu_to_node(cpu0) == cpu_to_node(cpu1);\\n}\\n\\n/**\\n * workqueue_init_topology - initialize CPU pods for unbound workqueues\\n *\\n * This is the third step of three-staged workqueue subsystem initialization and\\n * invoked after SMP and topology information are fully initialized. It\\n * initializes the unbound CPU pods accordingly.\\n */\\nvoid __init workqueue_init_topology(void)\\n{\\n\\tstruct workqueue_struct *wq;\\n\\tint cpu;\\n\\n\\tinit_pod_type(&wq_pod_types[WQ_AFFN_CPU], cpus_dont_share);\\n\\tinit_pod_type(&wq_pod_types[WQ_AFFN_SMT], cpus_share_smt);\\n\\tinit_pod_type(&wq_pod_types[WQ_AFFN_CACHE], cpus_share_cache);\\n\\tinit_pod_type(&wq_pod_types[WQ_AFFN_NUMA], cpus_share_numa);\\n\\n\\twq_topo_initialized = true;\\n\\n\\tmutex_lock(&wq_pool_mutex);\\n\\n\\t/*\\n\\t * Workqueues allocated earlier would have all CPUs sharing the default\\n\\t * worker pool. Explicitly call unbound_wq_update_pwq() on all workqueue\\n\\t * and CPU combinations to apply per-pod sharing.\\n\\t */\\n\\tlist_for_each_entry(wq, &workqueues, list) {\\n\\t\\tfor_each_online_cpu(cpu)\\n\\t\\t\\tunbound_wq_update_pwq(wq, cpu);\\n\\t\\tif (wq->flags & WQ_UNBOUND) {\\n\\t\\t\\tmutex_lock(&wq->mutex);\\n\\t\\t\\twq_update_node_max_active(wq, -1);\\n\\t\\t\\tmutex_unlock(&wq->mutex);\\n\\t\\t}\\n\\t}\\n\\n\\tmutex_unlock(&wq_pool_mutex);\\n}\\n\\nvoid __warn_flushing_systemwide_wq(void)\\n{\\n\\tpr_warn(\"WARNING: Flushing system-wide workqueues will be prohibited in near future.\\\\n\");\\n\\tdump_stack();\\n}\\nEXPORT_SYMBOL(__warn_flushing_systemwide_wq);\\n\\nstatic int __init workqueue_unbound_cpus_setup(char *str)\\n{\\n\\tif (cpulist_parse(str, &wq_cmdline_cpumask) < 0) {\\n\\t\\tcpumask_clear(&wq_cmdline_cpumask);\\n\\t\\tpr_warn(\"workqueue.unbound_cpus: incorrect CPU range, using default\\\\n\");\\n\\t}\\n\\n\\treturn 1;\\n}\\n__setup(\"workqueue.unbound_cpus=\", workqueue_unbound_cpus_setup);\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n *  Copyright (C) 2004 IBM Corporation\\n *\\n *  Author: Serge Hallyn <serue@us.ibm.com>\\n */\\n\\n#include <linux/export.h>\\n#include <linux/uts.h>\\n#include <linux/utsname.h>\\n#include <linux/err.h>\\n#include <linux/slab.h>\\n#include <linux/cred.h>\\n#include <linux/user_namespace.h>\\n#include <linux/proc_ns.h>\\n#include <linux/sched/task.h>\\n\\nstatic struct kmem_cache *uts_ns_cache __ro_after_init;\\n\\nstatic struct ucounts *inc_uts_namespaces(struct user_namespace *ns)\\n{\\n\\treturn inc_ucount(ns, current_euid(), UCOUNT_UTS_NAMESPACES);\\n}\\n\\nstatic void dec_uts_namespaces(struct ucounts *ucounts)\\n{\\n\\tdec_ucount(ucounts, UCOUNT_UTS_NAMESPACES);\\n}\\n\\nstatic struct uts_namespace *create_uts_ns(void)\\n{\\n\\tstruct uts_namespace *uts_ns;\\n\\n\\tuts_ns = kmem_cache_alloc(uts_ns_cache, GFP_KERNEL);\\n\\tif (uts_ns)\\n\\t\\trefcount_set(&uts_ns->ns.count, 1);\\n\\treturn uts_ns;\\n}\\n\\n/*\\n * Clone a new ns copying an original utsname, setting refcount to 1\\n * @old_ns: namespace to clone\\n * Return ERR_PTR(-ENOMEM) on error (failure to allocate), new ns otherwise\\n */\\nstatic struct uts_namespace *clone_uts_ns(struct user_namespace *user_ns,\\n\\t\\t\\t\\t\\t  struct uts_namespace *old_ns)\\n{\\n\\tstruct uts_namespace *ns;\\n\\tstruct ucounts *ucounts;\\n\\tint err;\\n\\n\\terr = -ENOSPC;\\n\\tucounts = inc_uts_namespaces(user_ns);\\n\\tif (!ucounts)\\n\\t\\tgoto fail;\\n\\n\\terr = -ENOMEM;\\n\\tns = create_uts_ns();\\n\\tif (!ns)\\n\\t\\tgoto fail_dec;\\n\\n\\terr = ns_alloc_inum(&ns->ns);\\n\\tif (err)\\n\\t\\tgoto fail_free;\\n\\n\\tns->ucounts = ucounts;\\n\\tns->ns.ops = &utsns_operations;\\n\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(&ns->name, &old_ns->name, sizeof(ns->name));\\n\\tns->user_ns = get_user_ns(user_ns);\\n\\tup_read(&uts_sem);\\n\\treturn ns;\\n\\nfail_free:\\n\\tkmem_cache_free(uts_ns_cache, ns);\\nfail_dec:\\n\\tdec_uts_namespaces(ucounts);\\nfail:\\n\\treturn ERR_PTR(err);\\n}\\n\\n/*\\n * Copy task tsk\\'s utsname namespace, or clone it if flags\\n * specifies CLONE_NEWUTS.  In latter case, changes to the\\n * utsname of this process won\\'t be seen by parent, and vice\\n * versa.\\n */\\nstruct uts_namespace *copy_utsname(unsigned long flags,\\n\\tstruct user_namespace *user_ns, struct uts_namespace *old_ns)\\n{\\n\\tstruct uts_namespace *new_ns;\\n\\n\\tBUG_ON(!old_ns);\\n\\tget_uts_ns(old_ns);\\n\\n\\tif (!(flags & CLONE_NEWUTS))\\n\\t\\treturn old_ns;\\n\\n\\tnew_ns = clone_uts_ns(user_ns, old_ns);\\n\\n\\tput_uts_ns(old_ns);\\n\\treturn new_ns;\\n}\\n\\nvoid free_uts_ns(struct uts_namespace *ns)\\n{\\n\\tdec_uts_namespaces(ns->ucounts);\\n\\tput_user_ns(ns->user_ns);\\n\\tns_free_inum(&ns->ns);\\n\\tkmem_cache_free(uts_ns_cache, ns);\\n}\\n\\nstatic inline struct uts_namespace *to_uts_ns(struct ns_common *ns)\\n{\\n\\treturn container_of(ns, struct uts_namespace, ns);\\n}\\n\\nstatic struct ns_common *utsns_get(struct task_struct *task)\\n{\\n\\tstruct uts_namespace *ns = NULL;\\n\\tstruct nsproxy *nsproxy;\\n\\n\\ttask_lock(task);\\n\\tnsproxy = task->nsproxy;\\n\\tif (nsproxy) {\\n\\t\\tns = nsproxy->uts_ns;\\n\\t\\tget_uts_ns(ns);\\n\\t}\\n\\ttask_unlock(task);\\n\\n\\treturn ns ? &ns->ns : NULL;\\n}\\n\\nstatic void utsns_put(struct ns_common *ns)\\n{\\n\\tput_uts_ns(to_uts_ns(ns));\\n}\\n\\nstatic int utsns_install(struct nsset *nsset, struct ns_common *new)\\n{\\n\\tstruct nsproxy *nsproxy = nsset->nsproxy;\\n\\tstruct uts_namespace *ns = to_uts_ns(new);\\n\\n\\tif (!ns_capable(ns->user_ns, CAP_SYS_ADMIN) ||\\n\\t    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tget_uts_ns(ns);\\n\\tput_uts_ns(nsproxy->uts_ns);\\n\\tnsproxy->uts_ns = ns;\\n\\treturn 0;\\n}\\n\\nstatic struct user_namespace *utsns_owner(struct ns_common *ns)\\n{\\n\\treturn to_uts_ns(ns)->user_ns;\\n}\\n\\nconst struct proc_ns_operations utsns_operations = {\\n\\t.name\\t\\t= \"uts\",\\n\\t.type\\t\\t= CLONE_NEWUTS,\\n\\t.get\\t\\t= utsns_get,\\n\\t.put\\t\\t= utsns_put,\\n\\t.install\\t= utsns_install,\\n\\t.owner\\t\\t= utsns_owner,\\n};\\n\\nvoid __init uts_ns_init(void)\\n{\\n\\tuts_ns_cache = kmem_cache_create_usercopy(\\n\\t\\t\\t\"uts_namespace\", sizeof(struct uts_namespace), 0,\\n\\t\\t\\tSLAB_PANIC|SLAB_ACCOUNT,\\n\\t\\t\\toffsetof(struct uts_namespace, name),\\n\\t\\t\\tsizeof_field(struct uts_namespace, name),\\n\\t\\t\\tNULL);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n\\n#include <linux/export.h>\\n#include <linux/nsproxy.h>\\n#include <linux/slab.h>\\n#include <linux/sched/signal.h>\\n#include <linux/user_namespace.h>\\n#include <linux/proc_ns.h>\\n#include <linux/highuid.h>\\n#include <linux/cred.h>\\n#include <linux/securebits.h>\\n#include <linux/security.h>\\n#include <linux/keyctl.h>\\n#include <linux/key-type.h>\\n#include <keys/user-type.h>\\n#include <linux/seq_file.h>\\n#include <linux/fs.h>\\n#include <linux/uaccess.h>\\n#include <linux/ctype.h>\\n#include <linux/projid.h>\\n#include <linux/fs_struct.h>\\n#include <linux/bsearch.h>\\n#include <linux/sort.h>\\n\\nstatic struct kmem_cache *user_ns_cachep __ro_after_init;\\nstatic DEFINE_MUTEX(userns_state_mutex);\\n\\nstatic bool new_idmap_permitted(const struct file *file,\\n\\t\\t\\t\\tstruct user_namespace *ns, int cap_setid,\\n\\t\\t\\t\\tstruct uid_gid_map *map);\\nstatic void free_user_ns(struct work_struct *work);\\n\\nstatic struct ucounts *inc_user_namespaces(struct user_namespace *ns, kuid_t uid)\\n{\\n\\treturn inc_ucount(ns, uid, UCOUNT_USER_NAMESPACES);\\n}\\n\\nstatic void dec_user_namespaces(struct ucounts *ucounts)\\n{\\n\\treturn dec_ucount(ucounts, UCOUNT_USER_NAMESPACES);\\n}\\n\\nstatic void set_cred_user_ns(struct cred *cred, struct user_namespace *user_ns)\\n{\\n\\t/* Start with the same capabilities as init but useless for doing\\n\\t * anything as the capabilities are bound to the new user namespace.\\n\\t */\\n\\tcred->securebits = SECUREBITS_DEFAULT;\\n\\tcred->cap_inheritable = CAP_EMPTY_SET;\\n\\tcred->cap_permitted = CAP_FULL_SET;\\n\\tcred->cap_effective = CAP_FULL_SET;\\n\\tcred->cap_ambient = CAP_EMPTY_SET;\\n\\tcred->cap_bset = CAP_FULL_SET;\\n#ifdef CONFIG_KEYS\\n\\tkey_put(cred->request_key_auth);\\n\\tcred->request_key_auth = NULL;\\n#endif\\n\\t/* tgcred will be cleared in our caller bc CLONE_THREAD won\\'t be set */\\n\\tcred->user_ns = user_ns;\\n}\\n\\nstatic unsigned long enforced_nproc_rlimit(void)\\n{\\n\\tunsigned long limit = RLIM_INFINITY;\\n\\n\\t/* Is RLIMIT_NPROC currently enforced? */\\n\\tif (!uid_eq(current_uid(), GLOBAL_ROOT_UID) ||\\n\\t    (current_user_ns() != &init_user_ns))\\n\\t\\tlimit = rlimit(RLIMIT_NPROC);\\n\\n\\treturn limit;\\n}\\n\\n/*\\n * Create a new user namespace, deriving the creator from the user in the\\n * passed credentials, and replacing that user with the new root user for the\\n * new namespace.\\n *\\n * This is called by copy_creds(), which will finish setting the target task\\'s\\n * credentials.\\n */\\nint create_user_ns(struct cred *new)\\n{\\n\\tstruct user_namespace *ns, *parent_ns = new->user_ns;\\n\\tkuid_t owner = new->euid;\\n\\tkgid_t group = new->egid;\\n\\tstruct ucounts *ucounts;\\n\\tint ret, i;\\n\\n\\tret = -ENOSPC;\\n\\tif (parent_ns->level > 32)\\n\\t\\tgoto fail;\\n\\n\\tucounts = inc_user_namespaces(parent_ns, owner);\\n\\tif (!ucounts)\\n\\t\\tgoto fail;\\n\\n\\t/*\\n\\t * Verify that we can not violate the policy of which files\\n\\t * may be accessed that is specified by the root directory,\\n\\t * by verifying that the root directory is at the root of the\\n\\t * mount namespace which allows all files to be accessed.\\n\\t */\\n\\tret = -EPERM;\\n\\tif (current_chrooted())\\n\\t\\tgoto fail_dec;\\n\\n\\t/* The creator needs a mapping in the parent user namespace\\n\\t * or else we won\\'t be able to reasonably tell userspace who\\n\\t * created a user_namespace.\\n\\t */\\n\\tret = -EPERM;\\n\\tif (!kuid_has_mapping(parent_ns, owner) ||\\n\\t    !kgid_has_mapping(parent_ns, group))\\n\\t\\tgoto fail_dec;\\n\\n\\tret = security_create_user_ns(new);\\n\\tif (ret < 0)\\n\\t\\tgoto fail_dec;\\n\\n\\tret = -ENOMEM;\\n\\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\\n\\tif (!ns)\\n\\t\\tgoto fail_dec;\\n\\n\\tns->parent_could_setfcap = cap_raised(new->cap_effective, CAP_SETFCAP);\\n\\tret = ns_alloc_inum(&ns->ns);\\n\\tif (ret)\\n\\t\\tgoto fail_free;\\n\\tns->ns.ops = &userns_operations;\\n\\n\\trefcount_set(&ns->ns.count, 1);\\n\\t/* Leave the new->user_ns reference with the new user namespace. */\\n\\tns->parent = parent_ns;\\n\\tns->level = parent_ns->level + 1;\\n\\tns->owner = owner;\\n\\tns->group = group;\\n\\tINIT_WORK(&ns->work, free_user_ns);\\n\\tfor (i = 0; i < UCOUNT_COUNTS; i++) {\\n\\t\\tns->ucount_max[i] = INT_MAX;\\n\\t}\\n\\tset_userns_rlimit_max(ns, UCOUNT_RLIMIT_NPROC, enforced_nproc_rlimit());\\n\\tset_userns_rlimit_max(ns, UCOUNT_RLIMIT_MSGQUEUE, rlimit(RLIMIT_MSGQUEUE));\\n\\tset_userns_rlimit_max(ns, UCOUNT_RLIMIT_SIGPENDING, rlimit(RLIMIT_SIGPENDING));\\n\\tset_userns_rlimit_max(ns, UCOUNT_RLIMIT_MEMLOCK, rlimit(RLIMIT_MEMLOCK));\\n\\tns->ucounts = ucounts;\\n\\n\\t/* Inherit USERNS_SETGROUPS_ALLOWED from our parent */\\n\\tmutex_lock(&userns_state_mutex);\\n\\tns->flags = parent_ns->flags;\\n\\tmutex_unlock(&userns_state_mutex);\\n\\n#ifdef CONFIG_KEYS\\n\\tINIT_LIST_HEAD(&ns->keyring_name_list);\\n\\tinit_rwsem(&ns->keyring_sem);\\n#endif\\n\\tret = -ENOMEM;\\n\\tif (!setup_userns_sysctls(ns))\\n\\t\\tgoto fail_keyring;\\n\\n\\tset_cred_user_ns(new, ns);\\n\\treturn 0;\\nfail_keyring:\\n#ifdef CONFIG_PERSISTENT_KEYRINGS\\n\\tkey_put(ns->persistent_keyring_register);\\n#endif\\n\\tns_free_inum(&ns->ns);\\nfail_free:\\n\\tkmem_cache_free(user_ns_cachep, ns);\\nfail_dec:\\n\\tdec_user_namespaces(ucounts);\\nfail:\\n\\treturn ret;\\n}\\n\\nint unshare_userns(unsigned long unshare_flags, struct cred **new_cred)\\n{\\n\\tstruct cred *cred;\\n\\tint err = -ENOMEM;\\n\\n\\tif (!(unshare_flags & CLONE_NEWUSER))\\n\\t\\treturn 0;\\n\\n\\tcred = prepare_creds();\\n\\tif (cred) {\\n\\t\\terr = create_user_ns(cred);\\n\\t\\tif (err)\\n\\t\\t\\tput_cred(cred);\\n\\t\\telse\\n\\t\\t\\t*new_cred = cred;\\n\\t}\\n\\n\\treturn err;\\n}\\n\\nstatic void free_user_ns(struct work_struct *work)\\n{\\n\\tstruct user_namespace *parent, *ns =\\n\\t\\tcontainer_of(work, struct user_namespace, work);\\n\\n\\tdo {\\n\\t\\tstruct ucounts *ucounts = ns->ucounts;\\n\\t\\tparent = ns->parent;\\n\\t\\tif (ns->gid_map.nr_extents > UID_GID_MAP_MAX_BASE_EXTENTS) {\\n\\t\\t\\tkfree(ns->gid_map.forward);\\n\\t\\t\\tkfree(ns->gid_map.reverse);\\n\\t\\t}\\n\\t\\tif (ns->uid_map.nr_extents > UID_GID_MAP_MAX_BASE_EXTENTS) {\\n\\t\\t\\tkfree(ns->uid_map.forward);\\n\\t\\t\\tkfree(ns->uid_map.reverse);\\n\\t\\t}\\n\\t\\tif (ns->projid_map.nr_extents > UID_GID_MAP_MAX_BASE_EXTENTS) {\\n\\t\\t\\tkfree(ns->projid_map.forward);\\n\\t\\t\\tkfree(ns->projid_map.reverse);\\n\\t\\t}\\n#if IS_ENABLED(CONFIG_BINFMT_MISC)\\n\\t\\tkfree(ns->binfmt_misc);\\n#endif\\n\\t\\tretire_userns_sysctls(ns);\\n\\t\\tkey_free_user_ns(ns);\\n\\t\\tns_free_inum(&ns->ns);\\n\\t\\tkmem_cache_free(user_ns_cachep, ns);\\n\\t\\tdec_user_namespaces(ucounts);\\n\\t\\tns = parent;\\n\\t} while (refcount_dec_and_test(&parent->ns.count));\\n}\\n\\nvoid __put_user_ns(struct user_namespace *ns)\\n{\\n\\tschedule_work(&ns->work);\\n}\\nEXPORT_SYMBOL(__put_user_ns);\\n\\n/*\\n * struct idmap_key - holds the information necessary to find an idmapping in a\\n * sorted idmap array. It is passed to cmp_map_id() as first argument.\\n */\\nstruct idmap_key {\\n\\tbool map_up; /* true  -> id from kid; false -> kid from id */\\n\\tu32 id; /* id to find */\\n\\tu32 count; /* == 0 unless used with map_id_range_down() */\\n};\\n\\n/*\\n * cmp_map_id - Function to be passed to bsearch() to find the requested\\n * idmapping. Expects struct idmap_key to be passed via @k.\\n */\\nstatic int cmp_map_id(const void *k, const void *e)\\n{\\n\\tu32 first, last, id2;\\n\\tconst struct idmap_key *key = k;\\n\\tconst struct uid_gid_extent *el = e;\\n\\n\\tid2 = key->id + key->count - 1;\\n\\n\\t/* handle map_id_{down,up}() */\\n\\tif (key->map_up)\\n\\t\\tfirst = el->lower_first;\\n\\telse\\n\\t\\tfirst = el->first;\\n\\n\\tlast = first + el->count - 1;\\n\\n\\tif (key->id >= first && key->id <= last &&\\n\\t    (id2 >= first && id2 <= last))\\n\\t\\treturn 0;\\n\\n\\tif (key->id < first || id2 < first)\\n\\t\\treturn -1;\\n\\n\\treturn 1;\\n}\\n\\n/*\\n * map_id_range_down_max - Find idmap via binary search in ordered idmap array.\\n * Can only be called if number of mappings exceeds UID_GID_MAP_MAX_BASE_EXTENTS.\\n */\\nstatic struct uid_gid_extent *\\nmap_id_range_down_max(unsigned extents, struct uid_gid_map *map, u32 id, u32 count)\\n{\\n\\tstruct idmap_key key;\\n\\n\\tkey.map_up = false;\\n\\tkey.count = count;\\n\\tkey.id = id;\\n\\n\\treturn bsearch(&key, map->forward, extents,\\n\\t\\t       sizeof(struct uid_gid_extent), cmp_map_id);\\n}\\n\\n/*\\n * map_id_range_down_base - Find idmap via binary search in static extent array.\\n * Can only be called if number of mappings is equal or less than\\n * UID_GID_MAP_MAX_BASE_EXTENTS.\\n */\\nstatic struct uid_gid_extent *\\nmap_id_range_down_base(unsigned extents, struct uid_gid_map *map, u32 id, u32 count)\\n{\\n\\tunsigned idx;\\n\\tu32 first, last, id2;\\n\\n\\tid2 = id + count - 1;\\n\\n\\t/* Find the matching extent */\\n\\tfor (idx = 0; idx < extents; idx++) {\\n\\t\\tfirst = map->extent[idx].first;\\n\\t\\tlast = first + map->extent[idx].count - 1;\\n\\t\\tif (id >= first && id <= last &&\\n\\t\\t    (id2 >= first && id2 <= last))\\n\\t\\t\\treturn &map->extent[idx];\\n\\t}\\n\\treturn NULL;\\n}\\n\\nstatic u32 map_id_range_down(struct uid_gid_map *map, u32 id, u32 count)\\n{\\n\\tstruct uid_gid_extent *extent;\\n\\tunsigned extents = map->nr_extents;\\n\\tsmp_rmb();\\n\\n\\tif (extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\\n\\t\\textent = map_id_range_down_base(extents, map, id, count);\\n\\telse\\n\\t\\textent = map_id_range_down_max(extents, map, id, count);\\n\\n\\t/* Map the id or note failure */\\n\\tif (extent)\\n\\t\\tid = (id - extent->first) + extent->lower_first;\\n\\telse\\n\\t\\tid = (u32) -1;\\n\\n\\treturn id;\\n}\\n\\nu32 map_id_down(struct uid_gid_map *map, u32 id)\\n{\\n\\treturn map_id_range_down(map, id, 1);\\n}\\n\\n/*\\n * map_id_up_base - Find idmap via binary search in static extent array.\\n * Can only be called if number of mappings is equal or less than\\n * UID_GID_MAP_MAX_BASE_EXTENTS.\\n */\\nstatic struct uid_gid_extent *\\nmap_id_up_base(unsigned extents, struct uid_gid_map *map, u32 id)\\n{\\n\\tunsigned idx;\\n\\tu32 first, last;\\n\\n\\t/* Find the matching extent */\\n\\tfor (idx = 0; idx < extents; idx++) {\\n\\t\\tfirst = map->extent[idx].lower_first;\\n\\t\\tlast = first + map->extent[idx].count - 1;\\n\\t\\tif (id >= first && id <= last)\\n\\t\\t\\treturn &map->extent[idx];\\n\\t}\\n\\treturn NULL;\\n}\\n\\n/*\\n * map_id_up_max - Find idmap via binary search in ordered idmap array.\\n * Can only be called if number of mappings exceeds UID_GID_MAP_MAX_BASE_EXTENTS.\\n */\\nstatic struct uid_gid_extent *\\nmap_id_up_max(unsigned extents, struct uid_gid_map *map, u32 id)\\n{\\n\\tstruct idmap_key key;\\n\\n\\tkey.map_up = true;\\n\\tkey.count = 1;\\n\\tkey.id = id;\\n\\n\\treturn bsearch(&key, map->reverse, extents,\\n\\t\\t       sizeof(struct uid_gid_extent), cmp_map_id);\\n}\\n\\nu32 map_id_up(struct uid_gid_map *map, u32 id)\\n{\\n\\tstruct uid_gid_extent *extent;\\n\\tunsigned extents = map->nr_extents;\\n\\tsmp_rmb();\\n\\n\\tif (extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\\n\\t\\textent = map_id_up_base(extents, map, id);\\n\\telse\\n\\t\\textent = map_id_up_max(extents, map, id);\\n\\n\\t/* Map the id or note failure */\\n\\tif (extent)\\n\\t\\tid = (id - extent->lower_first) + extent->first;\\n\\telse\\n\\t\\tid = (u32) -1;\\n\\n\\treturn id;\\n}\\n\\n/**\\n *\\tmake_kuid - Map a user-namespace uid pair into a kuid.\\n *\\t@ns:  User namespace that the uid is in\\n *\\t@uid: User identifier\\n *\\n *\\tMaps a user-namespace uid pair into a kernel internal kuid,\\n *\\tand returns that kuid.\\n *\\n *\\tWhen there is no mapping defined for the user-namespace uid\\n *\\tpair INVALID_UID is returned.  Callers are expected to test\\n *\\tfor and handle INVALID_UID being returned.  INVALID_UID\\n *\\tmay be tested for using uid_valid().\\n */\\nkuid_t make_kuid(struct user_namespace *ns, uid_t uid)\\n{\\n\\t/* Map the uid to a global kernel uid */\\n\\treturn KUIDT_INIT(map_id_down(&ns->uid_map, uid));\\n}\\nEXPORT_SYMBOL(make_kuid);\\n\\n/**\\n *\\tfrom_kuid - Create a uid from a kuid user-namespace pair.\\n *\\t@targ: The user namespace we want a uid in.\\n *\\t@kuid: The kernel internal uid to start with.\\n *\\n *\\tMap @kuid into the user-namespace specified by @targ and\\n *\\treturn the resulting uid.\\n *\\n *\\tThere is always a mapping into the initial user_namespace.\\n *\\n *\\tIf @kuid has no mapping in @targ (uid_t)-1 is returned.\\n */\\nuid_t from_kuid(struct user_namespace *targ, kuid_t kuid)\\n{\\n\\t/* Map the uid from a global kernel uid */\\n\\treturn map_id_up(&targ->uid_map, __kuid_val(kuid));\\n}\\nEXPORT_SYMBOL(from_kuid);\\n\\n/**\\n *\\tfrom_kuid_munged - Create a uid from a kuid user-namespace pair.\\n *\\t@targ: The user namespace we want a uid in.\\n *\\t@kuid: The kernel internal uid to start with.\\n *\\n *\\tMap @kuid into the user-namespace specified by @targ and\\n *\\treturn the resulting uid.\\n *\\n *\\tThere is always a mapping into the initial user_namespace.\\n *\\n *\\tUnlike from_kuid from_kuid_munged never fails and always\\n *\\treturns a valid uid.  This makes from_kuid_munged appropriate\\n *\\tfor use in syscalls like stat and getuid where failing the\\n *\\tsystem call and failing to provide a valid uid are not an\\n *\\toptions.\\n *\\n *\\tIf @kuid has no mapping in @targ overflowuid is returned.\\n */\\nuid_t from_kuid_munged(struct user_namespace *targ, kuid_t kuid)\\n{\\n\\tuid_t uid;\\n\\tuid = from_kuid(targ, kuid);\\n\\n\\tif (uid == (uid_t) -1)\\n\\t\\tuid = overflowuid;\\n\\treturn uid;\\n}\\nEXPORT_SYMBOL(from_kuid_munged);\\n\\n/**\\n *\\tmake_kgid - Map a user-namespace gid pair into a kgid.\\n *\\t@ns:  User namespace that the gid is in\\n *\\t@gid: group identifier\\n *\\n *\\tMaps a user-namespace gid pair into a kernel internal kgid,\\n *\\tand returns that kgid.\\n *\\n *\\tWhen there is no mapping defined for the user-namespace gid\\n *\\tpair INVALID_GID is returned.  Callers are expected to test\\n *\\tfor and handle INVALID_GID being returned.  INVALID_GID may be\\n *\\ttested for using gid_valid().\\n */\\nkgid_t make_kgid(struct user_namespace *ns, gid_t gid)\\n{\\n\\t/* Map the gid to a global kernel gid */\\n\\treturn KGIDT_INIT(map_id_down(&ns->gid_map, gid));\\n}\\nEXPORT_SYMBOL(make_kgid);\\n\\n/**\\n *\\tfrom_kgid - Create a gid from a kgid user-namespace pair.\\n *\\t@targ: The user namespace we want a gid in.\\n *\\t@kgid: The kernel internal gid to start with.\\n *\\n *\\tMap @kgid into the user-namespace specified by @targ and\\n *\\treturn the resulting gid.\\n *\\n *\\tThere is always a mapping into the initial user_namespace.\\n *\\n *\\tIf @kgid has no mapping in @targ (gid_t)-1 is returned.\\n */\\ngid_t from_kgid(struct user_namespace *targ, kgid_t kgid)\\n{\\n\\t/* Map the gid from a global kernel gid */\\n\\treturn map_id_up(&targ->gid_map, __kgid_val(kgid));\\n}\\nEXPORT_SYMBOL(from_kgid);\\n\\n/**\\n *\\tfrom_kgid_munged - Create a gid from a kgid user-namespace pair.\\n *\\t@targ: The user namespace we want a gid in.\\n *\\t@kgid: The kernel internal gid to start with.\\n *\\n *\\tMap @kgid into the user-namespace specified by @targ and\\n *\\treturn the resulting gid.\\n *\\n *\\tThere is always a mapping into the initial user_namespace.\\n *\\n *\\tUnlike from_kgid from_kgid_munged never fails and always\\n *\\treturns a valid gid.  This makes from_kgid_munged appropriate\\n *\\tfor use in syscalls like stat and getgid where failing the\\n *\\tsystem call and failing to provide a valid gid are not options.\\n *\\n *\\tIf @kgid has no mapping in @targ overflowgid is returned.\\n */\\ngid_t from_kgid_munged(struct user_namespace *targ, kgid_t kgid)\\n{\\n\\tgid_t gid;\\n\\tgid = from_kgid(targ, kgid);\\n\\n\\tif (gid == (gid_t) -1)\\n\\t\\tgid = overflowgid;\\n\\treturn gid;\\n}\\nEXPORT_SYMBOL(from_kgid_munged);\\n\\n/**\\n *\\tmake_kprojid - Map a user-namespace projid pair into a kprojid.\\n *\\t@ns:  User namespace that the projid is in\\n *\\t@projid: Project identifier\\n *\\n *\\tMaps a user-namespace uid pair into a kernel internal kuid,\\n *\\tand returns that kuid.\\n *\\n *\\tWhen there is no mapping defined for the user-namespace projid\\n *\\tpair INVALID_PROJID is returned.  Callers are expected to test\\n *\\tfor and handle INVALID_PROJID being returned.  INVALID_PROJID\\n *\\tmay be tested for using projid_valid().\\n */\\nkprojid_t make_kprojid(struct user_namespace *ns, projid_t projid)\\n{\\n\\t/* Map the uid to a global kernel uid */\\n\\treturn KPROJIDT_INIT(map_id_down(&ns->projid_map, projid));\\n}\\nEXPORT_SYMBOL(make_kprojid);\\n\\n/**\\n *\\tfrom_kprojid - Create a projid from a kprojid user-namespace pair.\\n *\\t@targ: The user namespace we want a projid in.\\n *\\t@kprojid: The kernel internal project identifier to start with.\\n *\\n *\\tMap @kprojid into the user-namespace specified by @targ and\\n *\\treturn the resulting projid.\\n *\\n *\\tThere is always a mapping into the initial user_namespace.\\n *\\n *\\tIf @kprojid has no mapping in @targ (projid_t)-1 is returned.\\n */\\nprojid_t from_kprojid(struct user_namespace *targ, kprojid_t kprojid)\\n{\\n\\t/* Map the uid from a global kernel uid */\\n\\treturn map_id_up(&targ->projid_map, __kprojid_val(kprojid));\\n}\\nEXPORT_SYMBOL(from_kprojid);\\n\\n/**\\n *\\tfrom_kprojid_munged - Create a projiid from a kprojid user-namespace pair.\\n *\\t@targ: The user namespace we want a projid in.\\n *\\t@kprojid: The kernel internal projid to start with.\\n *\\n *\\tMap @kprojid into the user-namespace specified by @targ and\\n *\\treturn the resulting projid.\\n *\\n *\\tThere is always a mapping into the initial user_namespace.\\n *\\n *\\tUnlike from_kprojid from_kprojid_munged never fails and always\\n *\\treturns a valid projid.  This makes from_kprojid_munged\\n *\\tappropriate for use in syscalls like stat and where\\n *\\tfailing the system call and failing to provide a valid projid are\\n *\\tnot an options.\\n *\\n *\\tIf @kprojid has no mapping in @targ OVERFLOW_PROJID is returned.\\n */\\nprojid_t from_kprojid_munged(struct user_namespace *targ, kprojid_t kprojid)\\n{\\n\\tprojid_t projid;\\n\\tprojid = from_kprojid(targ, kprojid);\\n\\n\\tif (projid == (projid_t) -1)\\n\\t\\tprojid = OVERFLOW_PROJID;\\n\\treturn projid;\\n}\\nEXPORT_SYMBOL(from_kprojid_munged);\\n\\n\\nstatic int uid_m_show(struct seq_file *seq, void *v)\\n{\\n\\tstruct user_namespace *ns = seq->private;\\n\\tstruct uid_gid_extent *extent = v;\\n\\tstruct user_namespace *lower_ns;\\n\\tuid_t lower;\\n\\n\\tlower_ns = seq_user_ns(seq);\\n\\tif ((lower_ns == ns) && lower_ns->parent)\\n\\t\\tlower_ns = lower_ns->parent;\\n\\n\\tlower = from_kuid(lower_ns, KUIDT_INIT(extent->lower_first));\\n\\n\\tseq_printf(seq, \"%10u %10u %10u\\\\n\",\\n\\t\\textent->first,\\n\\t\\tlower,\\n\\t\\textent->count);\\n\\n\\treturn 0;\\n}\\n\\nstatic int gid_m_show(struct seq_file *seq, void *v)\\n{\\n\\tstruct user_namespace *ns = seq->private;\\n\\tstruct uid_gid_extent *extent = v;\\n\\tstruct user_namespace *lower_ns;\\n\\tgid_t lower;\\n\\n\\tlower_ns = seq_user_ns(seq);\\n\\tif ((lower_ns == ns) && lower_ns->parent)\\n\\t\\tlower_ns = lower_ns->parent;\\n\\n\\tlower = from_kgid(lower_ns, KGIDT_INIT(extent->lower_first));\\n\\n\\tseq_printf(seq, \"%10u %10u %10u\\\\n\",\\n\\t\\textent->first,\\n\\t\\tlower,\\n\\t\\textent->count);\\n\\n\\treturn 0;\\n}\\n\\nstatic int projid_m_show(struct seq_file *seq, void *v)\\n{\\n\\tstruct user_namespace *ns = seq->private;\\n\\tstruct uid_gid_extent *extent = v;\\n\\tstruct user_namespace *lower_ns;\\n\\tprojid_t lower;\\n\\n\\tlower_ns = seq_user_ns(seq);\\n\\tif ((lower_ns == ns) && lower_ns->parent)\\n\\t\\tlower_ns = lower_ns->parent;\\n\\n\\tlower = from_kprojid(lower_ns, KPROJIDT_INIT(extent->lower_first));\\n\\n\\tseq_printf(seq, \"%10u %10u %10u\\\\n\",\\n\\t\\textent->first,\\n\\t\\tlower,\\n\\t\\textent->count);\\n\\n\\treturn 0;\\n}\\n\\nstatic void *m_start(struct seq_file *seq, loff_t *ppos,\\n\\t\\t     struct uid_gid_map *map)\\n{\\n\\tloff_t pos = *ppos;\\n\\tunsigned extents = map->nr_extents;\\n\\tsmp_rmb();\\n\\n\\tif (pos >= extents)\\n\\t\\treturn NULL;\\n\\n\\tif (extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\\n\\t\\treturn &map->extent[pos];\\n\\n\\treturn &map->forward[pos];\\n}\\n\\nstatic void *uid_m_start(struct seq_file *seq, loff_t *ppos)\\n{\\n\\tstruct user_namespace *ns = seq->private;\\n\\n\\treturn m_start(seq, ppos, &ns->uid_map);\\n}\\n\\nstatic void *gid_m_start(struct seq_file *seq, loff_t *ppos)\\n{\\n\\tstruct user_namespace *ns = seq->private;\\n\\n\\treturn m_start(seq, ppos, &ns->gid_map);\\n}\\n\\nstatic void *projid_m_start(struct seq_file *seq, loff_t *ppos)\\n{\\n\\tstruct user_namespace *ns = seq->private;\\n\\n\\treturn m_start(seq, ppos, &ns->projid_map);\\n}\\n\\nstatic void *m_next(struct seq_file *seq, void *v, loff_t *pos)\\n{\\n\\t(*pos)++;\\n\\treturn seq->op->start(seq, pos);\\n}\\n\\nstatic void m_stop(struct seq_file *seq, void *v)\\n{\\n\\treturn;\\n}\\n\\nconst struct seq_operations proc_uid_seq_operations = {\\n\\t.start = uid_m_start,\\n\\t.stop = m_stop,\\n\\t.next = m_next,\\n\\t.show = uid_m_show,\\n};\\n\\nconst struct seq_operations proc_gid_seq_operations = {\\n\\t.start = gid_m_start,\\n\\t.stop = m_stop,\\n\\t.next = m_next,\\n\\t.show = gid_m_show,\\n};\\n\\nconst struct seq_operations proc_projid_seq_operations = {\\n\\t.start = projid_m_start,\\n\\t.stop = m_stop,\\n\\t.next = m_next,\\n\\t.show = projid_m_show,\\n};\\n\\nstatic bool mappings_overlap(struct uid_gid_map *new_map,\\n\\t\\t\\t     struct uid_gid_extent *extent)\\n{\\n\\tu32 upper_first, lower_first, upper_last, lower_last;\\n\\tunsigned idx;\\n\\n\\tupper_first = extent->first;\\n\\tlower_first = extent->lower_first;\\n\\tupper_last = upper_first + extent->count - 1;\\n\\tlower_last = lower_first + extent->count - 1;\\n\\n\\tfor (idx = 0; idx < new_map->nr_extents; idx++) {\\n\\t\\tu32 prev_upper_first, prev_lower_first;\\n\\t\\tu32 prev_upper_last, prev_lower_last;\\n\\t\\tstruct uid_gid_extent *prev;\\n\\n\\t\\tif (new_map->nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\\n\\t\\t\\tprev = &new_map->extent[idx];\\n\\t\\telse\\n\\t\\t\\tprev = &new_map->forward[idx];\\n\\n\\t\\tprev_upper_first = prev->first;\\n\\t\\tprev_lower_first = prev->lower_first;\\n\\t\\tprev_upper_last = prev_upper_first + prev->count - 1;\\n\\t\\tprev_lower_last = prev_lower_first + prev->count - 1;\\n\\n\\t\\t/* Does the upper range intersect a previous extent? */\\n\\t\\tif ((prev_upper_first <= upper_last) &&\\n\\t\\t    (prev_upper_last >= upper_first))\\n\\t\\t\\treturn true;\\n\\n\\t\\t/* Does the lower range intersect a previous extent? */\\n\\t\\tif ((prev_lower_first <= lower_last) &&\\n\\t\\t    (prev_lower_last >= lower_first))\\n\\t\\t\\treturn true;\\n\\t}\\n\\treturn false;\\n}\\n\\n/*\\n * insert_extent - Safely insert a new idmap extent into struct uid_gid_map.\\n * Takes care to allocate a 4K block of memory if the number of mappings exceeds\\n * UID_GID_MAP_MAX_BASE_EXTENTS.\\n */\\nstatic int insert_extent(struct uid_gid_map *map, struct uid_gid_extent *extent)\\n{\\n\\tstruct uid_gid_extent *dest;\\n\\n\\tif (map->nr_extents == UID_GID_MAP_MAX_BASE_EXTENTS) {\\n\\t\\tstruct uid_gid_extent *forward;\\n\\n\\t\\t/* Allocate memory for 340 mappings. */\\n\\t\\tforward = kmalloc_array(UID_GID_MAP_MAX_EXTENTS,\\n\\t\\t\\t\\t\\tsizeof(struct uid_gid_extent),\\n\\t\\t\\t\\t\\tGFP_KERNEL);\\n\\t\\tif (!forward)\\n\\t\\t\\treturn -ENOMEM;\\n\\n\\t\\t/* Copy over memory. Only set up memory for the forward pointer.\\n\\t\\t * Defer the memory setup for the reverse pointer.\\n\\t\\t */\\n\\t\\tmemcpy(forward, map->extent,\\n\\t\\t       map->nr_extents * sizeof(map->extent[0]));\\n\\n\\t\\tmap->forward = forward;\\n\\t\\tmap->reverse = NULL;\\n\\t}\\n\\n\\tif (map->nr_extents < UID_GID_MAP_MAX_BASE_EXTENTS)\\n\\t\\tdest = &map->extent[map->nr_extents];\\n\\telse\\n\\t\\tdest = &map->forward[map->nr_extents];\\n\\n\\t*dest = *extent;\\n\\tmap->nr_extents++;\\n\\treturn 0;\\n}\\n\\n/* cmp function to sort() forward mappings */\\nstatic int cmp_extents_forward(const void *a, const void *b)\\n{\\n\\tconst struct uid_gid_extent *e1 = a;\\n\\tconst struct uid_gid_extent *e2 = b;\\n\\n\\tif (e1->first < e2->first)\\n\\t\\treturn -1;\\n\\n\\tif (e1->first > e2->first)\\n\\t\\treturn 1;\\n\\n\\treturn 0;\\n}\\n\\n/* cmp function to sort() reverse mappings */\\nstatic int cmp_extents_reverse(const void *a, const void *b)\\n{\\n\\tconst struct uid_gid_extent *e1 = a;\\n\\tconst struct uid_gid_extent *e2 = b;\\n\\n\\tif (e1->lower_first < e2->lower_first)\\n\\t\\treturn -1;\\n\\n\\tif (e1->lower_first > e2->lower_first)\\n\\t\\treturn 1;\\n\\n\\treturn 0;\\n}\\n\\n/*\\n * sort_idmaps - Sorts an array of idmap entries.\\n * Can only be called if number of mappings exceeds UID_GID_MAP_MAX_BASE_EXTENTS.\\n */\\nstatic int sort_idmaps(struct uid_gid_map *map)\\n{\\n\\tif (map->nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\\n\\t\\treturn 0;\\n\\n\\t/* Sort forward array. */\\n\\tsort(map->forward, map->nr_extents, sizeof(struct uid_gid_extent),\\n\\t     cmp_extents_forward, NULL);\\n\\n\\t/* Only copy the memory from forward we actually need. */\\n\\tmap->reverse = kmemdup_array(map->forward, map->nr_extents,\\n\\t\\t\\t\\t     sizeof(struct uid_gid_extent), GFP_KERNEL);\\n\\tif (!map->reverse)\\n\\t\\treturn -ENOMEM;\\n\\n\\t/* Sort reverse array. */\\n\\tsort(map->reverse, map->nr_extents, sizeof(struct uid_gid_extent),\\n\\t     cmp_extents_reverse, NULL);\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * verify_root_map() - check the uid 0 mapping\\n * @file: idmapping file\\n * @map_ns: user namespace of the target process\\n * @new_map: requested idmap\\n *\\n * If a process requests mapping parent uid 0 into the new ns, verify that the\\n * process writing the map had the CAP_SETFCAP capability as the target process\\n * will be able to write fscaps that are valid in ancestor user namespaces.\\n *\\n * Return: true if the mapping is allowed, false if not.\\n */\\nstatic bool verify_root_map(const struct file *file,\\n\\t\\t\\t    struct user_namespace *map_ns,\\n\\t\\t\\t    struct uid_gid_map *new_map)\\n{\\n\\tint idx;\\n\\tconst struct user_namespace *file_ns = file->f_cred->user_ns;\\n\\tstruct uid_gid_extent *extent0 = NULL;\\n\\n\\tfor (idx = 0; idx < new_map->nr_extents; idx++) {\\n\\t\\tif (new_map->nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\\n\\t\\t\\textent0 = &new_map->extent[idx];\\n\\t\\telse\\n\\t\\t\\textent0 = &new_map->forward[idx];\\n\\t\\tif (extent0->lower_first == 0)\\n\\t\\t\\tbreak;\\n\\n\\t\\textent0 = NULL;\\n\\t}\\n\\n\\tif (!extent0)\\n\\t\\treturn true;\\n\\n\\tif (map_ns == file_ns) {\\n\\t\\t/* The process unshared its ns and is writing to its own\\n\\t\\t * /proc/self/uid_map.  User already has full capabilites in\\n\\t\\t * the new namespace.  Verify that the parent had CAP_SETFCAP\\n\\t\\t * when it unshared.\\n\\t\\t * */\\n\\t\\tif (!file_ns->parent_could_setfcap)\\n\\t\\t\\treturn false;\\n\\t} else {\\n\\t\\t/* Process p1 is writing to uid_map of p2, who is in a child\\n\\t\\t * user namespace to p1\\'s.  Verify that the opener of the map\\n\\t\\t * file has CAP_SETFCAP against the parent of the new map\\n\\t\\t * namespace */\\n\\t\\tif (!file_ns_capable(file, map_ns->parent, CAP_SETFCAP))\\n\\t\\t\\treturn false;\\n\\t}\\n\\n\\treturn true;\\n}\\n\\nstatic ssize_t map_write(struct file *file, const char __user *buf,\\n\\t\\t\\t size_t count, loff_t *ppos,\\n\\t\\t\\t int cap_setid,\\n\\t\\t\\t struct uid_gid_map *map,\\n\\t\\t\\t struct uid_gid_map *parent_map)\\n{\\n\\tstruct seq_file *seq = file->private_data;\\n\\tstruct user_namespace *map_ns = seq->private;\\n\\tstruct uid_gid_map new_map;\\n\\tunsigned idx;\\n\\tstruct uid_gid_extent extent;\\n\\tchar *kbuf, *pos, *next_line;\\n\\tssize_t ret;\\n\\n\\t/* Only allow < page size writes at the beginning of the file */\\n\\tif ((*ppos != 0) || (count >= PAGE_SIZE))\\n\\t\\treturn -EINVAL;\\n\\n\\t/* Slurp in the user data */\\n\\tkbuf = memdup_user_nul(buf, count);\\n\\tif (IS_ERR(kbuf))\\n\\t\\treturn PTR_ERR(kbuf);\\n\\n\\t/*\\n\\t * The userns_state_mutex serializes all writes to any given map.\\n\\t *\\n\\t * Any map is only ever written once.\\n\\t *\\n\\t * An id map fits within 1 cache line on most architectures.\\n\\t *\\n\\t * On read nothing needs to be done unless you are on an\\n\\t * architecture with a crazy cache coherency model like alpha.\\n\\t *\\n\\t * There is a one time data dependency between reading the\\n\\t * count of the extents and the values of the extents.  The\\n\\t * desired behavior is to see the values of the extents that\\n\\t * were written before the count of the extents.\\n\\t *\\n\\t * To achieve this smp_wmb() is used on guarantee the write\\n\\t * order and smp_rmb() is guaranteed that we don\\'t have crazy\\n\\t * architectures returning stale data.\\n\\t */\\n\\tmutex_lock(&userns_state_mutex);\\n\\n\\tmemset(&new_map, 0, sizeof(struct uid_gid_map));\\n\\n\\tret = -EPERM;\\n\\t/* Only allow one successful write to the map */\\n\\tif (map->nr_extents != 0)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Adjusting namespace settings requires capabilities on the target.\\n\\t */\\n\\tif (cap_valid(cap_setid) && !file_ns_capable(file, map_ns, CAP_SYS_ADMIN))\\n\\t\\tgoto out;\\n\\n\\t/* Parse the user data */\\n\\tret = -EINVAL;\\n\\tpos = kbuf;\\n\\tfor (; pos; pos = next_line) {\\n\\n\\t\\t/* Find the end of line and ensure I don\\'t look past it */\\n\\t\\tnext_line = strchr(pos, \\'\\\\n\\');\\n\\t\\tif (next_line) {\\n\\t\\t\\t*next_line = \\'\\\\0\\';\\n\\t\\t\\tnext_line++;\\n\\t\\t\\tif (*next_line == \\'\\\\0\\')\\n\\t\\t\\t\\tnext_line = NULL;\\n\\t\\t}\\n\\n\\t\\tpos = skip_spaces(pos);\\n\\t\\textent.first = simple_strtoul(pos, &pos, 10);\\n\\t\\tif (!isspace(*pos))\\n\\t\\t\\tgoto out;\\n\\n\\t\\tpos = skip_spaces(pos);\\n\\t\\textent.lower_first = simple_strtoul(pos, &pos, 10);\\n\\t\\tif (!isspace(*pos))\\n\\t\\t\\tgoto out;\\n\\n\\t\\tpos = skip_spaces(pos);\\n\\t\\textent.count = simple_strtoul(pos, &pos, 10);\\n\\t\\tif (*pos && !isspace(*pos))\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/* Verify there is not trailing junk on the line */\\n\\t\\tpos = skip_spaces(pos);\\n\\t\\tif (*pos != \\'\\\\0\\')\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/* Verify we have been given valid starting values */\\n\\t\\tif ((extent.first == (u32) -1) ||\\n\\t\\t    (extent.lower_first == (u32) -1))\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/* Verify count is not zero and does not cause the\\n\\t\\t * extent to wrap\\n\\t\\t */\\n\\t\\tif ((extent.first + extent.count) <= extent.first)\\n\\t\\t\\tgoto out;\\n\\t\\tif ((extent.lower_first + extent.count) <=\\n\\t\\t     extent.lower_first)\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/* Do the ranges in extent overlap any previous extents? */\\n\\t\\tif (mappings_overlap(&new_map, &extent))\\n\\t\\t\\tgoto out;\\n\\n\\t\\tif ((new_map.nr_extents + 1) == UID_GID_MAP_MAX_EXTENTS &&\\n\\t\\t    (next_line != NULL))\\n\\t\\t\\tgoto out;\\n\\n\\t\\tret = insert_extent(&new_map, &extent);\\n\\t\\tif (ret < 0)\\n\\t\\t\\tgoto out;\\n\\t\\tret = -EINVAL;\\n\\t}\\n\\t/* Be very certain the new map actually exists */\\n\\tif (new_map.nr_extents == 0)\\n\\t\\tgoto out;\\n\\n\\tret = -EPERM;\\n\\t/* Validate the user is allowed to use user id\\'s mapped to. */\\n\\tif (!new_idmap_permitted(file, map_ns, cap_setid, &new_map))\\n\\t\\tgoto out;\\n\\n\\tret = -EPERM;\\n\\t/* Map the lower ids from the parent user namespace to the\\n\\t * kernel global id space.\\n\\t */\\n\\tfor (idx = 0; idx < new_map.nr_extents; idx++) {\\n\\t\\tstruct uid_gid_extent *e;\\n\\t\\tu32 lower_first;\\n\\n\\t\\tif (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS)\\n\\t\\t\\te = &new_map.extent[idx];\\n\\t\\telse\\n\\t\\t\\te = &new_map.forward[idx];\\n\\n\\t\\tlower_first = map_id_range_down(parent_map,\\n\\t\\t\\t\\t\\t\\te->lower_first,\\n\\t\\t\\t\\t\\t\\te->count);\\n\\n\\t\\t/* Fail if we can not map the specified extent to\\n\\t\\t * the kernel global id space.\\n\\t\\t */\\n\\t\\tif (lower_first == (u32) -1)\\n\\t\\t\\tgoto out;\\n\\n\\t\\te->lower_first = lower_first;\\n\\t}\\n\\n\\t/*\\n\\t * If we want to use binary search for lookup, this clones the extent\\n\\t * array and sorts both copies.\\n\\t */\\n\\tret = sort_idmaps(&new_map);\\n\\tif (ret < 0)\\n\\t\\tgoto out;\\n\\n\\t/* Install the map */\\n\\tif (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS) {\\n\\t\\tmemcpy(map->extent, new_map.extent,\\n\\t\\t       new_map.nr_extents * sizeof(new_map.extent[0]));\\n\\t} else {\\n\\t\\tmap->forward = new_map.forward;\\n\\t\\tmap->reverse = new_map.reverse;\\n\\t}\\n\\tsmp_wmb();\\n\\tmap->nr_extents = new_map.nr_extents;\\n\\n\\t*ppos = count;\\n\\tret = count;\\nout:\\n\\tif (ret < 0 && new_map.nr_extents > UID_GID_MAP_MAX_BASE_EXTENTS) {\\n\\t\\tkfree(new_map.forward);\\n\\t\\tkfree(new_map.reverse);\\n\\t\\tmap->forward = NULL;\\n\\t\\tmap->reverse = NULL;\\n\\t\\tmap->nr_extents = 0;\\n\\t}\\n\\n\\tmutex_unlock(&userns_state_mutex);\\n\\tkfree(kbuf);\\n\\treturn ret;\\n}\\n\\nssize_t proc_uid_map_write(struct file *file, const char __user *buf,\\n\\t\\t\\t   size_t size, loff_t *ppos)\\n{\\n\\tstruct seq_file *seq = file->private_data;\\n\\tstruct user_namespace *ns = seq->private;\\n\\tstruct user_namespace *seq_ns = seq_user_ns(seq);\\n\\n\\tif (!ns->parent)\\n\\t\\treturn -EPERM;\\n\\n\\tif ((seq_ns != ns) && (seq_ns != ns->parent))\\n\\t\\treturn -EPERM;\\n\\n\\treturn map_write(file, buf, size, ppos, CAP_SETUID,\\n\\t\\t\\t &ns->uid_map, &ns->parent->uid_map);\\n}\\n\\nssize_t proc_gid_map_write(struct file *file, const char __user *buf,\\n\\t\\t\\t   size_t size, loff_t *ppos)\\n{\\n\\tstruct seq_file *seq = file->private_data;\\n\\tstruct user_namespace *ns = seq->private;\\n\\tstruct user_namespace *seq_ns = seq_user_ns(seq);\\n\\n\\tif (!ns->parent)\\n\\t\\treturn -EPERM;\\n\\n\\tif ((seq_ns != ns) && (seq_ns != ns->parent))\\n\\t\\treturn -EPERM;\\n\\n\\treturn map_write(file, buf, size, ppos, CAP_SETGID,\\n\\t\\t\\t &ns->gid_map, &ns->parent->gid_map);\\n}\\n\\nssize_t proc_projid_map_write(struct file *file, const char __user *buf,\\n\\t\\t\\t      size_t size, loff_t *ppos)\\n{\\n\\tstruct seq_file *seq = file->private_data;\\n\\tstruct user_namespace *ns = seq->private;\\n\\tstruct user_namespace *seq_ns = seq_user_ns(seq);\\n\\n\\tif (!ns->parent)\\n\\t\\treturn -EPERM;\\n\\n\\tif ((seq_ns != ns) && (seq_ns != ns->parent))\\n\\t\\treturn -EPERM;\\n\\n\\t/* Anyone can set any valid project id no capability needed */\\n\\treturn map_write(file, buf, size, ppos, -1,\\n\\t\\t\\t &ns->projid_map, &ns->parent->projid_map);\\n}\\n\\nstatic bool new_idmap_permitted(const struct file *file,\\n\\t\\t\\t\\tstruct user_namespace *ns, int cap_setid,\\n\\t\\t\\t\\tstruct uid_gid_map *new_map)\\n{\\n\\tconst struct cred *cred = file->f_cred;\\n\\n\\tif (cap_setid == CAP_SETUID && !verify_root_map(file, ns, new_map))\\n\\t\\treturn false;\\n\\n\\t/* Don\\'t allow mappings that would allow anything that wouldn\\'t\\n\\t * be allowed without the establishment of unprivileged mappings.\\n\\t */\\n\\tif ((new_map->nr_extents == 1) && (new_map->extent[0].count == 1) &&\\n\\t    uid_eq(ns->owner, cred->euid)) {\\n\\t\\tu32 id = new_map->extent[0].lower_first;\\n\\t\\tif (cap_setid == CAP_SETUID) {\\n\\t\\t\\tkuid_t uid = make_kuid(ns->parent, id);\\n\\t\\t\\tif (uid_eq(uid, cred->euid))\\n\\t\\t\\t\\treturn true;\\n\\t\\t} else if (cap_setid == CAP_SETGID) {\\n\\t\\t\\tkgid_t gid = make_kgid(ns->parent, id);\\n\\t\\t\\tif (!(ns->flags & USERNS_SETGROUPS_ALLOWED) &&\\n\\t\\t\\t    gid_eq(gid, cred->egid))\\n\\t\\t\\t\\treturn true;\\n\\t\\t}\\n\\t}\\n\\n\\t/* Allow anyone to set a mapping that doesn\\'t require privilege */\\n\\tif (!cap_valid(cap_setid))\\n\\t\\treturn true;\\n\\n\\t/* Allow the specified ids if we have the appropriate capability\\n\\t * (CAP_SETUID or CAP_SETGID) over the parent user namespace.\\n\\t * And the opener of the id file also has the appropriate capability.\\n\\t */\\n\\tif (ns_capable(ns->parent, cap_setid) &&\\n\\t    file_ns_capable(file, ns->parent, cap_setid))\\n\\t\\treturn true;\\n\\n\\treturn false;\\n}\\n\\nint proc_setgroups_show(struct seq_file *seq, void *v)\\n{\\n\\tstruct user_namespace *ns = seq->private;\\n\\tunsigned long userns_flags = READ_ONCE(ns->flags);\\n\\n\\tseq_printf(seq, \"%s\\\\n\",\\n\\t\\t   (userns_flags & USERNS_SETGROUPS_ALLOWED) ?\\n\\t\\t   \"allow\" : \"deny\");\\n\\treturn 0;\\n}\\n\\nssize_t proc_setgroups_write(struct file *file, const char __user *buf,\\n\\t\\t\\t     size_t count, loff_t *ppos)\\n{\\n\\tstruct seq_file *seq = file->private_data;\\n\\tstruct user_namespace *ns = seq->private;\\n\\tchar kbuf[8], *pos;\\n\\tbool setgroups_allowed;\\n\\tssize_t ret;\\n\\n\\t/* Only allow a very narrow range of strings to be written */\\n\\tret = -EINVAL;\\n\\tif ((*ppos != 0) || (count >= sizeof(kbuf)))\\n\\t\\tgoto out;\\n\\n\\t/* What was written? */\\n\\tret = -EFAULT;\\n\\tif (copy_from_user(kbuf, buf, count))\\n\\t\\tgoto out;\\n\\tkbuf[count] = \\'\\\\0\\';\\n\\tpos = kbuf;\\n\\n\\t/* What is being requested? */\\n\\tret = -EINVAL;\\n\\tif (strncmp(pos, \"allow\", 5) == 0) {\\n\\t\\tpos += 5;\\n\\t\\tsetgroups_allowed = true;\\n\\t}\\n\\telse if (strncmp(pos, \"deny\", 4) == 0) {\\n\\t\\tpos += 4;\\n\\t\\tsetgroups_allowed = false;\\n\\t}\\n\\telse\\n\\t\\tgoto out;\\n\\n\\t/* Verify there is not trailing junk on the line */\\n\\tpos = skip_spaces(pos);\\n\\tif (*pos != \\'\\\\0\\')\\n\\t\\tgoto out;\\n\\n\\tret = -EPERM;\\n\\tmutex_lock(&userns_state_mutex);\\n\\tif (setgroups_allowed) {\\n\\t\\t/* Enabling setgroups after setgroups has been disabled\\n\\t\\t * is not allowed.\\n\\t\\t */\\n\\t\\tif (!(ns->flags & USERNS_SETGROUPS_ALLOWED))\\n\\t\\t\\tgoto out_unlock;\\n\\t} else {\\n\\t\\t/* Permanently disabling setgroups after setgroups has\\n\\t\\t * been enabled by writing the gid_map is not allowed.\\n\\t\\t */\\n\\t\\tif (ns->gid_map.nr_extents != 0)\\n\\t\\t\\tgoto out_unlock;\\n\\t\\tns->flags &= ~USERNS_SETGROUPS_ALLOWED;\\n\\t}\\n\\tmutex_unlock(&userns_state_mutex);\\n\\n\\t/* Report a successful write */\\n\\t*ppos = count;\\n\\tret = count;\\nout:\\n\\treturn ret;\\nout_unlock:\\n\\tmutex_unlock(&userns_state_mutex);\\n\\tgoto out;\\n}\\n\\nbool userns_may_setgroups(const struct user_namespace *ns)\\n{\\n\\tbool allowed;\\n\\n\\tmutex_lock(&userns_state_mutex);\\n\\t/* It is not safe to use setgroups until a gid mapping in\\n\\t * the user namespace has been established.\\n\\t */\\n\\tallowed = ns->gid_map.nr_extents != 0;\\n\\t/* Is setgroups allowed? */\\n\\tallowed = allowed && (ns->flags & USERNS_SETGROUPS_ALLOWED);\\n\\tmutex_unlock(&userns_state_mutex);\\n\\n\\treturn allowed;\\n}\\n\\n/*\\n * Returns true if @child is the same namespace or a descendant of\\n * @ancestor.\\n */\\nbool in_userns(const struct user_namespace *ancestor,\\n\\t       const struct user_namespace *child)\\n{\\n\\tconst struct user_namespace *ns;\\n\\tfor (ns = child; ns->level > ancestor->level; ns = ns->parent)\\n\\t\\t;\\n\\treturn (ns == ancestor);\\n}\\n\\nbool current_in_userns(const struct user_namespace *target_ns)\\n{\\n\\treturn in_userns(target_ns, current_user_ns());\\n}\\nEXPORT_SYMBOL(current_in_userns);\\n\\nstatic inline struct user_namespace *to_user_ns(struct ns_common *ns)\\n{\\n\\treturn container_of(ns, struct user_namespace, ns);\\n}\\n\\nstatic struct ns_common *userns_get(struct task_struct *task)\\n{\\n\\tstruct user_namespace *user_ns;\\n\\n\\trcu_read_lock();\\n\\tuser_ns = get_user_ns(__task_cred(task)->user_ns);\\n\\trcu_read_unlock();\\n\\n\\treturn user_ns ? &user_ns->ns : NULL;\\n}\\n\\nstatic void userns_put(struct ns_common *ns)\\n{\\n\\tput_user_ns(to_user_ns(ns));\\n}\\n\\nstatic int userns_install(struct nsset *nsset, struct ns_common *ns)\\n{\\n\\tstruct user_namespace *user_ns = to_user_ns(ns);\\n\\tstruct cred *cred;\\n\\n\\t/* Don\\'t allow gaining capabilities by reentering\\n\\t * the same user namespace.\\n\\t */\\n\\tif (user_ns == current_user_ns())\\n\\t\\treturn -EINVAL;\\n\\n\\t/* Tasks that share a thread group must share a user namespace */\\n\\tif (!thread_group_empty(current))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (current->fs->users != 1)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (!ns_capable(user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tcred = nsset_cred(nsset);\\n\\tif (!cred)\\n\\t\\treturn -EINVAL;\\n\\n\\tput_user_ns(cred->user_ns);\\n\\tset_cred_user_ns(cred, get_user_ns(user_ns));\\n\\n\\tif (set_cred_ucounts(cred) < 0)\\n\\t\\treturn -EINVAL;\\n\\n\\treturn 0;\\n}\\n\\nstruct ns_common *ns_get_owner(struct ns_common *ns)\\n{\\n\\tstruct user_namespace *my_user_ns = current_user_ns();\\n\\tstruct user_namespace *owner, *p;\\n\\n\\t/* See if the owner is in the current user namespace */\\n\\towner = p = ns->ops->owner(ns);\\n\\tfor (;;) {\\n\\t\\tif (!p)\\n\\t\\t\\treturn ERR_PTR(-EPERM);\\n\\t\\tif (p == my_user_ns)\\n\\t\\t\\tbreak;\\n\\t\\tp = p->parent;\\n\\t}\\n\\n\\treturn &get_user_ns(owner)->ns;\\n}\\n\\nstatic struct user_namespace *userns_owner(struct ns_common *ns)\\n{\\n\\treturn to_user_ns(ns)->parent;\\n}\\n\\nconst struct proc_ns_operations userns_operations = {\\n\\t.name\\t\\t= \"user\",\\n\\t.type\\t\\t= CLONE_NEWUSER,\\n\\t.get\\t\\t= userns_get,\\n\\t.put\\t\\t= userns_put,\\n\\t.install\\t= userns_install,\\n\\t.owner\\t\\t= userns_owner,\\n\\t.get_parent\\t= ns_get_owner,\\n};\\n\\nstatic __init int user_namespaces_init(void)\\n{\\n\\tuser_ns_cachep = KMEM_CACHE(user_namespace, SLAB_PANIC | SLAB_ACCOUNT);\\n\\treturn 0;\\n}\\nsubsys_initcall(user_namespaces_init);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Detect hard and soft lockups on a system\\n *\\n * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.\\n *\\n * Note: Most of this code is borrowed heavily from the original softlockup\\n * detector, so thanks to Ingo for the initial implementation.\\n * Some chunks also taken from the old x86-specific nmi watchdog code, thanks\\n * to those contributors as well.\\n */\\n\\n#define pr_fmt(fmt) \"watchdog: \" fmt\\n\\n#include <linux/cpu.h>\\n#include <linux/init.h>\\n#include <linux/irq.h>\\n#include <linux/irqdesc.h>\\n#include <linux/kernel_stat.h>\\n#include <linux/kvm_para.h>\\n#include <linux/math64.h>\\n#include <linux/mm.h>\\n#include <linux/module.h>\\n#include <linux/nmi.h>\\n#include <linux/stop_machine.h>\\n#include <linux/sysctl.h>\\n#include <linux/tick.h>\\n\\n#include <linux/sched/clock.h>\\n#include <linux/sched/debug.h>\\n#include <linux/sched/isolation.h>\\n\\n#include <asm/irq_regs.h>\\n\\nstatic DEFINE_MUTEX(watchdog_mutex);\\n\\n#if defined(CONFIG_HARDLOCKUP_DETECTOR) || defined(CONFIG_HARDLOCKUP_DETECTOR_SPARC64)\\n# define WATCHDOG_HARDLOCKUP_DEFAULT\\t1\\n#else\\n# define WATCHDOG_HARDLOCKUP_DEFAULT\\t0\\n#endif\\n\\n#define NUM_SAMPLE_PERIODS\\t5\\n\\nunsigned long __read_mostly watchdog_enabled;\\nint __read_mostly watchdog_user_enabled = 1;\\nstatic int __read_mostly watchdog_hardlockup_user_enabled = WATCHDOG_HARDLOCKUP_DEFAULT;\\nstatic int __read_mostly watchdog_softlockup_user_enabled = 1;\\nint __read_mostly watchdog_thresh = 10;\\nstatic int __read_mostly watchdog_hardlockup_available;\\n\\nstruct cpumask watchdog_cpumask __read_mostly;\\nunsigned long *watchdog_cpumask_bits = cpumask_bits(&watchdog_cpumask);\\n\\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\\n\\n# ifdef CONFIG_SMP\\nint __read_mostly sysctl_hardlockup_all_cpu_backtrace;\\n# endif /* CONFIG_SMP */\\n\\n/*\\n * Should we panic when a soft-lockup or hard-lockup occurs:\\n */\\nunsigned int __read_mostly hardlockup_panic =\\n\\t\\t\\tIS_ENABLED(CONFIG_BOOTPARAM_HARDLOCKUP_PANIC);\\n/*\\n * We may not want to enable hard lockup detection by default in all cases,\\n * for example when running the kernel as a guest on a hypervisor. In these\\n * cases this function can be called to disable hard lockup detection. This\\n * function should only be executed once by the boot processor before the\\n * kernel command line parameters are parsed, because otherwise it is not\\n * possible to override this in hardlockup_panic_setup().\\n */\\nvoid __init hardlockup_detector_disable(void)\\n{\\n\\twatchdog_hardlockup_user_enabled = 0;\\n}\\n\\nstatic int __init hardlockup_panic_setup(char *str)\\n{\\nnext:\\n\\tif (!strncmp(str, \"panic\", 5))\\n\\t\\thardlockup_panic = 1;\\n\\telse if (!strncmp(str, \"nopanic\", 7))\\n\\t\\thardlockup_panic = 0;\\n\\telse if (!strncmp(str, \"0\", 1))\\n\\t\\twatchdog_hardlockup_user_enabled = 0;\\n\\telse if (!strncmp(str, \"1\", 1))\\n\\t\\twatchdog_hardlockup_user_enabled = 1;\\n\\telse if (!strncmp(str, \"r\", 1))\\n\\t\\thardlockup_config_perf_event(str + 1);\\n\\twhile (*(str++)) {\\n\\t\\tif (*str == \\',\\') {\\n\\t\\t\\tstr++;\\n\\t\\t\\tgoto next;\\n\\t\\t}\\n\\t}\\n\\treturn 1;\\n}\\n__setup(\"nmi_watchdog=\", hardlockup_panic_setup);\\n\\n#endif /* CONFIG_HARDLOCKUP_DETECTOR */\\n\\n#if defined(CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER)\\n\\nstatic DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);\\nstatic DEFINE_PER_CPU(int, hrtimer_interrupts_saved);\\nstatic DEFINE_PER_CPU(bool, watchdog_hardlockup_warned);\\nstatic DEFINE_PER_CPU(bool, watchdog_hardlockup_touched);\\nstatic unsigned long hard_lockup_nmi_warn;\\n\\nnotrace void arch_touch_nmi_watchdog(void)\\n{\\n\\t/*\\n\\t * Using __raw here because some code paths have\\n\\t * preemption enabled.  If preemption is enabled\\n\\t * then interrupts should be enabled too, in which\\n\\t * case we shouldn\\'t have to worry about the watchdog\\n\\t * going off.\\n\\t */\\n\\traw_cpu_write(watchdog_hardlockup_touched, true);\\n}\\nEXPORT_SYMBOL(arch_touch_nmi_watchdog);\\n\\nvoid watchdog_hardlockup_touch_cpu(unsigned int cpu)\\n{\\n\\tper_cpu(watchdog_hardlockup_touched, cpu) = true;\\n}\\n\\nstatic bool is_hardlockup(unsigned int cpu)\\n{\\n\\tint hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));\\n\\n\\tif (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)\\n\\t\\treturn true;\\n\\n\\t/*\\n\\t * NOTE: we don\\'t need any fancy atomic_t or READ_ONCE/WRITE_ONCE\\n\\t * for hrtimer_interrupts_saved. hrtimer_interrupts_saved is\\n\\t * written/read by a single CPU.\\n\\t */\\n\\tper_cpu(hrtimer_interrupts_saved, cpu) = hrint;\\n\\n\\treturn false;\\n}\\n\\nstatic void watchdog_hardlockup_kick(void)\\n{\\n\\tint new_interrupts;\\n\\n\\tnew_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));\\n\\twatchdog_buddy_check_hardlockup(new_interrupts);\\n}\\n\\nvoid watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)\\n{\\n\\tif (per_cpu(watchdog_hardlockup_touched, cpu)) {\\n\\t\\tper_cpu(watchdog_hardlockup_touched, cpu) = false;\\n\\t\\treturn;\\n\\t}\\n\\n\\t/*\\n\\t * Check for a hardlockup by making sure the CPU\\'s timer\\n\\t * interrupt is incrementing. The timer interrupt should have\\n\\t * fired multiple times before we overflow\\'d. If it hasn\\'t\\n\\t * then this is a good indication the cpu is stuck\\n\\t */\\n\\tif (is_hardlockup(cpu)) {\\n\\t\\tunsigned int this_cpu = smp_processor_id();\\n\\t\\tunsigned long flags;\\n\\n\\t\\t/* Only print hardlockups once. */\\n\\t\\tif (per_cpu(watchdog_hardlockup_warned, cpu))\\n\\t\\t\\treturn;\\n\\n\\t\\t/*\\n\\t\\t * Prevent multiple hard-lockup reports if one cpu is already\\n\\t\\t * engaged in dumping all cpu back traces.\\n\\t\\t */\\n\\t\\tif (sysctl_hardlockup_all_cpu_backtrace) {\\n\\t\\t\\tif (test_and_set_bit_lock(0, &hard_lockup_nmi_warn))\\n\\t\\t\\t\\treturn;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * NOTE: we call printk_cpu_sync_get_irqsave() after printing\\n\\t\\t * the lockup message. While it would be nice to serialize\\n\\t\\t * that printout, we really want to make sure that if some\\n\\t\\t * other CPU somehow locked up while holding the lock associated\\n\\t\\t * with printk_cpu_sync_get_irqsave() that we can still at least\\n\\t\\t * get the message about the lockup out.\\n\\t\\t */\\n\\t\\tpr_emerg(\"Watchdog detected hard LOCKUP on cpu %d\\\\n\", cpu);\\n\\t\\tprintk_cpu_sync_get_irqsave(flags);\\n\\n\\t\\tprint_modules();\\n\\t\\tprint_irqtrace_events(current);\\n\\t\\tif (cpu == this_cpu) {\\n\\t\\t\\tif (regs)\\n\\t\\t\\t\\tshow_regs(regs);\\n\\t\\t\\telse\\n\\t\\t\\t\\tdump_stack();\\n\\t\\t\\tprintk_cpu_sync_put_irqrestore(flags);\\n\\t\\t} else {\\n\\t\\t\\tprintk_cpu_sync_put_irqrestore(flags);\\n\\t\\t\\ttrigger_single_cpu_backtrace(cpu);\\n\\t\\t}\\n\\n\\t\\tif (sysctl_hardlockup_all_cpu_backtrace) {\\n\\t\\t\\ttrigger_allbutcpu_cpu_backtrace(cpu);\\n\\t\\t\\tif (!hardlockup_panic)\\n\\t\\t\\t\\tclear_bit_unlock(0, &hard_lockup_nmi_warn);\\n\\t\\t}\\n\\n\\t\\tif (hardlockup_panic)\\n\\t\\t\\tnmi_panic(regs, \"Hard LOCKUP\");\\n\\n\\t\\tper_cpu(watchdog_hardlockup_warned, cpu) = true;\\n\\t} else {\\n\\t\\tper_cpu(watchdog_hardlockup_warned, cpu) = false;\\n\\t}\\n}\\n\\n#else /* CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER */\\n\\nstatic inline void watchdog_hardlockup_kick(void) { }\\n\\n#endif /* !CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER */\\n\\n/*\\n * These functions can be overridden based on the configured hardlockdup detector.\\n *\\n * watchdog_hardlockup_enable/disable can be implemented to start and stop when\\n * softlockup watchdog start and stop. The detector must select the\\n * SOFTLOCKUP_DETECTOR Kconfig.\\n */\\nvoid __weak watchdog_hardlockup_enable(unsigned int cpu) { }\\n\\nvoid __weak watchdog_hardlockup_disable(unsigned int cpu) { }\\n\\n/*\\n * Watchdog-detector specific API.\\n *\\n * Return 0 when hardlockup watchdog is available, negative value otherwise.\\n * Note that the negative value means that a delayed probe might\\n * succeed later.\\n */\\nint __weak __init watchdog_hardlockup_probe(void)\\n{\\n\\treturn -ENODEV;\\n}\\n\\n/**\\n * watchdog_hardlockup_stop - Stop the watchdog for reconfiguration\\n *\\n * The reconfiguration steps are:\\n * watchdog_hardlockup_stop();\\n * update_variables();\\n * watchdog_hardlockup_start();\\n */\\nvoid __weak watchdog_hardlockup_stop(void) { }\\n\\n/**\\n * watchdog_hardlockup_start - Start the watchdog after reconfiguration\\n *\\n * Counterpart to watchdog_hardlockup_stop().\\n *\\n * The following variables have been updated in update_variables() and\\n * contain the currently valid configuration:\\n * - watchdog_enabled\\n * - watchdog_thresh\\n * - watchdog_cpumask\\n */\\nvoid __weak watchdog_hardlockup_start(void) { }\\n\\n/**\\n * lockup_detector_update_enable - Update the sysctl enable bit\\n *\\n * Caller needs to make sure that the hard watchdogs are off, so this\\n * can\\'t race with watchdog_hardlockup_disable().\\n */\\nstatic void lockup_detector_update_enable(void)\\n{\\n\\twatchdog_enabled = 0;\\n\\tif (!watchdog_user_enabled)\\n\\t\\treturn;\\n\\tif (watchdog_hardlockup_available && watchdog_hardlockup_user_enabled)\\n\\t\\twatchdog_enabled |= WATCHDOG_HARDLOCKUP_ENABLED;\\n\\tif (watchdog_softlockup_user_enabled)\\n\\t\\twatchdog_enabled |= WATCHDOG_SOFTOCKUP_ENABLED;\\n}\\n\\n#ifdef CONFIG_SOFTLOCKUP_DETECTOR\\n\\n/*\\n * Delay the soflockup report when running a known slow code.\\n * It does _not_ affect the timestamp of the last successdul reschedule.\\n */\\n#define SOFTLOCKUP_DELAY_REPORT\\tULONG_MAX\\n\\n#ifdef CONFIG_SMP\\nint __read_mostly sysctl_softlockup_all_cpu_backtrace;\\n#endif\\n\\nstatic struct cpumask watchdog_allowed_mask __read_mostly;\\n\\n/* Global variables, exported for sysctl */\\nunsigned int __read_mostly softlockup_panic =\\n\\t\\t\\tIS_ENABLED(CONFIG_BOOTPARAM_SOFTLOCKUP_PANIC);\\n\\nstatic bool softlockup_initialized __read_mostly;\\nstatic u64 __read_mostly sample_period;\\n\\n/* Timestamp taken after the last successful reschedule. */\\nstatic DEFINE_PER_CPU(unsigned long, watchdog_touch_ts);\\n/* Timestamp of the last softlockup report. */\\nstatic DEFINE_PER_CPU(unsigned long, watchdog_report_ts);\\nstatic DEFINE_PER_CPU(struct hrtimer, watchdog_hrtimer);\\nstatic DEFINE_PER_CPU(bool, softlockup_touch_sync);\\nstatic unsigned long soft_lockup_nmi_warn;\\n\\nstatic int __init softlockup_panic_setup(char *str)\\n{\\n\\tsoftlockup_panic = simple_strtoul(str, NULL, 0);\\n\\treturn 1;\\n}\\n__setup(\"softlockup_panic=\", softlockup_panic_setup);\\n\\nstatic int __init nowatchdog_setup(char *str)\\n{\\n\\twatchdog_user_enabled = 0;\\n\\treturn 1;\\n}\\n__setup(\"nowatchdog\", nowatchdog_setup);\\n\\nstatic int __init nosoftlockup_setup(char *str)\\n{\\n\\twatchdog_softlockup_user_enabled = 0;\\n\\treturn 1;\\n}\\n__setup(\"nosoftlockup\", nosoftlockup_setup);\\n\\nstatic int __init watchdog_thresh_setup(char *str)\\n{\\n\\tget_option(&str, &watchdog_thresh);\\n\\treturn 1;\\n}\\n__setup(\"watchdog_thresh=\", watchdog_thresh_setup);\\n\\nstatic void __lockup_detector_cleanup(void);\\n\\n#ifdef CONFIG_SOFTLOCKUP_DETECTOR_INTR_STORM\\nenum stats_per_group {\\n\\tSTATS_SYSTEM,\\n\\tSTATS_SOFTIRQ,\\n\\tSTATS_HARDIRQ,\\n\\tSTATS_IDLE,\\n\\tNUM_STATS_PER_GROUP,\\n};\\n\\nstatic const enum cpu_usage_stat tracked_stats[NUM_STATS_PER_GROUP] = {\\n\\tCPUTIME_SYSTEM,\\n\\tCPUTIME_SOFTIRQ,\\n\\tCPUTIME_IRQ,\\n\\tCPUTIME_IDLE,\\n};\\n\\nstatic DEFINE_PER_CPU(u16, cpustat_old[NUM_STATS_PER_GROUP]);\\nstatic DEFINE_PER_CPU(u8, cpustat_util[NUM_SAMPLE_PERIODS][NUM_STATS_PER_GROUP]);\\nstatic DEFINE_PER_CPU(u8, cpustat_tail);\\n\\n/*\\n * We don\\'t need nanosecond resolution. A granularity of 16ms is\\n * sufficient for our precision, allowing us to use u16 to store\\n * cpustats, which will roll over roughly every ~1000 seconds.\\n * 2^24 ~= 16 * 10^6\\n */\\nstatic u16 get_16bit_precision(u64 data_ns)\\n{\\n\\treturn data_ns >> 24LL; /* 2^24ns ~= 16.8ms */\\n}\\n\\nstatic void update_cpustat(void)\\n{\\n\\tint i;\\n\\tu8 util;\\n\\tu16 old_stat, new_stat;\\n\\tstruct kernel_cpustat kcpustat;\\n\\tu64 *cpustat = kcpustat.cpustat;\\n\\tu8 tail = __this_cpu_read(cpustat_tail);\\n\\tu16 sample_period_16 = get_16bit_precision(sample_period);\\n\\n\\tkcpustat_cpu_fetch(&kcpustat, smp_processor_id());\\n\\n\\tfor (i = 0; i < NUM_STATS_PER_GROUP; i++) {\\n\\t\\told_stat = __this_cpu_read(cpustat_old[i]);\\n\\t\\tnew_stat = get_16bit_precision(cpustat[tracked_stats[i]]);\\n\\t\\tutil = DIV_ROUND_UP(100 * (new_stat - old_stat), sample_period_16);\\n\\t\\t__this_cpu_write(cpustat_util[tail][i], util);\\n\\t\\t__this_cpu_write(cpustat_old[i], new_stat);\\n\\t}\\n\\n\\t__this_cpu_write(cpustat_tail, (tail + 1) % NUM_SAMPLE_PERIODS);\\n}\\n\\nstatic void print_cpustat(void)\\n{\\n\\tint i, group;\\n\\tu8 tail = __this_cpu_read(cpustat_tail);\\n\\tu64 sample_period_second = sample_period;\\n\\n\\tdo_div(sample_period_second, NSEC_PER_SEC);\\n\\n\\t/*\\n\\t * Outputting the \"watchdog\" prefix on every line is redundant and not\\n\\t * concise, and the original alarm information is sufficient for\\n\\t * positioning in logs, hence here printk() is used instead of pr_crit().\\n\\t */\\n\\tprintk(KERN_CRIT \"CPU#%d Utilization every %llus during lockup:\\\\n\",\\n\\t       smp_processor_id(), sample_period_second);\\n\\n\\tfor (i = 0; i < NUM_SAMPLE_PERIODS; i++) {\\n\\t\\tgroup = (tail + i) % NUM_SAMPLE_PERIODS;\\n\\t\\tprintk(KERN_CRIT \"\\\\t#%d: %3u%% system,\\\\t%3u%% softirq,\\\\t\"\\n\\t\\t\\t\"%3u%% hardirq,\\\\t%3u%% idle\\\\n\", i + 1,\\n\\t\\t\\t__this_cpu_read(cpustat_util[group][STATS_SYSTEM]),\\n\\t\\t\\t__this_cpu_read(cpustat_util[group][STATS_SOFTIRQ]),\\n\\t\\t\\t__this_cpu_read(cpustat_util[group][STATS_HARDIRQ]),\\n\\t\\t\\t__this_cpu_read(cpustat_util[group][STATS_IDLE]));\\n\\t}\\n}\\n\\n#define HARDIRQ_PERCENT_THRESH          50\\n#define NUM_HARDIRQ_REPORT              5\\nstruct irq_counts {\\n\\tint irq;\\n\\tu32 counts;\\n};\\n\\nstatic DEFINE_PER_CPU(bool, snapshot_taken);\\n\\n/* Tabulate the most frequent interrupts. */\\nstatic void tabulate_irq_count(struct irq_counts *irq_counts, int irq, u32 counts, int rank)\\n{\\n\\tint i;\\n\\tstruct irq_counts new_count = {irq, counts};\\n\\n\\tfor (i = 0; i < rank; i++) {\\n\\t\\tif (counts > irq_counts[i].counts)\\n\\t\\t\\tswap(new_count, irq_counts[i]);\\n\\t}\\n}\\n\\n/*\\n * If the hardirq time exceeds HARDIRQ_PERCENT_THRESH% of the sample_period,\\n * then the cause of softlockup might be interrupt storm. In this case, it\\n * would be useful to start interrupt counting.\\n */\\nstatic bool need_counting_irqs(void)\\n{\\n\\tu8 util;\\n\\tint tail = __this_cpu_read(cpustat_tail);\\n\\n\\ttail = (tail + NUM_HARDIRQ_REPORT - 1) % NUM_HARDIRQ_REPORT;\\n\\tutil = __this_cpu_read(cpustat_util[tail][STATS_HARDIRQ]);\\n\\treturn util > HARDIRQ_PERCENT_THRESH;\\n}\\n\\nstatic void start_counting_irqs(void)\\n{\\n\\tif (!__this_cpu_read(snapshot_taken)) {\\n\\t\\tkstat_snapshot_irqs();\\n\\t\\t__this_cpu_write(snapshot_taken, true);\\n\\t}\\n}\\n\\nstatic void stop_counting_irqs(void)\\n{\\n\\t__this_cpu_write(snapshot_taken, false);\\n}\\n\\nstatic void print_irq_counts(void)\\n{\\n\\tunsigned int i, count;\\n\\tstruct irq_counts irq_counts_sorted[NUM_HARDIRQ_REPORT] = {\\n\\t\\t{-1, 0}, {-1, 0}, {-1, 0}, {-1, 0}, {-1, 0}\\n\\t};\\n\\n\\tif (__this_cpu_read(snapshot_taken)) {\\n\\t\\tfor_each_active_irq(i) {\\n\\t\\t\\tcount = kstat_get_irq_since_snapshot(i);\\n\\t\\t\\ttabulate_irq_count(irq_counts_sorted, i, count, NUM_HARDIRQ_REPORT);\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * Outputting the \"watchdog\" prefix on every line is redundant and not\\n\\t\\t * concise, and the original alarm information is sufficient for\\n\\t\\t * positioning in logs, hence here printk() is used instead of pr_crit().\\n\\t\\t */\\n\\t\\tprintk(KERN_CRIT \"CPU#%d Detect HardIRQ Time exceeds %d%%. Most frequent HardIRQs:\\\\n\",\\n\\t\\t       smp_processor_id(), HARDIRQ_PERCENT_THRESH);\\n\\n\\t\\tfor (i = 0; i < NUM_HARDIRQ_REPORT; i++) {\\n\\t\\t\\tif (irq_counts_sorted[i].irq == -1)\\n\\t\\t\\t\\tbreak;\\n\\n\\t\\t\\tprintk(KERN_CRIT \"\\\\t#%u: %-10u\\\\tirq#%d\\\\n\",\\n\\t\\t\\t       i + 1, irq_counts_sorted[i].counts,\\n\\t\\t\\t       irq_counts_sorted[i].irq);\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * If the hardirq time is less than HARDIRQ_PERCENT_THRESH% in the last\\n\\t\\t * sample_period, then we suspect the interrupt storm might be subsiding.\\n\\t\\t */\\n\\t\\tif (!need_counting_irqs())\\n\\t\\t\\tstop_counting_irqs();\\n\\t}\\n}\\n\\nstatic void report_cpu_status(void)\\n{\\n\\tprint_cpustat();\\n\\tprint_irq_counts();\\n}\\n#else\\nstatic inline void update_cpustat(void) { }\\nstatic inline void report_cpu_status(void) { }\\nstatic inline bool need_counting_irqs(void) { return false; }\\nstatic inline void start_counting_irqs(void) { }\\nstatic inline void stop_counting_irqs(void) { }\\n#endif\\n\\n/*\\n * Hard-lockup warnings should be triggered after just a few seconds. Soft-\\n * lockups can have false positives under extreme conditions. So we generally\\n * want a higher threshold for soft lockups than for hard lockups. So we couple\\n * the thresholds with a factor: we make the soft threshold twice the amount of\\n * time the hard threshold is.\\n */\\nstatic int get_softlockup_thresh(void)\\n{\\n\\treturn watchdog_thresh * 2;\\n}\\n\\n/*\\n * Returns seconds, approximately.  We don\\'t need nanosecond\\n * resolution, and we don\\'t need to waste time with a big divide when\\n * 2^30ns == 1.074s.\\n */\\nstatic unsigned long get_timestamp(void)\\n{\\n\\treturn running_clock() >> 30LL;  /* 2^30 ~= 10^9 */\\n}\\n\\nstatic void set_sample_period(void)\\n{\\n\\t/*\\n\\t * convert watchdog_thresh from seconds to ns\\n\\t * the divide by 5 is to give hrtimer several chances (two\\n\\t * or three with the current relation between the soft\\n\\t * and hard thresholds) to increment before the\\n\\t * hardlockup detector generates a warning\\n\\t */\\n\\tsample_period = get_softlockup_thresh() * ((u64)NSEC_PER_SEC / NUM_SAMPLE_PERIODS);\\n\\twatchdog_update_hrtimer_threshold(sample_period);\\n}\\n\\nstatic void update_report_ts(void)\\n{\\n\\t__this_cpu_write(watchdog_report_ts, get_timestamp());\\n}\\n\\n/* Commands for resetting the watchdog */\\nstatic void update_touch_ts(void)\\n{\\n\\t__this_cpu_write(watchdog_touch_ts, get_timestamp());\\n\\tupdate_report_ts();\\n}\\n\\n/**\\n * touch_softlockup_watchdog_sched - touch watchdog on scheduler stalls\\n *\\n * Call when the scheduler may have stalled for legitimate reasons\\n * preventing the watchdog task from executing - e.g. the scheduler\\n * entering idle state.  This should only be used for scheduler events.\\n * Use touch_softlockup_watchdog() for everything else.\\n */\\nnotrace void touch_softlockup_watchdog_sched(void)\\n{\\n\\t/*\\n\\t * Preemption can be enabled.  It doesn\\'t matter which CPU\\'s watchdog\\n\\t * report period gets restarted here, so use the raw_ operation.\\n\\t */\\n\\traw_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);\\n}\\n\\nnotrace void touch_softlockup_watchdog(void)\\n{\\n\\ttouch_softlockup_watchdog_sched();\\n\\twq_watchdog_touch(raw_smp_processor_id());\\n}\\nEXPORT_SYMBOL(touch_softlockup_watchdog);\\n\\nvoid touch_all_softlockup_watchdogs(void)\\n{\\n\\tint cpu;\\n\\n\\t/*\\n\\t * watchdog_mutex cannpt be taken here, as this might be called\\n\\t * from (soft)interrupt context, so the access to\\n\\t * watchdog_allowed_cpumask might race with a concurrent update.\\n\\t *\\n\\t * The watchdog time stamp can race against a concurrent real\\n\\t * update as well, the only side effect might be a cycle delay for\\n\\t * the softlockup check.\\n\\t */\\n\\tfor_each_cpu(cpu, &watchdog_allowed_mask) {\\n\\t\\tper_cpu(watchdog_report_ts, cpu) = SOFTLOCKUP_DELAY_REPORT;\\n\\t\\twq_watchdog_touch(cpu);\\n\\t}\\n}\\n\\nvoid touch_softlockup_watchdog_sync(void)\\n{\\n\\t__this_cpu_write(softlockup_touch_sync, true);\\n\\t__this_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);\\n}\\n\\nstatic int is_softlockup(unsigned long touch_ts,\\n\\t\\t\\t unsigned long period_ts,\\n\\t\\t\\t unsigned long now)\\n{\\n\\tif ((watchdog_enabled & WATCHDOG_SOFTOCKUP_ENABLED) && watchdog_thresh) {\\n\\t\\t/*\\n\\t\\t * If period_ts has not been updated during a sample_period, then\\n\\t\\t * in the subsequent few sample_periods, period_ts might also not\\n\\t\\t * be updated, which could indicate a potential softlockup. In\\n\\t\\t * this case, if we suspect the cause of the potential softlockup\\n\\t\\t * might be interrupt storm, then we need to count the interrupts\\n\\t\\t * to find which interrupt is storming.\\n\\t\\t */\\n\\t\\tif (time_after_eq(now, period_ts + get_softlockup_thresh() / NUM_SAMPLE_PERIODS) &&\\n\\t\\t    need_counting_irqs())\\n\\t\\t\\tstart_counting_irqs();\\n\\n\\t\\t/*\\n\\t\\t * A poorly behaving BPF scheduler can live-lock the system into\\n\\t\\t * soft lockups. Tell sched_ext to try ejecting the BPF\\n\\t\\t * scheduler when close to a soft lockup.\\n\\t\\t */\\n\\t\\tif (time_after_eq(now, period_ts + get_softlockup_thresh() * 3 / 4))\\n\\t\\t\\tscx_softlockup(now - touch_ts);\\n\\n\\t\\t/* Warn about unreasonable delays. */\\n\\t\\tif (time_after(now, period_ts + get_softlockup_thresh()))\\n\\t\\t\\treturn now - touch_ts;\\n\\t}\\n\\treturn 0;\\n}\\n\\n/* watchdog detector functions */\\nstatic DEFINE_PER_CPU(struct completion, softlockup_completion);\\nstatic DEFINE_PER_CPU(struct cpu_stop_work, softlockup_stop_work);\\n\\n/*\\n * The watchdog feed function - touches the timestamp.\\n *\\n * It only runs once every sample_period seconds (4 seconds by\\n * default) to reset the softlockup timestamp. If this gets delayed\\n * for more than 2*watchdog_thresh seconds then the debug-printout\\n * triggers in watchdog_timer_fn().\\n */\\nstatic int softlockup_fn(void *data)\\n{\\n\\tupdate_touch_ts();\\n\\tstop_counting_irqs();\\n\\tcomplete(this_cpu_ptr(&softlockup_completion));\\n\\n\\treturn 0;\\n}\\n\\n/* watchdog kicker functions */\\nstatic enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)\\n{\\n\\tunsigned long touch_ts, period_ts, now;\\n\\tstruct pt_regs *regs = get_irq_regs();\\n\\tint duration;\\n\\tint softlockup_all_cpu_backtrace = sysctl_softlockup_all_cpu_backtrace;\\n\\tunsigned long flags;\\n\\n\\tif (!watchdog_enabled)\\n\\t\\treturn HRTIMER_NORESTART;\\n\\n\\twatchdog_hardlockup_kick();\\n\\n\\t/* kick the softlockup detector */\\n\\tif (completion_done(this_cpu_ptr(&softlockup_completion))) {\\n\\t\\treinit_completion(this_cpu_ptr(&softlockup_completion));\\n\\t\\tstop_one_cpu_nowait(smp_processor_id(),\\n\\t\\t\\t\\tsoftlockup_fn, NULL,\\n\\t\\t\\t\\tthis_cpu_ptr(&softlockup_stop_work));\\n\\t}\\n\\n\\t/* .. and repeat */\\n\\thrtimer_forward_now(hrtimer, ns_to_ktime(sample_period));\\n\\n\\t/*\\n\\t * Read the current timestamp first. It might become invalid anytime\\n\\t * when a virtual machine is stopped by the host or when the watchog\\n\\t * is touched from NMI.\\n\\t */\\n\\tnow = get_timestamp();\\n\\t/*\\n\\t * If a virtual machine is stopped by the host it can look to\\n\\t * the watchdog like a soft lockup. This function touches the watchdog.\\n\\t */\\n\\tkvm_check_and_clear_guest_paused();\\n\\t/*\\n\\t * The stored timestamp is comparable with @now only when not touched.\\n\\t * It might get touched anytime from NMI. Make sure that is_softlockup()\\n\\t * uses the same (valid) value.\\n\\t */\\n\\tperiod_ts = READ_ONCE(*this_cpu_ptr(&watchdog_report_ts));\\n\\n\\tupdate_cpustat();\\n\\n\\t/* Reset the interval when touched by known problematic code. */\\n\\tif (period_ts == SOFTLOCKUP_DELAY_REPORT) {\\n\\t\\tif (unlikely(__this_cpu_read(softlockup_touch_sync))) {\\n\\t\\t\\t/*\\n\\t\\t\\t * If the time stamp was touched atomically\\n\\t\\t\\t * make sure the scheduler tick is up to date.\\n\\t\\t\\t */\\n\\t\\t\\t__this_cpu_write(softlockup_touch_sync, false);\\n\\t\\t\\tsched_clock_tick();\\n\\t\\t}\\n\\n\\t\\tupdate_report_ts();\\n\\t\\treturn HRTIMER_RESTART;\\n\\t}\\n\\n\\t/* Check for a softlockup. */\\n\\ttouch_ts = __this_cpu_read(watchdog_touch_ts);\\n\\tduration = is_softlockup(touch_ts, period_ts, now);\\n\\tif (unlikely(duration)) {\\n\\t\\t/*\\n\\t\\t * Prevent multiple soft-lockup reports if one cpu is already\\n\\t\\t * engaged in dumping all cpu back traces.\\n\\t\\t */\\n\\t\\tif (softlockup_all_cpu_backtrace) {\\n\\t\\t\\tif (test_and_set_bit_lock(0, &soft_lockup_nmi_warn))\\n\\t\\t\\t\\treturn HRTIMER_RESTART;\\n\\t\\t}\\n\\n\\t\\t/* Start period for the next softlockup warning. */\\n\\t\\tupdate_report_ts();\\n\\n\\t\\tprintk_cpu_sync_get_irqsave(flags);\\n\\t\\tpr_emerg(\"BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\\\\n\",\\n\\t\\t\\tsmp_processor_id(), duration,\\n\\t\\t\\tcurrent->comm, task_pid_nr(current));\\n\\t\\treport_cpu_status();\\n\\t\\tprint_modules();\\n\\t\\tprint_irqtrace_events(current);\\n\\t\\tif (regs)\\n\\t\\t\\tshow_regs(regs);\\n\\t\\telse\\n\\t\\t\\tdump_stack();\\n\\t\\tprintk_cpu_sync_put_irqrestore(flags);\\n\\n\\t\\tif (softlockup_all_cpu_backtrace) {\\n\\t\\t\\ttrigger_allbutcpu_cpu_backtrace(smp_processor_id());\\n\\t\\t\\tif (!softlockup_panic)\\n\\t\\t\\t\\tclear_bit_unlock(0, &soft_lockup_nmi_warn);\\n\\t\\t}\\n\\n\\t\\tadd_taint(TAINT_SOFTLOCKUP, LOCKDEP_STILL_OK);\\n\\t\\tif (softlockup_panic)\\n\\t\\t\\tpanic(\"softlockup: hung tasks\");\\n\\t}\\n\\n\\treturn HRTIMER_RESTART;\\n}\\n\\nstatic void watchdog_enable(unsigned int cpu)\\n{\\n\\tstruct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);\\n\\tstruct completion *done = this_cpu_ptr(&softlockup_completion);\\n\\n\\tWARN_ON_ONCE(cpu != smp_processor_id());\\n\\n\\tinit_completion(done);\\n\\tcomplete(done);\\n\\n\\t/*\\n\\t * Start the timer first to prevent the hardlockup watchdog triggering\\n\\t * before the timer has a chance to fire.\\n\\t */\\n\\thrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);\\n\\thrtimer->function = watchdog_timer_fn;\\n\\thrtimer_start(hrtimer, ns_to_ktime(sample_period),\\n\\t\\t      HRTIMER_MODE_REL_PINNED_HARD);\\n\\n\\t/* Initialize timestamp */\\n\\tupdate_touch_ts();\\n\\t/* Enable the hardlockup detector */\\n\\tif (watchdog_enabled & WATCHDOG_HARDLOCKUP_ENABLED)\\n\\t\\twatchdog_hardlockup_enable(cpu);\\n}\\n\\nstatic void watchdog_disable(unsigned int cpu)\\n{\\n\\tstruct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);\\n\\n\\tWARN_ON_ONCE(cpu != smp_processor_id());\\n\\n\\t/*\\n\\t * Disable the hardlockup detector first. That prevents that a large\\n\\t * delay between disabling the timer and disabling the hardlockup\\n\\t * detector causes a false positive.\\n\\t */\\n\\twatchdog_hardlockup_disable(cpu);\\n\\thrtimer_cancel(hrtimer);\\n\\twait_for_completion(this_cpu_ptr(&softlockup_completion));\\n}\\n\\nstatic int softlockup_stop_fn(void *data)\\n{\\n\\twatchdog_disable(smp_processor_id());\\n\\treturn 0;\\n}\\n\\nstatic void softlockup_stop_all(void)\\n{\\n\\tint cpu;\\n\\n\\tif (!softlockup_initialized)\\n\\t\\treturn;\\n\\n\\tfor_each_cpu(cpu, &watchdog_allowed_mask)\\n\\t\\tsmp_call_on_cpu(cpu, softlockup_stop_fn, NULL, false);\\n\\n\\tcpumask_clear(&watchdog_allowed_mask);\\n}\\n\\nstatic int softlockup_start_fn(void *data)\\n{\\n\\twatchdog_enable(smp_processor_id());\\n\\treturn 0;\\n}\\n\\nstatic void softlockup_start_all(void)\\n{\\n\\tint cpu;\\n\\n\\tcpumask_copy(&watchdog_allowed_mask, &watchdog_cpumask);\\n\\tfor_each_cpu(cpu, &watchdog_allowed_mask)\\n\\t\\tsmp_call_on_cpu(cpu, softlockup_start_fn, NULL, false);\\n}\\n\\nint lockup_detector_online_cpu(unsigned int cpu)\\n{\\n\\tif (cpumask_test_cpu(cpu, &watchdog_allowed_mask))\\n\\t\\twatchdog_enable(cpu);\\n\\treturn 0;\\n}\\n\\nint lockup_detector_offline_cpu(unsigned int cpu)\\n{\\n\\tif (cpumask_test_cpu(cpu, &watchdog_allowed_mask))\\n\\t\\twatchdog_disable(cpu);\\n\\treturn 0;\\n}\\n\\nstatic void __lockup_detector_reconfigure(void)\\n{\\n\\tcpus_read_lock();\\n\\twatchdog_hardlockup_stop();\\n\\n\\tsoftlockup_stop_all();\\n\\tset_sample_period();\\n\\tlockup_detector_update_enable();\\n\\tif (watchdog_enabled && watchdog_thresh)\\n\\t\\tsoftlockup_start_all();\\n\\n\\twatchdog_hardlockup_start();\\n\\tcpus_read_unlock();\\n\\t/*\\n\\t * Must be called outside the cpus locked section to prevent\\n\\t * recursive locking in the perf code.\\n\\t */\\n\\t__lockup_detector_cleanup();\\n}\\n\\nvoid lockup_detector_reconfigure(void)\\n{\\n\\tmutex_lock(&watchdog_mutex);\\n\\t__lockup_detector_reconfigure();\\n\\tmutex_unlock(&watchdog_mutex);\\n}\\n\\n/*\\n * Create the watchdog infrastructure and configure the detector(s).\\n */\\nstatic __init void lockup_detector_setup(void)\\n{\\n\\t/*\\n\\t * If sysctl is off and watchdog got disabled on the command line,\\n\\t * nothing to do here.\\n\\t */\\n\\tlockup_detector_update_enable();\\n\\n\\tif (!IS_ENABLED(CONFIG_SYSCTL) &&\\n\\t    !(watchdog_enabled && watchdog_thresh))\\n\\t\\treturn;\\n\\n\\tmutex_lock(&watchdog_mutex);\\n\\t__lockup_detector_reconfigure();\\n\\tsoftlockup_initialized = true;\\n\\tmutex_unlock(&watchdog_mutex);\\n}\\n\\n#else /* CONFIG_SOFTLOCKUP_DETECTOR */\\nstatic void __lockup_detector_reconfigure(void)\\n{\\n\\tcpus_read_lock();\\n\\twatchdog_hardlockup_stop();\\n\\tlockup_detector_update_enable();\\n\\twatchdog_hardlockup_start();\\n\\tcpus_read_unlock();\\n}\\nvoid lockup_detector_reconfigure(void)\\n{\\n\\t__lockup_detector_reconfigure();\\n}\\nstatic inline void lockup_detector_setup(void)\\n{\\n\\t__lockup_detector_reconfigure();\\n}\\n#endif /* !CONFIG_SOFTLOCKUP_DETECTOR */\\n\\nstatic void __lockup_detector_cleanup(void)\\n{\\n\\tlockdep_assert_held(&watchdog_mutex);\\n\\thardlockup_detector_perf_cleanup();\\n}\\n\\n/**\\n * lockup_detector_cleanup - Cleanup after cpu hotplug or sysctl changes\\n *\\n * Caller must not hold the cpu hotplug rwsem.\\n */\\nvoid lockup_detector_cleanup(void)\\n{\\n\\tmutex_lock(&watchdog_mutex);\\n\\t__lockup_detector_cleanup();\\n\\tmutex_unlock(&watchdog_mutex);\\n}\\n\\n/**\\n * lockup_detector_soft_poweroff - Interface to stop lockup detector(s)\\n *\\n * Special interface for parisc. It prevents lockup detector warnings from\\n * the default pm_poweroff() function which busy loops forever.\\n */\\nvoid lockup_detector_soft_poweroff(void)\\n{\\n\\twatchdog_enabled = 0;\\n}\\n\\n#ifdef CONFIG_SYSCTL\\n\\n/* Propagate any changes to the watchdog infrastructure */\\nstatic void proc_watchdog_update(void)\\n{\\n\\t/* Remove impossible cpus to keep sysctl output clean. */\\n\\tcpumask_and(&watchdog_cpumask, &watchdog_cpumask, cpu_possible_mask);\\n\\t__lockup_detector_reconfigure();\\n}\\n\\n/*\\n * common function for watchdog, nmi_watchdog and soft_watchdog parameter\\n *\\n * caller             | table->data points to            | \\'which\\'\\n * -------------------|----------------------------------|-------------------------------\\n * proc_watchdog      | watchdog_user_enabled            | WATCHDOG_HARDLOCKUP_ENABLED |\\n *                    |                                  | WATCHDOG_SOFTOCKUP_ENABLED\\n * -------------------|----------------------------------|-------------------------------\\n * proc_nmi_watchdog  | watchdog_hardlockup_user_enabled | WATCHDOG_HARDLOCKUP_ENABLED\\n * -------------------|----------------------------------|-------------------------------\\n * proc_soft_watchdog | watchdog_softlockup_user_enabled | WATCHDOG_SOFTOCKUP_ENABLED\\n */\\nstatic int proc_watchdog_common(int which, const struct ctl_table *table, int write,\\n\\t\\t\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tint err, old, *param = table->data;\\n\\n\\tmutex_lock(&watchdog_mutex);\\n\\n\\told = *param;\\n\\tif (!write) {\\n\\t\\t/*\\n\\t\\t * On read synchronize the userspace interface. This is a\\n\\t\\t * racy snapshot.\\n\\t\\t */\\n\\t\\t*param = (watchdog_enabled & which) != 0;\\n\\t\\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\\n\\t\\t*param = old;\\n\\t} else {\\n\\t\\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\\n\\t\\tif (!err && old != READ_ONCE(*param))\\n\\t\\t\\tproc_watchdog_update();\\n\\t}\\n\\tmutex_unlock(&watchdog_mutex);\\n\\treturn err;\\n}\\n\\n/*\\n * /proc/sys/kernel/watchdog\\n */\\nstatic int proc_watchdog(const struct ctl_table *table, int write,\\n\\t\\t\\t void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED |\\n\\t\\t\\t\\t    WATCHDOG_SOFTOCKUP_ENABLED,\\n\\t\\t\\t\\t    table, write, buffer, lenp, ppos);\\n}\\n\\n/*\\n * /proc/sys/kernel/nmi_watchdog\\n */\\nstatic int proc_nmi_watchdog(const struct ctl_table *table, int write,\\n\\t\\t\\t     void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tif (!watchdog_hardlockup_available && write)\\n\\t\\treturn -ENOTSUPP;\\n\\treturn proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED,\\n\\t\\t\\t\\t    table, write, buffer, lenp, ppos);\\n}\\n\\n#ifdef CONFIG_SOFTLOCKUP_DETECTOR\\n/*\\n * /proc/sys/kernel/soft_watchdog\\n */\\nstatic int proc_soft_watchdog(const struct ctl_table *table, int write,\\n\\t\\t\\t      void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn proc_watchdog_common(WATCHDOG_SOFTOCKUP_ENABLED,\\n\\t\\t\\t\\t    table, write, buffer, lenp, ppos);\\n}\\n#endif\\n\\n/*\\n * /proc/sys/kernel/watchdog_thresh\\n */\\nstatic int proc_watchdog_thresh(const struct ctl_table *table, int write,\\n\\t\\t\\t\\tvoid *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tint err, old;\\n\\n\\tmutex_lock(&watchdog_mutex);\\n\\n\\told = READ_ONCE(watchdog_thresh);\\n\\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\\n\\n\\tif (!err && write && old != READ_ONCE(watchdog_thresh))\\n\\t\\tproc_watchdog_update();\\n\\n\\tmutex_unlock(&watchdog_mutex);\\n\\treturn err;\\n}\\n\\n/*\\n * The cpumask is the mask of possible cpus that the watchdog can run\\n * on, not the mask of cpus it is actually running on.  This allows the\\n * user to specify a mask that will include cpus that have not yet\\n * been brought online, if desired.\\n */\\nstatic int proc_watchdog_cpumask(const struct ctl_table *table, int write,\\n\\t\\t\\t\\t void *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tint err;\\n\\n\\tmutex_lock(&watchdog_mutex);\\n\\n\\terr = proc_do_large_bitmap(table, write, buffer, lenp, ppos);\\n\\tif (!err && write)\\n\\t\\tproc_watchdog_update();\\n\\n\\tmutex_unlock(&watchdog_mutex);\\n\\treturn err;\\n}\\n\\nstatic const int sixty = 60;\\n\\nstatic struct ctl_table watchdog_sysctls[] = {\\n\\t{\\n\\t\\t.procname       = \"watchdog\",\\n\\t\\t.data\\t\\t= &watchdog_user_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler   = proc_watchdog,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"watchdog_thresh\",\\n\\t\\t.data\\t\\t= &watchdog_thresh,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_watchdog_thresh,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= (void *)&sixty,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"watchdog_cpumask\",\\n\\t\\t.data\\t\\t= &watchdog_cpumask_bits,\\n\\t\\t.maxlen\\t\\t= NR_CPUS,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_watchdog_cpumask,\\n\\t},\\n#ifdef CONFIG_SOFTLOCKUP_DETECTOR\\n\\t{\\n\\t\\t.procname       = \"soft_watchdog\",\\n\\t\\t.data\\t\\t= &watchdog_softlockup_user_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler   = proc_soft_watchdog,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"softlockup_panic\",\\n\\t\\t.data\\t\\t= &softlockup_panic,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname\\t= \"softlockup_all_cpu_backtrace\",\\n\\t\\t.data\\t\\t= &sysctl_softlockup_all_cpu_backtrace,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#endif /* CONFIG_SMP */\\n#endif\\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\\n\\t{\\n\\t\\t.procname\\t= \"hardlockup_panic\",\\n\\t\\t.data\\t\\t= &hardlockup_panic,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname\\t= \"hardlockup_all_cpu_backtrace\",\\n\\t\\t.data\\t\\t= &sysctl_hardlockup_all_cpu_backtrace,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n#endif /* CONFIG_SMP */\\n#endif\\n};\\n\\nstatic struct ctl_table watchdog_hardlockup_sysctl[] = {\\n\\t{\\n\\t\\t.procname       = \"nmi_watchdog\",\\n\\t\\t.data\\t\\t= &watchdog_hardlockup_user_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler   = proc_nmi_watchdog,\\n\\t\\t.extra1\\t\\t= SYSCTL_ZERO,\\n\\t\\t.extra2\\t\\t= SYSCTL_ONE,\\n\\t},\\n};\\n\\nstatic void __init watchdog_sysctl_init(void)\\n{\\n\\tregister_sysctl_init(\"kernel\", watchdog_sysctls);\\n\\n\\tif (watchdog_hardlockup_available)\\n\\t\\twatchdog_hardlockup_sysctl[0].mode = 0644;\\n\\tregister_sysctl_init(\"kernel\", watchdog_hardlockup_sysctl);\\n}\\n\\n#else\\n#define watchdog_sysctl_init() do { } while (0)\\n#endif /* CONFIG_SYSCTL */\\n\\nstatic void __init lockup_detector_delay_init(struct work_struct *work);\\nstatic bool allow_lockup_detector_init_retry __initdata;\\n\\nstatic struct work_struct detector_work __initdata =\\n\\t\\t__WORK_INITIALIZER(detector_work, lockup_detector_delay_init);\\n\\nstatic void __init lockup_detector_delay_init(struct work_struct *work)\\n{\\n\\tint ret;\\n\\n\\tret = watchdog_hardlockup_probe();\\n\\tif (ret) {\\n\\t\\tif (ret == -ENODEV)\\n\\t\\t\\tpr_info(\"NMI not fully supported\\\\n\");\\n\\t\\telse\\n\\t\\t\\tpr_info(\"Delayed init of the lockup detector failed: %d\\\\n\", ret);\\n\\t\\tpr_info(\"Hard watchdog permanently disabled\\\\n\");\\n\\t\\treturn;\\n\\t}\\n\\n\\tallow_lockup_detector_init_retry = false;\\n\\n\\twatchdog_hardlockup_available = true;\\n\\tlockup_detector_setup();\\n}\\n\\n/*\\n * lockup_detector_retry_init - retry init lockup detector if possible.\\n *\\n * Retry hardlockup detector init. It is useful when it requires some\\n * functionality that has to be initialized later on a particular\\n * platform.\\n */\\nvoid __init lockup_detector_retry_init(void)\\n{\\n\\t/* Must be called before late init calls */\\n\\tif (!allow_lockup_detector_init_retry)\\n\\t\\treturn;\\n\\n\\tschedule_work(&detector_work);\\n}\\n\\n/*\\n * Ensure that optional delayed hardlockup init is proceed before\\n * the init code and memory is freed.\\n */\\nstatic int __init lockup_detector_check(void)\\n{\\n\\t/* Prevent any later retry. */\\n\\tallow_lockup_detector_init_retry = false;\\n\\n\\t/* Make sure no work is pending. */\\n\\tflush_work(&detector_work);\\n\\n\\twatchdog_sysctl_init();\\n\\n\\treturn 0;\\n\\n}\\nlate_initcall_sync(lockup_detector_check);\\n\\nvoid __init lockup_detector_init(void)\\n{\\n\\tif (tick_nohz_full_enabled())\\n\\t\\tpr_info(\"Disabling watchdog on nohz_full cores by default\\\\n\");\\n\\n\\tcpumask_copy(&watchdog_cpumask,\\n\\t\\t     housekeeping_cpumask(HK_TYPE_TIMER));\\n\\n\\tif (!watchdog_hardlockup_probe())\\n\\t\\twatchdog_hardlockup_available = true;\\n\\telse\\n\\t\\tallow_lockup_detector_init_retry = true;\\n\\n\\tlockup_detector_setup();\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0-only\\n/*\\n * Uniprocessor-only support functions.  The counterpart to kernel/smp.c\\n */\\n\\n#include <linux/interrupt.h>\\n#include <linux/kernel.h>\\n#include <linux/export.h>\\n#include <linux/smp.h>\\n#include <linux/hypervisor.h>\\n\\nint smp_call_function_single(int cpu, void (*func) (void *info), void *info,\\n\\t\\t\\t\\tint wait)\\n{\\n\\tunsigned long flags;\\n\\n\\tif (cpu != 0)\\n\\t\\treturn -ENXIO;\\n\\n\\tlocal_irq_save(flags);\\n\\tfunc(info);\\n\\tlocal_irq_restore(flags);\\n\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(smp_call_function_single);\\n\\nint smp_call_function_single_async(int cpu, call_single_data_t *csd)\\n{\\n\\tunsigned long flags;\\n\\n\\tlocal_irq_save(flags);\\n\\tcsd->func(csd->info);\\n\\tlocal_irq_restore(flags);\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(smp_call_function_single_async);\\n\\n/*\\n * Preemption is disabled here to make sure the cond_func is called under the\\n * same conditions in UP and SMP.\\n */\\nvoid on_each_cpu_cond_mask(smp_cond_func_t cond_func, smp_call_func_t func,\\n\\t\\t\\t   void *info, bool wait, const struct cpumask *mask)\\n{\\n\\tunsigned long flags;\\n\\n\\tpreempt_disable();\\n\\tif ((!cond_func || cond_func(0, info)) && cpumask_test_cpu(0, mask)) {\\n\\t\\tlocal_irq_save(flags);\\n\\t\\tfunc(info);\\n\\t\\tlocal_irq_restore(flags);\\n\\t}\\n\\tpreempt_enable();\\n}\\nEXPORT_SYMBOL(on_each_cpu_cond_mask);\\n\\nint smp_call_on_cpu(unsigned int cpu, int (*func)(void *), void *par, bool phys)\\n{\\n\\tint ret;\\n\\n\\tif (cpu != 0)\\n\\t\\treturn -ENXIO;\\n\\n\\tif (phys)\\n\\t\\thypervisor_pin_vcpu(0);\\n\\tret = func(par);\\n\\tif (phys)\\n\\t\\thypervisor_pin_vcpu(-1);\\n\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(smp_call_on_cpu);\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/* Watch queue and general notification mechanism, built on pipes\\n *\\n * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.\\n * Written by David Howells (dhowells@redhat.com)\\n *\\n * See Documentation/core-api/watch_queue.rst\\n */\\n\\n#define pr_fmt(fmt) \"watchq: \" fmt\\n#include <linux/module.h>\\n#include <linux/init.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/printk.h>\\n#include <linux/miscdevice.h>\\n#include <linux/fs.h>\\n#include <linux/mm.h>\\n#include <linux/pagemap.h>\\n#include <linux/poll.h>\\n#include <linux/uaccess.h>\\n#include <linux/vmalloc.h>\\n#include <linux/file.h>\\n#include <linux/security.h>\\n#include <linux/cred.h>\\n#include <linux/sched/signal.h>\\n#include <linux/watch_queue.h>\\n#include <linux/pipe_fs_i.h>\\n\\nMODULE_DESCRIPTION(\"Watch queue\");\\nMODULE_AUTHOR(\"Red Hat, Inc.\");\\n\\n#define WATCH_QUEUE_NOTE_SIZE 128\\n#define WATCH_QUEUE_NOTES_PER_PAGE (PAGE_SIZE / WATCH_QUEUE_NOTE_SIZE)\\n\\n/*\\n * This must be called under the RCU read-lock, which makes\\n * sure that the wqueue still exists. It can then take the lock,\\n * and check that the wqueue hasn\\'t been destroyed, which in\\n * turn makes sure that the notification pipe still exists.\\n */\\nstatic inline bool lock_wqueue(struct watch_queue *wqueue)\\n{\\n\\tspin_lock_bh(&wqueue->lock);\\n\\tif (unlikely(!wqueue->pipe)) {\\n\\t\\tspin_unlock_bh(&wqueue->lock);\\n\\t\\treturn false;\\n\\t}\\n\\treturn true;\\n}\\n\\nstatic inline void unlock_wqueue(struct watch_queue *wqueue)\\n{\\n\\tspin_unlock_bh(&wqueue->lock);\\n}\\n\\nstatic void watch_queue_pipe_buf_release(struct pipe_inode_info *pipe,\\n\\t\\t\\t\\t\\t struct pipe_buffer *buf)\\n{\\n\\tstruct watch_queue *wqueue = (struct watch_queue *)buf->private;\\n\\tstruct page *page;\\n\\tunsigned int bit;\\n\\n\\t/* We need to work out which note within the page this refers to, but\\n\\t * the note might have been maximum size, so merely ANDing the offset\\n\\t * off doesn\\'t work.  OTOH, the note must\\'ve been more than zero size.\\n\\t */\\n\\tbit = buf->offset + buf->len;\\n\\tif ((bit & (WATCH_QUEUE_NOTE_SIZE - 1)) == 0)\\n\\t\\tbit -= WATCH_QUEUE_NOTE_SIZE;\\n\\tbit /= WATCH_QUEUE_NOTE_SIZE;\\n\\n\\tpage = buf->page;\\n\\tbit += page->index;\\n\\n\\tset_bit(bit, wqueue->notes_bitmap);\\n\\tgeneric_pipe_buf_release(pipe, buf);\\n}\\n\\n// No try_steal function => no stealing\\n#define watch_queue_pipe_buf_try_steal NULL\\n\\n/* New data written to a pipe may be appended to a buffer with this type. */\\nstatic const struct pipe_buf_operations watch_queue_pipe_buf_ops = {\\n\\t.release\\t= watch_queue_pipe_buf_release,\\n\\t.try_steal\\t= watch_queue_pipe_buf_try_steal,\\n\\t.get\\t\\t= generic_pipe_buf_get,\\n};\\n\\n/*\\n * Post a notification to a watch queue.\\n *\\n * Must be called with the RCU lock for reading, and the\\n * watch_queue lock held, which guarantees that the pipe\\n * hasn\\'t been released.\\n */\\nstatic bool post_one_notification(struct watch_queue *wqueue,\\n\\t\\t\\t\\t  struct watch_notification *n)\\n{\\n\\tvoid *p;\\n\\tstruct pipe_inode_info *pipe = wqueue->pipe;\\n\\tstruct pipe_buffer *buf;\\n\\tstruct page *page;\\n\\tunsigned int head, tail, mask, note, offset, len;\\n\\tbool done = false;\\n\\n\\tspin_lock_irq(&pipe->rd_wait.lock);\\n\\n\\tmask = pipe->ring_size - 1;\\n\\thead = pipe->head;\\n\\ttail = pipe->tail;\\n\\tif (pipe_full(head, tail, pipe->ring_size))\\n\\t\\tgoto lost;\\n\\n\\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);\\n\\tif (note >= wqueue->nr_notes)\\n\\t\\tgoto lost;\\n\\n\\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];\\n\\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;\\n\\tget_page(page);\\n\\tlen = n->info & WATCH_INFO_LENGTH;\\n\\tp = kmap_atomic(page);\\n\\tmemcpy(p + offset, n, len);\\n\\tkunmap_atomic(p);\\n\\n\\tbuf = &pipe->bufs[head & mask];\\n\\tbuf->page = page;\\n\\tbuf->private = (unsigned long)wqueue;\\n\\tbuf->ops = &watch_queue_pipe_buf_ops;\\n\\tbuf->offset = offset;\\n\\tbuf->len = len;\\n\\tbuf->flags = PIPE_BUF_FLAG_WHOLE;\\n\\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */\\n\\n\\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {\\n\\t\\tspin_unlock_irq(&pipe->rd_wait.lock);\\n\\t\\tBUG();\\n\\t}\\n\\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\\n\\tdone = true;\\n\\nout:\\n\\tspin_unlock_irq(&pipe->rd_wait.lock);\\n\\tif (done)\\n\\t\\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\\n\\treturn done;\\n\\nlost:\\n\\tbuf = &pipe->bufs[(head - 1) & mask];\\n\\tbuf->flags |= PIPE_BUF_FLAG_LOSS;\\n\\tgoto out;\\n}\\n\\n/*\\n * Apply filter rules to a notification.\\n */\\nstatic bool filter_watch_notification(const struct watch_filter *wf,\\n\\t\\t\\t\\t      const struct watch_notification *n)\\n{\\n\\tconst struct watch_type_filter *wt;\\n\\tunsigned int st_bits = sizeof(wt->subtype_filter[0]) * 8;\\n\\tunsigned int st_index = n->subtype / st_bits;\\n\\tunsigned int st_bit = 1U << (n->subtype % st_bits);\\n\\tint i;\\n\\n\\tif (!test_bit(n->type, wf->type_filter))\\n\\t\\treturn false;\\n\\n\\tfor (i = 0; i < wf->nr_filters; i++) {\\n\\t\\twt = &wf->filters[i];\\n\\t\\tif (n->type == wt->type &&\\n\\t\\t    (wt->subtype_filter[st_index] & st_bit) &&\\n\\t\\t    (n->info & wt->info_mask) == wt->info_filter)\\n\\t\\t\\treturn true;\\n\\t}\\n\\n\\treturn false; /* If there is a filter, the default is to reject. */\\n}\\n\\n/**\\n * __post_watch_notification - Post an event notification\\n * @wlist: The watch list to post the event to.\\n * @n: The notification record to post.\\n * @cred: The creds of the process that triggered the notification.\\n * @id: The ID to match on the watch.\\n *\\n * Post a notification of an event into a set of watch queues and let the users\\n * know.\\n *\\n * The size of the notification should be set in n->info & WATCH_INFO_LENGTH and\\n * should be in units of sizeof(*n).\\n */\\nvoid __post_watch_notification(struct watch_list *wlist,\\n\\t\\t\\t       struct watch_notification *n,\\n\\t\\t\\t       const struct cred *cred,\\n\\t\\t\\t       u64 id)\\n{\\n\\tconst struct watch_filter *wf;\\n\\tstruct watch_queue *wqueue;\\n\\tstruct watch *watch;\\n\\n\\tif (((n->info & WATCH_INFO_LENGTH) >> WATCH_INFO_LENGTH__SHIFT) == 0) {\\n\\t\\tWARN_ON(1);\\n\\t\\treturn;\\n\\t}\\n\\n\\trcu_read_lock();\\n\\n\\thlist_for_each_entry_rcu(watch, &wlist->watchers, list_node) {\\n\\t\\tif (watch->id != id)\\n\\t\\t\\tcontinue;\\n\\t\\tn->info &= ~WATCH_INFO_ID;\\n\\t\\tn->info |= watch->info_id;\\n\\n\\t\\twqueue = rcu_dereference(watch->queue);\\n\\t\\twf = rcu_dereference(wqueue->filter);\\n\\t\\tif (wf && !filter_watch_notification(wf, n))\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (security_post_notification(watch->cred, cred, n) < 0)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tif (lock_wqueue(wqueue)) {\\n\\t\\t\\tpost_one_notification(wqueue, n);\\n\\t\\t\\tunlock_wqueue(wqueue);\\n\\t\\t}\\n\\t}\\n\\n\\trcu_read_unlock();\\n}\\nEXPORT_SYMBOL(__post_watch_notification);\\n\\n/*\\n * Allocate sufficient pages to preallocation for the requested number of\\n * notifications.\\n */\\nlong watch_queue_set_size(struct pipe_inode_info *pipe, unsigned int nr_notes)\\n{\\n\\tstruct watch_queue *wqueue = pipe->watch_queue;\\n\\tstruct page **pages;\\n\\tunsigned long *bitmap;\\n\\tunsigned long user_bufs;\\n\\tint ret, i, nr_pages;\\n\\n\\tif (!wqueue)\\n\\t\\treturn -ENODEV;\\n\\tif (wqueue->notes)\\n\\t\\treturn -EBUSY;\\n\\n\\tif (nr_notes < 1 ||\\n\\t    nr_notes > 512) /* TODO: choose a better hard limit */\\n\\t\\treturn -EINVAL;\\n\\n\\tnr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1);\\n\\tnr_pages /= WATCH_QUEUE_NOTES_PER_PAGE;\\n\\tuser_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_pages);\\n\\n\\tif (nr_pages > pipe->max_usage &&\\n\\t    (too_many_pipe_buffers_hard(user_bufs) ||\\n\\t     too_many_pipe_buffers_soft(user_bufs)) &&\\n\\t    pipe_is_unprivileged_user()) {\\n\\t\\tret = -EPERM;\\n\\t\\tgoto error;\\n\\t}\\n\\n\\tnr_notes = nr_pages * WATCH_QUEUE_NOTES_PER_PAGE;\\n\\tret = pipe_resize_ring(pipe, roundup_pow_of_two(nr_notes));\\n\\tif (ret < 0)\\n\\t\\tgoto error;\\n\\n\\tret = -ENOMEM;\\n\\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\\n\\tif (!pages)\\n\\t\\tgoto error;\\n\\n\\tfor (i = 0; i < nr_pages; i++) {\\n\\t\\tpages[i] = alloc_page(GFP_KERNEL);\\n\\t\\tif (!pages[i])\\n\\t\\t\\tgoto error_p;\\n\\t\\tpages[i]->index = i * WATCH_QUEUE_NOTES_PER_PAGE;\\n\\t}\\n\\n\\tbitmap = bitmap_alloc(nr_notes, GFP_KERNEL);\\n\\tif (!bitmap)\\n\\t\\tgoto error_p;\\n\\n\\tbitmap_fill(bitmap, nr_notes);\\n\\twqueue->notes = pages;\\n\\twqueue->notes_bitmap = bitmap;\\n\\twqueue->nr_pages = nr_pages;\\n\\twqueue->nr_notes = nr_notes;\\n\\treturn 0;\\n\\nerror_p:\\n\\twhile (--i >= 0)\\n\\t\\t__free_page(pages[i]);\\n\\tkfree(pages);\\nerror:\\n\\t(void) account_pipe_buffers(pipe->user, nr_pages, pipe->nr_accounted);\\n\\treturn ret;\\n}\\n\\n/*\\n * Set the filter on a watch queue.\\n */\\nlong watch_queue_set_filter(struct pipe_inode_info *pipe,\\n\\t\\t\\t    struct watch_notification_filter __user *_filter)\\n{\\n\\tstruct watch_notification_type_filter *tf;\\n\\tstruct watch_notification_filter filter;\\n\\tstruct watch_type_filter *q;\\n\\tstruct watch_filter *wfilter;\\n\\tstruct watch_queue *wqueue = pipe->watch_queue;\\n\\tint ret, nr_filter = 0, i;\\n\\n\\tif (!wqueue)\\n\\t\\treturn -ENODEV;\\n\\n\\tif (!_filter) {\\n\\t\\t/* Remove the old filter */\\n\\t\\twfilter = NULL;\\n\\t\\tgoto set;\\n\\t}\\n\\n\\t/* Grab the user\\'s filter specification */\\n\\tif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)\\n\\t\\treturn -EFAULT;\\n\\tif (filter.nr_filters == 0 ||\\n\\t    filter.nr_filters > 16 ||\\n\\t    filter.__reserved != 0)\\n\\t\\treturn -EINVAL;\\n\\n\\ttf = memdup_array_user(_filter->filters, filter.nr_filters, sizeof(*tf));\\n\\tif (IS_ERR(tf))\\n\\t\\treturn PTR_ERR(tf);\\n\\n\\tret = -EINVAL;\\n\\tfor (i = 0; i < filter.nr_filters; i++) {\\n\\t\\tif ((tf[i].info_filter & ~tf[i].info_mask) ||\\n\\t\\t    tf[i].info_mask & WATCH_INFO_LENGTH)\\n\\t\\t\\tgoto err_filter;\\n\\t\\t/* Ignore any unknown types */\\n\\t\\tif (tf[i].type >= WATCH_TYPE__NR)\\n\\t\\t\\tcontinue;\\n\\t\\tnr_filter++;\\n\\t}\\n\\n\\t/* Now we need to build the internal filter from only the relevant\\n\\t * user-specified filters.\\n\\t */\\n\\tret = -ENOMEM;\\n\\twfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);\\n\\tif (!wfilter)\\n\\t\\tgoto err_filter;\\n\\twfilter->nr_filters = nr_filter;\\n\\n\\tq = wfilter->filters;\\n\\tfor (i = 0; i < filter.nr_filters; i++) {\\n\\t\\tif (tf[i].type >= WATCH_TYPE__NR)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tq->type\\t\\t\\t= tf[i].type;\\n\\t\\tq->info_filter\\t\\t= tf[i].info_filter;\\n\\t\\tq->info_mask\\t\\t= tf[i].info_mask;\\n\\t\\tq->subtype_filter[0]\\t= tf[i].subtype_filter[0];\\n\\t\\t__set_bit(q->type, wfilter->type_filter);\\n\\t\\tq++;\\n\\t}\\n\\n\\tkfree(tf);\\nset:\\n\\tpipe_lock(pipe);\\n\\twfilter = rcu_replace_pointer(wqueue->filter, wfilter,\\n\\t\\t\\t\\t      lockdep_is_held(&pipe->mutex));\\n\\tpipe_unlock(pipe);\\n\\tif (wfilter)\\n\\t\\tkfree_rcu(wfilter, rcu);\\n\\treturn 0;\\n\\nerr_filter:\\n\\tkfree(tf);\\n\\treturn ret;\\n}\\n\\nstatic void __put_watch_queue(struct kref *kref)\\n{\\n\\tstruct watch_queue *wqueue =\\n\\t\\tcontainer_of(kref, struct watch_queue, usage);\\n\\tstruct watch_filter *wfilter;\\n\\tint i;\\n\\n\\tfor (i = 0; i < wqueue->nr_pages; i++)\\n\\t\\t__free_page(wqueue->notes[i]);\\n\\tkfree(wqueue->notes);\\n\\tbitmap_free(wqueue->notes_bitmap);\\n\\n\\twfilter = rcu_access_pointer(wqueue->filter);\\n\\tif (wfilter)\\n\\t\\tkfree_rcu(wfilter, rcu);\\n\\tkfree_rcu(wqueue, rcu);\\n}\\n\\n/**\\n * put_watch_queue - Dispose of a ref on a watchqueue.\\n * @wqueue: The watch queue to unref.\\n */\\nvoid put_watch_queue(struct watch_queue *wqueue)\\n{\\n\\tkref_put(&wqueue->usage, __put_watch_queue);\\n}\\nEXPORT_SYMBOL(put_watch_queue);\\n\\nstatic void free_watch(struct rcu_head *rcu)\\n{\\n\\tstruct watch *watch = container_of(rcu, struct watch, rcu);\\n\\n\\tput_watch_queue(rcu_access_pointer(watch->queue));\\n\\tatomic_dec(&watch->cred->user->nr_watches);\\n\\tput_cred(watch->cred);\\n\\tkfree(watch);\\n}\\n\\nstatic void __put_watch(struct kref *kref)\\n{\\n\\tstruct watch *watch = container_of(kref, struct watch, usage);\\n\\n\\tcall_rcu(&watch->rcu, free_watch);\\n}\\n\\n/*\\n * Discard a watch.\\n */\\nstatic void put_watch(struct watch *watch)\\n{\\n\\tkref_put(&watch->usage, __put_watch);\\n}\\n\\n/**\\n * init_watch - Initialise a watch\\n * @watch: The watch to initialise.\\n * @wqueue: The queue to assign.\\n *\\n * Initialise a watch and set the watch queue.\\n */\\nvoid init_watch(struct watch *watch, struct watch_queue *wqueue)\\n{\\n\\tkref_init(&watch->usage);\\n\\tINIT_HLIST_NODE(&watch->list_node);\\n\\tINIT_HLIST_NODE(&watch->queue_node);\\n\\trcu_assign_pointer(watch->queue, wqueue);\\n}\\n\\nstatic int add_one_watch(struct watch *watch, struct watch_list *wlist, struct watch_queue *wqueue)\\n{\\n\\tconst struct cred *cred;\\n\\tstruct watch *w;\\n\\n\\thlist_for_each_entry(w, &wlist->watchers, list_node) {\\n\\t\\tstruct watch_queue *wq = rcu_access_pointer(w->queue);\\n\\t\\tif (wqueue == wq && watch->id == w->id)\\n\\t\\t\\treturn -EBUSY;\\n\\t}\\n\\n\\tcred = current_cred();\\n\\tif (atomic_inc_return(&cred->user->nr_watches) > task_rlimit(current, RLIMIT_NOFILE)) {\\n\\t\\tatomic_dec(&cred->user->nr_watches);\\n\\t\\treturn -EAGAIN;\\n\\t}\\n\\n\\twatch->cred = get_cred(cred);\\n\\trcu_assign_pointer(watch->watch_list, wlist);\\n\\n\\tkref_get(&wqueue->usage);\\n\\tkref_get(&watch->usage);\\n\\thlist_add_head(&watch->queue_node, &wqueue->watches);\\n\\thlist_add_head_rcu(&watch->list_node, &wlist->watchers);\\n\\treturn 0;\\n}\\n\\n/**\\n * add_watch_to_object - Add a watch on an object to a watch list\\n * @watch: The watch to add\\n * @wlist: The watch list to add to\\n *\\n * @watch->queue must have been set to point to the queue to post notifications\\n * to and the watch list of the object to be watched.  @watch->cred must also\\n * have been set to the appropriate credentials and a ref taken on them.\\n *\\n * The caller must pin the queue and the list both and must hold the list\\n * locked against racing watch additions/removals.\\n */\\nint add_watch_to_object(struct watch *watch, struct watch_list *wlist)\\n{\\n\\tstruct watch_queue *wqueue;\\n\\tint ret = -ENOENT;\\n\\n\\trcu_read_lock();\\n\\n\\twqueue = rcu_access_pointer(watch->queue);\\n\\tif (lock_wqueue(wqueue)) {\\n\\t\\tspin_lock(&wlist->lock);\\n\\t\\tret = add_one_watch(watch, wlist, wqueue);\\n\\t\\tspin_unlock(&wlist->lock);\\n\\t\\tunlock_wqueue(wqueue);\\n\\t}\\n\\n\\trcu_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(add_watch_to_object);\\n\\n/**\\n * remove_watch_from_object - Remove a watch or all watches from an object.\\n * @wlist: The watch list to remove from\\n * @wq: The watch queue of interest (ignored if @all is true)\\n * @id: The ID of the watch to remove (ignored if @all is true)\\n * @all: True to remove all objects\\n *\\n * Remove a specific watch or all watches from an object.  A notification is\\n * sent to the watcher to tell them that this happened.\\n */\\nint remove_watch_from_object(struct watch_list *wlist, struct watch_queue *wq,\\n\\t\\t\\t     u64 id, bool all)\\n{\\n\\tstruct watch_notification_removal n;\\n\\tstruct watch_queue *wqueue;\\n\\tstruct watch *watch;\\n\\tint ret = -EBADSLT;\\n\\n\\trcu_read_lock();\\n\\nagain:\\n\\tspin_lock(&wlist->lock);\\n\\thlist_for_each_entry(watch, &wlist->watchers, list_node) {\\n\\t\\tif (all ||\\n\\t\\t    (watch->id == id && rcu_access_pointer(watch->queue) == wq))\\n\\t\\t\\tgoto found;\\n\\t}\\n\\tspin_unlock(&wlist->lock);\\n\\tgoto out;\\n\\nfound:\\n\\tret = 0;\\n\\thlist_del_init_rcu(&watch->list_node);\\n\\trcu_assign_pointer(watch->watch_list, NULL);\\n\\tspin_unlock(&wlist->lock);\\n\\n\\t/* We now own the reference on watch that used to belong to wlist. */\\n\\n\\tn.watch.type = WATCH_TYPE_META;\\n\\tn.watch.subtype = WATCH_META_REMOVAL_NOTIFICATION;\\n\\tn.watch.info = watch->info_id | watch_sizeof(n.watch);\\n\\tn.id = id;\\n\\tif (id != 0)\\n\\t\\tn.watch.info = watch->info_id | watch_sizeof(n);\\n\\n\\twqueue = rcu_dereference(watch->queue);\\n\\n\\tif (lock_wqueue(wqueue)) {\\n\\t\\tpost_one_notification(wqueue, &n.watch);\\n\\n\\t\\tif (!hlist_unhashed(&watch->queue_node)) {\\n\\t\\t\\thlist_del_init_rcu(&watch->queue_node);\\n\\t\\t\\tput_watch(watch);\\n\\t\\t}\\n\\n\\t\\tunlock_wqueue(wqueue);\\n\\t}\\n\\n\\tif (wlist->release_watch) {\\n\\t\\tvoid (*release_watch)(struct watch *);\\n\\n\\t\\trelease_watch = wlist->release_watch;\\n\\t\\trcu_read_unlock();\\n\\t\\t(*release_watch)(watch);\\n\\t\\trcu_read_lock();\\n\\t}\\n\\tput_watch(watch);\\n\\n\\tif (all && !hlist_empty(&wlist->watchers))\\n\\t\\tgoto again;\\nout:\\n\\trcu_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(remove_watch_from_object);\\n\\n/*\\n * Remove all the watches that are contributory to a queue.  This has the\\n * potential to race with removal of the watches by the destruction of the\\n * objects being watched or with the distribution of notifications.\\n */\\nvoid watch_queue_clear(struct watch_queue *wqueue)\\n{\\n\\tstruct watch_list *wlist;\\n\\tstruct watch *watch;\\n\\tbool release;\\n\\n\\trcu_read_lock();\\n\\tspin_lock_bh(&wqueue->lock);\\n\\n\\t/*\\n\\t * This pipe can be freed by callers like free_pipe_info().\\n\\t * Removing this reference also prevents new notifications.\\n\\t */\\n\\twqueue->pipe = NULL;\\n\\n\\twhile (!hlist_empty(&wqueue->watches)) {\\n\\t\\twatch = hlist_entry(wqueue->watches.first, struct watch, queue_node);\\n\\t\\thlist_del_init_rcu(&watch->queue_node);\\n\\t\\t/* We now own a ref on the watch. */\\n\\t\\tspin_unlock_bh(&wqueue->lock);\\n\\n\\t\\t/* We can\\'t do the next bit under the queue lock as we need to\\n\\t\\t * get the list lock - which would cause a deadlock if someone\\n\\t\\t * was removing from the opposite direction at the same time or\\n\\t\\t * posting a notification.\\n\\t\\t */\\n\\t\\twlist = rcu_dereference(watch->watch_list);\\n\\t\\tif (wlist) {\\n\\t\\t\\tvoid (*release_watch)(struct watch *);\\n\\n\\t\\t\\tspin_lock(&wlist->lock);\\n\\n\\t\\t\\trelease = !hlist_unhashed(&watch->list_node);\\n\\t\\t\\tif (release) {\\n\\t\\t\\t\\thlist_del_init_rcu(&watch->list_node);\\n\\t\\t\\t\\trcu_assign_pointer(watch->watch_list, NULL);\\n\\n\\t\\t\\t\\t/* We now own a second ref on the watch. */\\n\\t\\t\\t}\\n\\n\\t\\t\\trelease_watch = wlist->release_watch;\\n\\t\\t\\tspin_unlock(&wlist->lock);\\n\\n\\t\\t\\tif (release) {\\n\\t\\t\\t\\tif (release_watch) {\\n\\t\\t\\t\\t\\trcu_read_unlock();\\n\\t\\t\\t\\t\\t/* This might need to call dput(), so\\n\\t\\t\\t\\t\\t * we have to drop all the locks.\\n\\t\\t\\t\\t\\t */\\n\\t\\t\\t\\t\\t(*release_watch)(watch);\\n\\t\\t\\t\\t\\trcu_read_lock();\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tput_watch(watch);\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\tput_watch(watch);\\n\\t\\tspin_lock_bh(&wqueue->lock);\\n\\t}\\n\\n\\tspin_unlock_bh(&wqueue->lock);\\n\\trcu_read_unlock();\\n}\\n\\n/**\\n * get_watch_queue - Get a watch queue from its file descriptor.\\n * @fd: The fd to query.\\n */\\nstruct watch_queue *get_watch_queue(int fd)\\n{\\n\\tstruct pipe_inode_info *pipe;\\n\\tstruct watch_queue *wqueue = ERR_PTR(-EINVAL);\\n\\tCLASS(fd, f)(fd);\\n\\n\\tif (!fd_empty(f)) {\\n\\t\\tpipe = get_pipe_info(fd_file(f), false);\\n\\t\\tif (pipe && pipe->watch_queue) {\\n\\t\\t\\twqueue = pipe->watch_queue;\\n\\t\\t\\tkref_get(&wqueue->usage);\\n\\t\\t}\\n\\t}\\n\\n\\treturn wqueue;\\n}\\nEXPORT_SYMBOL(get_watch_queue);\\n\\n/*\\n * Initialise a watch queue\\n */\\nint watch_queue_init(struct pipe_inode_info *pipe)\\n{\\n\\tstruct watch_queue *wqueue;\\n\\n\\twqueue = kzalloc(sizeof(*wqueue), GFP_KERNEL);\\n\\tif (!wqueue)\\n\\t\\treturn -ENOMEM;\\n\\n\\twqueue->pipe = pipe;\\n\\tkref_init(&wqueue->usage);\\n\\tspin_lock_init(&wqueue->lock);\\n\\tINIT_HLIST_HEAD(&wqueue->watches);\\n\\n\\tpipe->watch_queue = wqueue;\\n\\treturn 0;\\n}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVDU1rWFDRVN"
      },
      "source": [
        "# convert characters to integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2D1eQ_8DVRg",
        "outputId": "b01042b4-0947-4eb9-cbe6-71a57d841120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of unique characters 99\n"
          ]
        }
      ],
      "source": [
        "# create character to index mapping\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print(f\"Total number of unique characters {len(chars)}\")\n",
        "\n",
        "char_indices = dict((c , i) for i , c in enumerate(chars))\n",
        "indices_char = dict((i , c) for i , c in enumerate(chars))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QPpmPuGDl2J",
        "outputId": "32b486ba-20b0-4b47-823d-0bc2c2e6cd7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '\\t',\n",
              " 1: '\\n',\n",
              " 2: ' ',\n",
              " 3: '!',\n",
              " 4: '\"',\n",
              " 5: '#',\n",
              " 6: '$',\n",
              " 7: '%',\n",
              " 8: '&',\n",
              " 9: \"'\",\n",
              " 10: '(',\n",
              " 11: ')',\n",
              " 12: '*',\n",
              " 13: '+',\n",
              " 14: ',',\n",
              " 15: '-',\n",
              " 16: '.',\n",
              " 17: '/',\n",
              " 18: '0',\n",
              " 19: '1',\n",
              " 20: '2',\n",
              " 21: '3',\n",
              " 22: '4',\n",
              " 23: '5',\n",
              " 24: '6',\n",
              " 25: '7',\n",
              " 26: '8',\n",
              " 27: '9',\n",
              " 28: ':',\n",
              " 29: ';',\n",
              " 30: '<',\n",
              " 31: '=',\n",
              " 32: '>',\n",
              " 33: '?',\n",
              " 34: '@',\n",
              " 35: 'A',\n",
              " 36: 'B',\n",
              " 37: 'C',\n",
              " 38: 'D',\n",
              " 39: 'E',\n",
              " 40: 'F',\n",
              " 41: 'G',\n",
              " 42: 'H',\n",
              " 43: 'I',\n",
              " 44: 'J',\n",
              " 45: 'K',\n",
              " 46: 'L',\n",
              " 47: 'M',\n",
              " 48: 'N',\n",
              " 49: 'O',\n",
              " 50: 'P',\n",
              " 51: 'Q',\n",
              " 52: 'R',\n",
              " 53: 'S',\n",
              " 54: 'T',\n",
              " 55: 'U',\n",
              " 56: 'V',\n",
              " 57: 'W',\n",
              " 58: 'X',\n",
              " 59: 'Y',\n",
              " 60: 'Z',\n",
              " 61: '[',\n",
              " 62: '\\\\',\n",
              " 63: ']',\n",
              " 64: '^',\n",
              " 65: '_',\n",
              " 66: '`',\n",
              " 67: 'a',\n",
              " 68: 'b',\n",
              " 69: 'c',\n",
              " 70: 'd',\n",
              " 71: 'e',\n",
              " 72: 'f',\n",
              " 73: 'g',\n",
              " 74: 'h',\n",
              " 75: 'i',\n",
              " 76: 'j',\n",
              " 77: 'k',\n",
              " 78: 'l',\n",
              " 79: 'm',\n",
              " 80: 'n',\n",
              " 81: 'o',\n",
              " 82: 'p',\n",
              " 83: 'q',\n",
              " 84: 'r',\n",
              " 85: 's',\n",
              " 86: 't',\n",
              " 87: 'u',\n",
              " 88: 'v',\n",
              " 89: 'w',\n",
              " 90: 'x',\n",
              " 91: 'y',\n",
              " 92: 'z',\n",
              " 93: '{',\n",
              " 94: '|',\n",
              " 95: '}',\n",
              " 96: '~',\n",
              " 97: 'å',\n",
              " 98: '∩'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indices_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzWbFlirD4wQ",
        "outputId": "1435676c-f99d-473d-f5a7-9cbb8e6bde8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size : 99\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocabulary size : {len(chars)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN5rWdXiEC9o"
      },
      "source": [
        "# Divide data in input(X) and output(Y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUjSNuRLEdM4"
      },
      "source": [
        "## Create sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHr6hrTsEbw5"
      },
      "outputs": [],
      "source": [
        "# define length for each sequence\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 50 # number of input characters (X) in each sequence\n",
        "STEP = 3 # increment between each sequence\n",
        "VOCAB_SIZE = len(chars) # total number of unique characters in dataset\n",
        "\n",
        "sentences = [] # X\n",
        "next_chars = [] # Y\n",
        "\n",
        "for i in range(0,len(text)-MAX_SEQUENCE_LENGTH,STEP):\n",
        "    sentences.append(text[i:i+MAX_SEQUENCE_LENGTH])\n",
        "    next_chars.append(text[i+MAX_SEQUENCE_LENGTH])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mV43ZKjFp5h",
        "outputId": "98e251d4-10ae-4389-bb9e-1555eee06db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples : 743543\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training samples : {len(sentences)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvpVRfKnNDUD"
      },
      "source": [
        "# Create input and output using the created sequences\n",
        "\n",
        "when you're not using the Embedding layer of the keras as the very fast layer , you need to convert your data in the following format :     \n",
        "\n",
        "input shape should be the form (# samples , # timesteps , # features )\n",
        "\n",
        "output shape should be the form (# samples , # timesteps , # features )\n",
        "\n",
        "samples : the number of data points (or sequences )\n",
        "\n",
        "timesteps : it's the length of the sequences of your data (the MAX_SEQ_LENGTH varriable)\n",
        "\n",
        "features : Number of features depends on the type of the problem , in this problem , features is the voccablurary size , that is , the dimensionality of the one-hot-encoding matrix using which each character is being represented . if you're working with images , features size will be (height , width , channels ) and the input shape will be ( training_samples , timesteps , height , width , channels )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17BGBcbqLL9G"
      },
      "outputs": [],
      "source": [
        "# create X and Y\n",
        "\n",
        "X = np.zeros((len(sentences),MAX_SEQUENCE_LENGTH,VOCAB_SIZE), dtype=np.bool_)\n",
        "y = np.zeros((len(sentences),VOCAB_SIZE),dtype=np.bool_)\n",
        "\n",
        "for i,sentence in enumerate(sentences):\n",
        "    for t,char in enumerate(sentence):\n",
        "        X[i,t,char_indices[char]] = 1\n",
        "    y[i,char_indices[next_chars[i]]] = 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLL77azEPZFd",
        "outputId": "b5652969-ccfa-4332-f66c-1b0be78a7114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of X (743543, 50, 99)\n",
            "shape of y (743543, 99)\n"
          ]
        }
      ],
      "source": [
        "print(f\"shape of X {X.shape}\")\n",
        "print(f\"shape of y {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BglmyuXxRNR9"
      },
      "source": [
        "Here , X is reshaped to (#samples,#timesteps,#features) . we have explicitly mentioned the third dimension (#features) because we won't use the Embedding() layer of Keras in this case since there are only 99 characters . characters can be represented as one_hot_encoded vector . there are no word embeddings for characters ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUAvSv76R4qs"
      },
      "source": [
        "# 2.LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMHCcEtxR4Mx",
        "outputId": "a746790a-8e4c-44ce-8f26-6020e077dd49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# define model architecture _ using a two-layer with 128 LSTM cells in each layer\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.7), input_shape=(MAX_SEQUENCE_LENGTH, VOCAB_SIZE)))\n",
        "model.add(Bidirectional(LSTM(128, dropout=0.5)))\n",
        "model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "06_UIVzKTycd",
        "outputId": "9881ef0b-aaf8-46f3-e7af-8841e7c64b50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">233,472</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m233,472\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m394,240\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                  │          \u001b[38;5;34m25,443\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">653,155</span> (2.49 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m653,155\u001b[0m (2.49 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">653,155</span> (2.49 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m653,155\u001b[0m (2.49 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# check model summary\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJY8UcYWVXqH",
        "outputId": "80606225-0c32-4958-ab73-d963920be874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m4755/5809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m13:15\u001b[0m 755ms/step - loss: 3.2664"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "model_training = model.fit(X,y,batch_size=128,epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfv2s5gAUuWF"
      },
      "outputs": [],
      "source": [
        "loss , accuracy = model.evaluate(X , y)\n",
        "print(f'Loss : {loss} \\n  Accuracy {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QQfMsmfV0DE"
      },
      "source": [
        "# 3.Generate code\n",
        "\n",
        "Create a function that will make next character predictions based on temperature . if temperature is greater than 1 , the generated characters will be more veratile and diverse . on the other hand , if temperature is less than one , the generated characters will be much conservative ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH4NwUjdXj9d"
      },
      "source": [
        "##1.Input:\n",
        "\n",
        "preds: This is an array representing the probabilities of the model predicting different characters. For example, it might contain the probabilities of the next character being 'a', 'b', 'c', and so on.\n",
        "temperature: This is a parameter that controls the \"creativity\" of the model. It defaults to 1.0.\n",
        "\n",
        "\n",
        "##2.Converting to NumPy array:\n",
        "\n",
        "This line first converts the preds input (which could be a list or another data structure) into a NumPy array. Then, it ensures that the data type of the array is 'float64' for numerical stability in the calculations that follow.\n",
        "\n",
        "\n",
        "##3.Applying Temperature:\n",
        "\n",
        "preds = np.log(preds) / temperature\n",
        "\n",
        "This is the core of the temperature scaling. It takes the logarithm of the probabilities and divides them by the temperature.\n",
        "\n",
        "Higher temperature (e.g., > 1.0): Makes the probabilities more uniform, increasing the chance of the model selecting less likely characters, leading to more surprising and diverse output.\n",
        "Lower temperature (e.g., < 1.0): Makes the probabilities more peaked, increasing the chance of the model sticking to its most confident predictions, leading to more conservative and predictable output.\n",
        "\n",
        "\n",
        "##4.Scaling Probabilities:\n",
        "\n",
        "exp_preds = np.exp(preds)\n",
        "   preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "\n",
        "These lines first exponentiate the modified probabilities (preds) and then normalize them (divide by their sum) to ensure they add up to 1 and still represent valid probabilities.\n",
        "\n",
        "\n",
        "##5.Making a Choice:\n",
        "\n",
        "\n",
        "probas = np.random.multinomial(1, preds, 1)\n",
        "\n",
        "\n",
        "This line uses a multinomial distribution (like rolling a weighted die) to make a random choice of the next character based on the adjusted probabilities (preds). The result (probas) is a one-hot encoded array, meaning it has a 1 in the position of the chosen character and 0s elsewhere.\n",
        "\n",
        "\n",
        "##6.Returning the Selection:\n",
        "\n",
        "\n",
        "return np.argmax(probas)\n",
        "\n",
        "\n",
        "Finally, the function returns the index of the selected character (the position of the 1 in the probas array), which can then be used to retrieve the actual character from the indices_char dictionary created earlier in the code.\n",
        "\n",
        "In short, the sample function introduces randomness and variability in the text generation process using the temperature parameter, allowing the model to produce more creative and less repetitive output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGNeGXeIV2el"
      },
      "outputs": [],
      "source": [
        "# define a function to sample next word from a probability array based on temperature\n",
        "\n",
        "def sample(preds,temperature = 1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds)/temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds/np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1,preds,1)\n",
        "  return np.argmax(probas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ0j4fxPXEbt"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "In summary, the numbers 1, 9, 10, and 0 in the results array\n",
        "represent the number of times each of\n",
        "the three possible outcomes occurred in the two simulated multinomial experiments.\n",
        "Because outcome 2 had a much higher probability (0.9),\n",
        "it occurred most frequently in the results.\n",
        "'''\n",
        "\n",
        "\n",
        "np.random.multinomial(10,[0.05,0.9,0.05],size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cApjtqdFa7If"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Generate\n",
        "start_index = random.randint(0,len(text)-MAX_SEQUENCE_LENGTH-1) # a random starting point within the text to get an initial sequence for text generation.\n",
        "\n",
        "# iterate through temperature\n",
        "for diversity in [0.2,0.5,1.0,1.2]:\n",
        "    print('-'*50,'diversity: ',diversity)\n",
        "    generated = \"\"\n",
        "    sentence = text[start_index:start_index+MAX_SEQUENCE_LENGTH]\n",
        "    generated += sentence\n",
        "    print('Generating with seed : \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "\n",
        "\n",
        "\n",
        "    #The inner for loop iterates 1000 times, generating 1000 characters.\n",
        "    for i in range(1000):\n",
        "      #Creates an empty array X_pred to hold the input sequence for prediction. It's shaped to fit the model's input requirements (1 sample, sequence length, vocabulary size).\n",
        "      X_pred = np.zeros((1,MAX_SEQUENCE_LENGTH,VOCAB_SIZE))\n",
        "      #The inner for loop with enumerate(sentence): Converts the current sentence into a numerical representation that the model can understand, storing it in X_pred. This essentially one-hot encodes the characters in the sentence\n",
        "      for t , char in enumerate(sentence):\n",
        "        X_pred[0,t,char_indices[char]] = 1\n",
        "      #Uses the trained model to predict the probability distribution of the next character\n",
        "      preds = model.predict(X_pred,verbose=0)[0]\n",
        "      #The sample function uses the predicted probabilities and the diversity value to select the index of the next character. Higher diversity leads to more unexpected choices.\n",
        "      next_index = sample(preds,diversity)\n",
        "      #Gets the actual next character using the next_index from the indices_char dictionary (which maps indices to characters)\n",
        "      next_char = indices_char[next_index]\n",
        "      #Adds the predicted next_char to the generated text.\n",
        "      generated += next_char\n",
        "      #Updates the sentence by removing the first character and adding the predicted next_char at the end. This creates a sliding window for the next prediction\n",
        "      sentence = sentence[1:] + next_char\n",
        "      #: Prints the generated character to the console immediately.\n",
        "      sys.stdout.write(next_char)\n",
        "      sys.stdout.flush()\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}